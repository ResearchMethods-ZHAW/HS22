[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Research Methods",
    "section": "",
    "text": "Das Modul „Research Methods” vermittelt vertiefte Methodenkompetenzen für praxisorientiertes und angewandtes wissenschaftliches Arbeiten im Fachbereich „Umwelt und Natürliche Ressourcen” auf MSc-Niveau. Die Studierenden erarbeiten sich vertiefte Methodenkompetenzen für die analytische Betrachtung der Zusammenhänge im Gesamtsystem „Umwelt und Natürliche Ressourcen”. Die Studierenden erlernen die methodischen Kompetenzen, auf denen die nachfolgenden Module im MSc Programm UNR aufbauen. Das Modul vermittelt einerseits allgemeine, fächerübergreifende methodische Kompetenzen (z.B. Wissenschaftstheorie, computer-gestützte Datenverar-beitung und Statistik).\nHier werden die Unterlagen für die R-Übungsteile bereitgestellt. Es werden sukzessive sowohl Demo-Files, Aufgabenstellungen und Lösungen veröffentlicht.\nDiese Website wurde am 2022-08-30 23:29:50 zum letzten Mal aktualisiert."
  },
  {
    "objectID": "prepro/Prepro1_Demo.html",
    "href": "prepro/Prepro1_Demo.html",
    "title": "Prepro 1: Demo",
    "section": "",
    "text": "Datentypen\n\nNumerics\nUnter die Kategorie numeric fallen in R zwei Datentypen:\n\ndouble: Gleitkommazahl (z.B. 10.3, 7.3)\ninteger: Ganzzahl (z.B. 10, 7)\n\n\nDoubles\nFolgendermassen wird eine Gleitkommazahl einer Variabel zuweisen:\n\nx <- 10.3\n\nx\n\n[1] 10.3\n\ntypeof(x)\n\n[1] \"double\"\n\n\nStatt <-kann auch = verwendet werden. Dies funktioniert aber nicht in allen Situationen, und ist zudem leicht mit == zu verwechseln.\n\ny = 7.3\n\ny\n\n[1] 7.3\n\n\nOhne explizite Zuweisung nimmt R immer den Datentyp doublean:\n\nz <- 42\ntypeof(z)\n\n[1] \"double\"\n\nis.integer(z)\n\n[1] FALSE\n\nis.numeric(z)\n\n[1] TRUE\n\nis.double(z)\n\n[1] TRUE\n\n\n\n\n\nGanzzahl / Integer\nErst wenn man eine Zahl explizit als integer definiert (mit as.integer() oder L), wird sie auch als solches abgespeichert.\n\na <- as.integer(z)\nis.numeric(a)\n\n[1] TRUE\n\nis.integer(a)\n\n[1] TRUE\n\nc <- 8L\nis.numeric(c)\n\n[1] TRUE\n\nis.integer(c)\n\n[1] TRUE\n\n\n\ntypeof(a)\n\n[1] \"integer\"\n\nis.numeric(a)\n\n[1] TRUE\n\nis.integer(a)\n\n[1] TRUE\n\n\nMit c() können eine Reihe von Werten in einer Variabel zugewiesen werden (als vector). Es gibt zudem auch character vectors.\n\nvector <- c(10,20,33,42,54,66,77)\nvector\n\n[1] 10 20 33 42 54 66 77\n\nvector[5]\n\n[1] 54\n\nvector[2:4]\n\n[1] 20 33 42\n\nvector2 <- vector[2:4]\n\nEine Ganzzahl kann explizit mit as.integer() definiert werden.\n\na <- as.integer(7)\nb <- as.integer(3.14)\na\n\n[1] 7\n\nb\n\n[1] 3\n\ntypeof(a)\n\n[1] \"integer\"\n\ntypeof(b)\n\n[1] \"integer\"\n\nis.integer(a)\n\n[1] TRUE\n\nis.integer(b)\n\n[1] TRUE\n\n\nEine Zeichenkette kann als Zahl eingelesen werden.\n\nc <- as.integer(\"3.14\")\nc\n\n[1] 3\n\ntypeof(c)\n\n[1] \"integer\"\n\n\n\n\nLogische Abfragen\nWird auch auch als boolesch (Eng. boolean) bezeichnet.\n\ne <- 3\nf <- 6\ng <- e > f\ne\n\n[1] 3\n\nf\n\n[1] 6\n\ng\n\n[1] FALSE\n\ntypeof(g)\n\n[1] \"logical\"\n\n\n\n\nLogische Operationen\n\nsonnig <- TRUE\ntrocken <- FALSE\n\nsonnig & !trocken\n\n[1] TRUE\n\n\nOft braucht man auch das Gegenteil / die Negation eines Wertes. Dies wird mittels ! erreicht\n\nu <- TRUE\nv <- !u \nv\n\n[1] FALSE\n\n\n\n\nZeichenketten\nZeichenketten (Eng. character) stellen Text dar\n\ns <- as.character(3.14)\ns\n\n[1] \"3.14\"\n\ntypeof(s)\n\n[1] \"character\"\n\n\nZeichenketten verbinden / zusammenfügen (Eng. concatenate)\n\nfname <- \"Hans\"\nlname <- \"Muster\"\npaste(fname,lname)\n\n[1] \"Hans Muster\"\n\nfname2 <- \"hans\"\nfname == fname2\n\n[1] FALSE\n\n\n\n\nFactors\nMit Factors wird in R eine Sammlung von Zeichenketten bezeichnet, die sich wiederholen, z.B. Wochentage (es gibt nur 7 unterschiedliche Werte für “Wochentage”).\n\nwochentage <- c(\"Donnerstag\",\"Freitag\",\"Samstag\",\"Sonntag\",\"Montag\",\"Dienstag\",\"Mittwoch\",\n                \"Donnerstag\",\"Freitag\",\"Samstag\",\"Sonntag\", \"Montag\",\"Dienstag\",\"Mittwoch\")\n\ntypeof(wochentage)\n\n[1] \"character\"\n\nwochentage_fac <- as.factor(wochentage)\n\nwochentage\n\n [1] \"Donnerstag\" \"Freitag\"    \"Samstag\"    \"Sonntag\"    \"Montag\"    \n [6] \"Dienstag\"   \"Mittwoch\"   \"Donnerstag\" \"Freitag\"    \"Samstag\"   \n[11] \"Sonntag\"    \"Montag\"     \"Dienstag\"   \"Mittwoch\"  \n\nwochentage_fac\n\n [1] Donnerstag Freitag    Samstag    Sonntag    Montag     Dienstag  \n [7] Mittwoch   Donnerstag Freitag    Samstag    Sonntag    Montag    \n[13] Dienstag   Mittwoch  \nLevels: Dienstag Donnerstag Freitag Mittwoch Montag Samstag Sonntag\n\n\nWie man oben sieht, unterscheiden sich character vectors und factors v.a. dadurch, dass letztere über sogenannte levels verfügt. Diese levels entsprechen den Eindeutigen (unique) Werten.\n\nlevels(wochentage_fac)\n\n[1] \"Dienstag\"   \"Donnerstag\" \"Freitag\"    \"Mittwoch\"   \"Montag\"    \n[6] \"Samstag\"    \"Sonntag\"   \n\nunique(wochentage)\n\n[1] \"Donnerstag\" \"Freitag\"    \"Samstag\"    \"Sonntag\"    \"Montag\"    \n[6] \"Dienstag\"   \"Mittwoch\"  \n\n\nZudem ist fällt auf, dass die Reihenfolge der Wohentag alphabetisch sortiert ist. Wie diese sortiert werden zeigen wir an einem anderen Beispiel:\n\nzahlen <- factor(c(\"null\",\"eins\",\"zwei\",\"drei\"))\n\nzahlen\n\n[1] null eins zwei drei\nLevels: drei eins null zwei\n\n\nOffensichtlich sollten diese factors geordnet sein, R weiss davon aber nichts. Eine Ordnung kann man mit dem Befehl ordered = T festlegen.\nBeachtet: ordered = T kann nur bei der Funktion factor() spezifiziert werden, nicht bei as.factor(). Ansonsten sind factor() und as.factor() sehr ähnlich.\n\nzahlen <- factor(zahlen,ordered = TRUE)\n\nzahlen\n\n[1] null eins zwei drei\nLevels: drei < eins < null < zwei\n\n\nBeachtet das “<”-Zeichen zwischen den Levels. Die Zahlen werden nicht in der korrekten Reihenfolge, sondern Alphabetisch geordnet. Die richtige Reihenfolge kann man mit levels = festlegen.\n\nzahlen <- factor(zahlen,ordered = T,levels = c(\"null\",\"eins\",\"zwei\",\"drei\",\"vier\"))\n\nzahlen\n\n[1] null eins zwei drei\nLevels: null < eins < zwei < drei < vier\n\n\nWie auch schon erwähnt werden factors als character Vektor dargestellt, aber als Integers gespeichert. Das führt zu einem scheinbaren Wiederspruch wenn man den Datentyp auf unterschiedliche Weise abfragt.\n\ntypeof(zahlen)\n\n[1] \"integer\"\n\nis.integer(zahlen)\n\n[1] FALSE\n\n\nMit typeof() wird eben diese Form der Speicherung abgefragt und deshalb mit integer beantwortet. Da es sich aber nicht um einen eigentlichen Integer Vektor handelt, wird die Frage is.integer() mit FALSE beantwortet. Das ist etwas verwirrend, beruht aber darauf, dass die beiden Funktionen die Frage von unterschiedlichen Perspektiven beantworten. In diesem Fall schafft class() Klarheit:\n\nclass(zahlen)\n\n[1] \"ordered\" \"factor\" \n\n\nWirklich verwirrend wird es, wenn factors in numeric umgewandelt werden sollen.\n\nzahlen\n\n[1] null eins zwei drei\nLevels: null < eins < zwei < drei < vier\n\nas.integer(zahlen)\n\n[1] 1 2 3 4\n\n\nDas die Übersetzung der auf Deutsch ausgeschriebenen Nummern in nummerische Zahlen nicht funktionieren würde, war ja klar. Weniger klar ist es jedoch, wenn die factors bereits aus nummerischen Zahlen bestehen.\n\nzahlen2 <- factor(c(\"10\",\"20\"))\nas.integer(zahlen2)\n\n[1] 1 2\n\n\nIn diesem Fall müssen die factors erstmals in character umgewandelt werden.\n\nas.integer(as.character(zahlen2))\n\n[1] 10 20\n\n\n\n\nZeit/Datum\nUm in R mit Datum/Zeit Datentypen umzugehen, müssen sie als POSIXct eingelesen werden (es gibt alternativ noch POSIXlt, aber diese ignorieren wir mal). Anders als Beispielsweise bei Excel, sollten in R Datum und Uhrzeit immer in einer Spalte gespeichert werden.\n\ndatum <- \"2017-10-01 13:45:10\"\n\nas.POSIXct(datum)\n\n[1] \"2017-10-01 13:45:10 UTC\"\n\n\nWenn das die Zeichenkette in dem obigen Format (Jahr-Monat-Tag Stunde:Minute:Sekunde) daher kommt, braucht as.POSIXctkeine weiteren Informationen. Sollte das Format von dem aber Abweichen, muss man der Funktion das genaue Schema jedoch mitteilen. Der Syntax dafür kann via ?strptime nachgeschlagen werden.\n\ndatum <- \"01.10.2017 13:45\"\n\nas.POSIXct(datum,format = \"%d.%m.%Y %H:%M\")\n\n[1] \"2017-10-01 13:45:00 UTC\"\n\ndatum <- as.POSIXct(datum,format = \"%d.%m.%Y %H:%M\")\n\nBeachtet, dass in den den obigen Beispiel R automatisch eine Zeitzone angenommen hat (CEST). R geht davon aus, dass die Zeitzone der System Timezone (Sys.timezone()) entspricht.\n\nstrftime(datum, format = \"%m\")\n\n[1] \"10\"\n\nstrftime(datum, format = \"%b\")\n\n[1] \"Oct\"\n\nstrftime(datum, format = \"%B\")\n\n[1] \"October\"\n\n\n\n\n\nData Frames und Conveniance Variabeln\nEine data.frame ist die gängigste Art, Tabellarische Daten zu speichern.\n\ndf <- data.frame(\n  Stadt = c(\"Zürich\",\"Genf\",\"Basel\",\"Bern\",\"Lausanne\"),\n  Einwohner = c(396027,194565,175131,140634,135629),\n  Ankunft = c(\"1.1.2017 10:00\",\"1.1.2017 14:00\",\n              \"1.1.2017 13:00\",\"1.1.2017 18:00\",\"1.1.2017 21:00\")\n)\n\nstr(df)\n\n'data.frame':   5 obs. of  3 variables:\n $ Stadt    : chr  \"Zürich\" \"Genf\" \"Basel\" \"Bern\" ...\n $ Einwohner: num  396027 194565 175131 140634 135629\n $ Ankunft  : chr  \"1.1.2017 10:00\" \"1.1.2017 14:00\" \"1.1.2017 13:00\" \"1.1.2017 18:00\" ...\n\n\nIn der obigen data.frame wurde die Spalte Einwohner als Fliesskommazahl abgespeichert. Dies ist zwar nicht tragisch, aber da wir wissen das es sich hier sicher um Ganzzahlen handelt, können wir das korrigieren. Wichtiger ist aber, dass wir die Ankunftszeit (SpalteAnkunft) von einem Factor in ein Zeitformat (POSIXct) umwandeln.\n\ndf$Einwohner <- as.integer(df$Einwohner)\n\ndf$Einwohner\n\n[1] 396027 194565 175131 140634 135629\n\ndf$Ankunft <- as.POSIXct(df$Ankunft, format = \"%d.%m.%Y %H:%M\")\n\ndf$Ankunft\n\n[1] \"2017-01-01 10:00:00 UTC\" \"2017-01-01 14:00:00 UTC\"\n[3] \"2017-01-01 13:00:00 UTC\" \"2017-01-01 18:00:00 UTC\"\n[5] \"2017-01-01 21:00:00 UTC\"\n\n\nDiese Rohdaten können nun helfen, um Hilfsvariablen (convenience variables) zu erstellen. Z.B. können wir die Städte einteilen in gross, mittel und klein.\n\ndf$Groesse[df$Einwohner > 300000] <- \"gross\"\ndf$Groesse[df$Einwohner <= 300000 & df$Einwohner > 150000] <- \"mittel\"\ndf$Groesse[df$Einwohner <= 150000] <- \"klein\"\n\nOder aber, die Ankunftszeit kann von der Spalte Ankunftabgeleitet werden. Dazu brauchen wir aber das Package lubridate\n\nlibrary(lubridate)\n\n\ndf$Ankunft_stunde <- hour(df$Ankunft)"
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#arbeiten-mit-rstudio-project",
    "href": "prepro/Prepro1_Uebung.html#arbeiten-mit-rstudio-project",
    "title": "PrePro 1: Übung",
    "section": "Arbeiten mit RStudio “Project”",
    "text": "Arbeiten mit RStudio “Project”\nWir empfehlen die Verwendung von “Projects” innerhalb von RStudio. RStudio legt für jedes Projekt dann einen Ordner an, in welches die Projekt-Datei abgelegt wird (Dateiendung .Rproj). Sollen innerhalb des Projekts dann R-Skripts geladen oder erzeugt werden, werden diese dann auch im angelegten Ordner abgelegt. Mehr zu RStudio Projects findet ihr hier.\nDas Verwenden von Projects bringt verschiedene Vorteile, wie zum Beispiel:\n\nFestlegen der Working Directory ohne die Verwendung des expliziten Pfades (setwd()). Das ist sinnvoll, da sich dieser Pfad ändern kann (Zusammenarbeit mit anderen Usern, Ausführung des Scripts zu einem späteren Zeitpunkt)\nAutomatisches Zwischenspeichern geöffneter Scripts und Wiederherstellung der geöffneten Scripts bei der nächsten Session\nFestlegen verschiedener projektspezifischer Optionen\nVerwendung von Versionsverwaltungssystemen (git oder SVN)"
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#arbeiten-mit-libraries-packages",
    "href": "prepro/Prepro1_Uebung.html#arbeiten-mit-libraries-packages",
    "title": "PrePro 1: Übung",
    "section": "Arbeiten mit Libraries / Packages",
    "text": "Arbeiten mit Libraries / Packages\nR ist ohne Zusatzpackete nicht mehr denkbar. Die allermeisten Packages werden auf CRAN gehostet und können leicht mittels install.packages() installiert werden. Eine sehr wichtige Sammlung von Packages wird von RStudio entwickelt. Unter dem Namen Tidyverse werden eine Reihe von Packages angeboten, den R-Alltag enorm erleichtert. Wir werden später näher auf das “Tidy”-Universum eingehen, an dieser Stelle können wir die Sammlung einfach mal die wichtigsten Packages aus tidyverse installieren (heute werden wir davon nur einen kleinen Teil benutzen).\n\n\n\nUm ein package in R verwenden zu können, gibt es zwei Möglichkeiten:\n\nentweder man lädt es zu Beginn der R-session mittles library(dplyr) (ohne Anführungs- und Schlusszeichen).\noder man ruft eine function mit vorangestelltem Packetname sowie zwei Doppelpunkten auf. dplyr::filter() ruft die Funktion filter() des Packets dplyr auf.\n\nLetztere Notation ist vor allem dann sinnvoll, wenn sich zwei unterschiedliche Funktionen mit dem gleichen namen in verschiedenen pacakges existieren. filter() existiert als Funktion einersits im package dplyr sowie in stats. Dieses Phänomen nennt man “masking”.\nZu Beginn laden wir die nötigen Pakete :\n\nlibrary(readr)\nlibrary(lubridate)\n\n# Im Unterschied zu install.packages() werden bei library()\n# keine Anführungs- und Schlusszeichen gesetzt.\n\ndplyr liefert viele Funktionen, für die es in der normalen R-Umgebung (“base R”) keine wirkliche Alternative gibt. Andere Funktionen sind alternativen zu Base-R Funktionen (read_csv statt read.csv, read_delim statt read.delim.\nDiese verhalten sich leicht anders als Base-R Funktionen: Sie treffen weniger Annahmen und sind etwas konservativer. Wir verwenden oft Tidyverse Funktionen, ihr könnt aber selber entscheiden welche Version ihr benutzt."
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#aufgabe-1",
    "href": "prepro/Prepro1_Uebung.html#aufgabe-1",
    "title": "PrePro 1: Übung",
    "section": "Aufgabe 1",
    "text": "Aufgabe 1\nErstelle eine data.frame mit nachstehenden Daten.\nTipps:\n\nEine leere data.frame zu erstellen ist schwieriger als wenn erstellen und befüllen der data.frame in einem Schritt erfolgt\nR ist dafür gedacht, Spalte für Spalte zu arbeiten (warum?), nicht Reihe für Reihe. Versuche dich an dieses Schema zu halten.\n\n\n\n\n\n\n\n\n\nTierart\nAnzahl\nGewicht\nGeschlecht\nBeschreibung\n\n\n\n\nFuchs\n2\n4.4\nm\nRötlich\n\n\nBär\n5\n40.3\nf\nBraun, gross\n\n\nHase\n1\n1.1\nm\nklein, mit langen Ohren\n\n\nElch\n3\n120.0\nm\nLange Beine, Schaufelgeweih"
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#aufgabe-2",
    "href": "prepro/Prepro1_Uebung.html#aufgabe-2",
    "title": "PrePro 1: Übung",
    "section": "Aufgabe 2",
    "text": "Aufgabe 2\nWas für Datentypen wurden (in Aufgabe 1) von R automatisch angenommen? Sind diese sinnvoll?\n\n## 'data.frame':    4 obs. of  5 variables:\n##  $ Tierart     : chr  \"Fuchs\" \"Bär\" \"Hase\" \"Elch\"\n##  $ Anzahl      : num  2 5 1 3\n##  $ Gewicht     : num  4.4 40.3 1.1 120\n##  $ Geschlecht  : chr  \"m\" \"f\" \"m\" \"m\"\n##  $ Beschreibung: chr  \"Rötlich\" \"Braun, gross\" \"klein, mit langen Ohren\" \"Lange Beine, Schaufelgeweih\"\n## [1] \"double\""
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#aufgabe-3",
    "href": "prepro/Prepro1_Uebung.html#aufgabe-3",
    "title": "PrePro 1: Übung",
    "section": "Aufgabe 3",
    "text": "Aufgabe 3\nNutze die Spalte Gewicht um die Tiere in 3 Gewichtskategorien einzuteilen:\n\nleicht: < 5kg\nmittel: 5 - 100 kg\nschwer: > 100kg"
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#aufgabe-4",
    "href": "prepro/Prepro1_Uebung.html#aufgabe-4",
    "title": "PrePro 1: Übung",
    "section": "Aufgabe 4",
    "text": "Aufgabe 4\nImportiere den Datensatz weather.csv (Rechtsklick -> Speichern Unter, Quelle MeteoSchweiz). Es handelt sich dabei um die stündlich gemittelten Temperaturdaten an verschiedenen Standorten in der Schweiz. Wir empfehlen read_csv() anstelle von read.csv().\nAchtung! read_csv erwartet leicht andere inputs als read.csv, schaut euch die Hilfe dazu an (?read_csv).\n\n\n\n\n\n\n\n\n\n\n\nstn\ntime\ntre200h0\n\n\n\n\nABO\n2000010100\n-2.6\n\n\nABO\n2000010101\n-2.5\n\n\nABO\n2000010102\n-3.1\n\n\nABO\n2000010103\n-2.4\n\n\nABO\n2000010104\n-2.5\n\n\nABO\n2000010105\n-3.0\n\n\nABO\n2000010106\n-3.7\n\n\nABO\n2000010107\n-4.4\n\n\nABO\n2000010108\n-4.1\n\n\nABO\n2000010109\n-4.1"
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#aufgabe-5",
    "href": "prepro/Prepro1_Uebung.html#aufgabe-5",
    "title": "PrePro 1: Übung",
    "section": "Aufgabe 5",
    "text": "Aufgabe 5\nSchau dir die Rückmeldung von read_csv()an. Sind die Daten korrekt interpretiert worden?"
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#aufgabe-6",
    "href": "prepro/Prepro1_Uebung.html#aufgabe-6",
    "title": "PrePro 1: Übung",
    "section": "Aufgabe 6",
    "text": "Aufgabe 6\nDie Spalte time ist eine Datum/Zeitangabe im Format JJJJMMTTHH (siehe meta.txt). Damit R dies als Datum-/Zeitangabe erkennt, müssen wir die Spalte in einem R-Format (POSIXct) einlesen und dabei R mitteilen, wie sie aktuell formatiert ist. Lies die Spalte mit as.POSIXct() ein und spezifiziere sowohl format wie auch tz.\nTipps:\n\nWenn keine Zeitzone festgelegt wird, trifft as.POSIXct() eine Annahme (basierend auf Sys.timezone()). In unserem Fall handelt es sich aber um Werte in UTC (siehe metadata.csv)\nas.POSIXcterwartet character: Wenn du eine Fehlermeldung hast die 'origin' must be supplied (o.ä) heisst, hast du der Funktion vermutlich einen Numeric übergeben.\n\n\n\n\n\n\n\nDie neue Tabelle sollte so aussehen\n\n\nstn\ntime\ntre200h0\n\n\n\n\nABO\n2000-01-01 00:00:00\n-2.6\n\n\nABO\n2000-01-01 01:00:00\n-2.5\n\n\nABO\n2000-01-01 02:00:00\n-3.1\n\n\nABO\n2000-01-01 03:00:00\n-2.4\n\n\nABO\n2000-01-01 04:00:00\n-2.5\n\n\nABO\n2000-01-01 05:00:00\n-3.0\n\n\nABO\n2000-01-01 06:00:00\n-3.7\n\n\nABO\n2000-01-01 07:00:00\n-4.4\n\n\nABO\n2000-01-01 08:00:00\n-4.1\n\n\nABO\n2000-01-01 09:00:00\n-4.1"
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#aufgabe-7",
    "href": "prepro/Prepro1_Uebung.html#aufgabe-7",
    "title": "PrePro 1: Übung",
    "section": "Aufgabe 7",
    "text": "Aufgabe 7\nErstelle zwei neue Spalten mit Wochentag (Montag, Dienstag, etc) und Kalenderwoche. Verwende dazu die neu erstellte POSIXct-Spalte\n\n\n\n\n\n\n\n\nstn\ntime\ntre200h0\nwochentag\nkw\n\n\n\n\nABO\n2000-01-01 00:00:00\n-2.6\nSat\n1\n\n\nABO\n2000-01-01 01:00:00\n-2.5\nSat\n1\n\n\nABO\n2000-01-01 02:00:00\n-3.1\nSat\n1\n\n\nABO\n2000-01-01 03:00:00\n-2.4\nSat\n1\n\n\nABO\n2000-01-01 04:00:00\n-2.5\nSat\n1\n\n\nABO\n2000-01-01 05:00:00\n-3.0\nSat\n1\n\n\nABO\n2000-01-01 06:00:00\n-3.7\nSat\n1\n\n\nABO\n2000-01-01 07:00:00\n-4.4\nSat\n1\n\n\nABO\n2000-01-01 08:00:00\n-4.1\nSat\n1\n\n\nABO\n2000-01-01 09:00:00\n-4.1\nSat\n1"
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#aufgabe-8",
    "href": "prepro/Prepro1_Uebung.html#aufgabe-8",
    "title": "PrePro 1: Übung",
    "section": "Aufgabe 8",
    "text": "Aufgabe 8\nErstelle eine neue Spalte basierend auf die Temperaturwerte mit der Einteilung “kalt” (unter Null Grad) und “warm” (über Null Grad)\n\n\n\n\n\n\n\n\nstn\ntime\ntre200h0\nwochentag\nkw\ntemp_kat\n\n\n\n\nABO\n2000-01-01 00:00:00\n-2.6\nSat\n1\nkalt\n\n\nABO\n2000-01-01 01:00:00\n-2.5\nSat\n1\nkalt\n\n\nABO\n2000-01-01 02:00:00\n-3.1\nSat\n1\nkalt\n\n\nABO\n2000-01-01 03:00:00\n-2.4\nSat\n1\nkalt\n\n\nABO\n2000-01-01 04:00:00\n-2.5\nSat\n1\nkalt\n\n\nABO\n2000-01-01 05:00:00\n-3.0\nSat\n1\nkalt\n\n\nABO\n2000-01-01 06:00:00\n-3.7\nSat\n1\nkalt\n\n\nABO\n2000-01-01 07:00:00\n-4.4\nSat\n1\nkalt\n\n\nABO\n2000-01-01 08:00:00\n-4.1\nSat\n1\nkalt\n\n\nABO\n2000-01-01 09:00:00\n-4.1\nSat\n1\nkalt"
  },
  {
    "objectID": "prepro/Prepro2_Demo.html#piping",
    "href": "prepro/Prepro2_Demo.html#piping",
    "title": "Prepro 2: Demo",
    "section": "Piping",
    "text": "Piping\nGegeben ist ein character string (diary). Wir wollen aus diesem Text die Temperaturangabe aus dem String extrahieren und danach den Wert von Kelvin in Celsius nach der folgenden Formel umwandeln und zum Schluss den Mittelwert über all diese Werte berechnen.\n\\[°C = K - 273.15\\]\n\ndiary <- c(\n  \"The temperature is 310° Kelvin\",\n  \"The temperature is 322° Kelvin\",\n  \"The temperature is 410° Kelvin\"\n)\n\ndiary\n\n[1] \"The temperature is 310° Kelvin\" \"The temperature is 322° Kelvin\"\n[3] \"The temperature is 410° Kelvin\"\n\n\nDafür haben wir eine Hilfsfunktion subtrahieren, welche zwei Werte annimmt, den minuend und den subtrahend:\n\nsubtrahieren <- function(minuend, subtrahend){\n  minuend - subtrahend\n}\n\nZudem brauchen wir die Funktion substr(), welche aus einem character einen teil “raus schnipseln” kann.\n\n# Wenn die Buchstaben einzelne _Elemente_ eines Vektors wären, würden wir diese\n# folgendermassen subsetten:\n\ncharvec1 <- c(\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\")\ncharvec1[4:6]\n\n[1] \"d\" \"e\" \"f\"\n\n# Aber wenn diese in einem einzigen character gespeichert sind, brauchen wir substr:\ncharvec2 <- \"abcdefgh\"\nsubstr(charvec2, 4, 6)\n\n[1] \"def\"\n\n\nÜbersetzt in R-Code entsteht folgende Operation:\n\n# 1. Nimm diary\n# 2. Extrahiere auf jeder Zeile die Werte 20 bis 22\n# 3. Konvertiere \"character\" zu \"numeric\"\n# 4. Subtrahiere 273.15\n# 5. Berechne den Mittlwert\n\noutput <- mean(subtrahieren(as.numeric(substr(diary, 20, 22)),273.15))\n#                                             \\_1_/\n#                                      \\________2__________/\n#                           \\___________________3___________/\n#              \\________________________________4__________________/\n#         \\_____________________________________5____________________/\n\nDie ganze Operation liest sich etwas leichter, wenn diese sequentiell notiert wird:\n\ntemp <- substr(diary, 20, 22)       # 1, 2\ntemp <- as.numeric(temp)            # 3\ntemp <- subtrahieren(temp, 273.15)  # 4\noutput <- mean(temp)                # 5\n\nUmständlich ist dabei einfach, dass die Zwischenresultate immer abgespeichert und in der darauf folgenden Operation wieder abgerufen werden müssen. Hier kommt “piping” ins Spiel: Mit “piping” wird der Output der einen Funktion der erste Parameter der darauf folgenden Funktion.\n\nlibrary(magrittr)\n\ndiary %>%                            # 1\n  substr(20, 22) %>%                 # 2\n  as.numeric() %>%                   # 3 \n  subtrahieren(273.15) %>%           # 4\n  mean()                             # 5\n\n[1] 74.18333\n\n\nNoch ein Hinweis: die %>% Pipe Operation aus magrittr wurde in R so beliebt, dass in R 4.1 ein “base R pipe” eingeführt. Diese sieht folgendermassen aus:\n\ndiary |>                             # 1\n  substr(20, 22) |>                  # 2\n  as.numeric() |>                    # 3 \n  subtrahieren(273.15) |>            # 4\n  mean()                             # 5\n\n[1] 74.18333"
  },
  {
    "objectID": "prepro/Prepro2_Demo.html#joins",
    "href": "prepro/Prepro2_Demo.html#joins",
    "title": "Prepro 2: Demo",
    "section": "Joins",
    "text": "Joins\n\nstudierende <- data.frame(\n  Matrikel_Nr = c(100002, 100003, 200003),\n  Studi = c(\"Patrick\", \"Manuela\", \"Eva\"),\n  PLZ = c(8006, 8001, 8820)\n)\n\nstudierende\n\n  Matrikel_Nr   Studi  PLZ\n1      100002 Patrick 8006\n2      100003 Manuela 8001\n3      200003     Eva 8820\n\nortschaften <- data.frame(\n  PLZ = c(8003, 8006, 8810, 8820),\n  Ortsname = c(\"Zürich\", \"Zürich\", \"Horgen\", \"Wädenswil\")\n)\n\nortschaften\n\n   PLZ  Ortsname\n1 8003    Zürich\n2 8006    Zürich\n3 8810    Horgen\n4 8820 Wädenswil\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\ninner_join(studierende, ortschaften, by = \"PLZ\")\n\n  Matrikel_Nr   Studi  PLZ  Ortsname\n1      100002 Patrick 8006    Zürich\n2      200003     Eva 8820 Wädenswil\n\nleft_join(studierende, ortschaften, by = \"PLZ\")\n\n  Matrikel_Nr   Studi  PLZ  Ortsname\n1      100002 Patrick 8006    Zürich\n2      100003 Manuela 8001      <NA>\n3      200003     Eva 8820 Wädenswil\n\nright_join(studierende, ortschaften, by = \"PLZ\")\n\n  Matrikel_Nr   Studi  PLZ  Ortsname\n1      100002 Patrick 8006    Zürich\n2      200003     Eva 8820 Wädenswil\n3          NA    <NA> 8003    Zürich\n4          NA    <NA> 8810    Horgen\n\nfull_join(studierende, ortschaften, by = \"PLZ\")\n\n  Matrikel_Nr   Studi  PLZ  Ortsname\n1      100002 Patrick 8006    Zürich\n2      100003 Manuela 8001      <NA>\n3      200003     Eva 8820 Wädenswil\n4          NA    <NA> 8003    Zürich\n5          NA    <NA> 8810    Horgen\n\n\n\nstudierende <- data.frame(\n  Matrikel_Nr = c(100002, 100003, 200003),\n  Studi = c(\"Patrick\", \"Manuela\", \"Pascal\"),\n  Wohnort = c(8006, 8001, 8006)\n)\n\nleft_join(studierende, ortschaften, by = c(\"Wohnort\" = \"PLZ\"))\n\n  Matrikel_Nr   Studi Wohnort Ortsname\n1      100002 Patrick    8006   Zürich\n2      100003 Manuela    8001     <NA>\n3      200003  Pascal    8006   Zürich"
  },
  {
    "objectID": "prepro/Prepro2_Uebung_A.html",
    "href": "prepro/Prepro2_Uebung_A.html",
    "title": "Prepro 2: Übung A",
    "section": "",
    "text": "Für die Musterlösungen werden wir die nachstehenden Libraries verwenden:"
  },
  {
    "objectID": "prepro/Prepro2_Uebung_A.html#aufgabe-1",
    "href": "prepro/Prepro2_Uebung_A.html#aufgabe-1",
    "title": "Prepro 2: Übung A",
    "section": "Aufgabe 1",
    "text": "Aufgabe 1\nLade die Wetterdaten von letzer Woche runter (weather.csv, Quelle MeteoSchweiz) und importiere sie in R. Sorge dafür, dass die Spalten korrekt formatiert sind (stn als factor, time als POSIXct, tre200h0 als numeric.)"
  },
  {
    "objectID": "prepro/Prepro2_Uebung_A.html#aufgabe-2",
    "href": "prepro/Prepro2_Uebung_A.html#aufgabe-2",
    "title": "Prepro 2: Übung A",
    "section": "Aufgabe 2",
    "text": "Aufgabe 2\nLade metadata herunter und lade es ebenfalls als csv ein.\nHinweis: Wenn Umlaute und Sonderzeichen nicht korrekt dargestellt werden (z.B. in Genève), hat das vermutlich mit der Zeichencodierung zu tun. Das File ist aktuell in ‘ANSI’ Codiert, welche für gewisse Betriebssysteme / R-Versionen ein Problem darstellt. Um das Problem zu umgehen muss man das File mit einem Editor öffnen (Windows ‘Editor’ oder ‘Notepad++’, Mac: ‘TextEdit’) und mit einer neuen Codierung (z.B ‘UTF-8’) abspeichern. Danach kann die Codierung spezifitiert werden (bei read_delim(): mitlocale = locale(encoding = “UTF-8”)`)"
  },
  {
    "objectID": "prepro/Prepro2_Uebung_A.html#aufgabe-3",
    "href": "prepro/Prepro2_Uebung_A.html#aufgabe-3",
    "title": "Prepro 2: Übung A",
    "section": "Aufgabe 3",
    "text": "Aufgabe 3\nDie x-/y-Koordinaten sind aktuell in einer Spalte erfasst. Um mit den Koordinaten sinnvoll arbeiten zu können, brauchen wir die Koordinaten getrennt. Trenne die x und y Koordinaten aus der Spalte Koordinaten.\n\nSchritt: verwende stringr::str_split_fixed um die Spalte in eine matrix zu überführen\nSchritt: benenne die Spalten der matrix in x und y um\nSchritt: nutze cbind um die matrix mit der data.frame zu verbinden"
  },
  {
    "objectID": "prepro/Prepro2_Uebung_A.html#aufgabe-4",
    "href": "prepro/Prepro2_Uebung_A.html#aufgabe-4",
    "title": "Prepro 2: Übung A",
    "section": "Aufgabe 4",
    "text": "Aufgabe 4\nNun wollen wir den Datensatz wettermit den Informationen aus metadata anreichern. Uns interessiert aber nur das Stationskürzel, der Name, die x/y Koordinaten sowie die Meereshöhe. Lösche die nicht benötigten Spalten (oder selektiere die benötigten Spalten)."
  },
  {
    "objectID": "prepro/Prepro2_Uebung_A.html#aufgabe-5",
    "href": "prepro/Prepro2_Uebung_A.html#aufgabe-5",
    "title": "Prepro 2: Übung A",
    "section": "Aufgabe 5",
    "text": "Aufgabe 5\nNun ist der Datensatz metadata genügend vorbereitet. Jetzt kann er mit dem Datensatz wetter verbunden werden. Überlege dir, welcher Join dafür sinnvoll ist und mit welchem Attribut wir “joinen” können.\nNutze die Join-Möglichkeiten von dplyr (Hilfe via ?dplyr::join) um die Datensätze wetter und metadata zu verbinden."
  },
  {
    "objectID": "prepro/Prepro2_Uebung_A.html#aufgabe-6",
    "href": "prepro/Prepro2_Uebung_A.html#aufgabe-6",
    "title": "Prepro 2: Übung A",
    "section": "Aufgabe 6",
    "text": "Aufgabe 6\nErstelle eine neue Spalte month welche den jeweiligen Monat (aus time) beinhaltet. Nutze dafür die Funktion lubridate::month()."
  },
  {
    "objectID": "prepro/Prepro2_Uebung_A.html#aufgabe-7",
    "href": "prepro/Prepro2_Uebung_A.html#aufgabe-7",
    "title": "Prepro 2: Übung A",
    "section": "Aufgabe 7",
    "text": "Aufgabe 7\nBerechne mit der Spalte month die Durchschnittstemperatur pro Monat."
  },
  {
    "objectID": "prepro/Prepro2_Uebung_B.html#aufgabe-1",
    "href": "prepro/Prepro2_Uebung_B.html#aufgabe-1",
    "title": "Prepro 2: Übung B",
    "section": "Aufgabe 1",
    "text": "Aufgabe 1\nGegeben sind die Daten von drei Sensoren (sensor1.csv, sensor2.csv, sensor3.csv). Lade die Datensätze runter und lese sie ein."
  },
  {
    "objectID": "prepro/Prepro2_Uebung_B.html#aufgabe-2",
    "href": "prepro/Prepro2_Uebung_B.html#aufgabe-2",
    "title": "Prepro 2: Übung B",
    "section": "Aufgabe 2",
    "text": "Aufgabe 2\nErstelle aus den 3 Dataframes eine einzige Dataframe, die aussieht wie unten dargestellt. Nutze dafür zwei joins aus dplyr um 3 data.frames miteinander zu verbinden. Bereinige im Anschluss die Spaltennamen (wie geht das?).\n\n\n\n\n\n\n\n\nDatetime\nsensor1\nsensor2\nsensor3\n\n\n\n\n16102017_1800\n23.5\n13.5\n26.5\n\n\n17102017_1800\n25.4\n24.4\n24.4\n\n\n18102017_1800\n12.4\n22.4\n13.4\n\n\n19102017_1800\n5.4\n12.4\n7.4\n\n\n23102017_1800\n23.5\n13.5\nNA\n\n\n24102017_1800\n21.3\n11.3\nNA"
  },
  {
    "objectID": "prepro/Prepro2_Uebung_B.html#aufgabe-3",
    "href": "prepro/Prepro2_Uebung_B.html#aufgabe-3",
    "title": "Prepro 2: Übung B",
    "section": "Aufgabe 3",
    "text": "Aufgabe 3\nImportiere die Datei sensor_1_fail.csv in R.\n\n\n\nsensor_fail.csv hat eine Variabel SensorStatus: 1 bedeutet der Sensor misst, 0 bedeutet der Sensor miss nicht. Fälschlicherweise wurde auch dann der Messwert Temp = 0 erfasst, wenn Sensorstatus = 0. Richtig wäre hier NA (not available). Korrigiere den Datensatz entsprechend.\n\n\n\n\n\nSensor\nTemp\nHum_%\nDatetime\nSensorStatus\n\n\n\n\nSen102\n0.6\n98\n16102017_1800\n1\n\n\nSen102\n0.3\n96\n17102017_1800\n1\n\n\nSen102\n0.0\n87\n18102017_1800\n1\n\n\nSen102\n0.0\n86\n19102017_1800\n0\n\n\nSen102\n0.0\n98\n23102017_1800\n0\n\n\nSen102\n0.0\n98\n24102017_1800\n0\n\n\nSen102\n0.0\n96\n25102017_1800\n1\n\n\nSen103\n-0.3\n87\n26102017_1800\n1\n\n\nSen103\n-0.7\n98\n27102017_1800\n1\n\n\nSen103\n-1.2\n98\n28102017_1800\n1"
  },
  {
    "objectID": "prepro/Prepro2_Uebung_B.html#aufgabe-4",
    "href": "prepro/Prepro2_Uebung_B.html#aufgabe-4",
    "title": "Prepro 2: Übung B",
    "section": "Aufgabe 4",
    "text": "Aufgabe 4\nWarum spielt das es eine Rolle, ob 0 oder NA erfasst wird? Berechne die Mittlere der Temperatur / Feuchtigkeit nach der Korrektur."
  },
  {
    "objectID": "prepro/Prepro2_Uebung_B.html#musterlösung",
    "href": "prepro/Prepro2_Uebung_B.html#musterlösung",
    "title": "Prepro 2: Übung B",
    "section": "Musterlösung",
    "text": "Musterlösung"
  },
  {
    "objectID": "prepro/Prepro3_Demo.html",
    "href": "prepro/Prepro3_Demo.html",
    "title": "Prepro 3: Demo",
    "section": "",
    "text": "In dieser DEMO möchten wir weitere Werkzeuge aus dem Tidyverse vorstellen und mit Beispielen illustrieren. Die tidyverse-Tools erleichtern den Umgang mit Daten ungeheuer und haben sich mittlerweile zu einem “must have” im Umgang mit Daten in R entwickelt.\nWir können Euch nicht sämtliche Möglichkeiten von tidyverse zeigen. Wir fokussieren uns deshalb auf weitere wichtige Komponenten 1 und zeigen zusätzliche Funktionalitäten, die wir oft verwenden und Euch ggf. noch nicht bekannt sind. Wer sich vertieft mit dem Thema auseinandersetzen möchte, der sollte sich unbedingt das Buch Wickham and Grolemund (2017) beschaffen. Eine umfangreiche, aber nicht ganz vollständige Version gibt es online1 , das vollständige eBook kann über die Bibliothek bezogen werden2.\nWir benötigen dazu folgende Packages:"
  },
  {
    "objectID": "prepro/Prepro3_Demo.html#split-apply-combine-beispiel-1",
    "href": "prepro/Prepro3_Demo.html#split-apply-combine-beispiel-1",
    "title": "Prepro 3: Demo",
    "section": "Split-Apply-Combine (Beispiel 1)",
    "text": "Split-Apply-Combine (Beispiel 1)\n\ndf <- data.frame(\n  key = c(\"A\",\"B\",\"C\",\"A\",\"B\",\"C\",\"A\",\"B\",\"C\"),\n  data = c(0, 5, 10, 5, 10, 15, 10, 15, 20)\n)\n\ndf \n\n  key data\n1   A    0\n2   B    5\n3   C   10\n4   A    5\n5   B   10\n6   C   15\n7   A   10\n8   B   15\n9   C   20\n\nsry <- df %>%\n  group_by(key) %>%\n  summarise(Summe = sum(data))\n\nsry <- df %>%\n  group_by(key) %>%\n  summarise(\n    Summe = sum(data),\n    Minimum = min(data),\n    Maximun = max(data)\n    )"
  },
  {
    "objectID": "prepro/Prepro3_Demo.html#split-apply-combine-beispiel-2",
    "href": "prepro/Prepro3_Demo.html#split-apply-combine-beispiel-2",
    "title": "Prepro 3: Demo",
    "section": "Split-Apply-Combine (Beispiel 2)",
    "text": "Split-Apply-Combine (Beispiel 2)\n\n?mtcars\n\n\nmtcars <- mtcars                          # Einlesen des Beispielsdatensatzes mtcars\nby_cyl <- group_by(mtcars, cyl)           # Gruppieren nach Anzahl Zylinder (cyl)\nsummarise(by_cyl,mean_mpg = mean(mpg))    # Berechnen des Mittelwerts pro \n\n# A tibble: 3 × 2\n    cyl mean_mpg\n  <dbl>    <dbl>\n1     4     26.7\n2     6     19.7\n3     8     15.1\n\n                                          # Zylinder-Gruppe, resp. Kategorie\n\nby_cyl_2 <- group_by(mtcars,cyl,am)       # Gruppieren nach Anzahl Zylinder (cyl) \n                                          # UND Schaltung automatisch oder manuell (am)\n\nsummarise(by_cyl_2,mean_mpg = mean(mpg))\n\n`summarise()` has grouped output by 'cyl'. You can override using the `.groups`\nargument.\n\n\n# A tibble: 6 × 3\n# Groups:   cyl [3]\n    cyl    am mean_mpg\n  <dbl> <dbl>    <dbl>\n1     4     0     22.9\n2     4     1     28.1\n3     6     0     19.1\n4     6     1     20.6\n5     8     0     15.0\n6     8     1     15.4"
  },
  {
    "objectID": "prepro/Prepro3_Demo.html#split-apply-combine-beispiel-3",
    "href": "prepro/Prepro3_Demo.html#split-apply-combine-beispiel-3",
    "title": "Prepro 3: Demo",
    "section": "Split-Apply-Combine (Beispiel 3)",
    "text": "Split-Apply-Combine (Beispiel 3)\n\nDaten Laden\nWir laden die Wetterdaten (Quelle MeteoSchweiz) von der letzten Übung.\n\nwetter <- read_csv(here(\"data\",\"weather.csv\"),\n                  col_types = cols(\n                    col_factor(levels = NULL),    \n                    col_datetime(format = \"%Y%m%d%H\"),\n                    col_double()\n                    )\n                  )\n\n\n\nKennwerte berechnen\nWir möchten den Mittelwert aller gemessenen Temperaturwerte berechnen. Dazu könnten wir folgenden Befehl verwenden:\n\nmean(wetter$tre200h0, na.rm = TRUE) \n\n[1] 6.324744\n\n\nDie Option na.rm = T bedeutet, dass NA Werte von der Berechnung ausgeschlossen werden sollen.\nMit der selben Herangehensweise können diverse Werte berechnet werden (z.B. das Maximum (max()), Minimum (min()), Median (median()) u.v.m.).\nDiese Herangehensweise funktioniert nur dann gut, wenn wir die Kennwerte über alle Beobachtungen für eine Variable (Spalte) berechnen wollen. Sobald wir die Beobachtungen gruppieren wollen, wird es schwierig. Zum Beispiel, wenn wir die durchschnittliche Temperatur pro Monat berechnen wollen.\n\n\nConvenience Variablen\nUm diese Aufgabe zu lösen, muss zuerst den Monat extrahiert werden (der Monat ist die convenience variabel). Hierfür brauchen wir die Funktion lubridate::month().\nNun kann kann die convenience Variable “Month” erstellt werden. Ohne dpylr wird eine neue Spalte folgendermassen hinzugefügt.\n\nwetter$month <- month(wetter$time)\n\nMit dplyr (siehe 3) sieht der gleiche Befehl folgendermassen aus:\n\nwetter <- mutate(wetter,month = month(time))\n\nDer grosse Vorteil von dplyr ist an dieser Stelle noch nicht ersichtlich. Dieser wird aber später klar.\n\n\nKennwerte nach Gruppen berechnen\nUm mit base R den Mittelwert pro Monat zu berechnen, kann man zuerst ein Subset mit [] erstellen und davon den Mittelwerb berechnen, z.B. folgendermassen:\n\nmean(wetter$tre200h0[wetter$month == 1], na.rm = TRUE)\n\n[1] -1.963239\n\n\nDies müssen wir pro Monat wiederholen, was natürlich sehr umständlich ist. Deshalb nutzen wir das package dplyr. Damit geht die Aufgabe (Temperaturmittel pro Monat berechnen) folgendermassen:\n\nsummarise(group_by(wetter,month),temp_mittel = mean(tre200h0, na.rm = TRUE))\n\n# A tibble: 12 × 2\n   month temp_mittel\n   <dbl>       <dbl>\n 1     1      -1.96 \n 2     2       0.355\n 3     3       2.97 \n 4     4       4.20 \n 5     5      11.0  \n 6     6      12.4  \n 7     7      13.0  \n 8     8      15.0  \n 9     9       9.49 \n10    10       8.79 \n11    11       1.21 \n12    12      -0.898\n\n\n\n\nVerketten vs. verschachteln\nAuf Deutsch übersetzt heisst die obige Operation folgendermassen:\n\nnimm den Datensatz wetter\nBilde Gruppen pro Jahr (group_by(wetter,year))\nBerechne das Temperaturmittel (mean(tre200h0))\n\nDiese Übersetzung R-> Deutsch unterscheidet sich vor allem darin, dass die Operation auf Deutsch verkettet ausgesprochen wird (Operation 1->2->3) während der Computer verschachtelt liest 3(2(1)). Um R näher an die gesprochene Sprache zu bringen, kann man den %>%-Operator verwenden (siehe 4).\n\n# 1 nimm den Datensatz \"wetter\"\n# 2 Bilde Gruppen pro Monat\n# 3 berechne das Temperaturmittel \n\nsummarise(group_by(wetter,month),temp_mittel = mean(tre200h0))\n#                  \\_1_/\n#         \\__________2_________/\n#\\___________________3_______________________________________/\n\n# wird zu:\n\nwetter %>%                                 # 1\n  group_by(month) %>%                      # 2\n  summarise(temp_mittel = mean(tre200h0))  # 3\n\nDieses Verketten mittels %>% (genannt “pipe”) macht den Code einiges schreib- und leserfreundlicher, und wir werden ihn in den nachfolgenden Übungen verwenden. Die “pipe” wird mit dem package magrittr bereitgestellt und mit dplyr mitinstalliert.\nZu dplyr gibt es etliche Tutorials online (siehe5), deshalb werden wir diese Tools nicht in allen Details erläutern. Nur noch folgenden wichtigen Unterschied zu zwei wichtigen Funktionen in dpylr: mutate() und summarise().\n\nsummarise() fasst einen Datensatz zusammen. Dabei reduziert sich die Anzahl Beobachtungen (Zeilen) auf die Anzahl Gruppen (z.B. eine zusammengefasste Beobachtung (Zeile) pro Jahr). Zudem reduziert sich die Anzahl Variablen (Spalten) auf diejenigen, die in der “summarise” Funktion spezifiziert wurde (z.B. temp_mittel).\nmit mutate wird ein data.frame vom Umfang her belassen, es werden lediglich zusätzliche Variablen (Spalten) hinzugefügt (siehe Beispiel unten).\n\n\n# Maximal und minimal Temperatur pro Kalenderwoche\nweather_summary <- wetter %>%               #1) nimm den Datensatz \"wetter\"\n  filter(month == 1) %>%                    #2) filter auf den Monat Januar\n  mutate(day = day(time)) %>%               #3) erstelle eine neue Spalte \"day\"\n  group_by(day) %>%                         #4) Nutze die neue Spalte um Gruppen zu bilden\n  summarise(\n    temp_max = max(tre200h0, na.rm = TRUE), #5) Berechne das Maximum \n    temp_min = min(tre200h0, na.rm = TRUE)  #6) Berechne das Minimum\n    )   \n\nweather_summary\n\n# A tibble: 31 × 3\n     day temp_max temp_min\n   <int>    <dbl>    <dbl>\n 1     1      5.8     -4.4\n 2     2      2.8     -4.3\n 3     3      4.2     -3.1\n 4     4      4.7     -2.8\n 5     5     11.4     -0.6\n 6     6      6.7     -1.6\n 7     7      2.9     -2.8\n 8     8      0.2     -3.6\n 9     9      2.1     -8.8\n10    10      1.6     -2.4\n# … with 21 more rows"
  },
  {
    "objectID": "prepro/Prepro3_Demo.html#reshaping-data",
    "href": "prepro/Prepro3_Demo.html#reshaping-data",
    "title": "Prepro 3: Demo",
    "section": "Reshaping data",
    "text": "Reshaping data\n\nBreit -> lang\nDie Umformung von Tabellen breit->lang erfolgt mittels tidyr(siehe 6). Auch dieses package funktioniert wunderbar mit piping (%>%).\n\nweather_summary %>%\n  pivot_longer(c(temp_max,temp_min))\n\n# A tibble: 62 × 3\n     day name     value\n   <int> <chr>    <dbl>\n 1     1 temp_max   5.8\n 2     1 temp_min  -4.4\n 3     2 temp_max   2.8\n 4     2 temp_min  -4.3\n 5     3 temp_max   4.2\n 6     3 temp_min  -3.1\n 7     4 temp_max   4.7\n 8     4 temp_min  -2.8\n 9     5 temp_max  11.4\n10     5 temp_min  -0.6\n# … with 52 more rows\n\n\nIm Befehl pivot_longer() müssen wir festlegen, welche Spalten zusammengefasst werden sollen (hier: temp_max,temp_min,temp_mean). Alternativ können wir angeben, welche Spalten wir nicht zusammenfassen wollen:\n\nweather_summary %>%\n  pivot_longer(-day)\n\n# A tibble: 62 × 3\n     day name     value\n   <int> <chr>    <dbl>\n 1     1 temp_max   5.8\n 2     1 temp_min  -4.4\n 3     2 temp_max   2.8\n 4     2 temp_min  -4.3\n 5     3 temp_max   4.2\n 6     3 temp_min  -3.1\n 7     4 temp_max   4.7\n 8     4 temp_min  -2.8\n 9     5 temp_max  11.4\n10     5 temp_min  -0.6\n# … with 52 more rows\n\n\nWenn wir die Namen neuen Spalten festlegen wollen (anstelle von name und value) erreichen wir dies mit names_to bzw. values_to:\n\nweather_summary_long <- weather_summary %>%\n  pivot_longer(-day, names_to = \"Messtyp\", values_to = \"Messwert\")\n\nDie ersten 6 Zeilen von weather_summary_long:\n\n\n\n\n\nday\nMesstyp\nMesswert\n\n\n\n\n1\ntemp_max\n5.8\n\n\n1\ntemp_min\n-4.4\n\n\n2\ntemp_max\n2.8\n\n\n2\ntemp_min\n-4.3\n\n\n3\ntemp_max\n4.2\n\n\n3\ntemp_min\n-3.1\n\n\n\n\n\nDie ersten 6 Zeilen von wetter_sry:\n\n\n\n\n\nday\ntemp_max\ntemp_min\n\n\n\n\n1\n5.8\n-4.4\n\n\n2\n2.8\n-4.3\n\n\n3\n4.2\n-3.1\n\n\n4\n4.7\n-2.8\n\n\n5\n11.4\n-0.6\n\n\n6\n6.7\n-1.6\n\n\n\n\n\nBeachte: weather_summary_long umfasst 62 Beobachtungen (Zeilen), das sind doppelt soviel wie weather_summary, da wir ja zwei Spalten zusammengefasst haben.\n\nnrow(weather_summary)\n\n[1] 31\n\nnrow(weather_summary_long)\n\n[1] 62\n\n\nLange Tabellen sind in verschiedenen Situationen praktischer. Beispielsweise ist das Visualisieren mittels ggplot2 (dieses Package werdet ihr im Block “InfoVis” kennenlernen) mit long tables wesentlich einfacher.\n\nggplot(weather_summary_long, aes(day,Messwert, colour = Messtyp)) +\n  geom_line()\n\n\n\n\n\n\nLang -> breit\nDas Gegenstück zu pivot_longer ist pivot_wider. Mit dieser Funktion können wir eine lange Tabelle in eine breite überführen. Dazu müssen wir in names_from angeben, aus welcher Spalte die neuen Spaltennamen erstellt werden sollen (names_from) und aus welcher Spalte die Werte entstammen sollen (values_from):\n\nweather_summary_long %>%\n  pivot_wider(names_from = Messtyp, values_from = Messwert)\n\n# A tibble: 31 × 3\n     day temp_max temp_min\n   <int>    <dbl>    <dbl>\n 1     1      5.8     -4.4\n 2     2      2.8     -4.3\n 3     3      4.2     -3.1\n 4     4      4.7     -2.8\n 5     5     11.4     -0.6\n 6     6      6.7     -1.6\n 7     7      2.9     -2.8\n 8     8      0.2     -3.6\n 9     9      2.1     -8.8\n10    10      1.6     -2.4\n# … with 21 more rows\n\n\nZum Vergleich: mit einer wide table müssen wir in ggplot2 jede Spalte einzeln plotten. Dies ist bei wenigen Variabeln wie hier noch nicht problematisch, aber bei einer hohen Anzahl wir dies schnell mühsam.\n\nggplot(weather_summary) +\n  geom_line(aes(day, temp_max)) +\n  geom_line(aes(day, temp_min))\n\n\n\n\n\n\n\n\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for Data Science. O’Reilly. https://ebookcentral.proquest.com/lib/zhaw/detail.action?docID=4770093."
  },
  {
    "objectID": "prepro/Prepro3_Uebung.html#aufgabe-1",
    "href": "prepro/Prepro3_Uebung.html#aufgabe-1",
    "title": "Prepro 3: Übung",
    "section": "Aufgabe 1",
    "text": "Aufgabe 1\nGegeben sei ein Datensatz “sensors_combined.csv”, mit den Temperaturwerten von drei verschiedenen Sensoren. Lade diesen Datensatz herunter, importiere ihn als csv in R (als sensors_combined).\nFormatiere die Datetime Spalte in POSIXct um. Verwende dazu die Funktion as.POSIXct (lies mit ?strftime() nochmal nach wie du das spezfische Format (die “Schablone”) festlegen kannst.\n\nlibrary(readr)\n\nsensors_combined <- read_csv(here(\"data\",\"sensors_combined.csv\"))\n\nRows: 16 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Datetime\ndbl (3): sensor1, sensor2, sensor3\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsensors_combined$Datetime <- as.POSIXct(sensors_combined$Datetime, format = \"%d%m%Y_%H%M\")"
  },
  {
    "objectID": "prepro/Prepro3_Uebung.html#aufgabe-2",
    "href": "prepro/Prepro3_Uebung.html#aufgabe-2",
    "title": "Prepro 3: Übung",
    "section": "Aufgabe 2",
    "text": "Aufgabe 2\nÜberführe die Tabelle in ein langes Format (verwende dazu die Funktion pivot_longer aus tidyr) und speichere den output als sensors_long.\nTipp:\n\nim Argument cols kannst du entweder die Spalten auflisten, die “pivotiert” werden sollen.\nAlternativ kannst du (mit vorangestelltem Minuszeichen, -) die Spalte, bezeichnen, die nicht pivotiert werden soll.\nIn beiden Fällen musst du die Spalten weder mit Anführungs- und Schlusszeichen noch mit dem $-Zeichen versehen.\n\n\nlibrary(tidyr)\n\n# Variante 1 (Spalten abwählen)\nsensors_long <- pivot_longer(sensors_combined, -Datetime) \n\n# Variante 2 (Spalten anwählen)\nsensors_long <- pivot_longer(sensors_combined, c(sensor1:sensor3))"
  },
  {
    "objectID": "prepro/Prepro3_Uebung.html#aufgabe-3",
    "href": "prepro/Prepro3_Uebung.html#aufgabe-3",
    "title": "Prepro 3: Übung",
    "section": "Aufgabe 3",
    "text": "Aufgabe 3\nGruppiere sensors_long nach der neuen Spalte wo die Sensor-Information enthalten ist (default: name) mit group_by und berechne die mittlere Temperatur pro Sensor (summarise). Hinweis: Beide Funktionen sind Teil des Packages dplyr.\nDer Output sieht folgendermassen aus:\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nsensors_long %>%\n  group_by(name) %>%\n  summarise(temp_mean = mean(value, na.rm = TRUE))\n\n# A tibble: 3 × 2\n  name    temp_mean\n  <chr>       <dbl>\n1 sensor1      14.7\n2 sensor2      12.0\n3 sensor3      14.4"
  },
  {
    "objectID": "prepro/Prepro3_Uebung.html#aufgabe-4",
    "href": "prepro/Prepro3_Uebung.html#aufgabe-4",
    "title": "Prepro 3: Übung",
    "section": "Aufgabe 4",
    "text": "Aufgabe 4\nErstelle für sensors_long eine neue convenience Variabel month welche den Monat beinhaltet (Tipp: verwende dazu die Funktion month aus lubridate). Gruppiere nun nach month und Sensor und berechne die mittlere Temperatur.\n\nlibrary(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\nsensors_long %>%\n  mutate(month = month(Datetime)) %>%\n  group_by(month, name) %>%\n  summarise(temp_mean = mean(value, na.rm = TRUE))\n\n`summarise()` has grouped output by 'month'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 6 × 3\n# Groups:   month [2]\n  month name    temp_mean\n  <dbl> <chr>       <dbl>\n1    10 sensor1     14.7 \n2    10 sensor2     12.7 \n3    10 sensor3     14.4 \n4    11 sensor1    NaN   \n5    11 sensor2      8.87\n6    11 sensor3    NaN"
  },
  {
    "objectID": "prepro/Prepro3_Uebung.html#aufgabe-5",
    "href": "prepro/Prepro3_Uebung.html#aufgabe-5",
    "title": "Prepro 3: Übung",
    "section": "Aufgabe 5",
    "text": "Aufgabe 5\nLade jetzt nochmal den Datensatz wetter.csv (Quelle MeteoSchweiz) herunter und importiere ihn als CSV mit den Korrekten Spaltentypen (stn als factor, time als POSIXct, tre200h0 als double).\n\nweather <- read_csv(here(\"data\",\"weather.csv\"), col_types = cols(col_factor(), col_datetime(\"%Y%m%d%H\"), col_double()))"
  },
  {
    "objectID": "prepro/Prepro3_Uebung.html#aufgabe-6",
    "href": "prepro/Prepro3_Uebung.html#aufgabe-6",
    "title": "Prepro 3: Übung",
    "section": "Aufgabe 6",
    "text": "Aufgabe 6\nErstelle nun eine convenience Variabel für die Kalenderwoche pro Messung (lubridate::isoweek). Berechne im Anschluss den mittleren Temperaturwert pro Kalenderwoche.\n\nweather_summary <- weather %>%\n  mutate(week = isoweek(time)) %>%\n  group_by(week) %>%\n  summarise(\n    temp_mean = mean(tre200h0, na.rm = TRUE)\n  )\n\nVisualisiere im Anschluss das Resultat:\n\nplot(weather_summary$week, weather_summary$temp_mean, type = \"l\")"
  },
  {
    "objectID": "prepro/Prepro3_Uebung.html#aufgabe-7",
    "href": "prepro/Prepro3_Uebung.html#aufgabe-7",
    "title": "Prepro 3: Übung",
    "section": "Aufgabe 7",
    "text": "Aufgabe 7\nIn der vorherigen Aufgabe haben wir die mittlere Temperatur pro Kalenderwoche über alle Jahre (2000 und 2001) berechnet. Wenn wir die Jahre aber miteinander vergleichen wollen, müssen wir das Jahr als zusätzliche convenience Variabel erstellen und danach gruppieren. Versuche dies mit den Wetterdaten und visualisiere den Output anschliessend.\n\nweather_summary2 <- weather %>%\n  mutate(\n    week = week(time),\n    year = year(time)\n    ) %>%\n  group_by(year, week) %>%\n  summarise(\n    temp_mean = mean(tre200h0, na.rm = TRUE)\n  )\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\n\n\nplot(weather_summary2$week, weather_summary2$temp_mean, type = \"l\")\n\n\n\n\nbaseplot mag keine long tables und macht aus den beiden Jahren eine kontinuierliche Linie"
  },
  {
    "objectID": "prepro/Prepro3_Uebung.html#aufgabe-8",
    "href": "prepro/Prepro3_Uebung.html#aufgabe-8",
    "title": "Prepro 3: Übung",
    "section": "Aufgabe 8",
    "text": "Aufgabe 8\nÜberführe den Output aus der letzten Übung in eine wide table. Nun lassen sich die beiden Jahre viel besser miteinander vergleichen.\n\nweather_summary2 <- weather_summary2 %>%\n  pivot_wider(names_from = year, values_from = temp_mean,names_prefix = \"year\")\n\n\nplot(weather_summary2$week, weather_summary2$year2000, type = \"l\",col = \"blue\")\nlines(weather_summary2$week, weather_summary2$year2001, type = \"l\",col = \"red\")"
  },
  {
    "objectID": "infovis/Infovis1_Demo.html",
    "href": "infovis/Infovis1_Demo.html",
    "title": "Infovis 1: Demo A",
    "section": "",
    "text": "Als erstes laden wir den Temperaturdatensatz ein. Es handelt sich dabei um eine leicht modifizierte Variante der Daten aus PrePro1 und PrePro2."
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#base-plot-vs.-ggplot",
    "href": "infovis/Infovis1_Demo.html#base-plot-vs.-ggplot",
    "title": "Infovis 1: Demo A",
    "section": "Base-plot vs. ggplot",
    "text": "Base-plot vs. ggplot\nUm in “base-R” einen Scatterplot zu erstellen wo Datum der Temperatur gegenübersteht, gehen wir wie folgt vor:\n\nplot(temperature$time, temperature$SHA, type = \"l\", col = \"red\")\nlines(temperature$time, temperature$ZER, col = \"blue\")\n\n\n\n\nIn ggplot sieht das etwas anders und auf den ersten Blick etwas komplizierter aus: Ein plot wird durch den Befehl ggplot() initiiert. Hier wird einerseits der Datensatz festgelegt, auf dem der Plot beruht (data =), sowie die Variablen innerhalb des Datensatzes, die Einfluss auf den Plot ausüben (mapping = aes()).\n\n# Datensatz: \"temperature\" | Beeinflussende Variabeln: \"time\" und \"temp\"\nggplot(data = temperature, mapping = aes(time,SHA))             \n\n\n\n\nWeiter braucht es mindestens ein “Layer” der beschreibt, wie die Daten dargestellt werden sollen (z.B. geom_point()). Anders als bei “Piping” (%>%) wird ein Layer mit + hinzugefügt.\n\nggplot(data = temperature, mapping = aes(time,SHA)) +         \n  # Layer: \"geom_point\" entspricht Punkten in einem Scatterplot \n  geom_point()                                    \n\n\n\n\nDa ggplot die Eingaben in der Reihenfolge data = und dann mapping =erwartet, können wir diese Spezifizierungen auch weglassen.\n\nggplot(temperature, aes(time,SHA)) +\n  geom_point()"
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#long-vs.-wide",
    "href": "infovis/Infovis1_Demo.html#long-vs.-wide",
    "title": "Infovis 1: Demo A",
    "section": "Long vs. wide",
    "text": "Long vs. wide\nWie wir in PrePro 2 bereits erwähnt haben, ist ggplot2 auf long tables ausgelegt. Wir überführen deshalb an dieser Stelle die breite in eine lange Tabelle:\n\ntemperature_long <- pivot_longer(temperature, -time, names_to = \"station\", values_to = \"temp\")\n\nNun wollen wir die unterschiedlichen Stationen unterschiedlich einfärben. Da wir Variablen definieren wollen, welche Einfluss auf die Grafik haben sollen, gehört diese Information in aes().\n\nggplot(temperature_long, aes(time,temp, colour = station)) +\n  geom_point()\n\n\n\n\nWir können noch einen Layer mit Linien hinzufügen:\n\nggplot(temperature_long, aes(time,temp, colour = station)) +\n  geom_point()+\n  geom_line()"
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#beschriftungen-labels",
    "href": "infovis/Infovis1_Demo.html#beschriftungen-labels",
    "title": "Infovis 1: Demo A",
    "section": "Beschriftungen (labels)",
    "text": "Beschriftungen (labels)\nWeiter können wir die Achsen beschriften und einen Titel hinzufügen. Zudem lasse ich die Punkte (geom_point()) nun weg, da mir diese nicht gefallen.\n\nggplot(temperature_long, aes(time,temp, colour = station)) +\n  geom_line() +\n  labs(\n    x = \"Zeit\",\n    y = \"Temperatur in Grad C°\", \n    title = \"Temperaturdaten Schweiz\",\n    subtitle = \"2001 bis 2002\",\n    color = \"Station\"\n    )"
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#split-apply-combine",
    "href": "infovis/Infovis1_Demo.html#split-apply-combine",
    "title": "Infovis 1: Demo A",
    "section": "Split Apply Combine",
    "text": "Split Apply Combine\nIm obigen Plot fällt auf, dass stündliche Werte eine zu hohe Auflösung ist, wenn wir daten über 2 Jahre visualisieren. Mit Split Apply Combine (PrePro 3) können wir die Auflösung unserer Daten verändern:\n\ntemperature_day <- temperature_long %>%\n  mutate(time = as.Date(time)) \n\ntemperature_day\n\n# A tibble: 35,088 × 3\n   time       station  temp\n   <date>     <chr>   <dbl>\n 1 2000-01-01 SHA       0.2\n 2 2000-01-01 ZER      -8.8\n 3 2000-01-01 SHA       0.3\n 4 2000-01-01 ZER      -8.7\n 5 2000-01-01 SHA       0.3\n 6 2000-01-01 ZER      -9  \n 7 2000-01-01 SHA       0.3\n 8 2000-01-01 ZER      -8.7\n 9 2000-01-01 SHA       0.4\n10 2000-01-01 ZER      -8.5\n# … with 35,078 more rows\n\ntemperature_day <- temperature_day %>%\n  group_by(station, time) %>%\n  summarise(temp = mean(temp))\n\n`summarise()` has grouped output by 'station'. You can override using the\n`.groups` argument."
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#xy-achse-anpassen",
    "href": "infovis/Infovis1_Demo.html#xy-achse-anpassen",
    "title": "Infovis 1: Demo A",
    "section": "X/Y-Achse anpassen",
    "text": "X/Y-Achse anpassen\nMan kann auch Einfluss auf die x-/y-Achsen nehmen. Dabei muss man zuerst festlegen, was für ein Achsentyp der Plot hat (vorher hat ggplot eine Annahme auf der Basis der Daten getroffen).\nBei unserer y-Achse handelt es sich um numerische Daten, ggplot nennt diese: scale_y_continuous(). Unter ggplot2.tidyverse.org findet man noch andere x/y-Achsentypen (scale_x_irgenwas bzw. scale_y_irgendwas).\n\nggplot(temperature_day, aes(time,temp, colour = station)) +\n  geom_line() +\n  labs(\n    x = \"Zeit\",\n    y = \"Temperatur in Grad C°\", \n    title = \"Temperaturdaten Schweiz\",\n    subtitle = \"2001 bis 2002\",\n    color = \"Station\"\n    ) +    \n  scale_y_continuous(limits = c(-30,30))    # y-Achsenabschnitt bestimmen\n\n\n\n\nDas gleiche Spiel kann man für die y-Achse betreiben. Bei unserer y-Achse handelt es sich ja um Datumsangaben. ggplot nennt diese: scale_x_date().\n\nggplot(temperature_day, aes(time,temp, colour = station)) +\n  geom_line() +\n  labs(\n    x = \"Zeit\",\n    y = \"Temperatur in Grad C°\", \n    title = \"Temperaturdaten Schweiz\",\n    subtitle = \"2001 bis 2002\",\n    color = \"Station\"\n    ) +    \n  scale_y_continuous(limits = c(-30,30)) +\n  scale_x_date(date_breaks = \"3 months\", \n                   date_labels = \"%b\")"
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#themes",
    "href": "infovis/Infovis1_Demo.html#themes",
    "title": "Infovis 1: Demo A",
    "section": "Themes",
    "text": "Themes\nMit theme verändert man das allgmeine Layout der Plots. Beispielsweise kann man mit theme_classic() ggplot-Grafiken etwas weniger “Poppig” erscheinen lassen: so sind sie besser für Bachelor- / Masterarbeiten sowie Publikationen geeignet. theme_classic() kann man indiviudell pro Plot anwenden, oder für die aktuelle Session global setzen (s.u.)\nIndividuell pro Plot:\n\nggplot(temperature_day, aes(time,temp, colour = station)) +\n  geom_line() +\n  theme_classic()\n\nGlobal (für alle nachfolgenden Plots der aktuellen Session):\n\ntheme_set(theme_classic())"
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#facets-small-multiples",
    "href": "infovis/Infovis1_Demo.html#facets-small-multiples",
    "title": "Infovis 1: Demo A",
    "section": "Facets / Small Multiples",
    "text": "Facets / Small Multiples\nSehr praktisch sind auch die Funktionen für “Small multiples”. Dies erreicht man mit facet_wrap() (oder facet_grid(), mehr dazu später). Man muss mit einem Tilde-Symbol “~” nur festlegen, welche Variable für das Aufteilen des Plots in kleinere Subplots verantwortlich sein soll.\n\nggplot(temperature_day, aes(time,temp, colour = station)) +\n  geom_line() +\n  labs(\n    x = \"Zeit\",\n    y = \"Temperatur in Grad C°\", \n    title = \"Temperaturdaten Schweiz\",\n    subtitle = \"2001 bis 2002\",\n    color = \"Station\"\n    ) +    \n  scale_y_continuous(limits = c(-30,30)) +\n  scale_x_date(date_breaks = \"3 months\", \n                   date_labels = \"%b\") +\n  facet_wrap(station~.)\n\n\n\n\nAuch facet_wrap kann man auf seine Bedürfnisse anpassen: Beispielweise kann man mit ncol = die Anzahl facets pro Zeile bestimmen.\nZudem brauchen wir die Legende nicht mehr, da der Stationsnamen über jedem Facet steht. Ich setze deshalb theme(legend.position=\"none\")\n\nggplot(temperature_day, aes(time,temp, colour = station)) +\n  geom_line() +\n  labs(\n    x = \"Zeit\",\n    y = \"Temperatur in Grad C°\", \n    title = \"Temperaturdaten Schweiz\",\n    subtitle = \"2001 bis 2002\"\n    ) +  \n  scale_y_continuous(limits = c(-30,30)) +\n  scale_x_date(date_breaks = \"3 months\", \n                   date_labels = \"%b\") +\n  facet_wrap(~station,ncol = 1) +\n  theme(legend.position=\"none\")"
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#in-variabel-abspeichern-und-exportieren",
    "href": "infovis/Infovis1_Demo.html#in-variabel-abspeichern-und-exportieren",
    "title": "Infovis 1: Demo A",
    "section": "In Variabel abspeichern und Exportieren",
    "text": "In Variabel abspeichern und Exportieren\nGenau wie data.frames und andere Objekte, kann man einen ganzen Plot auch in einer Variabel speichern. Dies kann nützlich sein um einen Plot zu exportieren (als png, jpg usw.) oder sukzessive erweitern wie in diesem Beispiel.\n\np <- ggplot(temperature_day, aes(time,temp, colour = station)) +\n  geom_line() +\n  labs(\n    x = \"Zeit\",\n    y = \"Temperatur in Grad C°\", \n    title = \"Temperaturdaten Schweiz\",\n    subtitle = \"2001 bis 2002\"\n    ) +\n  scale_y_continuous(limits = c(-30,30)) +\n  scale_x_date(date_breaks = \"3 months\", \n                   date_labels = \"%b\") +\n  facet_wrap(~station,ncol = 1)\n  # ich habe an dieser Stelle theme(legend.position=\"none\") entfernt\n\nFolgendermassen kann ich den Plot als png-File abspeichern (ohne Angabe von “plot =” wird einfach der letzte Plot gespeichert)\n\nggsave(filename = \"plot.png\",plot = p)\n\n.. und so kann ich einen bestehenden Plot (in einer Variabel) mit einem Layer / einer Option erweitern\n\np +\n  theme(legend.position=\"none\")\n\nWie üblich wurde diese Änderung nicht gespeichert, sondern nur das Resultat davon ausgeben. Wenn die Änderung in meinem Plot (in der Variabel) abspeichern will, muss ich die Variabel überschreiben:\n\np <- p +\n  theme(legend.position=\"none\")"
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#smoothing",
    "href": "infovis/Infovis1_Demo.html#smoothing",
    "title": "Infovis 1: Demo A",
    "section": "Smoothing",
    "text": "Smoothing\nMit geom_smooth() kann ggplot eine Trendlinie auf der Baiss von Punktdaten berechnen. Die zugrunde liegende statistische Methode kann selbst gewählt werden (ohne Angabe verwendet ggplot bei < 1’000 Messungen stats::loess, ansonsten mgcv::gam)\n\np <- p +\n  geom_smooth(colour = \"black\")\n\np\n\n`geom_smooth()` using method = 'loess' and formula 'y ~ x'"
  },
  {
    "objectID": "infovis/Infovis1_Script_eda.html",
    "href": "infovis/Infovis1_Script_eda.html",
    "title": "Infovis 1: Demo B",
    "section": "",
    "text": "library(ggplot2)\nlibrary(dplyr)\nlibrary(scales)\n\n# create some data about age and height of people\npeople <- data.frame(\n  ID = c(1:30),\n  \n  age = c(5.0, 7.0, 6.5 ,9.0, 8.0, 5.0, 8.6, 7.5, 9.0, 6.0,\n          63.5 ,65.7, 57.6, 98.6, 76.5, 78.0, 93.4, 77.5, 256.6, 512.3,\n          15.5, 18.6, 18.5, 22.8, 28.5, 39.5, 55.9, 50.3, 31.9, 41.3),\n  \n  height = c(0.85, 0.93, 1.1, 1.25, 1.33, 1.17, 1.32, 0.82, 0.89, 1.13,\n             1.62, 1.87, 1.67, 1.76, 1.56, 1.71, 1.65, 1.55, 1.87, 1.69,\n             1.49, 1.68, 1.41, 1.55, 1.84, 1.69, 0.85, 1.65, 1.94, 1.80),\n  \n  weight = c(45.5, 54.3, 76.5, 60.4, 43.4, 36.4, 50.3, 27.8, 34.7, 47.6,\n             84.3, 90.4, 76.5, 55.6, 54.3, 83.2, 80.7, 55.6, 87.6, 69.5,\n             48.0, 55.6, 47.6, 60.5, 54.3, 59.5, 34.5, 55.4, 100.4, 110.3)\n)\n\n# build a scatterplot for a first inspection\nggplot(people, aes(x=age, y=height)) + \n  geom_point() \n\n\n\nggplot(people, aes(x=age, y=height)) + \n  geom_point() +\n  scale_y_continuous(limits = c(0.75, 2))\n\n\n\n# Go to help page: http://docs.ggplot2.org/current/ -> Search for icon of fit-line\n# http://docs.ggplot2.org/current/geom_smooth.html\n\n# build a scatterplot for a first inspection, with regression line\nggplot(people, aes(x=age, y=height)) + \n  geom_point() + \n  scale_y_continuous(limits=c(0, 2.0)) +\n  geom_smooth()\n\nWarning: Removed 41 rows containing missing values (geom_smooth).\n\n\n\n\n# stem and leaf plot\nstem(people$height)\n\n\n  The decimal point is 1 digit(s) to the left of the |\n\n   8 | 25593\n  10 | 037\n  12 | 523\n  14 | 19556\n  16 | 255789916\n  18 | 04774\n\nstem(people$height, scale=2)\n\n\n  The decimal point is 1 digit(s) to the left of the |\n\n   8 | 2559\n   9 | 3\n  10 | \n  11 | 037\n  12 | 5\n  13 | 23\n  14 | 19\n  15 | 556\n  16 | 2557899\n  17 | 16\n  18 | 0477\n  19 | 4\n\n# explore the two variables with box-whiskerplots\nsummary(people$age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   5.00    8.70   30.20   59.14   65.15  512.30 \n\nboxplot(people$age)\n\n\n\nsummary(people$height)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.820   1.190   1.555   1.455   1.690   1.940 \n\nboxplot(people$height)\n\n\n\n# explore data with a histgram\nggplot(people, aes(x=age)) + \n  geom_histogram(binwidth=20)  \n\n\n\ndensity(x = people$height)\n\n\nCall:\n    density.default(x = people$height)\n\nData: people$height (30 obs.);  Bandwidth 'bw' = 0.1576\n\n       x                y           \n Min.   :0.3472   Min.   :0.001593  \n 1st Qu.:0.8636   1st Qu.:0.102953  \n Median :1.3800   Median :0.510601  \n Mean   :1.3800   Mean   :0.483553  \n 3rd Qu.:1.8964   3rd Qu.:0.722660  \n Max.   :2.4128   Max.   :1.216350  \n\n# re-expression: use log or sqrt axes\n#\n# Find here guideline about scaling axes \n# http://www.cookbook-r.com/Graphs/Axes_(ggplot2)/\n# http://docs.ggplot2.org/0.9.3.1/scale_continuous.html\n\n# logarithmic axis: respond to skewness in the data, e.g. log10 \nggplot(people, aes(x=age, y=height)) + \n  geom_point() + \n  scale_y_continuous(limits=c(0, 2.0)) +\n  geom_smooth() +\n  scale_x_log10()\n\n\n\n# outliers: Remove very small and very old people\n\npeopleClean <- people %>%\n  filter(ID != 27) %>%    # Diese Person war zu klein.\n  filter(age < 100)       # Fehler in der Erhebung des Alters\n\nggplot(peopleClean, aes(x=age)) + \n  geom_histogram(binwidth=10)\n\n\n\nggplot(peopleClean, aes(x=age, y=height)) + \n  geom_point() + \n  scale_y_continuous(limits=c(0, 2.0)) +\n  geom_smooth()\n\n\n\n# with custom binwidth\nggplot(peopleClean, aes(x=age)) + \n  geom_histogram(binwidth=10) + \n  theme_bw() # specifying the theme\n\n\n\n# quadratic axis\nggplot(peopleClean, aes(x=age, y=height)) + \n  geom_point() + scale_y_continuous(limits=c(0, 2.0)) +\n  geom_smooth(method=\"lm\", fill='lightblue', size=0.5, alpha=0.5) + \n  scale_x_sqrt()\n\n\n\n# filter \"teenies\": No trend\nfilter(peopleClean, age < 15) %>%\n  ggplot(aes(x=age, y=height)) + \n  geom_point() + \n  scale_y_continuous(limits=c(0, 2.0)) +\n  geom_smooth(method=\"lm\", fill='lightblue', size=0.5, alpha=0.5)\n\n\n\n# filter \"teenies\": No trend\npeopleClean %>%\n  filter(age > 55) %>%\n  ggplot(aes(x=age, y=height)) + \n  geom_point() + \n  scale_y_continuous(limits=c(0, 2.0)) +\n  geom_smooth(method=\"lm\", fill='lightblue', size=0.5, alpha=0.5)\n\n\n\n# Onwards towards multidimensional data\n\n# Finally, make a scatterplot matrix\npairs(peopleClean[,2:4], panel=panel.smooth)\n\npairs(peopleClean[,2:4], panel=panel.smooth)"
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html",
    "href": "infovis/Infovis1_Uebung.html",
    "title": "Infovis 1: Übung",
    "section": "",
    "text": "In dieser Übung geht es darum, die Grafiken aus dem Blog-post Kovic (2014) zu rekonstruieren. Schau dir die Grafiken in dem Blogpost durch. Freundlicherweise wurden im Blogbeitrag die ggplot2 Standardeinstellungen benutzt, was die Rekonstruktion relativ einfach macht. Die Links im Text verweisen auf die Originalgrafik, die eingebetteten Plots sind meine eigenen Rekonstruktionen.\nImportiere als erstes den Datensatz initiative_masseneinwanderung_kanton.csv (dieser ist auch auf der Blog-Seite verfügbar)."
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#aufgabe-1",
    "href": "infovis/Infovis1_Uebung.html#aufgabe-1",
    "title": "Infovis 1: Übung",
    "section": "Aufgabe 1",
    "text": "Aufgabe 1\nRekonstruiere Grafik 1 (Kovic 2014). Erstelle dazu einen Scatterplot wo der Ausländeranteil der Kantone dem Ja-Anteil gegenüber gestellt wird. Speichere den Plot einer Variabel plot1.\n\nnutze ggplot(kanton, aes(auslanderanteil, ja_anteil)) um den ggplot zu initiieren. Füge danach ein einen Punkte Layer hinzu (geom_point())\nnutze coord_fixed() um die beiden Achsen in ein fixes Verhältnis zu setzen (1:1).\nOptional:\n\nsetze die Achsen Start- und Endwerte mittels scale_y_continuous bzw. scale_x_continuous.\nSetze analog Kovic (2014) die breaks (0.0, 0.1…0.7) manuell (innerhalb scale_*_continuous)\n\n\nRekonstruktion:"
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#aufgabe-2",
    "href": "infovis/Infovis1_Uebung.html#aufgabe-2",
    "title": "Infovis 1: Übung",
    "section": "Aufgabe 2",
    "text": "Aufgabe 2\nRekonstruiere Grafik 2. Erweitere dazu plot1 mit einer Trendlinie."
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#aufgabe-3",
    "href": "infovis/Infovis1_Uebung.html#aufgabe-3",
    "title": "Infovis 1: Übung",
    "section": "Aufgabe 3",
    "text": "Aufgabe 3\nImportiere die Gemeindedaten initiative_masseneinwanderung_gemeinde.csv. Rekonstruiere danach Grafik 3 indem du den Ausländeranteil aller Gemeinden dem Ja-Stimmen-Anteil gegenüber. Speichere den Plot als plot2"
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#aufgabe-4",
    "href": "infovis/Infovis1_Uebung.html#aufgabe-4",
    "title": "Infovis 1: Übung",
    "section": "Aufgabe 4",
    "text": "Aufgabe 4\nRekonstruiere Grafik 4 indem plot2 mit einer Trendlinie erweitert wird."
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#aufgabe-5",
    "href": "infovis/Infovis1_Uebung.html#aufgabe-5",
    "title": "Infovis 1: Übung",
    "section": "Aufgabe 5",
    "text": "Aufgabe 5\nRekonstruiere Grafik 5 indem plot2 mit facetting erweitert wird. Die Facets sollen die einzelnen Kantone sein. Speichere den Plot als plot3."
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#aufgabe-6",
    "href": "infovis/Infovis1_Uebung.html#aufgabe-6",
    "title": "Infovis 1: Übung",
    "section": "Aufgabe 6",
    "text": "Aufgabe 6\nRekonstruiere Grafik 6 indem plot3 mit einer Trendlinie erweitert wird.\nRekonstruktion:"
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#aufgabe-7",
    "href": "infovis/Infovis1_Uebung.html#aufgabe-7",
    "title": "Infovis 1: Übung",
    "section": "Aufgabe 7",
    "text": "Aufgabe 7\nRekonstruiere Grafik 7 indem plot2mit facetting erweitert wird. Die Facets sollen nun den Grössen-Quantilen entsprechen. Speichere den Plot unter plot4.\nRekonstruktion:"
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#aufgabe-8",
    "href": "infovis/Infovis1_Uebung.html#aufgabe-8",
    "title": "Infovis 1: Übung",
    "section": "Aufgabe 8",
    "text": "Aufgabe 8\nRekonstruiere Grafik 8 indem plot4 mit einer Trendlinie ausgestattet wird."
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#aufgabe-9-optional-fortgeschritten",
    "href": "infovis/Infovis1_Uebung.html#aufgabe-9-optional-fortgeschritten",
    "title": "Infovis 1: Übung",
    "section": "Aufgabe 9 (Optional, fortgeschritten)",
    "text": "Aufgabe 9 (Optional, fortgeschritten)\nRekonstruiere die Korrelationstabelle.\nTipp: - Nutze group_by() und summarise() - Nutze cor.test() um den Korrelationskoeffizienten sowie den p-Wert zu erhalten. - Mit $estimate und $p.value können die entsprechenden Werte direkt angesprochen werden\nHinweis: aus bisher unerklärlichen Gründen weiche gewisse meiner Werte leicht von den Berechnungen aus Kovic (2014) ab.\n\n\n\n\n\n\nLegende: * = p<0.05, ** = p<0.01, ***=p<0.001.\n\n\nkanton\nKorr.Koeffizient\nSignifikanz\n\n\n\n\nAG\n-0.2362552\n***\n\n\nAI\n-0.7828022\n-\n\n\nAR\n-0.0892817\n-\n\n\nBE\n-0.4422003\n***\n\n\nBL\n-0.2919712\n**\n\n\nBS\n-0.9935385\n-\n\n\nFR\n-0.4217634\n***\n\n\nGE\n0.3753004\n*\n\n\nGL\n-0.4070120\n-\n\n\nGR\n-0.0426607\n-\n\n\nJU\n-0.2252540\n-\n\n\nLU\n-0.3048455\n**\n\n\nNE\n-0.5214180\n***\n\n\nNW\n-0.2018174\n-\n\n\nOW\n-0.4813090\n-\n\n\nSG\n-0.2449093\n*\n\n\nSH\n-0.2995527\n-\n\n\nSO\n-0.0533442\n-\n\n\nSZ\n-0.7259276\n***\n\n\nTG\n-0.5522862\n***\n\n\nTI\n0.1512509\n-\n\n\nUR\n-0.3848167\n-\n\n\nVD\n-0.2685301\n***\n\n\nVS\n-0.1736954\n*\n\n\nZG\n0.0407166\n-\n\n\nZH\n-0.2744683\n***\n\n\n\n\n\n\n\n\n\n\n\nKovic, Marko. 2014. “Je Weniger Ausländer, Desto Mehr Ja-Stimmen? Wirklich?” Tagesanzeiger Datenblog. https://blog.tagesanzeiger.ch/datenblog/index.php/668/je-weniger-auslaender-desto-mehr-ja-stimmen-wirklich."
  },
  {
    "objectID": "infovis/Infovis2_Uebung_A.html",
    "href": "infovis/Infovis2_Uebung_A.html",
    "title": "Infovis 2: Übung A",
    "section": "",
    "text": "Für die heutige Übung brauchst du den Datensatz temperature_2005.csv. Dabei handelt es sich wieder um Teperaturwerte verschiedener Stationen, diesmal aus dem Jahr 2005. Das Datum ist so formatiert, dass R (isbesondere read_csv) es korrekt als datetime erkennen und als POSIXct einlesen sollte."
  },
  {
    "objectID": "infovis/Infovis2_Uebung_A.html#aufgabe-1",
    "href": "infovis/Infovis2_Uebung_A.html#aufgabe-1",
    "title": "Infovis 2: Übung A",
    "section": "Aufgabe 1",
    "text": "Aufgabe 1\nMache aus der wide table eine long table die wie folgt aussieht.\n\n\n\n\n\ntime\nstation\ntemperature\n\n\n\n\n2005-01-01\nALT\n1.3\n\n\n2005-01-01\nBUS\n1.5\n\n\n2005-01-01\nGVE\n1.1\n\n\n2005-01-01\nINT\n0.2\n\n\n2005-01-01\nOTL\n2.2\n\n\n2005-01-01\nLUG\n1.7\n\n\n\n\n\nLade anschliessend temperature_2005_metadata.csv herunter und verbinde die beiden Datensätze mit einem left_join via station (bzw. stn).\n\n\nRows: 24 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): stn, Name\ndbl (3): Meereshoehe, x, y\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "infovis/Infovis2_Uebung_A.html#aufgabe-2",
    "href": "infovis/Infovis2_Uebung_A.html#aufgabe-2",
    "title": "Infovis 2: Übung A",
    "section": "Aufgabe 2",
    "text": "Aufgabe 2\nErstelle ein Scatterplot (time vs. temperature) wobei die Punkte aufgrund ihrer Meereshöhe eingefärbt werden sollen. Tiefe Werte sollen dabei blau eingefärbt werden und hohe Werte rot (scale_color_gradient). Verkleinere die Punkte um übermässiges Überplotten der Punkten zu vermeiden (size =). Weiter sollen auf der x-Achse im Abstand von 3 Monaten der jeweilige Monat vermerkt sein (date_breaks bzw. date_labels von scale_x_datetime()).\n\n\nWarning: Removed 1 rows containing missing values (geom_point)."
  },
  {
    "objectID": "infovis/Infovis2_Uebung_A.html#aufgabe-3",
    "href": "infovis/Infovis2_Uebung_A.html#aufgabe-3",
    "title": "Infovis 2: Übung A",
    "section": "Aufgabe 3",
    "text": "Aufgabe 3\nErstelle eine Zusatzvariabel Date mit dem Datum der jeweiligen Messung ( mit as.Date). Nutze diese Spalte um die Tagesmitteltemperatur pro Station zu berechnen (mit summarise()).\nUm die Metadaten (Name, Meereshoehe, x, y) nicht zu verlieren kannst du den Join aus der ersten Übung wieder ausführen. Alternativ (schneller aber auch schwerer zu verstehen) kannst du diese Variabeln innerhalb deines group_by verwenden.\n\n\n`summarise()` has grouped output by 'time', 'station', 'Name', 'Meereshoehe',\n'x'. You can override using the `.groups` argument."
  },
  {
    "objectID": "infovis/Infovis2_Uebung_A.html#aufgabe-4",
    "href": "infovis/Infovis2_Uebung_A.html#aufgabe-4",
    "title": "Infovis 2: Übung A",
    "section": "Aufgabe 4",
    "text": "Aufgabe 4\nWiederhole nun den Plot aus der ersten Aufgabe mit den aggregierten Daten aus der vorherigen Aufgabe. Um die labels korrekt zu setzen musst du scale_x_datetime mit scale_x_date ersetzen.\n\n\nWarning: Removed 1 rows containing missing values (geom_point)."
  },
  {
    "objectID": "infovis/Infovis2_Uebung_A.html#aufgabe-5",
    "href": "infovis/Infovis2_Uebung_A.html#aufgabe-5",
    "title": "Infovis 2: Übung A",
    "section": "Aufgabe 5",
    "text": "Aufgabe 5\nFüge am obigen Plot eine schwarze, gestrichelte Trendlinie hinzu.\n\n\nWarning: Removed 1 rows containing non-finite values (stat_smooth).\n\n\nWarning: Removed 1 rows containing missing values (geom_point)."
  },
  {
    "objectID": "infovis/Infovis2_Uebung_A.html#aufgabe-6",
    "href": "infovis/Infovis2_Uebung_A.html#aufgabe-6",
    "title": "Infovis 2: Übung A",
    "section": "Aufgabe 6",
    "text": "Aufgabe 6\nPositioniere die Legende oberhalb des Plots (nutze dazu theme() mit legend.position).\n\n\nWarning: Removed 1 rows containing non-finite values (stat_smooth).\n\n\nWarning: Removed 1 rows containing missing values (geom_point)."
  },
  {
    "objectID": "infovis/Infovis2_Uebung_A.html#aufgabe-7-optional-fortgeschritten",
    "href": "infovis/Infovis2_Uebung_A.html#aufgabe-7-optional-fortgeschritten",
    "title": "Infovis 2: Übung A",
    "section": "Aufgabe 7 (optional, fortgeschritten)",
    "text": "Aufgabe 7 (optional, fortgeschritten)\nFüge den Temperaturwerten auf der y-Ache ein °C hinzu (siehe unten und studiere diesen Tipp zur Hilfe).\n\n\nWarning: Removed 1 rows containing non-finite values (stat_smooth).\n\n\nWarning: Removed 1 rows containing missing values (geom_point)."
  },
  {
    "objectID": "infovis/Infovis2_Uebung_A.html#aufgabe-8",
    "href": "infovis/Infovis2_Uebung_A.html#aufgabe-8",
    "title": "Infovis 2: Übung A",
    "section": "Aufgabe 8",
    "text": "Aufgabe 8\nJetzt verlassen wir den scatterplot und machen einen Boxplot mit den Temperaturdaten. Färbe die Boxplots wieder in Abhängigkeit der Meereshöhe ein.\n\nBeachte den Unterschied zwischen colour = und fill =\nBeachte den Unterschied zwischen facet_wrap() und facet_grid()\nfacet_grid() braucht übrigens noch einen Punkt (.) zur Tilde (~).\nBeachte den Unterschied zwischen “.~” und “~.” bei facet_grid()\nverschiebe nach Bedarf die Legende\n\n\n\nWarning: Removed 1 rows containing non-finite values (stat_boxplot)."
  },
  {
    "objectID": "infovis/Infovis2_Uebung_A.html#aufgabe-9",
    "href": "infovis/Infovis2_Uebung_A.html#aufgabe-9",
    "title": "Infovis 2: Übung A",
    "section": "Aufgabe 9",
    "text": "Aufgabe 9\nAls letzter wichtiger Plottyp noch zwei Übungen zum Histogramm. Erstelle ein Histogramm geom_histogram() mit den Temperaturwerten. Teile dazu die Stationen in verschiedene Höhenlagen ein (Tieflage [< 400 m], Mittellage [400 - 600 m] und Hochlage [> 600 m]). Vergleiche die Verteilung der Temperaturwerte in den verschiedenen Lagen mit einem Histogramm.\nTip: Nutze cut um die Stationen in die drei Gruppen aufzuteilen\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "infovis/Infovis2_Uebung_A.html#musterlösung",
    "href": "infovis/Infovis2_Uebung_A.html#musterlösung",
    "title": "Infovis 2: Übung A",
    "section": "Musterlösung",
    "text": "Musterlösung"
  },
  {
    "objectID": "infovis/Infovis2_Uebung_B.html",
    "href": "infovis/Infovis2_Uebung_B.html",
    "title": "Infovis 2: Übung B",
    "section": "",
    "text": "In dieser Übung bauen wir einige etwas unübliche Plots aus der Vorlesung nach. Dafür verwenden wir Datensätze, die in R bereits integriert sind. Eine Liste dieser Datensätze findet man hier oder mit der Hilfe ?datasets.\nDazu verwenden wir nach wie vor ggplot2, aber mit einigen Tricks."
  },
  {
    "objectID": "infovis/Infovis2_Uebung_B.html#aufgabe-1-parallel-coordinate-plots",
    "href": "infovis/Infovis2_Uebung_B.html#aufgabe-1-parallel-coordinate-plots",
    "title": "Infovis 2: Übung B",
    "section": "Aufgabe 1: Parallel coordinate plots",
    "text": "Aufgabe 1: Parallel coordinate plots\nErstelle einen parallel coordinate plot. Dafür eignet sich der integrierte Datensatz mtcars. Extrahiere die Fahrzeugnamen mit rownames_to_column.\nZudem müssen die Werte jeweiles auf eine gemeinsame Skala normalisiert werden. Hierfür kannst du die Funktion scales::rescale verwenden.\n\nmtcars2 <- mtcars %>%\n  tibble::rownames_to_column(\"car\") %>%\n  pivot_longer(-car)\n\nmtcars2 <- mtcars2 %>%\n  group_by(name) %>%\n  mutate(value_scaled = scales::rescale(value))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\nMazda RX4\n21.0\n6\n160\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\nMazda RX4 Wag\n21.0\n6\n160\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\nDatsun 710\n22.8\n4\n108\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\nHornet 4 Drive\n21.4\n6\n258\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\nHornet Sportabout\n18.7\n8\n360\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n\n\nValiant\n18.1\n6\n225\n105\n2.76\n3.460\n20.22\n1\n0\n3\n1\n\n\n\n\n\nSo sieht der fertige Plot aus:\n\nmtcars2 <- mtcars2 %>%\n  group_by(car) %>%\n  mutate(gear = value[name == \"gear\"])\n\nggplot(mtcars2, aes(name, value_scaled, group = car, color = factor(gear))) +\n  geom_point() +\n  geom_line() +\n  theme_minimal() +\n  theme(legend.position = \"none\", axis.title.y = element_blank())"
  },
  {
    "objectID": "infovis/Infovis2_Uebung_B.html#aufgabe-2-polar-plot-mit-biber-daten",
    "href": "infovis/Infovis2_Uebung_B.html#aufgabe-2-polar-plot-mit-biber-daten",
    "title": "Infovis 2: Übung B",
    "section": "Aufgabe 2: Polar Plot mit Biber Daten",
    "text": "Aufgabe 2: Polar Plot mit Biber Daten\nPolar Plots eignen sich unter anderem für Daten, die zyklischer Natur sind, wie zum Beispiel zeitlich geprägte Daten (Tages-, Wochen-, oder Jahresrhythmen). Aus den Beispiels-Datensätzen habe ich zwei Datensätze gefunden, die zeitlich geprägt sind:\n\nbeaver1 und beaver2 AirPassenger\n\nBeide Datensätze müssen noch etwas umgeformt werden, bevor wir sie für einen Radialplot verwenden können. In Aufgabe 2 verwenden wir die Biber-Datensätze, in der nächsten Aufgabe (3) die Passagier-Daten.\nWenn wir die Daten von beiden Bibern verwenden wollen, müssen wir diese noch zusammenfügen:\n\nbeaver1_new <- beaver1 %>%\n  mutate(beaver = \"nr1\")\n\nbeaver2_new <- beaver2 %>%\n  mutate(beaver = \"nr2\")\n\nbeaver_new <- rbind(beaver1_new,beaver2_new)\n\nZudem müssen wir die Zeitangabe noch anpassen: Gemäss der Datenbeschreibung handelt es sich bei der Zeitangabe um ein sehr programmier-unfreundliches Format. 3:30 wird als “0330” notiert. Wir müssen diese Zeitangabe, noch in ein Dezimalsystem umwandeln:\n\nbeaver_new <- beaver_new %>%\n  mutate(\n    hour_dec = (time/100)%/%1,         # Ganze Stunden (mittels ganzzaliger Division)\n    min_dec = (time/100)%%1/0.6,       # Dezimalminuten (15 min wird zu 0.25, via Modulo)\n    hour_min_dec = hour_dec+min_dec    # Dezimal-Zeitangabe (03:30 wird zu 3.5)\n    ) \n\nDer Datensatz:\n\n\n\n\n\nday\ntime\ntemp\nactiv\nbeaver\nhour_dec\nmin_dec\nhour_min_dec\n\n\n\n\n346\n840\n36.33\n0\nnr1\n8\n0.6666667\n8.666667\n\n\n346\n850\n36.34\n0\nnr1\n8\n0.8333333\n8.833333\n\n\n346\n900\n36.35\n0\nnr1\n9\n0.0000000\n9.000000\n\n\n346\n910\n36.42\n0\nnr1\n9\n0.1666667\n9.166667\n\n\n346\n920\n36.55\n0\nnr1\n9\n0.3333333\n9.333333\n\n\n346\n930\n36.69\n0\nnr1\n9\n0.5000000\n9.500000\n\n\n\n\n\nSo sieht der fertige Plot aus:\n\n# Lösung Aufgabe 2\n\nbeaver_new %>%\n  ggplot(aes(hour_min_dec, temp, color = beaver)) +\n  geom_point() +\n  scale_x_continuous(breaks = seq(0,23,2)) +\n  coord_polar() +\n  theme_minimal() +\n  theme(axis.title =  element_blank())"
  },
  {
    "objectID": "infovis/Infovis2_Uebung_B.html#aufgabe-3-raster-visualisierung-mit-flugpassagieren",
    "href": "infovis/Infovis2_Uebung_B.html#aufgabe-3-raster-visualisierung-mit-flugpassagieren",
    "title": "Infovis 2: Übung B",
    "section": "Aufgabe 3: Raster Visualisierung mit Flugpassagieren",
    "text": "Aufgabe 3: Raster Visualisierung mit Flugpassagieren\nAnalog Aufgabe 2, dieses Mal mit dem Datensatz AirPassanger\nAirPassengers kommt in einem Format daher, das ich selbst noch gar nicht kannte. Es sieht zwar aus wie ein data.frame oder eine matrix, ist aber von der Klasse ts.\n\nAirPassengers\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n1949 112 118 132 129 121 135 148 148 136 119 104 118\n1950 115 126 141 135 125 149 170 170 158 133 114 140\n1951 145 150 178 163 172 178 199 199 184 162 146 166\n1952 171 180 193 181 183 218 230 242 209 191 172 194\n1953 196 196 236 235 229 243 264 272 237 211 180 201\n1954 204 188 235 227 234 264 302 293 259 229 203 229\n1955 242 233 267 269 270 315 364 347 312 274 237 278\n1956 284 277 317 313 318 374 413 405 355 306 271 306\n1957 315 301 356 348 355 422 465 467 404 347 305 336\n1958 340 318 362 348 363 435 491 505 404 359 310 337\n1959 360 342 406 396 420 472 548 559 463 407 362 405\n1960 417 391 419 461 472 535 622 606 508 461 390 432\n\nclass(AirPassengers)\n\n[1] \"ts\"\n\n\nDamit wir den Datensatz verwenden können, müssen wir ihn zuerst in eine matrix umwandeln. Wie das geht habe ich hier erfahren.\n\nAirPassengers2 <- tapply(AirPassengers, list(year = floor(time(AirPassengers)), month = month.abb[cycle(AirPassengers)]), c)\n\nAus der matrix muss noch ein Dataframe her, zudem müssen wir aus der breiten Tabelle eine lange Tabelle machen:\n\nAirPassengers3 <- AirPassengers2 %>%\n  as.data.frame() %>%\n  tibble::rownames_to_column(\"year\") %>%\n  pivot_longer(-year, names_to = \"month\", values_to = \"n\") %>%\n  mutate(\n    # ich nutze einen billigen Trick um ausgeschriebene Monate in Nummern umzuwandeln\n    month = factor(month, levels = month.abb,ordered = T),\n    month_numb = as.integer(month),\n    year = as.integer(year)\n  )\n\nSo sieht der fertige Plot aus:"
  },
  {
    "objectID": "stat1-4/Statistik1_Demo.html",
    "href": "stat1-4/Statistik1_Demo.html",
    "title": "Stat1: Demo",
    "section": "",
    "text": "Demoskript als Download\n\nBinomialtest\nIn Klammern übergibt man die Anzahl der Erfolge und die Stichprobengrösse\n\nbinom.test(43, 100)\n\n\n    Exact binomial test\n\ndata:  43 and 100\nnumber of successes = 43, number of trials = 100, p-value = 0.1933\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.3313910 0.5328663\nsample estimates:\nprobability of success \n                  0.43 \n\nbinom.test(57, 100)\n\n\n    Exact binomial test\n\ndata:  57 and 100\nnumber of successes = 57, number of trials = 100, p-value = 0.1933\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.4671337 0.6686090\nsample estimates:\nprobability of success \n                  0.57 \n\n\n\n\nChi-Quadrat-Test & Fishers Test\nErmitteln des kritischen Wertes\n\nqchisq(0.95, 1)\n\n[1] 3.841459\n\n\nDirekter Test in R (dazu Werte als Matrix nötig)\n\ncount <- matrix(c(38, 14, 11, 51), nrow = 2)\ncount\n\n     [,1] [,2]\n[1,]   38   11\n[2,]   14   51\n\nchisq.test(count)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  count\nX-squared = 33.112, df = 1, p-value = 8.7e-09\n\nfisher.test(count)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  count\np-value = 2.099e-09\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n  4.746351 34.118920\nsample estimates:\nodds ratio \n  12.22697 \n\n\n\n\nt-Test\n\na <- c(20, 19, 25, 10, 8, 15, 13, 18, 11, 14)\nb <- c(12, 15, 16, 7, 8, 10, 12, 11, 13, 10)\nblume <- data.frame(a,b)\nblume\n\n    a  b\n1  20 12\n2  19 15\n3  25 16\n4  10  7\n5   8  8\n6  15 10\n7  13 12\n8  18 11\n9  11 13\n10 14 10\n\nsummary(blume)\n\n       a               b        \n Min.   : 8.00   Min.   : 7.00  \n 1st Qu.:11.50   1st Qu.:10.00  \n Median :14.50   Median :11.50  \n Mean   :15.30   Mean   :11.40  \n 3rd Qu.:18.75   3rd Qu.:12.75  \n Max.   :25.00   Max.   :16.00  \n\nboxplot(blume$a, blume$b)\n\n\n\nboxplot(blume)\n\n\n\nhist(blume$a)\n\n\n\nhist(blume$b)\n\n\n\n\nzweiseitiger t-Test\n\nt.test(blume$a, blume$b)\n\n\n    Welch Two Sample t-test\n\ndata:  blume$a and blume$b\nt = 2.0797, df = 13.907, p-value = 0.05654\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.1245926  7.9245926\nsample estimates:\nmean of x mean of y \n     15.3      11.4 \n\n\neinseitiger t-Test\n\nt.test(blume$a, blume$b, alternative = \"greater\") #einseitig\n\n\n    Welch Two Sample t-test\n\ndata:  blume$a and blume$b\nt = 2.0797, df = 13.907, p-value = 0.02827\nalternative hypothesis: true difference in means is greater than 0\n95 percent confidence interval:\n 0.5954947       Inf\nsample estimates:\nmean of x mean of y \n     15.3      11.4 \n\nt.test(blume$a, blume$b, alternative = \"less\") #einseitig\n\n\n    Welch Two Sample t-test\n\ndata:  blume$a and blume$b\nt = 2.0797, df = 13.907, p-value = 0.9717\nalternative hypothesis: true difference in means is less than 0\n95 percent confidence interval:\n     -Inf 7.204505\nsample estimates:\nmean of x mean of y \n     15.3      11.4 \n\n\nklassischer t-Test vs. Welch Test\n\n# Varianzen gleich, klassischer t-Test\nt.test(blume$a, blume$b, var.equal = T) \n\n\n    Two Sample t-test\n\ndata:  blume$a and blume$b\nt = 2.0797, df = 18, p-value = 0.05212\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.03981237  7.83981237\nsample estimates:\nmean of x mean of y \n     15.3      11.4 \n\n# Varianzen ungleich, Welch's t-Test, ist auch default, d.h. wenn var.equal \n# nicht  definiert wird, wird ein Welch's t-Test ausgeführt. \nt.test(blume$a, blume$b, var.equal = F) \n\n\n    Welch Two Sample t-test\n\ndata:  blume$a and blume$b\nt = 2.0797, df = 13.907, p-value = 0.05654\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.1245926  7.9245926\nsample estimates:\nmean of x mean of y \n     15.3      11.4 \n\n\ngepaarter t-Test\n\nt.test(blume$a, blume$b, paired = T)\n\n\n    Paired t-test\n\ndata:  blume$a and blume$b\nt = 3.4821, df = 9, p-value = 0.006916\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 1.366339 6.433661\nsample estimates:\nmean difference \n            3.9 \n\nt.test(blume$a, blume$b, paired = T, alternative = \"greater\")\n\n\n    Paired t-test\n\ndata:  blume$a and blume$b\nt = 3.4821, df = 9, p-value = 0.003458\nalternative hypothesis: true mean difference is greater than 0\n95 percent confidence interval:\n 1.846877      Inf\nsample estimates:\nmean difference \n            3.9 \n\n\nDas gleiche mit einem “long table”\n\ncultivar <- c(rep(\"a\", 10), rep(\"b\", 10))\nsize <- c(a, b)\nblume.long <- data.frame(cultivar, size)\n\nrm(size) #Befehl rm entfernt die nicht mehr benötitgten Objekte aus dem Workspace\nrm(cultivar)\n\nDas gleiche in einer Zeile\n\nblume.long <- data.frame(cultivar = c(rep(\"a\", 10), rep(\"b\", 10)), size = c(a, b))\nsummary(blume.long)             \n\n   cultivar              size      \n Length:20          Min.   : 7.00  \n Class :character   1st Qu.:10.00  \n Mode  :character   Median :12.50  \n                    Mean   :13.35  \n                    3rd Qu.:15.25  \n                    Max.   :25.00  \n\nhead(blume.long)\n\n  cultivar size\n1        a   20\n2        a   19\n3        a   25\n4        a   10\n5        a    8\n6        a   15\n\nboxplot(size~cultivar, data = blume.long)\n\n\n\nt.test(size~cultivar, blume.long, var.equal = T)\n\n\n    Two Sample t-test\n\ndata:  size by cultivar\nt = 2.0797, df = 18, p-value = 0.05212\nalternative hypothesis: true difference in means between group a and group b is not equal to 0\n95 percent confidence interval:\n -0.03981237  7.83981237\nsample estimates:\nmean in group a mean in group b \n           15.3            11.4 \n\n# gepaarter t-Test erster Wert von Cultivar a wird mit erstem Wert von Cultivar\n# b gepaart, zweiter Wert von a mit zweitem von b ect.\nt.test(size~cultivar, blume.long, paired = T)\n\n\n    Paired t-test\n\ndata:  size by cultivar\nt = 3.4821, df = 9, p-value = 0.006916\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 1.366339 6.433661\nsample estimates:\nmean difference \n            3.9 \n\n\n\n\nBase R vs. ggplot2\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n✔ tibble  3.1.8     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.1\n✔ readr   2.1.2     ✔ forcats 0.5.2\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nggplot(blume.long, aes(cultivar, size)) + geom_boxplot()\n\n\n\nggplot(blume.long, aes(cultivar, size)) + geom_boxplot() + theme_classic()\n\n\n\nggplot(blume.long, aes(cultivar, size)) + geom_boxplot(size = 1) + theme_classic()+\ntheme(axis.line = element_line(size = 1)) + theme(axis.title = element_text(size = 14))+\ntheme(axis.text = element_text(size = 14))\n\n\n\nggplot(blume.long, aes(cultivar, size)) + geom_boxplot(size=1) + theme_classic()+\n  theme(axis.line = element_line(size = 1), axis.ticks = element_line(size = 1), \n       axis.text = element_text(size = 20), axis.title = element_text(size = 20))\n\n\n\n\nDefinieren von mytheme mit allen gewünschten Settings, das man zu Beginn einer Sitzung einmal laden und dann immer wieder ausführen kann (statt des langen Codes)\n\nmytheme <- theme_classic() + \n  theme(axis.line = element_line(color = \"black\", size=1), \n        axis.text = element_text(size = 20, color = \"black\"), \n        axis.title = element_text(size = 20, color = \"black\"), \n        axis.ticks = element_line(size = 1, color = \"black\"), \n        axis.ticks.length = unit(.5, \"cm\"))\n\n\nggplot(blume.long, aes(cultivar, size)) + \n  geom_boxplot(size = 1) +\n  mytheme\n\n\n\nt_test <- t.test(size~cultivar, blume.long)\n\nggplot(blume.long, aes(cultivar, size)) + \n  geom_boxplot(size = 1) + \n  mytheme +\n  annotate(\"text\", x = \"b\", y = 24, \n  label = paste0(\"italic(p) == \", round(t_test$p.value, 3)), parse = TRUE, size = 8)\n\n\n\nggplot (blume.long, aes(cultivar,size)) + \n  geom_boxplot(size = 1) + \n  mytheme +\n  labs(x=\"Cultivar\",y=\"Size (cm)\")"
  },
  {
    "objectID": "stat1-4/Statistik1_Loesung.html#kommentierter-weg",
    "href": "stat1-4/Statistik1_Loesung.html#kommentierter-weg",
    "title": "Stat1: Loesung",
    "section": "kommentierter Weg",
    "text": "kommentierter Weg\nLade Daten von der Gästebefragung 2017 herunter (für Informationen zu den einzelnen Variablen, siehe diesen Link):\n\n2017_ZHAW_aggregated_menu_sales_NOVANIMAL.csv\n2019_ZHAW_vonRickenbach_cleaned_recorded_survey_dataset_NOVANIMAL_anonym.csv\n\n\nnova <- read_delim(file = here(\"data\",\"2017_ZHAW_aggregated_menu_sales_NOVANIMAL.csv\"), delim = \";\")\n\nRows: 26340 Columns: 27\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \";\"\nchr   (9): article_description, art_code, rab_descript, pay_description, sho...\ndbl  (16): transaction_id, qty_weight, total_amount, price_article, prop_pri...\ndttm  (1): trans_date\ndate  (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nnova_survey <- read_delim(file = here(\"data\",\"2019_ZHAW_vonRickenbach_cleaned_recorded_survey_dataset_NOVANIMAL_anonym.csv\"), delim = \";\")\n\nRows: 1175 Columns: 71\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \";\"\nchr  (13): Pseudonym, meal, gender, member, age_groups, Verpflegungstyp, pla...\ndbl  (56): choice_1, choice_2, choice_3, choice_4, choice_5, choice_6, choic...\ndate  (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n#überprüfe die Datenstruktur\nglimpse(nova_survey)\n\nRows: 1,175\nColumns: 71\n$ Pseudonym           <chr> \"2017_HS_F_0001\", \"2017_HS_F_0007\", \"2017_HS_F_000…\n$ meal                <chr> NA, \"Favorite\", \"Favorite\", \"Favorite\", \"Favorite\"…\n$ choice_1            <dbl> 4, 4, 3, 4, 3, 3, 4, 4, 3, 3, 3, 3, 4, 4, 4, 2, 3,…\n$ choice_2            <dbl> NA, 4, 1, 1, 3, 2, 1, 2, 2, 3, 4, 3, 4, 4, 4, 2, 1…\n$ choice_3            <dbl> NA, 4, 4, 4, 4, 4, 3, 4, 3, 3, 4, 3, 4, 4, 4, 2, 4…\n$ choice_4            <dbl> NA, 4, 2, 4, 3, 3, 4, 4, 3, 3, 4, 3, 4, 4, 2, 4, 4…\n$ choice_5            <dbl> NA, 2, 2, 4, 2, 2, 3, 3, 2, 2, 4, 2, 2, 4, 3, 2, 2…\n$ choice_6            <dbl> NA, 4, 4, 4, 3, 4, 2, 3, 2, 4, 1, 2, 4, 4, 3, 2, 4…\n$ choice_7            <dbl> NA, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1…\n$ choice_8            <dbl> NA, NA, 2, 4, 2, 1, 1, 1, 1, 3, 1, 1, 1, 4, 2, 2, …\n$ choice_9            <dbl> NA, NA, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 4, 1, 4, …\n$ choice_10           <dbl> 4, NA, 3, 3, 3, 3, 2, 4, 4, 3, 4, 3, 4, 4, 4, 2, 2…\n$ satis_1             <dbl> NA, 3, 3, 3, 4, 3, 3, 4, 4, 3, 4, 4, 4, 4, 4, 4, 4…\n$ satis_2             <dbl> 3, 3, 3, 3, 4, 3, 3, 4, 4, 3, 4, 3, 4, 4, 4, 3, 3,…\n$ satis_3             <dbl> NA, 2, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 3…\n$ ing_1               <dbl> 4, 4, 1, 1, 3, 2, 1, 3, 1, 3, 2, 3, 1, 4, 1, 2, 2,…\n$ ing_2               <dbl> 3, 2, 1, 1, 4, 2, 1, 2, 1, 2, 1, 2, 1, 4, 4, 1, 1,…\n$ ing_3               <dbl> 3, 2, 1, 1, -99, -99, 1, 2, -99, 2, -99, -99, 1, 2…\n$ ing_4               <dbl> 4, 3, 4, 3, 3, -99, 4, 4, -99, 3, NA, -99, 3, 4, 1…\n$ att_1               <dbl> 4, 3, 3, 2, 2, 1, 2, 3, 4, 2, 1, 1, 2, 4, 2, 3, 3,…\n$ att_2               <dbl> 4, 4, 4, 3, 3, 2, 3, 4, 4, 4, 4, 3, 4, 4, 4, 3, 3,…\n$ att_3               <dbl> 2, 3, 3, 2, 2, 2, 2, 2, 3, 2, 1, 1, 2, 4, 2, 2, 3,…\n$ att_4               <dbl> 4, 3, 4, 3, 3, 2, 3, 2, 4, 2, NA, 2, 2, 4, 1, 3, 4…\n$ att_5               <dbl> 4, 4, 4, 3, 3, 2, 3, 3, 4, 2, NA, 2, 3, 4, 1, 3, 3…\n$ att_6               <dbl> 1, 2, 4, 2, 1, 1, 2, 1, 3, 2, 1, 1, 1, 1, 1, NA, 4…\n$ att_7               <dbl> 1, 2, 1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, NA, 4…\n$ att_8               <dbl> 4, 4, 4, 3, 3, 3, 3, 4, 3, 3, 3, 2, 4, 4, 1, 3, 4,…\n$ att_9               <dbl> 3, 4, -99, 3, 4, 2, 4, 4, 4, 4, 4, -99, 3, 4, 1, 4…\n$ att_10              <dbl> 4, 2, -99, 3, 3, -99, 4, 4, 4, 2, -99, -99, 3, 4, …\n$ att_11              <dbl> 4, 3, NA, 2, 4, 3, 2, 4, 3, 4, 4, 3, 4, 2, 4, 3, 4…\n$ diet                <dbl> 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ allerg              <dbl> 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,…\n$ relig               <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,…\n$ meds                <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ tho_1               <dbl> 4, 4, 3, 4, 3, 3, 3, 4, 3, 4, 3, 2, 3, 4, 2, 3, 3,…\n$ tho_2               <dbl> 4, 3, 4, 4, 4, 3, 4, 4, 4, 4, 4, 3, 3, 4, 1, 4, 4,…\n$ tho_3               <dbl> 4, 3, 4, 4, 4, 2, 4, 3, 4, 4, 4, 3, 2, 4, 1, 4, 4,…\n$ tho_4               <dbl> 3, 3, 4, 4, 3, 3, 4, 3, 4, 4, 3, 3, 3, 4, 1, 4, 4,…\n$ tho_5               <dbl> 3, 3, 2, 2, 4, 2, 2, 4, 3, 3, 3, 2, 4, 4, 4, 2, 3,…\n$ tra_1               <dbl> 4, 4, 3, 4, 3, 3, 4, 4, 4, 4, 3, 3, 4, 4, 3, 3, 3,…\n$ tra_2               <dbl> 3, 3, 4, 4, 3, 3, 4, 4, 4, 4, 4, 3, 3, 4, 1, 4, 4,…\n$ tra_3               <dbl> 3, 3, 3, 4, NA, 2, 4, 3, 4, 4, 3, 3, 3, 4, 1, 4, 4…\n$ tra_4               <dbl> 4, NA, 4, 4, 4, 2, 4, 3, 4, 4, 3, 3, 3, 4, 1, 4, 4…\n$ tra_5               <dbl> 4, 4, 4, 4, 4, 3, 4, 4, 4, 4, 4, 3, 3, 4, 2, 4, 4,…\n$ meat                <dbl> 6, 6, 4, 4, 4, 4, 5, 5, 5, 3, 3, 3, 5, 5, 4, 3, 1,…\n$ milk                <dbl> 5, 6, 6, 5, 3, 3, 6, 5, 6, 5, 6, 6, 6, 5, 6, 4, 3,…\n$ veget               <dbl> 2, 3, 4, 4, 2, 3, 3, 3, 4, 5, 1, 4, 2, 1, 1, 5, 6,…\n$ veg                 <dbl> 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 1, 1, 2, 1, 1, 3, 4,…\n$ pensum              <dbl> 5, NA, 4, 2, 5, 5, 3, 5, 2, 4, 5, 4, 4, 4, 5, 3, 1…\n$ cant                <dbl> 3, 1, 4, 3, 5, 4, 3, 4, 5, 2, 3, 3, 3, 4, 3, 5, 2,…\n$ home                <dbl> 4, 1, 2, 3, 2, 1, 1, 3, 1, 4, 3, 4, 3, 1, 4, 1, 3,…\n$ other               <dbl> 1, 1, 1, 2, 1, 3, 1, 1, 1, 1, 3, 1, 3, 1, 1, 2, 1,…\n$ gender              <chr> \"Mann\", \"Mann\", \"Mann\", \"Mann\", \"Frau\", \"Mann\", \"M…\n$ member              <chr> \"Student/in\", \"Andere\", \"Mitarbeiter/in\", \"Student…\n$ fill                <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ age_groups          <chr> \"17- bis 25-jaehrig\", \"26- bis 34-jaehrig\", \"26- b…\n$ mensa               <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ sumNA               <dbl> 11, 8, 1, 0, 1, 0, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1…\n$ perNA               <dbl> 2.115385e+14, 1.538462e+14, 1.923077e+14, 0.000000…\n$ Verpflegungstyp     <chr> \"Selbstverpfleger\", NA, \"Mensagaenger\", \"Abwechsle…\n$ date                <date> 2017-10-17, 2017-10-17, 2017-10-17, 2017-10-17, 2…\n$ place               <chr> \"Gr\\xfcental\", \"Gr\\xfcental\", \"Gr\\xfcental\", \"Gr\\x…\n$ intervention        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ label_content       <chr> NA, \"Fleisch/Fisch\", \"Fleisch/Fisch\", \"Fleisch/Fis…\n$ meal_name           <chr> NA, \"Kalbsbratwurst\", \"Kalbsbratwurst\", \"Kalbsbrat…\n$ choice_add_d        <dbl> 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,…\n$ choice_add_v        <chr> NA, NA, \"Vegi-Men\\xfc\\x81 \\x81berzeugte nicht\", NA…\n$ code_choice         <chr> NA, NA, \"Unzufrieden mit Auswahl\", NA, NA, NA, NA,…\n$ Bemerkungen_d       <dbl> 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,…\n$ Bemerkungen_v       <chr> NA, NA, NA, NA, \"Sch\\xf6n w\\xe4ren mehr regionale …\n$ code_zu_Bemerkungen <chr> NA, NA, NA, NA, \"regional\", NA, NA, NA, NA, NA, \"A…\n\n## definiert mytheme für ggplot2 (verwendet dabei theme_classic())\n\nmytheme <- \n  theme_classic() + \n  theme(\n    axis.line = element_line(color = \"black\"), \n    axis.text = element_text(size = 12, color = \"black\"), \n    axis.title = element_text(size = 12, color = \"black\"), \n    axis.ticks = element_line(size = .75, color = \"black\"), \n    axis.ticks.length = unit(.5, \"cm\")\n    )\n\n\n# Als eine Möglichkeit, die Aufgabe 1.1 zu bearbeiten, nehmen wir hier den \n# Datensatz  der Gästebefragung NOVANIMAL und gehen der folgenden Frage nach: \n# Gibt es einen Zusammenhang zwischen Geschlecht und dem wahrgenommenen \n# Milchkonsum (viel vs. wenig Milch/-produkte)\n\n# die Variable wahrgenommener Milchkonsum muss \n# noch in 2 Kategorien zusammengefasst werden: geringer vs. hoher Milchkonsum\n\n# Variable  milk == wahrgenommener Milchkonsum \n# alles kleiner als 4 (3 inklusive) == geringer wahrgenommener Milchkonsum, \n#alles grösser als 3 (4 inklusive) == hoher wahrgenommener Milchkonsum\nnova2 <- nova_survey %>% \n  filter(gender != \"x\") %>% # x aus der Variable Geschlecht entfernen \n  mutate(milkcon = if_else(milk <= 3, \"wenig\", \"viel\")) %>% \n  select(gender, milkcon) %>% \n  drop_na() # alle Missings können gestrichen werden\n \n# mal anschauen\ntable(nova2)\n\n      milkcon\ngender viel wenig\n  Frau  428    64\n  Mann  580    68\n\n#achtung chi_squre erwartet matrix\nnova_mtx <- xtabs(~ gender + milkcon ,data = nova2) \n# da es in diesem fall keine kriteriumsvariable gibt, fehlt das y sozusagen\n\n#Chi-squared Test\nchi_sq <- chisq.test(nova_mtx)\nchi_sq\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  nova_mtx\nX-squared = 1.49, df = 1, p-value = 0.2222\n\n#visualisierung\nOP <- par(mfrow=c(1,2), \"mar\"=c(1,1,3,1))\nmosaicplot(chi_sq$observed, cex.axis =1 , main = \"Observed counts\")\nmosaicplot(chi_sq$expected, cex.axis =1 , main = \"Expected counts\\n(wenn geschlecht keinen einfluss hat)\")\n\n\n\npar(OP)\n\n#Fisher's Test nur mit 2X2 Kontingenztabelle möglich\nfisher.test(nova_mtx)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  nova_mtx\np-value = 0.1922\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 0.536215 1.147939\nsample estimates:\nodds ratio \n 0.7842283"
  },
  {
    "objectID": "stat1-4/Statistik1_Loesung.html#ergebnisse",
    "href": "stat1-4/Statistik1_Loesung.html#ergebnisse",
    "title": "Stat1: Loesung",
    "section": "Ergebnisse",
    "text": "Ergebnisse\nDer \\(\\chi^2\\)-Test sagt uns, dass das Geschlecht und der wahrgenommene Milchkonsum nicht zusammenhängen. Es gibt keine signifikante Unterscheide zwischen dem Geschlecht und dem wahrgenommenen Milchkonsum (\\(\\chi^2\\)(1) = 1.49, p = 0.222. Es sieht so aus, dass Männer leicht mehr angeben weniger Milch zu konsumieren (Tabelle 1). Die Ergebnisse müssen jedoch mit Vorsicht interpretiert werden, denn der \\(\\chi^2\\)-Test gibt uns nur an, dass ein signifikanter Unterschied zwischen Geschlecht und wahrgenommener Milchkonsum vorliegt. Um die Unterschiede innerhalb einer Gruppen (z.B. Geschlecht nach Alter) festzustellen bedarf es weiterer Analysen z. B. mit einer mehrfaktorieller ANOVA mit anschliessenden Post-hoc Tests (siehe Statistik 3).\n\n\n`summarise()` has grouped output by 'gender'. You can override using the\n`.groups` argument.\n\n\n\nWahrgenommener Milchkonsum nach Geschlecht\n\n\nGeschlecht\nwahr. Milchkonsum\nabsolute Werte\nwahr. Milchkonsum (%)\n\n\n\n\nFrau\nviel\n428\n87.0\n\n\nFrau\nwenig\n64\n13.0\n\n\nMann\nviel\n580\n89.5\n\n\nMann\nwenig\n68\n10.5"
  },
  {
    "objectID": "stat1-4/Statistik1_Loesung.html#methoden",
    "href": "stat1-4/Statistik1_Loesung.html#methoden",
    "title": "Stat1: Loesung",
    "section": "Methoden",
    "text": "Methoden\nZiel war es die aggregierten Verkaufszahlen zwischen den Interventions- und Basiswochen zu vergleichen. Die Annahme ist, dass die wöchentlichen Verkaufszahlen unabhängig sind. Daher können die Unterschiede zwischen den Verkaufszahlen pro Woche zwischen den beiden Bedingungen mittels t-Test geprüft werden. Obwohl die visuelle Inspektion keine schwerwiegenden Verletzungen der Modelvoraussetzung zeigte, wurde einen Welch t-Test gerechnet. Zudem muss gesagt werden, dass die Gruppengrösse hier jeweils mit n = 6 (Anzahl Wochen) eher klein ist. T-test liefern dennoch relativ reliable Resultate. Für mehr Infos dazu hier eine Studie."
  },
  {
    "objectID": "stat1-4/Statistik1_Loesung.html#ergebnisse-1",
    "href": "stat1-4/Statistik1_Loesung.html#ergebnisse-1",
    "title": "Stat1: Loesung",
    "section": "Ergebnisse",
    "text": "Ergebnisse\nIn den Basiswochen werden mehr Gerichte pro Woche verkauft als in den Interventionsowochen (siehe Abbildung 1). Die wöchentlichen Verkaufszahlen zwischen den Bedigungen (Basis oder Intervention) unterscheiden sich gemäss Welch t-Test jedoch nicht signifikant (t(10) = 0.272 , p = 0.791). Die Ergebnisse könnten mit einem \\(\\chi^2\\)-Test nochmals validiert werden, da die Gruppengrösse mit n = 6 doch eher klein ist.\n\n\n\n\n\nDie wöchentlichen Verkaufszahlen für die Interventions- und Basiswochen unterscheiden sich nicht signifikant."
  },
  {
    "objectID": "stat1-4/Statistik1_Novanimal.html",
    "href": "stat1-4/Statistik1_Novanimal.html",
    "title": "Stat1: NOVANIMAL",
    "section": "",
    "text": "Dazu wurde u.a. ein Experiment in zwei Hochschulmensen durchgeführt. Forschungsleitend war die Frage, wie die Gäste dazu bewogen werden können, häufiger vegetarische oder vegane Gerichte zu wählen. Konkret wurde untersucht, wie die Gäste auf ein verändertes Menü-Angebot mit einem höheren Anteil an vegetarischen und veganen Gerichten reagieren. Das Experiment fand während 12 Wochen statt und bestand aus zwei Mensazyklen à 6 Wochen. Über den gesamten Untersuchungszeitraum werden insgesamt 90 verschiedene Gerichte angeboten. In den 6 Referenz- bzw. Basiswochen wurden zwei fleisch- oder fischhaltige Menüs und ein vegetarisches Menü angeboten. In den 6 Interventionswochen wurde das Verhältnis umgekehrt und es wurden ein veganes, ein vegetarisches und ein fleisch- oder fischhaltiges Gericht angeboten. Basis- und Interventionsangebote wechselten wöchentlich ab. Während der gesamten 12 Wochen konnten die Gäste jeweils auf ein Buffet ausweichen und ihre Mahlzeit aus warmen und kalten Komponenten selber zusammenstellen. Die Gerichte wurden über drei vorgegebene Menülinien (F, K, W) randomisiert angeboten.\n\n\n\nDie Abbildung zeigt das Versuchsdesign der ersten 6 Experimentwochen (Kalenderwoche 40 bis 45).\n\n\nMehr Informationen über das Forschungsprojekt NOVANIMAL findet ihr auf der Webpage"
  },
  {
    "objectID": "stat1-4/Statistik1_Uebung.html",
    "href": "stat1-4/Statistik1_Uebung.html",
    "title": "Stat1: Übung",
    "section": "",
    "text": "Aufgabe 1.1: Assoziationstest\nBitte führt einen Assoziationstest zweier kategorialer Variablen (mit je zwei Ausprägungen) mit Chi-Quadrat und Fishers exaktem Test durch. Ihr habt zwei Möglichkeiten: (1) Ihr erhebt dazu selbst die Daten (wozu ihr euch auch in Teams zusammenschliessen könnt). Dabei könnt ihr sowohl Befragungen/Datenerhebung unter Mitstudierenden durchführen (etwa Nutzung Mac/Windows vs. männlich/weiblich) oder Daten zu anderen Objekten erheben. (2) Ihr nehmt zwei kategoriale Variablen aus einem der Novanimal-Datensätze (Feldexperiment oder Gästebefragung).  Bitte formuliert in beiden Fällen vor der Datenerhebung/Datenextraktion eine Hypothese, d.h. eine Erwartungshaltung, ob und welche Assoziation vorliegt und wenn ja warum. Bitte beachtet, dass ihr für die Form des Assoziationstests aus dem Kurs zwei binäre Variablen benötigt; wenn ihr also kategoriale Variablen mit mehr als zwei Ausprägungen habt, könnt ihr entweder Ausprägungen sinnvoll zusammenfassen oder seltene Ausprägungen im Test unberücksichtigt lassen.\n\n\nAufgabe 1.2: t-Test\nWerden in den Basis- und Interventionswochen unterschiedlich viele Gerichte verkauft?\n\nDefiniere die Null- (\\(H_0\\)) und die Alternativhypothese (\\(H_1\\)).\nFühre einen t-Test durch.\nWelche Form von t-Test musst Du anwenden: einseitig/zweiseitig resp. gepaart/ungepaart?\nWie gut sind die Voraussetzungen für einen t-Test erfüllt (z.B. Normalverteilung der Residuen und Varianzhomogenität)?\nStelle deine Ergebnisse angemessen dar, d.h. Text mit Abbildung und/oder Tabelle"
  },
  {
    "objectID": "stat1-4/Statistik2_Demo.html",
    "href": "stat1-4/Statistik2_Demo.html",
    "title": "Stat2: Demo",
    "section": "",
    "text": "Demoscript als Download"
  },
  {
    "objectID": "stat1-4/Statistik2_Demo.html#t-test-als-anova",
    "href": "stat1-4/Statistik2_Demo.html#t-test-als-anova",
    "title": "Stat2: Demo",
    "section": "t-test als ANOVA",
    "text": "t-test als ANOVA\n\na <- c(20, 19, 25, 10, 8, 15, 13 ,18, 11, 14)\nb <- c(12, 15, 16, 7, 8, 10, 12, 11, 13, 10)\n\nblume <- data.frame(cultivar = c(rep(\"a\", 10), rep(\"b\" , 10)), size = c(a, b))\n\npar(mfrow=c(1,1))\nboxplot(size~cultivar, xlab = \"Sorte\", ylab = \"Bluetengroesse [cm]\", data = blume)\n\n\n\nt.test(size~cultivar, blume, var.equal = T)\n\n\n    Two Sample t-test\n\ndata:  size by cultivar\nt = 2.0797, df = 18, p-value = 0.05212\nalternative hypothesis: true difference in means between group a and group b is not equal to 0\n95 percent confidence interval:\n -0.03981237  7.83981237\nsample estimates:\nmean in group a mean in group b \n           15.3            11.4 \n\naov(size~cultivar, data = blume)\n\nCall:\n   aov(formula = size ~ cultivar, data = blume)\n\nTerms:\n                cultivar Residuals\nSum of Squares     76.05    316.50\nDeg. of Freedom        1        18\n\nResidual standard error: 4.193249\nEstimated effects may be unbalanced\n\nsummary(aov(size~cultivar, data = blume))\n\n            Df Sum Sq Mean Sq F value Pr(>F)  \ncultivar     1   76.0   76.05   4.325 0.0521 .\nResiduals   18  316.5   17.58                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary.lm(aov(size~cultivar, data = blume))\n\n\nCall:\naov(formula = size ~ cultivar, data = blume)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-7.300 -2.575 -0.350  2.925  9.700 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   15.300      1.326   11.54 9.47e-10 ***\ncultivarb     -3.900      1.875   -2.08   0.0521 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.193 on 18 degrees of freedom\nMultiple R-squared:  0.1937,    Adjusted R-squared:  0.1489 \nF-statistic: 4.325 on 1 and 18 DF,  p-value: 0.05212"
  },
  {
    "objectID": "stat1-4/Statistik2_Demo.html#echte-anova",
    "href": "stat1-4/Statistik2_Demo.html#echte-anova",
    "title": "Stat2: Demo",
    "section": "Echte ANOVA",
    "text": "Echte ANOVA\n\nc <- c(30, 19, 31, 23, 18, 25, 26, 24, 17, 20)\n\nblume2 <- data.frame(cultivar = c(rep(\"a\", 10), rep(\"b\", 10), rep(\"c\", 10)), size = c(a, b, c))\nblume2$cultivar <- as.factor(blume2$cultivar)\n\nsummary(blume2)             \n\n cultivar      size      \n a:10     Min.   : 7.00  \n b:10     1st Qu.:11.25  \n c:10     Median :15.50  \n          Mean   :16.67  \n          3rd Qu.:20.00  \n          Max.   :31.00  \n\nhead(blume2)\n\n  cultivar size\n1        a   20\n2        a   19\n3        a   25\n4        a   10\n5        a    8\n6        a   15\n\npar(mfrow=c(1,1))\nboxplot(size~cultivar, xlab = \"Sorte\", ylab = \"Blütengrösse [cm]\", data = blume2)\n\n\n\naov(size~cultivar, data = blume2)\n\nCall:\n   aov(formula = size ~ cultivar, data = blume2)\n\nTerms:\n                cultivar Residuals\nSum of Squares  736.0667  528.6000\nDeg. of Freedom        2        27\n\nResidual standard error: 4.424678\nEstimated effects may be unbalanced\n\nsummary(aov(size~cultivar, data = blume2))\n\n            Df Sum Sq Mean Sq F value   Pr(>F)    \ncultivar     2  736.1   368.0    18.8 7.68e-06 ***\nResiduals   27  528.6    19.6                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary.lm(aov(size~cultivar, data=blume2))\n\n\nCall:\naov(formula = size ~ cultivar, data = blume2)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-7.300 -3.375 -0.300  2.700  9.700 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   15.300      1.399  10.935 2.02e-11 ***\ncultivarb     -3.900      1.979  -1.971 0.059065 .  \ncultivarc      8.000      1.979   4.043 0.000395 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.425 on 27 degrees of freedom\nMultiple R-squared:  0.582, Adjusted R-squared:  0.5511 \nF-statistic:  18.8 on 2 and 27 DF,  p-value: 7.683e-06\n\naov.1 <- aov(size~cultivar, data = blume2)\nsummary(aov.1)\n\n            Df Sum Sq Mean Sq F value   Pr(>F)    \ncultivar     2  736.1   368.0    18.8 7.68e-06 ***\nResiduals   27  528.6    19.6                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary.lm(aov.1)\n\n\nCall:\naov(formula = size ~ cultivar, data = blume2)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-7.300 -3.375 -0.300  2.700  9.700 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   15.300      1.399  10.935 2.02e-11 ***\ncultivarb     -3.900      1.979  -1.971 0.059065 .  \ncultivarc      8.000      1.979   4.043 0.000395 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.425 on 27 degrees of freedom\nMultiple R-squared:  0.582, Adjusted R-squared:  0.5511 \nF-statistic:  18.8 on 2 and 27 DF,  p-value: 7.683e-06\n\n#Berechnung Mittelwerte usw. zur Charakterisierung der Gruppen\naggregate(size~cultivar, blume2, function(x) c(Mean = mean(x), SD = sd(x), Min = min(x), Max = max(x)))\n\n  cultivar size.Mean   size.SD  size.Min  size.Max\n1        a 15.300000  5.207900  8.000000 25.000000\n2        b 11.400000  2.836273  7.000000 16.000000\n3        c 23.300000  4.854551 17.000000 31.000000\n\nlm.1 <- lm(size~cultivar, data = blume2)\nsummary(lm.1)\n\n\nCall:\nlm(formula = size ~ cultivar, data = blume2)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-7.300 -3.375 -0.300  2.700  9.700 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   15.300      1.399  10.935 2.02e-11 ***\ncultivarb     -3.900      1.979  -1.971 0.059065 .  \ncultivarc      8.000      1.979   4.043 0.000395 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.425 on 27 degrees of freedom\nMultiple R-squared:  0.582, Adjusted R-squared:  0.5511 \nF-statistic:  18.8 on 2 and 27 DF,  p-value: 7.683e-06"
  },
  {
    "objectID": "stat1-4/Statistik2_Demo.html#tukeys-posthoc-test",
    "href": "stat1-4/Statistik2_Demo.html#tukeys-posthoc-test",
    "title": "Stat2: Demo",
    "section": "Tukeys Posthoc-Test",
    "text": "Tukeys Posthoc-Test\n\nif(!require(agricolae)){install.packages(\"agricolae\")}\n\nLoading required package: agricolae\n\nlibrary(agricolae)\n\nHSD.test(aov.1, \"cultivar\", group = FALSE, console = T)\n\n\nStudy: aov.1 ~ \"cultivar\"\n\nHSD Test for size \n\nMean Square Error:  19.57778 \n\ncultivar,  means\n\n  size      std  r Min Max\na 15.3 5.207900 10   8  25\nb 11.4 2.836273 10   7  16\nc 23.3 4.854551 10  17  31\n\nAlpha: 0.05 ; DF Error: 27 \nCritical Value of Studentized Range: 3.506426 \n\nComparison between treatments means\n\n      difference pvalue signif.        LCL       UCL\na - b        3.9 0.1388          -1.006213  8.806213\na - c       -8.0 0.0011      ** -12.906213 -3.093787\nb - c      -11.9 0.0000     *** -16.806213 -6.993787"
  },
  {
    "objectID": "stat1-4/Statistik2_Demo.html#beispiel-posthoc-labels-in-plot",
    "href": "stat1-4/Statistik2_Demo.html#beispiel-posthoc-labels-in-plot",
    "title": "Stat2: Demo",
    "section": "Beispiel Posthoc-Labels in Plot",
    "text": "Beispiel Posthoc-Labels in Plot\n\naov.2 <- aov(Sepal.Width ~ Species, data = iris)\nHSD.test(aov.2, \"Species\", console = T)\n\n\nStudy: aov.2 ~ \"Species\"\n\nHSD Test for Sepal.Width \n\nMean Square Error:  0.1153878 \n\nSpecies,  means\n\n           Sepal.Width       std  r Min Max\nsetosa           3.428 0.3790644 50 2.3 4.4\nversicolor       2.770 0.3137983 50 2.0 3.4\nvirginica        2.974 0.3224966 50 2.2 3.8\n\nAlpha: 0.05 ; DF Error: 147 \nCritical Value of Studentized Range: 3.348424 \n\nMinimun Significant Difference: 0.1608553 \n\nTreatments with the same letter are not significantly different.\n\n           Sepal.Width groups\nsetosa           3.428      a\nvirginica        2.974      b\nversicolor       2.770      c\n\nboxplot(Sepal.Width ~ Species, data = iris)\n\n\n\nboxplot(Sepal.Width ~ Species, ylim = c(2, 5), data = iris)\ntext(1, 4.8, \"a\")\ntext(2, 4.8, \"c\")\ntext(3, 4.8, \"b\")\n\n\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n✔ tibble  3.1.8     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.1\n✔ readr   2.1.2     ✔ forcats 0.5.2\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nggplot(iris, aes(Species, Sepal.Width)) + geom_boxplot(size = 1) +\n  annotate(\"text\", y = 5, x = 1:3, label = c(\"a\", \"c\", \"b\"))"
  },
  {
    "objectID": "stat1-4/Statistik2_Demo.html#klassische-tests-der-modellannahmen-nicht-empfohlen",
    "href": "stat1-4/Statistik2_Demo.html#klassische-tests-der-modellannahmen-nicht-empfohlen",
    "title": "Stat2: Demo",
    "section": "Klassische Tests der Modellannahmen (NICHT EMPFOHLEN!!!)",
    "text": "Klassische Tests der Modellannahmen (NICHT EMPFOHLEN!!!)\n\nshapiro.test(blume2$size[blume2$cultivar == \"a\"])\n\n\n    Shapiro-Wilk normality test\n\ndata:  blume2$size[blume2$cultivar == \"a\"]\nW = 0.97304, p-value = 0.9175\n\nvar.test(blume2$size[blume2$cultivar == \"a\"], blume2$size[blume2$cultivar == \"b\"])\n\n\n    F test to compare two variances\n\ndata:  blume2$size[blume2$cultivar == \"a\"] and blume2$size[blume2$cultivar == \"b\"]\nF = 3.3715, num df = 9, denom df = 9, p-value = 0.08467\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n  0.8374446 13.5738284\nsample estimates:\nratio of variances \n          3.371547 \n\nif(!require(car)){install.packages(\"car\")}\n\nLoading required package: car\n\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nlibrary(car)\nleveneTest(blume2$size[blume2$cultivar == \"a\"], blume2$size[blume2$cultivar == \"b\"], center=mean)\n\nWarning in leveneTest.default(blume2$size[blume2$cultivar == \"a\"],\nblume2$size[blume2$cultivar == : blume2$size[blume2$cultivar == \"b\"] coerced to\nfactor.\n\n\nWarning in anova.lm(lm(resp ~ group)): ANOVA F-tests on an essentially perfect\nfit are unreliable\n\n\nLevene's Test for Homogeneity of Variance (center = mean)\n      Df    F value    Pr(>F)    \ngroup  7 3.0148e+30 < 2.2e-16 ***\n       2                         \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nwilcox.test(blume2$size[blume2$cultivar == \"a\"], blume2$size[blume2$cultivar == \"b\"])\n\nWarning in wilcox.test.default(blume2$size[blume2$cultivar == \"a\"],\nblume2$size[blume2$cultivar == : cannot compute exact p-value with ties\n\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  blume2$size[blume2$cultivar == \"a\"] and blume2$size[blume2$cultivar == \"b\"]\nW = 73, p-value = 0.08789\nalternative hypothesis: true location shift is not equal to 0"
  },
  {
    "objectID": "stat1-4/Statistik2_Demo.html#nicht-parametrische-alternativen-wenn-modellannahmen-der-anvoa-massiv-verletzt-sind",
    "href": "stat1-4/Statistik2_Demo.html#nicht-parametrische-alternativen-wenn-modellannahmen-der-anvoa-massiv-verletzt-sind",
    "title": "Stat2: Demo",
    "section": "Nicht-parametrische Alternativen, wenn Modellannahmen der ANVOA massiv verletzt sind",
    "text": "Nicht-parametrische Alternativen, wenn Modellannahmen der ANVOA massiv verletzt sind"
  },
  {
    "objectID": "stat1-4/Statistik2_Demo.html#zum-vergleich-normale-anova-noch-mal",
    "href": "stat1-4/Statistik2_Demo.html#zum-vergleich-normale-anova-noch-mal",
    "title": "Stat2: Demo",
    "section": "Zum Vergleich normale ANOVA noch mal",
    "text": "Zum Vergleich normale ANOVA noch mal\n\nsummary(aov(size~cultivar, data = blume2))\n\n            Df Sum Sq Mean Sq F value   Pr(>F)    \ncultivar     2  736.1   368.0    18.8 7.68e-06 ***\nResiduals   27  528.6    19.6                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "stat1-4/Statistik2_Demo.html#bei-starken-abweichungen-von-der-normalverteilung-aber-ähnlichen-varianzen",
    "href": "stat1-4/Statistik2_Demo.html#bei-starken-abweichungen-von-der-normalverteilung-aber-ähnlichen-varianzen",
    "title": "Stat2: Demo",
    "section": "Bei starken Abweichungen von der Normalverteilung, aber ähnlichen Varianzen",
    "text": "Bei starken Abweichungen von der Normalverteilung, aber ähnlichen Varianzen"
  },
  {
    "objectID": "stat1-4/Statistik2_Demo.html#kruskal-wallis-test",
    "href": "stat1-4/Statistik2_Demo.html#kruskal-wallis-test",
    "title": "Stat2: Demo",
    "section": "Kruskal-Wallis-Test",
    "text": "Kruskal-Wallis-Test\n\nkruskal.test(size~cultivar, data = blume2)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  size by cultivar\nKruskal-Wallis chi-squared = 16.686, df = 2, p-value = 0.0002381\n\nif(!require(FSA)){install.packages(\"FSA\")} \n\nLoading required package: FSA\n\n\nRegistered S3 methods overwritten by 'FSA':\n  method       from\n  confint.boot car \n  hist.boot    car \n\n\n## FSA v0.9.3. See citation('FSA') if used in publication.\n## Run fishR() for related website and fishR('IFAR') for related book.\n\n\n\nAttaching package: 'FSA'\n\n\nThe following object is masked from 'package:car':\n\n    bootCase\n\nlibrary(FSA)\n#korrigierte p-Werte nach Bejamini-Hochberg\ndunnTest(size~cultivar, method = \"bh\", data = blume2) \n\nDunn (1964) Kruskal-Wallis multiple comparison\n\n\n  p-values adjusted with the Benjamini-Hochberg method.\n\n\n  Comparison         Z      P.unadj        P.adj\n1      a - b  1.526210 1.269575e-01 0.1269575490\n2      a - c -2.518247 1.179407e-02 0.0176911039\n3      b - c -4.044457 5.244459e-05 0.0001573338"
  },
  {
    "objectID": "stat1-4/Statistik2_Demo.html#bei-erheblicher-heteroskedastizität-aber-relative-normalsymmetrisch-verteilten-residuen",
    "href": "stat1-4/Statistik2_Demo.html#bei-erheblicher-heteroskedastizität-aber-relative-normalsymmetrisch-verteilten-residuen",
    "title": "Stat2: Demo",
    "section": "Bei erheblicher Heteroskedastizität, aber relative normal/symmetrisch verteilten Residuen",
    "text": "Bei erheblicher Heteroskedastizität, aber relative normal/symmetrisch verteilten Residuen"
  },
  {
    "objectID": "stat1-4/Statistik2_Demo.html#welch-test",
    "href": "stat1-4/Statistik2_Demo.html#welch-test",
    "title": "Stat2: Demo",
    "section": "Welch-Test",
    "text": "Welch-Test\n\noneway.test(size~cultivar, var.equal = F, data = blume2)\n\n\n    One-way analysis of means (not assuming equal variances)\n\ndata:  size and cultivar\nF = 21.642, num df = 2.000, denom df = 16.564, p-value = 2.397e-05"
  },
  {
    "objectID": "stat1-4/Statistik2_Demo.html#faktorielle-anova",
    "href": "stat1-4/Statistik2_Demo.html#faktorielle-anova",
    "title": "Stat2: Demo",
    "section": "2-faktorielle ANOVA",
    "text": "2-faktorielle ANOVA\n\nd <- c(10, 12, 11, 13, 10, 25, 12, 30, 26, 13)\ne <- c(15, 13, 18, 11, 14, 25, 39, 38, 28, 24)\nf <- c(10, 12, 11, 13, 10, 9, 2, 4, 7, 13)\n\nblume3 <- data.frame(cultivar=c(rep(\"a\", 20), rep(\"b\", 20), rep(\"c\", 20)),\n                   house = c(rep(c(rep(\"yes\", 10), rep(\"no\", 10)), 3)),\n                  size = c(a, b, c, d, e, f))\n\n\nblume3\n\n\nboxplot(size~cultivar + house, data = blume3)\n\n\n\nsummary(aov(size~cultivar + house, data = blume3))\n\n            Df Sum Sq Mean Sq F value   Pr(>F)    \ncultivar     2  417.1   208.6   5.005     0.01 *  \nhouse        1  992.3   992.3  23.815 9.19e-06 ***\nResiduals   56 2333.2    41.7                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(aov(size~cultivar + house + cultivar:house, data = blume3)) \n\n               Df Sum Sq Mean Sq F value   Pr(>F)    \ncultivar        2  417.1   208.6   5.364   0.0075 ** \nhouse           1  992.3   992.3  25.520 5.33e-06 ***\ncultivar:house  2  233.6   116.8   3.004   0.0579 .  \nResiduals      54 2099.6    38.9                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#Kurzschreibweise: \"*\" bedeutet, dass Interaktion zwischen cultivar und house eingeschlossen wird\nsummary(aov(size~cultivar * house, data = blume3)) \n\n               Df Sum Sq Mean Sq F value   Pr(>F)    \ncultivar        2  417.1   208.6   5.364   0.0075 ** \nhouse           1  992.3   992.3  25.520 5.33e-06 ***\ncultivar:house  2  233.6   116.8   3.004   0.0579 .  \nResiduals      54 2099.6    38.9                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary.lm(aov(size~cultivar+house, data = blume3))\n\n\nCall:\naov(formula = size ~ cultivar + house, data = blume3)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-9.733 -4.696 -1.050  2.717 19.133 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    9.283      1.667   5.570 7.52e-07 ***\ncultivarb      6.400      2.041   3.135  0.00273 ** \ncultivarc      2.450      2.041   1.200  0.23509    \nhouseyes       8.133      1.667   4.880 9.19e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.455 on 56 degrees of freedom\nMultiple R-squared:  0.3766,    Adjusted R-squared:  0.3432 \nF-statistic: 11.28 on 3 and 56 DF,  p-value: 6.848e-06\n\ninteraction.plot(blume3$cultivar, blume3$house, blume3$size)\n\n\n\ninteraction.plot(blume3$house, blume3$cultivar, blume3$size)\n\n\n\nanova(lm(blume3$size~blume3$cultivar*blume3$house), lm(blume3$size~blume3$cultivar+blume3$house))\n\nAnalysis of Variance Table\n\nModel 1: blume3$size ~ blume3$cultivar * blume3$house\nModel 2: blume3$size ~ blume3$cultivar + blume3$house\n  Res.Df    RSS Df Sum of Sq      F  Pr(>F)  \n1     54 2099.6                              \n2     56 2333.2 -2   -233.63 3.0044 0.05792 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(lm(blume3$size~blume3$house), lm(blume3$size~blume3$cultivar * blume3$house))\n\nAnalysis of Variance Table\n\nModel 1: blume3$size ~ blume3$house\nModel 2: blume3$size ~ blume3$cultivar * blume3$house\n  Res.Df    RSS Df Sum of Sq      F   Pr(>F)   \n1     58 2750.3                                \n2     54 2099.6  4    650.73 4.1841 0.005045 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "stat1-4/Statistik2_Demo.html#korrelationen",
    "href": "stat1-4/Statistik2_Demo.html#korrelationen",
    "title": "Stat2: Demo",
    "section": "Korrelationen",
    "text": "Korrelationen\n\nlibrary(car)\n\nblume <- data.frame(a, b)\nscatterplot(a~b, blume)\n\n\n\ncor.test(a, b, method = \"pearson\", data = blume)\n\n\n    Pearson's product-moment correlation\n\ndata:  a and b\nt = 3.3678, df = 8, p-value = 0.009818\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.2628864 0.9414665\nsample estimates:\n      cor \n0.7657634 \n\ncor.test(a, b, method = \"spearman\", data = blume)\n\nWarning in cor.test.default(a, b, method = \"spearman\", data = blume): Cannot\ncompute exact p-value with ties\n\n\n\n    Spearman's rank correlation rho\n\ndata:  a and b\nS = 53.321, p-value = 0.03159\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.6768419 \n\ncor.test(a, b, method = \"kendall\", data = blume) \n\nWarning in cor.test.default(a, b, method = \"kendall\", data = blume): Cannot\ncompute exact p-value with ties\n\n\n\n    Kendall's rank correlation tau\n\ndata:  a and b\nz = 2.0738, p-value = 0.03809\nalternative hypothesis: true tau is not equal to 0\nsample estimates:\n      tau \n0.5228623 \n\n#Jetzt als Regression\nlm.2 <- lm(b~a)\nanova(lm.2)\n\nAnalysis of Variance Table\n\nResponse: b\n          Df Sum Sq Mean Sq F value   Pr(>F)   \na          1 42.455  42.455  11.342 0.009818 **\nResiduals  8 29.945   3.743                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(lm.2)\n\n\nCall:\nlm(formula = b ~ a)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.1897 -1.3388 -0.6067  1.3081  3.3933 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)   5.0193     1.9910   2.521  0.03575 * \na             0.4170     0.1238   3.368  0.00982 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.935 on 8 degrees of freedom\nMultiple R-squared:  0.5864,    Adjusted R-squared:  0.5347 \nF-statistic: 11.34 on 1 and 8 DF,  p-value: 0.009818\n\n#Model II-Regression\nif(!require(lmodel2)){install.packages(\"lmodel2\")} \n\nLoading required package: lmodel2\n\nlibrary(lmodel2)\nlmodel2(b~a)\n\nRMA was not requested: it will not be computed.\n\n\nNo permutation test will be performed\n\n\n\nModel II regression\n\nCall: lmodel2(formula = b ~ a)\n\nn = 10   r = 0.7657634   r-square = 0.5863936 \nParametric P-values:   2-tailed = 0.009817588    1-tailed = 0.004908794 \nAngle between the two OLS regression lines = 12.78218 degrees\n\nRegression results\n  Method Intercept     Slope Angle (degrees) P-perm (1-tailed)\n1    OLS  5.019254 0.4170422        22.63820                NA\n2     MA  4.288499 0.4648040        24.92919                NA\n3    SMA  3.067471 0.5446097        28.57314                NA\n\nConfidence intervals\n  Method 2.5%-Intercept 97.5%-Intercept 2.5%-Slope 97.5%-Slope\n1    OLS      0.4280737        9.610435  0.1314843   0.7026001\n2     MA     -1.4843783        8.769024  0.1719592   0.8421162\n3    SMA     -2.3775157        6.360555  0.3293755   0.9004912\n\nEigenvalues: 32.37967 2.786995 \n\nH statistic used for computing C.I. of MA: 0.0684968"
  },
  {
    "objectID": "stat1-4/Statistik2_Demo.html#beispiele-modelldiagnostik",
    "href": "stat1-4/Statistik2_Demo.html#beispiele-modelldiagnostik",
    "title": "Stat2: Demo",
    "section": "Beispiele Modelldiagnostik",
    "text": "Beispiele Modelldiagnostik\n\npar(mfrow=c(2, 2)) #4 Plots in einem Fenster\nplot(lm(b~a))\n\n\n\nif(!require(ggfortify)){install.packages(\"ggfortify\")}\n\nLoading required package: ggfortify\n\nlibrary(ggfortify)\nautoplot(lm(b~a))\n\n\n\n# Modellstatistik nicht OK\ng <- c(20, 19, 25, 10, 8, 15, 13, 18, 11, 14, 25, 39, 38, 28, 24)\nh <- c(12, 15, 10, 7, 8, 10, 12, 11, 13, 10, 25, 12, 30, 26, 13)\npar(mfrow = c(1, 1))\n\nplot(h~g,xlim = c(0, 40), ylim = c(0, 30))\nabline(lm(h~g))\n\n\n\npar(mfrow = c(2, 2))\nplot(lm(h~g))\n\n\n\n# Modelldiagnostik mit ggplot\ndf <- data.frame(g, h)\nggplot(df, aes(x = g, y = h)) + \n    # scale_x_continuous(limits = c(0,25)) +\n    # scale_y_continuous(limits = c(0,25)) +\n    geom_point() +\n    geom_smooth( method = \"lm\", color = \"black\", size = .5, se = F) + \n    theme_classic()\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\npar(mfrow=c(2, 2))\nautoplot(lm(h~g))"
  },
  {
    "objectID": "stat1-4/Statistik2_Loesung_1.html#lösungsweg",
    "href": "stat1-4/Statistik2_Loesung_1.html#lösungsweg",
    "title": "Stat2: Loesung",
    "section": "Lösungsweg",
    "text": "Lösungsweg\n\nSAR <- read.delim(here(\"data\",\"SAR.csv\"), sep = \";\")\n\n\nSAR\n\nExplorative Datenanalyse\n\nsummary(SAR)\n\n      area             richness    \n Min.   :  0.0001   Min.   : 1.00  \n 1st Qu.:  0.0010   1st Qu.: 4.00  \n Median :  0.1000   Median : 9.00  \n Mean   :  9.4017   Mean   :16.37  \n 3rd Qu.:  1.0000   3rd Qu.:24.00  \n Max.   :100.0000   Max.   :85.00  \n\nboxplot(SAR$area) # extrem rechtsschief\n\n\n\nboxplot(SAR$richness) # extrem rechtsschief\n\n\n\nplot(richness~area, data = SAR) # sieht nicht linear aus\n\n\n\n\nEinfaches lineares Modell\n\nlm.1 <- lm(richness~area, data = SAR)\nsummary(lm.1)\n\n\nCall:\nlm(formula = richness ~ area, data = SAR)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.567  -8.474  -3.503   6.112  35.317 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  11.4742     0.9582   11.97   <2e-16 ***\narea          0.5209     0.0342   15.23   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.27 on 154 degrees of freedom\nMultiple R-squared:  0.601, Adjusted R-squared:  0.5984 \nF-statistic: 231.9 on 1 and 154 DF,  p-value: < 2.2e-16\n\n\nModelldiagnostik\n\npar(mfrow = c(2, 2))\nplot(lm.1)\n\n\n\n\nErgebnisplot\n\npar(mfrow = c(1, 1))\nplot(SAR$area, SAR$richness, xlab = \"Area [m²]\", ylab = \"Species richness\")\nabline(lm(richness~area, data = SAR), col = \"red\") #Alternative 1\nabline(lm.1, col = \"red\") #Alternative 2\n\n\n\n\nLösung A: log-Transformation der abhängigen Variablen\n\npar(mfrow=c(1,2))\nboxplot(SAR$richness)\nboxplot(log10(SAR$richness))\n\n\n\nhist(SAR$richness)\nhist(log10(SAR$richness))\n\n\n\nSAR$log_richness <- log10(SAR$richness)\nlm.2 <- lm(log_richness~area, data = SAR)\nsummary(lm.2)\n\n\nCall:\nlm(formula = log_richness ~ area, data = SAR)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.85613 -0.34114 -0.01204  0.36365  0.75729 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.856116   0.036657   23.36  < 2e-16 ***\narea        0.010259   0.001309    7.84 6.94e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4313 on 154 degrees of freedom\nMultiple R-squared:  0.2853,    Adjusted R-squared:  0.2806 \nF-statistic: 61.47 on 1 and 154 DF,  p-value: 6.939e-13\n\n\nModelldiagnostik\n\npar(mfrow = c(2, 2))\nplot(lm.2)\n\n\n\n\n#sieht noch schlechter aus\nLösung B: log-Transformation beider Variablen\n\npar(mfrow=c(1,2))\nboxplot(SAR$area)\nboxplot(log10(SAR$area))\n\n\n\nhist(SAR$area)\nhist(log10(SAR$area))\n\n\n\nSAR$log_area <- log10(SAR$area)\nlm.3 <- lm(log_richness~log_area, data = SAR)\nsummary(lm.3)\n\n\nCall:\nlm(formula = log_richness ~ log_area, data = SAR)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.50241 -0.09353  0.02130  0.09965  0.40068 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 1.265730   0.015607   81.10   <2e-16 ***\nlog_area    0.254440   0.006926   36.73   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1633 on 154 degrees of freedom\nMultiple R-squared:  0.8976,    Adjusted R-squared:  0.8969 \nF-statistic:  1349 on 1 and 154 DF,  p-value: < 2.2e-16\n\n\nModelldiagnostik\n\npar(mfrow = c(2, 2))\nplot(lm.3)\n\n\n\n\ndas sieht jetzt sehr gut aus, bis auf zwei Ausreisser im QQ-Plot\nErgebnisplots C \n\npar(mfrow = c(1, 1))\nxv <- seq(0, 100, 0.1)\n\nErgebnisplots\n\npar(mfrow = c(1,1))\nxv <- seq(0,100,0.1)\n\nA. lineares Modell mit log-transformierter Abhaengiger\n\nplot(SAR$area, SAR$richness)\nyv1a <- 10^predict(lm.2, list(area = xv))\nlines(xv, yv1a, col = \"blue\")\n\n\n\n\nB. lineares Modell mit log-Transformation beider Variablen\n\nxvlog <- seq(-4,2,0.1)\nplot(SAR$log_area, SAR$log_richness, xlab = \"log10 (Fläche [m²])\", ylab = \"log10 (Artenreichtum)\")\nyv1b <- predict(lm.3, list(log_area = xvlog))\nlines(xvlog, yv1b, col = \"green\")\n\n\n\n\nB. lineares Modell mit log-Transformation beider Variablen (zurücktransformiert)\n\nplot(SAR$area, SAR$richness, xlab = \"Fläche [m²]\", ylab = \"Artenreichtum\")\nyv1b <- predict(lm.3, list(log_area = xv))\nlines(10^xv, 10^yv1b, col = \"green\")\n\n\n\n\nModelle im Vergleich\n\n#Modelle im Vergleich\nplot(SAR$area, SAR$richness)\nabline(lm.1, col=\"red\")\nlines(xv, yv1a, col=\"blue\")\nlines(10^xv, 10^yv1b, col=\"green\")"
  },
  {
    "objectID": "stat1-4/Statistik2_Loesung_2223s.html#kommentierter-lösungsweg",
    "href": "stat1-4/Statistik2_Loesung_2223s.html#kommentierter-lösungsweg",
    "title": "Stat2: Lösung 2223s",
    "section": "kommentierter Lösungsweg",
    "text": "kommentierter Lösungsweg\n\n\n\n\ndf <- nova # klone den originaler Datensatz\n\n# fasst die vier Inhalte der Gerichte zu drei Inhalten zusammen.\ndf %<>%\n  # Geflügel & Fisch zu fleischgerichte zählen\n  mutate(label_content = str_replace(label_content, \"Geflügel|Fisch\", \"Fleisch\")) %>% \n  # achtung reihenfolge spielt eine rolle, wegen des + (plus)\n  mutate(label_content = str_replace(label_content, \"Pflanzlich[+]|Pflanzlich\", \"Vegetarisch\"))\n\n# gruppiert Daten nach Menü-Inhalt und Woche\ndf %<>%\n    group_by(label_content, week) %>% \n    summarise(tot_sold = n()) %>%\n    drop_na() %>% \n    ungroup() # lasst die unbekannten Menü-Inhalte weg\n\n# überprüft die Voraussetzungen für eine ANOVA\n# Schaut euch die Verteilungen der Mittelwerte an (plus Standardabweichungen)\n# Sind Mittelwerte nahe bei Null? \n# Gäbe uns einen weiteren Hinweis auf eine spezielle Binomail-Verteilung \ndf %>% \n  split(.$label_content) %>% # teilt den Datensatz in 3 verschiedene Datensätze auf\n  purrr::map(~ psych::describe(.$tot_sold)) # mit map können andere Funktionen \n\n$Fleisch\n   vars  n    mean     sd median trimmed    mad min  max range skew kurtosis\nX1    1 12 1135.58 200.03   1088  1129.2 223.13 917 1418   501 0.19    -1.89\n      se\nX1 57.74\n\n$`Hot and Cold`\n   vars  n   mean    sd median trimmed   mad min max range skew kurtosis   se\nX1    1 12 308.33 23.53    310   307.3 30.39 276 351    75 0.32    -1.25 6.79\n\n$Vegetarisch\n   vars  n   mean     sd median trimmed    mad min  max range  skew kurtosis\nX1    1 12 739.25 213.54    710   741.8 323.95 449 1004   555 -0.01    -1.85\n      se\nX1 61.64\n\n# auf den Datensatz angewendet werden (alternative Funktionen sind aggregate oder apply)\n\n# Boxplot\nggplot(df, aes(x = label_content, y= tot_sold)) +\n  # Achtung: Reihenfolge spielt hier eine Rolle!\n  stat_boxplot(geom = \"errorbar\", width = 0.25) +\n  geom_boxplot(fill=\"white\", color = \"black\", size = 1, width = .5) +\n  labs(x = \"\\nMenü-Inhalt\", y = \"Anzahl verkaufte Gerichte pro Woche\\n\") +\n  # achtung erster Hinweis einer Varianzheterogenität, wegen den Hot&Cold Gerichten\n  mytheme\n\n\n\n#alternative mit base\nboxplot(df$tot_sold~df$label_content)\n\n\n\n# definiert das Modell (vgl. Skript Statistik 2)\nmodel <- aov(tot_sold ~ label_content, data = df)\n\nsummary.lm(model)\n\n\nCall:\naov(formula = tot_sold ~ label_content, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-290.250 -135.083    1.667  125.500  282.417 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                1135.58      48.92  23.211  < 2e-16 ***\nlabel_contentHot and Cold  -827.25      69.19 -11.956 1.54e-13 ***\nlabel_contentVegetarisch   -396.33      69.19  -5.728 2.15e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 169.5 on 33 degrees of freedom\nMultiple R-squared:  0.8125,    Adjusted R-squared:  0.8012 \nF-statistic: 71.52 on 2 and 33 DF,  p-value: 1.007e-12\n\n# überprüft die Modelvoraussetzungen\npar(mfrow = c(2,2))\nplot(model)\n\n\n\n\n Fazit: Inspektion der Modellvoraussetzung zeigt klare Verletzungen des Residualplots (zeigt einen “Trichter”, siehe Skript Statistik 2), D.h. die Voraussetzung der Homoskedastizität sind verletzt. Mögliche nächste Schritte:\n\nMenüinhalt “Buffet” aus der Analyse ausschliessen, da sowieso kein richtiger Menüinhalt (aber Informationsverlust)\nDatentransformation z.B. log-Transformation\nnicht-parametrischer Test (Achtung, auch dieser setzt Voraussetzungen voraus)\nein glm Model (general linear model) mit einer poisson/quasipoisson link Funktion (vgl. Skript Statistik 4), weitere Infos dazu Link \n\n\n# überprüft die Voraussetzungen des Welch-Tests:\n# Gibt es eine hohe Varianzheterogenität und ist die relative Verteilung der \n# Residuen gegeben? (siehe Statistik 2)\n# Ja Varianzheterogenität ist gegeben, aber die Verteilung der Residuen folgt \n# einem \"Trichter\", also keiner \"normalen/symmetrischen\" Verteilung um 0\n# Daher ziehe ich eine Transformation der AV einem nicht-parametrischen Test vor\n# für weitere Infos: \n# https://data.library.virginia.edu/interpreting-log-transformations-in-a-linear-model/\n\n# achtung hier log10, bei Rücktransformation achten\nmodel_log <- aov(log10(tot_sold) ~ label_content, data = df) \npar(mfrow = c(2,2))\nplot(model_log) # scheint ok zu sein\n\n\n\nsummary.lm(model_log) # Referenzkategorie ist Fleisch\n\n\nCall:\naov(formula = log10(tot_sold) ~ label_content, data = df)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.198920 -0.059343  0.003477  0.062579  0.150567 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                3.04908    0.02585 117.942  < 2e-16 ***\nlabel_contentHot and Cold -0.56121    0.03656 -15.350  < 2e-16 ***\nlabel_contentVegetarisch  -0.19792    0.03656  -5.413 5.45e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.08956 on 33 degrees of freedom\nMultiple R-squared:  0.8802,    Adjusted R-squared:  0.8729 \nF-statistic: 121.2 on 2 and 33 DF,  p-value: 6.238e-16\n\nTukeyHSD(model_log) # (vgl. Statistik 2)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = log10(tot_sold) ~ label_content, data = df)\n\n$label_content\n                               diff        lwr        upr   p adj\nHot and Cold-Fleisch     -0.5612085 -0.6509215 -0.4714955 0.0e+00\nVegetarisch-Fleisch      -0.1979175 -0.2876305 -0.1082044 1.6e-05\nVegetarisch-Hot and Cold  0.3632910  0.2735780  0.4530041 0.0e+00\n\n# Achtung Beta-Werte resp. Koeffinzienten sind nicht direkt interpretierbar\n# sie müssten zuerst wieder zurück transformiert werden, hier ein Beispiel dafür:\n# für Fleisch\n10^model_log$coefficients[1]\n\n(Intercept) \n   1119.655 \n\n# für Hot & Cold,\n10^(model_log$coefficients[1] + model_log$coefficients[2])\n\n(Intercept) \n   307.5216 \n\n# ist equivalent zu\n10^(model_log$coefficients[1]) * 10^(model_log$coefficients[2])\n\n(Intercept) \n   307.5216 \n\n# für Vegi\n10^(model_log$coefficients[1] + model_log$coefficients[3])\n\n(Intercept) \n   709.8501"
  },
  {
    "objectID": "stat1-4/Statistik2_Loesung_2223s.html#methoden",
    "href": "stat1-4/Statistik2_Loesung_2223s.html#methoden",
    "title": "Stat2: Lösung 2223s",
    "section": "Methoden",
    "text": "Methoden\nZiel war es, die Unterschiede in den wöchentlichen Verkaufszahlen pro Menüinhalt aufzuzeigen. Da die Responsevariable (Verkaufszahlen) “metrisch” und die Prädiktorvariable kategorial sind, wurde eine einfaktorielle ANOVA gerechnet. Die visuelle Inspektion des Modells zeigte insbesondere schwere Verletzungen der Homoskedastizität. Der Boxplot bestätigt dieser Befund. Weil die Voraussetzungen schwer verletzt sind, wurde eine log-Transformation der Responsevariable vorgenommen. Anschliessend wurde erneut eine ANOVA gerechnet und die Modelvoraussetzungen visuell inspiziert: Homoskedastizität und Normalverteilung der Residuen sind gegeben. Für mehr Informatinen zu log-Transformationen und Darstellung der Ergebnisse findet ihr hier"
  },
  {
    "objectID": "stat1-4/Statistik2_Loesung_2223s.html#ergebnisse",
    "href": "stat1-4/Statistik2_Loesung_2223s.html#ergebnisse",
    "title": "Stat2: Lösung 2223s",
    "section": "Ergebnisse",
    "text": "Ergebnisse\nDie Menüinhalte (Fleisch, Vegetarisch und Buffet) unterscheiden sich in den wöchentlichen Verkaufszahlen signifikant (F(2,15) = 121.22, p < .001). Die Abbildung 1 zeigt die wöchentlichen Verkaufszahlen pro Menüinhalt.\n\n\n\n\n\nDie wöchentlichen Verkaufzahlen unterscheiden sich je nach Menüinhalt stark. Das Modell wurde mit den log-tranformierten Daten gerechnet."
  },
  {
    "objectID": "stat1-4/Statistik2_Loesung_2223s.html#kommentierter-lösungsweg-1",
    "href": "stat1-4/Statistik2_Loesung_2223s.html#kommentierter-lösungsweg-1",
    "title": "Stat2: Lösung 2223s",
    "section": "kommentierter Lösungsweg",
    "text": "kommentierter Lösungsweg\nDownload 2017_ZHAW_individual_menu_sales_NOVANIMAL.csv\n\n\n\n\n# klone den originaler Datensatz\ndf <- nova \n\n# Daten vorbereiten\ndf %<>% # schaut euch das Package \"magrittr\" an\n  # ersetze Local mit einem leeren String\n  mutate(article_description = str_replace(article_description, \"Local \", \"\")) %>% \n  filter(article_description != \"Hot and Cold\") %>% # lasse Buffet Gerichte weg\n  filter(member != \"Spezialkarten\") %>% # Spezialkarten können vernachlässigt werden\n  #  fasse die zwei Menülinien \"World & Favorite\" zusammen\n  mutate(article_description = str_replace_all(article_description, \"Favorite|World\",\n                                               \"Fav_World\"))  \n# gruppiere Daten nach Menülinie, Geschlecht und Hochschulzugehörigkeit\ndf %<>%\n    group_by(article_description, member, week) %>% \n    summarise(tot_sold = n()) %>%\n    ungroup() %>% \n    drop_na()  # lasst die unbekannten Menü-Inhalte weg\n\n# überprüft die Voraussetzungen für eine ANOVA\n# Schaut euch die Verteilungen der Mittelwerte der Responsevariable an\n# Sind Mittelwerte nahe bei Null? Gäbe uns einen weiteren Hinweis auf \n# eine spezielle Binomial-Verteilung (vgl. Statistik 4)\ndf %>% \n  split(.$article_description) %>% # teilt den Datensatz in 3 verschiedene Datensätze auf\n  # mit map können andere Funktionen auf den Datensatz angewendet werden \n  # (alternative Funktionen sind aggregate oder apply)\n  purrr::map(~ psych::describe(.$tot_sold)) \n\n$Fav_World\n   vars  n   mean     sd median trimmed    mad min max range skew kurtosis   se\nX1    1 24 622.67 178.79  599.5   620.8 253.52 378 876   498 0.04    -1.88 36.5\n\n$Kitchen\n   vars  n  mean    sd median trimmed   mad min max range skew kurtosis   se\nX1    1 24 128.5 22.21  124.5   128.2 23.72  79 187   108 0.27     0.43 4.53\n\n# visualisiere dir dein Model, was siehst du? \n# sind möglicherweise gewiesse Voraussetzungen verletzt?\n# Boxplot\nggplot(df, aes(x = interaction(article_description, member), y= tot_sold)) + \n   # Achtung: Reihenfolge spielt hier eine Rolle!\n  stat_boxplot(geom = \"errorbar\", width = 0.25) +\n  geom_boxplot(fill=\"white\", color = \"black\", size = 1, width = .5) +\n  labs(x = \"\\nMenülinie nach Hochschulzugehörigkeit\", y = \"Anzahl verkaufte Gerichte\\n\") + \n  # ändere Gruppennamen händisch\n  scale_x_discrete(limits = c(\"Fav_World.Mitarbeitende\", \"Kitchen.Mitarbeitende\",\n                              \"Fav_World.Studierende\", \"Kitchen.Studierende\"),\n                   breaks = c(\"Fav_World.Mitarbeitende\", \"Fav_World.Studierende\",\n                              \"Kitchen.Mitarbeitende\",  \"Kitchen.Studierende\"),\n                   labels = c(\"Fav_World\\nMitarbeitende\", \"Fav_World\\nStudierende\",\n                              \"Kitchen\\nMitarbeitende\",  \"Kitchen\\nStudierende\")) +\n  mytheme # wie sind die Voraussetzungen erfüllt?\n\n\n\n\n\n# definiert das Modell (Skript Statistik 2)\nmodel <- aov(tot_sold ~ article_description * member, data = df)\n\nsummary.lm(model)\n\n\nCall:\naov(formula = tot_sold ~ article_description * member, data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-91.00 -17.33   0.50  14.83  83.00 \n\nCoefficients:\n                                             Estimate Std. Error t value\n(Intercept)                                   452.333      9.734   46.47\narticle_descriptionKitchen                   -327.000     13.766  -23.75\nmemberStudierende                             340.667     13.766   24.75\narticle_descriptionKitchen:memberStudierende -334.333     19.469  -17.17\n                                             Pr(>|t|)    \n(Intercept)                                    <2e-16 ***\narticle_descriptionKitchen                     <2e-16 ***\nmemberStudierende                              <2e-16 ***\narticle_descriptionKitchen:memberStudierende   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 33.72 on 44 degrees of freedom\nMultiple R-squared:  0.9864,    Adjusted R-squared:  0.9855 \nF-statistic:  1063 on 3 and 44 DF,  p-value: < 2.2e-16\n\n# überprüft die Modelvoraussetzungen (Statistik 2)\npar(mfrow = c(2,2)) # alternativ gäbe es die ggfortify::autoplot(model) funktion\nplot(model)\n\n\n\n\nFazit: Die Inspektion des Modells zeigt kleinere Verletzungen bei der Normalverteilung der Residuen (Q-Q Plot). Aufgrund keiner starken Verbesserung durch eine Transformation der Responsevariable, entscheide ich mich für eine ANOVA ohne log-tranformierten Responsevariablen (AV).\n\n# sieht aus, als ob die Voraussetzungen für eine Anova nur geringfügig verletzt sind\n# mögliche alternativen: \n# 0. keine Tranformation der AV (machen wir hier)\n# 1. log-transformation um die grossen werte zu minimieren (nur möglich, wenn \n# keine 0 enthalten sind und die Mittelwerte weit von 0 entfernt sind (bei uns wäre dieser Fall erfüllt)\n# => bei Zähldaten ist dies leider nicht immer gegeben)\n# 2. nicht parametrische Test z.B. Welch-Test, wenn hohe Varianzheterogenität \n# zwischen den Residuen\n\n#0) keine Tranformation\n# post-hov Vergleiche\nTukeyHSD(model)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = tot_sold ~ article_description * member, data = df)\n\n$article_description\n                       diff      lwr       upr p adj\nKitchen-Fav_World -494.1667 -513.785 -474.5484     0\n\n$member\n                           diff      lwr      upr p adj\nStudierende-Mitarbeitende 173.5 153.8817 193.1183     0\n\n$`article_description:member`\n                                                     diff        lwr        upr\nKitchen:Mitarbeitende-Fav_World:Mitarbeitende -327.000000 -363.75650 -290.24350\nFav_World:Studierende-Fav_World:Mitarbeitende  340.666667  303.91017  377.42317\nKitchen:Studierende-Fav_World:Mitarbeitende   -320.666667 -357.42317 -283.91017\nFav_World:Studierende-Kitchen:Mitarbeitende    667.666667  630.91017  704.42317\nKitchen:Studierende-Kitchen:Mitarbeitende        6.333333  -30.42317   43.08983\nKitchen:Studierende-Fav_World:Studierende     -661.333333 -698.08983 -624.57683\n                                                  p adj\nKitchen:Mitarbeitende-Fav_World:Mitarbeitende 0.0000000\nFav_World:Studierende-Fav_World:Mitarbeitende 0.0000000\nKitchen:Studierende-Fav_World:Mitarbeitende   0.0000000\nFav_World:Studierende-Kitchen:Mitarbeitende   0.0000000\nKitchen:Studierende-Kitchen:Mitarbeitende     0.9672944\nKitchen:Studierende-Fav_World:Studierende     0.0000000\n\n#1) Alterativ: log-transformation\nmodel_log <- aov(log10(tot_sold) ~ article_description * member, data = df)\n\nsummary.lm(model_log) # interaktion ist nun nicht mehr signifikant: vgl. \n\n\nCall:\naov(formula = log10(tot_sold) ~ article_description * member, \n    data = df)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.191372 -0.025043  0.003191  0.037604  0.182842 \n\nCoefficients:\n                                             Estimate Std. Error t value\n(Intercept)                                   2.65417    0.01696 156.533\narticle_descriptionKitchen                   -0.56517    0.02398 -23.569\nmemberStudierende                             0.24438    0.02398  10.191\narticle_descriptionKitchen:memberStudierende -0.21726    0.03391  -6.407\n                                             Pr(>|t|)    \n(Intercept)                                   < 2e-16 ***\narticle_descriptionKitchen                    < 2e-16 ***\nmemberStudierende                            3.71e-13 ***\narticle_descriptionKitchen:memberStudierende 8.51e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.05874 on 44 degrees of freedom\nMultiple R-squared:  0.9745,    Adjusted R-squared:  0.9728 \nF-statistic: 561.4 on 3 and 44 DF,  p-value: < 2.2e-16\n\n# nochmals euren Boxplot zu beginn, machen diese Koeffizienten sinn?\n\n# überprüft die Modelvoraussetzungen (vgl. Skript Statistik 2)\n# bringt aber keine wesentliche Verbesserung, daher bleibe ich bei den \n# untranfromierten Daten\npar(mfrow = c(2,2))\nplot(model_log)\n\n\n\n# post-hov Vergleiche\nTukeyHSD(model_log) # gibt sehr ähnliche Resultate im Vergleich zum nicht-transformierten Model\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = log10(tot_sold) ~ article_description * member, data = df)\n\n$article_description\n                        diff        lwr        upr p adj\nKitchen-Fav_World -0.6738029 -0.7079755 -0.6396302     0\n\n$member\n                               diff       lwr       upr p adj\nStudierende-Mitarbeitende 0.1357518 0.1015791 0.1699244     0\n\n$`article_description:member`\n                                                     diff         lwr\nKitchen:Mitarbeitende-Fav_World:Mitarbeitende -0.56517128 -0.62919652\nFav_World:Studierende-Fav_World:Mitarbeitende  0.24438333  0.18035809\nKitchen:Studierende-Fav_World:Mitarbeitende   -0.53805110 -0.60207634\nFav_World:Studierende-Kitchen:Mitarbeitende    0.80955461  0.74552937\nKitchen:Studierende-Kitchen:Mitarbeitende      0.02712017 -0.03690507\nKitchen:Studierende-Fav_World:Studierende     -0.78243444 -0.84645968\n                                                      upr     p adj\nKitchen:Mitarbeitende-Fav_World:Mitarbeitende -0.50114604 0.0000000\nFav_World:Studierende-Fav_World:Mitarbeitende  0.30840857 0.0000000\nKitchen:Studierende-Fav_World:Mitarbeitende   -0.47402586 0.0000000\nFav_World:Studierende-Kitchen:Mitarbeitende    0.87357985 0.0000000\nKitchen:Studierende-Kitchen:Mitarbeitende      0.09114541 0.6726112\nKitchen:Studierende-Fav_World:Studierende     -0.71840920 0.0000000"
  },
  {
    "objectID": "stat1-4/Statistik2_Loesung_2223s.html#methode",
    "href": "stat1-4/Statistik2_Loesung_2223s.html#methode",
    "title": "Stat2: Lösung 2223s",
    "section": "Methode",
    "text": "Methode\nZiel war es die Unterschiede zwischen den preisgünstigeren und teureren Menülinien und der Hochschulzugehörigkeit herauszufinden: Hierfür wurde eine ANOVA mit Interaktion gerechnet, da wir eine (quasi)-metrische Responsevariable und zwei Prädiktorvariablen (Menülinie und Hochschulzugehörigkeit) haben.\nDie Voraussetzungen für eine ANOVA waren im ersten Model nicht stark verletzt, lediglich die Normalverteilung der Residuen: Deshalb habe wurde auf eine log-Transformation der Responsevariable verzichtet. Anschliessend wurden noch post-hoc Einzelvergleiche nach Tukey durchgeführt.\nKleiner Exkurs: Verkaufsdaten sind Zähldaten und perse binomial-Verteilt, da es keine negativen Werte geben kann. Ich versuche immer folgende Fragen zu beantworten:\n\nWie weit ist der Mittelwert von “Null entfernt”? -> Wenn ja uns keine Voraussetzungen zur Normalverteilung gibt, kann auch eine Normalverteilung angenommen werden\nBeinhalten die Daten viele “Null’s”? -> Wenn ja muss eine spezielle binomial Verteilung angenommen werden, z.B. negative binomiale Transformation mit GLM (see Skript XY)"
  },
  {
    "objectID": "stat1-4/Statistik2_Loesung_2223s.html#ergebnisse-1",
    "href": "stat1-4/Statistik2_Loesung_2223s.html#ergebnisse-1",
    "title": "Stat2: Lösung 2223s",
    "section": "Ergebnisse",
    "text": "Ergebnisse\nDie wöchentlichen Verkaufszahlen der Menülinien unterscheiden sich nach Hochschulzugehörigkeit signifikant (F(3,44) = 561.42, p < .001). Inhaltich bedeutet dies, dass Studierende signifikant häufiger die preisgünstigere Menülinie “Favorite & World” als Mitarbeitende kaufen. Entgegen der Annahme gibt es aber keine signifikanten Unterschiede zwischen Studierende und Mitarbeitende bei dem Kauf der teureren Menülinie “Kitchen”. Über die möglichen Gründe können nur spekuliert werden, hierfür bedarf es weiteren Analysen z.B. mit dem Prädiktor “Menüinhalt”.\n\n\n\n\n\nBox-Whisker-Plots der wöchentlichen Verkaufszahlen pro Menü-Inhalte. Kleinbuchstaben bezeichnen homogene Gruppen auf p < .05 nach Tukeys post-hoc-Test."
  },
  {
    "objectID": "stat1-4/Statistik2_Loesung_23n.html",
    "href": "stat1-4/Statistik2_Loesung_23n.html",
    "title": "Stat2: Lösung 23n",
    "section": "",
    "text": "RCode als Download\nLoesungstext 2.3\n\nÜbungsaufgabe (hier so ausführlich formuliert, wie dies auch in der Klausur der Fall sein wird) - Laden Sie den Datensatz kormoran.csv mit ein Dieser enthält Tauchzeiten (hier ohne Einheit) von Kormoranen in Abhängigkeit von Jahreszeit und Unterart. Unterarten: Phalacrocorax carbo carbo (C) und Phalacrocorax carbo sinensis (S); Jahreszeiten: F = Frühling, S = Sommer, H = Herbst, W = Winter. - Ihre Gesamtaufgabe ist es, aus diesen Daten ein minimal adäquates Modell zu ermitteln, das diese Abhängigkeit beschreibt. - Bitte erklären und begründen Sie die einzelnen Schritte, die Sie unternehmen, um zu diesem Ergebnis zu kommen. Dazu erstellen Sie bitte ein Word-Dokument, in das Sie Schritt für Schritt den verwendeten R-Code, die dazu gehörigen Ausgaben von R, Ihre Interpretation derselben und die sich ergebenden Schlussfolgerungen für das weitere Vorgehen dokumentieren. - Dieser Ablauf sollte insbesondere beinhalten: - Überprüfen der Datenstruktur nach dem Einlesen, welches sind die abhängige(n) und welches die unabängige(n) Variablen, welches statistische Verfahren wenden Sie an? - Explorative Datenanalyse, um zu sehen, ob schon vor dem Start der Analysen Transformationen o.ä. vorgenommen werden sollten - Definition eines vollen Modelles, das nach statistischen Kritierien zum minimal adäquaten Modell reduziert wird - Durchführen der Modelldiagnostik, um zu entscheiden, ob das gewählte Vorgehen korrekt war oder ggf. angepasst werden muss - Generieren aller Zahlen, Statistiken und Tabellen, die für eine wiss. Ergebnisdarstellung benötigt werden - Formulieren Sie abschliessend einen Methoden- und Ergebnisteil (ggf. incl. adäquaten Abbildungen) zu dieser Untersuchung in der Form einer wissenschaftlichen Arbeit (ausformulierte schriftliche Zusammenfassung, mit je einem Absatz von ca. 60-100 Worten, resp. 3-8 Sätzen für den Methoden- und Ergebnisteil). D. h. alle wichtigen Informationen sollten enthalten sein, unnötige Redundanz dagegen vermieden werden. - Abzugeben sind am Ende (a) Ein lauffähiges R-Skript; (b) begründeter Lösungsweg (Kombination aus R-Code, R Output und dessen Interpretation) und (c) ausformulierter Methoden- und Ergebnisteil (für eine wiss. Arbeit).\nkommentierter Lösungsweg\n\n# Working directory muss angepasst werden\nkormoran <- read.delim(here(\"data\",\"kormoran.csv\"), sep = \";\", stringsAsFactors = T)  # \n\n# Ueberpruefen, ob Einlesen richtig funktioniert hat und welche Datenstruktur vorliegt\nstr(kormoran)\n\n'data.frame':   40 obs. of  4 variables:\n $ Obs       : int  1 2 3 4 5 6 7 8 9 10 ...\n $ Tauchzeit : num  9.5 11.9 13.4 13.8 15.3 15.5 15.6 16.7 16.8 18.7 ...\n $ Unterart  : Factor w/ 2 levels \"C\",\"S\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Jahreszeit: Factor w/ 4 levels \"F\",\"H\",\"S\",\"W\": 1 1 1 1 1 3 3 3 3 3 ...\n\nsummary(kormoran)\n\n      Obs          Tauchzeit     Unterart Jahreszeit\n Min.   : 1.00   Min.   : 9.50   C:20     F:10      \n 1st Qu.:10.75   1st Qu.:13.38   S:20     H:10      \n Median :20.50   Median :16.75            S:10      \n Mean   :20.50   Mean   :17.40            W:10      \n 3rd Qu.:30.25   3rd Qu.:20.77                      \n Max.   :40.00   Max.   :30.40                      \n\n\nMan erkennt, dass es sich um einen Dataframe mit einer metrischen (Tauchzeit) und zwei kategorialen (Unterart, Jahreszeit) Variablen handelt. Die adäquate Analyse (1 metrische Abhängige vs. 2 kategoriale Unabhängige) ist damit eine zweifaktorielle ANOVA Die Sortierung der Jahreszeiten (default: alphabetisch) ist inhaltlich aber nicht sinnvoll und sollte angepasst werden.\n\n# Umsortieren der Faktoren, damit sie in den Boxplots eine sinnvolle Reihung haben\nkormoran$Jahreszeit <- ordered(kormoran$Jahreszeit, levels = c(\"F\", \"S\", \"H\", \"W\"))\nkormoran$Jahreszeit\n\n [1] F F F F F S S S S S H H H H H W W W W W F F F F F S S S S S H H H H H W W W\n[39] W W\nLevels: F < S < H < W\n\n# Explorative Datenanalyse (zeigt uns die Gesamtverteilung)\nboxplot(kormoran$Tauchzeit)\n\n\n\n\nDas ist noch OK für parametrische Verfahren (Box ziemlich symmetrisch um Median, Whisker etwas asymmetrisch aber nicht kritisch). Wegen der leichten Asymmetrie (Linksschiefe) könnte man eine log-Transformation ausprobieren.\n\nboxplot(log10(kormoran$Tauchzeit))\n\n\n\n\nDer Gesamtboxplot für log10 sieht perfekt symmetrisch aus, das spräche also für eine log10-Transformation. De facto kommt es aber nicht auf den Gesamtboxplot an, sondern auf die einzelnen.\n\n# Explorative Datenanalyse \n# (Check auf Normalverteilung der Residuen und Varianzhomogenitaet)\nboxplot(Tauchzeit~Jahreszeit * Unterart, data = kormoran)\n\n\n\nboxplot(log10(Tauchzeit)~Jahreszeit * Unterart, data = kormoran)\n\n\n\n\nHier sieht mal die Verteilung für die untransformierten Daten, mal für die transformierten besser aus. Da die Transformation keine klare Verbesserung bringt, bleiben wir im Folgenden bei den untransformierten Daten, da diese leichter (direkter) interpretiert werden können\n\n# Vollständiges Modell mit Interaktion\naov.1 <- aov(Tauchzeit~Unterart * Jahreszeit, data = kormoran)\naov.1\n\nCall:\n   aov(formula = Tauchzeit ~ Unterart * Jahreszeit, data = kormoran)\n\nTerms:\n                Unterart Jahreszeit Unterart:Jahreszeit Residuals\nSum of Squares   106.929    756.170              11.009    84.992\nDeg. of Freedom        1          3                   3        32\n\nResidual standard error: 1.629724\nEstimated effects may be unbalanced\n\nsummary(aov.1)\n\n                    Df Sum Sq Mean Sq F value   Pr(>F)    \nUnterart             1  106.9  106.93  40.259 4.01e-07 ***\nJahreszeit           3  756.2  252.06  94.901 5.19e-16 ***\nUnterart:Jahreszeit  3   11.0    3.67   1.382    0.266    \nResiduals           32   85.0    2.66                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# p-Wert der Interaktion ist 0.266\n\nDas volle (maximale) Modell zeigt, dass es keine signifikante Interaktion zwischen Jahreszeit und Unterart gibt. Wir können das Modell also vereinfachen, indem wir die Interaktion herausnehmen (+ statt * in der Modellspezifikation)\n\n# Modellvereinfachung\naov.2 <- aov(Tauchzeit~Unterart + Jahreszeit, data = kormoran)\naov.2\n\nCall:\n   aov(formula = Tauchzeit ~ Unterart + Jahreszeit, data = kormoran)\n\nTerms:\n                Unterart Jahreszeit Residuals\nSum of Squares   106.929    756.170    96.001\nDeg. of Freedom        1          3        35\n\nResidual standard error: 1.656166\nEstimated effects may be unbalanced\n\nsummary(aov.2)\n\n            Df Sum Sq Mean Sq F value   Pr(>F)    \nUnterart     1  106.9  106.93   38.98 3.69e-07 ***\nJahreszeit   3  756.2  252.06   91.89  < 2e-16 ***\nResiduals   35   96.0    2.74                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIm so vereinfachten Modell sind alle verbleibenden Terme signifikant, wir sind also beim „minimal adäquaten Modell“ angelangt\n\n# Anderer Weg, um zu pruefen, ob man das komplexere Modell mit Interaktion behalten soll\nanova(aov.1, aov.2)\n\nAnalysis of Variance Table\n\nModel 1: Tauchzeit ~ Unterart * Jahreszeit\nModel 2: Tauchzeit ~ Unterart + Jahreszeit\n  Res.Df    RSS Df Sum of Sq      F Pr(>F)\n1     32 84.992                           \n2     35 96.001 -3   -11.009 1.3817 0.2661\n\n# In diesem Fall bekommen wir den gleichen p-Wert wie oben (0.266)\n\n# Modelldiagnostik\npar(mfrow = c(2, 2)) #alle vier Abbildungen in einem 2 x 2 Raster\nplot(aov.2)\n\n\n\n\n\ninfluence.measures(aov.2) # \n# kann man sich zusätzlich zum \"plot\" ansehen, um herauszufinden, \n# ob es evtl. sehr einflussreiche Werte mit Cook's D von 1 oder grösser gibt\n\nLinks oben ist alles bestens, d. h. keine Hinweise auf Varianzheterogenität („Keil“) oder Nichtlinearität („Banane“) Rechts oben ganz gut, allerdings weichen Punkte 1 und 20 deutlich von der optimalen Gerade ab -> aus diesem Grund können wir es doch noch mal mit der log10-Transformation versuchen (s.u.) Rechts unten: kein Punkt hat einen problematischen Einfluss (die roten Linien für Cook’s D > 0.5 und > 1 sind noch nicht einmal im Bildausschnitt.\n\n# Alternative mit log10\naov.3 <-aov(log10(Tauchzeit)~Unterart + Jahreszeit, data=kormoran)\naov.3\n\nCall:\n   aov(formula = log10(Tauchzeit) ~ Unterart + Jahreszeit, data = kormoran)\n\nTerms:\n                 Unterart Jahreszeit Residuals\nSum of Squares  0.0627004  0.4958434 0.0562031\nDeg. of Freedom         1          3        35\n\nResidual standard error: 0.04007247\nEstimated effects may be unbalanced\n\nsummary(aov.3)\n\n            Df Sum Sq Mean Sq F value   Pr(>F)    \nUnterart     1 0.0627 0.06270   39.05 3.64e-07 ***\nJahreszeit   3 0.4958 0.16528  102.93  < 2e-16 ***\nResiduals   35 0.0562 0.00161                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nplot(aov.3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nRechts oben: Punkt 20 jetzt auf der Linie, aber Punkt 1 weicht umso deutlicher ab -> keine Verbesserung -> wir bleiben bei den untransformierten Daten.\n\n# Ergebnisdarstellung\n\nDa wir keine Interaktion zwischen Unterart und Jahreszeit festgestellt haben, brauchen wir auch keinen Interaktionsplot (unnötig kompliziert), statt dessen können wir die Ergebnisse am besten mit zwei getrennten Plots für die beiden Faktoren darstellen. Bitte die Achsenbeschriftungen und den Tukey post-hoc-Test nicht vergessen.\n\npar(mfrow = c(1, 1)) #Zurückschalten auf Einzelplots\nif(!require(multcomp)){install.packages(\"multcomp\")} \n\nLoading required package: multcomp\n\n\nLoading required package: mvtnorm\n\n\nLoading required package: survival\n\n\nLoading required package: TH.data\n\n\nLoading required package: MASS\n\n\n\nAttaching package: 'TH.data'\n\n\nThe following object is masked from 'package:MASS':\n\n    geyser\n\nlibrary(multcomp)\n\nboxplot(Tauchzeit~Unterart, data = kormoran)\n\n\n\nletters <- cld(glht(aov.2, linfct = mcp(Jahreszeit = \"Tukey\")))\nboxplot(Tauchzeit~Jahreszeit, data = kormoran)\nmtext(letters$mcletters$Letters, at = 1:4)\n\n\n\n\nJetzt brauchen wir noch die Mittelwerte bzw. Effektgroessen\nFür den Ergebnistext brauchen wir auch noch Angaben zu den Effektgrössen. Hier sind zwei Möglichkeiten, um an sie zu gelangen.\n\naggregate(Tauchzeit~Jahreszeit, FUN = mean, data = kormoran)\n\n  Jahreszeit Tauchzeit\n1          F     11.86\n2          S     15.09\n3          H     19.23\n4          W     23.42\n\naggregate(Tauchzeit~Unterart, FUN = mean, data = kormoran)\n\n  Unterart Tauchzeit\n1        C    19.035\n2        S    15.765\n\nsummary(lm(Tauchzeit~Jahreszeit, data = kormoran))\n\n\nCall:\nlm(formula = Tauchzeit ~ Jahreszeit, data = kormoran)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.820 -1.617 -0.145  1.587  6.980 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   17.4000     0.3754  46.351  < 2e-16 ***\nJahreszeit.L   8.6804     0.7508  11.562 1.12e-13 ***\nJahreszeit.Q   0.4800     0.7508   0.639    0.527    \nJahreszeit.C  -0.1923     0.7508  -0.256    0.799    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.374 on 36 degrees of freedom\nMultiple R-squared:  0.7884,    Adjusted R-squared:  0.7708 \nF-statistic: 44.72 on 3 and 36 DF,  p-value: 3.156e-12\n\nsummary(lm(Tauchzeit~Unterart, data = kormoran))\n\n\nCall:\nlm(formula = Tauchzeit ~ Unterart, data = kormoran)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-9.535 -3.585 -0.335  3.760 11.365 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   19.035      1.059  17.976   <2e-16 ***\nUnterartS     -3.270      1.498  -2.184   0.0352 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.736 on 38 degrees of freedom\nMultiple R-squared:  0.1115,    Adjusted R-squared:  0.08811 \nF-statistic: 4.768 on 1 and 38 DF,  p-value: 0.03523"
  },
  {
    "objectID": "stat1-4/Statistik2_Loesung_Beispiel.html#musterloesung-beispiel",
    "href": "stat1-4/Statistik2_Loesung_Beispiel.html#musterloesung-beispiel",
    "title": "Stat2: Lösung Beispiel",
    "section": "Musterloesung Beispiel",
    "text": "Musterloesung Beispiel\n\nDatensatz decay.csv\nRCode als Download\nLoesungstext Beispiel\n\nÜbungsaufgabe (hier so ausführlich formuliert, wie dies auch in der Klausur der Fall sein wird)\n\nLaden Sie den Datensatz decay.csv. Dieser enthält die Zahl radioaktiver Zerfälle pro Zeiteinheit (amount) für Zeitpunkte (time) nach dem Start des Experimentes.\nErmitteln Sie ein statistisches Modell, dass die Zerfallshäufigkeit in Abhängigkeit von der Zeit beschreibt.\nBitte erklären und begründen Sie die einzelnen Schritte, die Sie unternehmen, um zu diesem Ergebnis zu kommen. Dazu erstellen Sie bitte ein Word-Dokument, in das Sie Schritt für Schritt den verwendeten R-Code, die dazu gehörigen Ausgaben von R, Ihre Interpretation derselben und die sich ergebenden Schlussfolgerungen für das weitere Vorgehen dokumentieren.\nDieser Ablauf sollte insbesondere beinhalten:\n\nÜberprüfen der Datenstruktur nach dem Einlesen, welches sind die abhängige(n) und welches die unabängige(n) Variablen\nExplorative Datenanalyse, um zu sehen, ob evtl. Dateneingabefehler vorliegen oder Datentransformationen vorgenommen werden sollten\nAuswahl und Begründung eines statistischen Verfahrens (es gibt hier mehrere statistisch korrekte Möglichkeiten!)\nErmittlung eines Modells\nDurchführen der Modelldiagnostik für das gewählte Modell\nGenerieren aller Zahlen, Statistiken und Tabellen, die für eine wiss. Ergebnisdarstellung benötigt werden\nFormulieren Sie abschliessend einen Methoden- und Ergebnisteil (ggf. incl. adäquaten Abbildungen) zu dieser Untersuchung in der Form einer wissenschaftlichen Arbeit (ausformulierte schriftliche Zusammenfassung, mit je einem Absatz von ca. 60-100 Worten, resp. 3-8 Sätzen für den Methoden- und Ergebnisteil). D. h. alle wichtigen Informationen sollten enthalten sein, unnötige Redundanz dagegen vermieden werden.\nAbzugeben sind am Ende (a) Ein lauffähiges R-Skript; (b) begründeter Lösungsweg (Kombination aus R-Code, R Output und dessen Interpretation) und (c) ausformulierter Methoden- und Ergebnisteil (für eine wiss. Arbeit).\n\n\nkommentierter Loesungsweg\n\nsummary(decay)\n\n      time          amount       \n Min.   : 0.0   Min.   :  8.196  \n 1st Qu.: 7.5   1st Qu.: 21.522  \n Median :15.0   Median : 35.015  \n Mean   :15.0   Mean   : 42.146  \n 3rd Qu.:22.5   3rd Qu.: 57.460  \n Max.   :30.0   Max.   :125.000  \n\nstr(decay)\n\n'data.frame':   31 obs. of  2 variables:\n $ time  : int  0 1 2 3 4 5 6 7 8 9 ...\n $ amount: num  125 100.2 70 83.5 100 ...\n\n\nMan erkennt, dass es 31 Beobachtungen für die Zeit als Integer von Zerfällen gibt, die als rationale Zahlen angegeben werden (dass die Zahl der Zerfälle nicht ganzzahlig ist, deutet darauf hin, dass sie möglicherweise nur in einem Teil des Zeitintervalls oder für einen Teil des betrachteten Raumes gemessen und dann hochgerechnet wurde.\nExplorative Datenanalyse\n\nboxplot(decay$time)\n\n\n\nboxplot(decay$amount)\n\n\n\nplot(amount~time, data=decay)\n\n\n\n\nWährend der Boxplot für time wunderbar symmetrisch ohne Ausreisser ist, zeigt amount eine stark rechtsschiefe (linkssteile) Verteilung mit einem Ausreiser. Das deutet schon an, dass ein einfaches lineares Modell vermutlich die Modellannahmen verletzen wird. Auch der einfache Scatterplot zeigt, dass ein lineares Modell wohl nicht adäquat ist. Wir rechnen aber erst einmal weiter.\nEinfaches lineares Modell\n\nlm.1 <- lm(amount~time, data = decay)\nsummary(lm.1)\n\n\nCall:\nlm(formula = amount ~ time, data = decay)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.065 -10.029  -2.058   5.107  40.447 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  84.5534     5.0277   16.82  < 2e-16 ***\ntime         -2.8272     0.2879   -9.82 9.94e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.34 on 29 degrees of freedom\nMultiple R-squared:  0.7688,    Adjusted R-squared:  0.7608 \nF-statistic: 96.44 on 1 and 29 DF,  p-value: 9.939e-11\n\n\nDas sieht erst einmal nach einem Supermodell aus, höchstsignifikant und mit einem hohen R² von fast 77%. ABER: wir müssen uns noch die Modelldiagnostik ansehen…\nModelldiagnostik\n\npar(mfrow = c(2, 2))\nplot(lm.1)\n\n\n\n\nHier zeigen die wichtigen oberen Plots beide massive Abweichungen vom „Soll“. Der Plot oben links zeigt eine „Banane“ und beim Q-Q-Plot oben rechts weichen die Punkte rechts der Mitte alle stark nach oben von der Solllinie ab. Wir haben unser Modell also offensichtlich falsch spezifiziert. Um eine Idee zu bekommen, was falsch ist, plotten wir noch, wie das Ergebnis dieses Modells aussähe:\nErgebnisplot\n\npar(mfrow = c(1, 1))\nplot(decay$time, decay$amount)\nabline(lm.1, col = \"red\")\n\n\n\n\nDie Punkte links liegen alle über der Regressionslinie, die in der Mitte darunter und die ganz rechts wieder systematisch darüber (darum im Diagnostikplot oben die „Banane“). Es liegt also offensichtlich keine lineare Beziehung vor, sondern eine curvilineare.\nUm diese korrekt zu analysieren, gibt es im Prinzip drei Möglichkeiten, wovon am zweiten Kurstag nur eine hatten, während die zweite und dritte in Statistik 3 und 4 folgten. Im Folgenden sind alle drei nacheinander dargestellt (in der Klausur würde es aber genügen, eine davon darzustellen, wenn die Aufgabenstellung wie oben lautet).\nVariante (1): Lineares Modell nach Transformation der abhängigen Variablen\nDass die Verteilung der abhängigen Variable nicht normal ist, haben wir ja schon bei der explorativen Datenanalyse am Anfang gesehen. Da sie stark linkssteil ist, zugleich aber keine Nullwerte enthält, bietet sich eine Logarithmustransformation an, hier z. B. mit dem natürlichen Logarithmus.\nLoesung 1: log-Transformation der abhaengigen Variablen\n\npar(mfrow = c(1, 2))\nboxplot(decay$amount)\nboxplot(log(decay$amount))\n\n\n\nhist(decay$amount)\nhist(log(decay$amount))\n\n\n\n\nDie log-transformierte Variante rechts sieht sowohl im Boxplot als auch im #Histogramm viel symmetrischer/besser normalverteilt aus. Damit ergibt sich #dann folgendes lineares Modell\n\nlm.2 <- lm(log(amount)~time, data = decay)\nsummary(lm.2)\n\n\nCall:\nlm(formula = log(amount) ~ time, data = decay)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.5935 -0.2043  0.0067  0.2198  0.6297 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  4.547386   0.100295   45.34  < 2e-16 ***\ntime        -0.068528   0.005743  -11.93 1.04e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.286 on 29 degrees of freedom\nMultiple R-squared:  0.8308,    Adjusted R-squared:  0.825 \nF-statistic: 142.4 on 1 and 29 DF,  p-value: 1.038e-12\n\n\nJetzt ist der R²-Wert noch höher und der p-Wert noch niedriger als im ursprünglichen linearen Modell ohne Transformation. Das erlaubt aber keine Aussage, da wir Äpfel mit Birnen vergleichen, da die abhängige Variable einmal untransformiert und einmal log-transformiert ist. Entscheidend ist die Modelldiagnostik.\nModelldiagnostik\n\npar(mfrow = c(2, 2))\nplot(lm.2)\n\n\n\n\nDer Q-Q-Plot sieht jetzt exzellent aus, der Plot rechts oben hat kaum noch eine Banane, nur noch einen leichten Keil. Insgesamt deutlich besser und auf jeden Fall ein statistisch korrektes Modell.\nLösungen 2 und 3 greifen auf Methoden von Statistik 3 und 4 zurück, sie sind hier nur zum Vergleich angeführt\nLoesung 2: quadratische Regression (kam erst in Statistik 3; koente fuer die Datenverteilung passen, entspricht aber nicht der physikalischen\nGesetzmaessigkeit\n\nmodel.quad <- lm(amount~time + I(time^2), data=  decay)\nsummary(model.quad)\n\n\nCall:\nlm(formula = amount ~ time + I(time^2), data = decay)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-22.302  -6.044  -1.603   4.224  20.581 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 106.38880    4.65627  22.849  < 2e-16 ***\ntime         -7.34485    0.71844 -10.223 5.90e-11 ***\nI(time^2)     0.15059    0.02314   6.507 4.73e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.205 on 28 degrees of freedom\nMultiple R-squared:  0.908, Adjusted R-squared:  0.9014 \nF-statistic: 138.1 on 2 and 28 DF,  p-value: 3.122e-15\n\n\nHier können wir R² mit dem ursprünglichen Modell vergleichen (beide haben amount als abhängige Grösse) und es sieht viel besser aus. Sowohl der lineare als auch der quadratische Term sind hochsignifikant. Sicherheitshalber vergleichen wir die beiden Modelle aber noch mittels ANOVA.\nVergleich mit dem einfachen Modell mittels ANOVA (es ginge auch AICc)\n\nanova(lm.1, model.quad)\n\nAnalysis of Variance Table\n\nModel 1: amount ~ time\nModel 2: amount ~ time + I(time^2)\n  Res.Df    RSS Df Sum of Sq      F    Pr(>F)    \n1     29 5960.6                                  \n2     28 2372.6  1    3588.1 42.344 4.727e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn der Tat ist das komplexere Modell (jenes mit dem quadratischen Term) höchstsignifikant besser. Jetzt brauchen wir noch die Modelldiagnostik.\nModelldiagnostik\n\npar(mfrow = c(2, 2))\nplot(model.quad)\n\n\n\n\nLoesung 3 (die beste, hatten wir aber am 2. Tag noch nicht; mit Startwerten muss man ggf. ausprobieren)\nmit Startwerten muss man ggf. ausprobieren)\n\nmodel.nls <- nls(amount~a*exp(-b*time), start=(list(a = 100, b = 1)),data = decay)\nsummary(model.nls)\n\n\nFormula: amount ~ a * exp(-b * time)\n\nParameters:\n   Estimate Std. Error t value Pr(>|t|)    \na 1.081e+02  4.993e+00   21.66  < 2e-16 ***\nb 8.019e-02  5.833e-03   13.75 3.12e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.243 on 29 degrees of freedom\n\nNumber of iterations to convergence: 8 \nAchieved convergence tolerance: 7.976e-06\n\n\nModelldiagnostik\n\nif(!require(nlstools)){install.packages(\"nlstools\")}\n\nLoading required package: nlstools\n\n\n\n'nlstools' has been loaded.\n\n\nIMPORTANT NOTICE: Most nonlinear regression models and data set examples\n\n\nrelated to predictive microbiolgy have been moved to the package 'nlsMicrobio'\n\nlibrary(nlstools)\nresiduals.nls <- nlsResiduals(model.nls)\nplot(residuals.nls)\n\n\n\n\nFür nls kann man nicht den normalen Plotbefehl für die Residualdiagnostik nehmen, sondern verwendet das Äquivalent aus nlstools. Die beiden entscheidenden Plots sind jetzt links oben und rechts unten. Der QQ-Plot hat im unteren Bereich einen kleinen Schönheitsfehler, aber ansonsten ist alles OK.\nDa alle drei Lösungen zumindest statistisch OK waren, sollen jetzt noch die zugehörigen Ergebnisplots erstellt werden.\nErgebnisplots\n\npar(mfrow = c(1, 1))\nxv <- seq(0, 30, 0.1)\n\n\nlineares Modell mit log-transformierter Abhaengiger\n\n\nplot(decay$time, decay$amount)\nyv1 <- exp(predict(lm.2, list(time = xv)))\nlines(xv, yv1, col = \"red\")\n\n\n\n\n\nquadratisches Modell\n\n\nplot(decay$time, decay$amount)\nyv2 <- predict(model.quad, list(time = xv))\nlines(xv, yv2, col=  \"blue\")\n\n\n\n\n\nnicht-lineares Modell\n\n\nplot(decay$time, decay$amount)\nyv3 <- predict(model.nls, list(time = xv))\nlines(xv, yv3, col = \"green\")\n\n\n\n\nOptisch betrachtet, geben (2) und (3) den empirischen Zusammenhang etwas besser wieder als (1), da sie im linken Bereich die hohen Werte besser treffen. Man könnte sogar meinen, bei Betrachtung der Daten, dass die Werte ab time = 28 wieder leicht ansteigen, was die quadratische Funktion wiedergibt. Wer sich aber mit Physik etwas auskennt, weiss, dass Version (2) physikalisch nicht zutrifft, da die Zerfallsrate mit der Zeit immer weiter abfällt. Aufgrund der kurzen Messreihe wäre eine quadratische Funktion trotzdem eine statistisch korrekte Interpretation. Mit längeren Messreihen würde sich jedoch schnell zeigen, dass sie nicht zutrifft."
  },
  {
    "objectID": "stat1-4/Statistik2_Uebung.html",
    "href": "stat1-4/Statistik2_Uebung.html",
    "title": "Stat2: Übung",
    "section": "",
    "text": "Abzugeben sind am Ende\na. lauffähiges R-Skript\nb. begründeter Lösungsweg (Kombination aus R-Code, R Output \n   und dessen Interpretation)\nc. ausformulierter Methoden- und Ergebnisteil (für eine wiss.Arbeit).\n\nBitte erklärt und begründet die einzelnen Schritte, die ihr unternehmt, um zu eurem Ergebnis zu kommen. Dazu erstellt bitte ein Word-Dokument, in dem ihr Schritt für Schritt den verwendeten R-Code, die dazu gehörigen Ausgaben von R, eure Interpretation derselben und die sich ergebenden Schlussfolgerungen für das weitere Vorgehen dokumentiert.\nDieser Ablauf sollte insbesondere beinhalten:\n\nÜberprüfen der Datenstruktur nach dem Einlesen, welches sind die abhängige(n) und welches die unabängige(n) Variablen etc.\nExplorative Datenanalyse, um zu sehen, ob evtl. Dateneingabefehler vorliegen oder Datentransformationen vorgenommen werden sollten\nAuswahl und Begründung eines statistischen Verfahrens\nBestimmung des vollständigen/maximalen Models\nSelektion des/der besten Models/Modelle\nDurchführen der Modelldiagnostik für dieses\nGenerieren aller Zahlen, Statistiken und Tabellen, die für eine wiss. Ergebnisdarstellung benötigt werden\n\nFormuliert abschliessend einen Methoden- und Ergebnisteil (ggf. incl. adäquaten Abbildungen/Tabellen) zu dieser Untersuchung in der Form einer wissenschaftlichen Arbeit (je einen ausformulierten Absatz von ca. 60-100 Worten bzw. 3-8 Sätzen). Alle wichtigen Informationen sollten enthalten sein, unnötige Redundanz dagegen vermieden werden.\n\n\n\nÜbung 2.1: Regression\nRegressionsanalyse mit SAR.csv\nDer Datensatz beschreibt die Zunahme der Artenzahlen (richness) von Pflanzen in Trockenrasen der Schweiz in Abhängigkeit von der Probeflächengrösse (area, hier in m²). Diese Beziehung bezeichnet man als Artenzahl-Areal-Kurve (Species-area relationship = SAR).\n\nLadet den Datensatz in R und macht eine explorative Datenanalyse.\nWählt unter den schon gelernten Methoden der Regressionsanalyse ein adäquates Vorgehen zur Analyse dieser Daten und führt diese dann durch.\nPrüft anhand der Residuen, ob die Modellvoraussetzungen erfüllt waren\nFalls die Modelldiagnostik negativ ausfällt, überlegt, welche Datentransformation helfen könnte, und rechnet neue Modelle mit einer oder ggf. mehreren Datentransformationen, bis ihr eine statistisch zufriedenstellende Lösung gefunden habt.\nStellt die erhaltenen Ergebnisse angemessen dar (Text, Abbildung und/oder Tabelle).\nKennt ihr ggf. noch eine andere geeignete Herangehensweise?\n\n\n\nÜbung 2.2: Einfaktorielle ANOVA\nANOVA mit novanimal_agg.csv\nFührt mit dem Datensatz novanimal.csv eine einfaktorielle ANOVA durch. Gibt es Unterschiede zwischen der Anzahl verkaufter Gerichte (Buffet, Fleisch oder Vegetarisch) pro Woche?\nHinweise für die Analysen:\n\nFasst die vier Inhalte der Gerichte zu drei Inhalten zusammen. Das heisst, dass die pflanzlichen Gerichte neu zu den vegetarischen Gerichten gezählt werden. Konkret könnt ihr das in R mit einer Umbenennung der Inhalte durchführen (z. B. mit stringr::str_replace()).\nDanach muss der Datensatz gruppiert und zusammengefasst werden.\nUnbekannte Menüinhalte können vernachlässigt werden.\nWie gut sind die Voraussetzungen für eine ANOVA erfüllt? Wären allenfalls auch nicht-parametrische Analysen zulässig?\nFührt anschliessend Post-hoc-Vergleiche durch.\nFasst die Ergebnisse in wenigen Sätzen zusammen und stellst die angemessen dar (Text mit Abbildung und/oder Tabelle)\n\n\n\nÜbung 2.3N: Mehrfaktorielle ANOVA (NatWis)\nANOVA mit kormoran.csv\nDer Datensatz enthält 40 Beobachtungen zu Tauchzeiten zweier Kormoranunterarten (C = Phalocrocorax carbo carbo und S = Phalacrocorax carbo sinensis) aus vier Jahreszeiten (F = Frühling, S = Sommer, H = Herbst, W = Winter).\n\nLest den Datensatz nach R ein und führt eine adäquate Analyse durch, um beantworten zu können, wie Unterart und Jahreszeit die Tauchzeit beeinflussen.\nStellt eure Ergebnisse dann angemessen dar (Text mit Abbildung und/oder Tabelle).\nGibt es eine Interaktion?\n\n\n\nÜbung 2.3S: Mehrfaktorielle ANOVA mit Interaktion (SozWis)\nANOVA mit novanimal_indiv.csv\nIn der Mensa gibt es zwei unterschiedliche Preisniveaus bzgl. den Gerichten: eine preisgünstigere Menülinie (“World” & “Favorite”) und eine teuere Menülinie (“Kitchen”). Gibt es Unterschiede zwischen dem Kauf von preisgünstigeren resp. teureren Menülinien betreffend Menüinhalt & Hochschulzugehörigkeit?\nHinweise für die Analysen:\n\nFasst die zwei günstigeren Menülinien “Favorite” & “World” zu einer Menülinie zusammen. Konkret könnt ihr das in R mit einer Umbenennung der Inhalte durchführen (z. B. mit stringr::str_replace() oder base::sub()).\nKleiner Hinweis: “Local” Gerichte könnt ihr zu den anderen Gerichten dazu zählen z.B. Local Favorite -> Favorite\nDanach muss der Datensatz gruppiert (nach Menülinie & Hochschulzugehörigkeit) und zusammengefasst werden.\nUnbekannte Menüinhalte können vernachlässigt werden.\nWie gut sind die Voraussetzungen für eine ANOVA erfüllt? Wären allenfalls auch nicht-parametrische Analysen zulässig?\nFührt anschliessend Post-hoc-Vergleiche durch.\nStellt eure Ergebnisse dann angemessen dar (Text mit Abbildung und/oder Tabelle)."
  },
  {
    "objectID": "stat1-4/Statistik3_Demo.html#ancova",
    "href": "stat1-4/Statistik3_Demo.html#ancova",
    "title": "Stat3: Demo",
    "section": "ANCOVA",
    "text": "ANCOVA\nExperiment zur Fruchtproduktion (“Fruit”) von Ipomopsis sp. (“Fruit”) in Abhängigkeit Ungrazedvon der Beweidung (Grazing mit 2 Levels: Grazed, Ungrazed) und korrigiert für die Pflanzengrösse vor der Beweidung (hier ausgedrückt als Durchmesser an der Spitze des Wurzelstock: “Root”)\n\ncompensation <- read.delim(here(\"data\",\"ipomopsis.csv\"), sep = \",\", stringsAsFactors = T)\n\n\nsummary(compensation)\n\n       X              Root            Fruit            Grazing  \n Min.   : 1.00   Min.   : 4.426   Min.   : 14.73   Grazed  :20  \n 1st Qu.:10.75   1st Qu.: 6.083   1st Qu.: 41.15   Ungrazed:20  \n Median :20.50   Median : 7.123   Median : 60.88                \n Mean   :20.50   Mean   : 7.181   Mean   : 59.41                \n 3rd Qu.:30.25   3rd Qu.: 8.510   3rd Qu.: 76.19                \n Max.   :40.00   Max.   :10.253   Max.   :116.05                \n\nplot(Fruit~Root, data = compensation)\n\n\n\nboxplot(Fruit~Grazing, data = compensation)\n\ntapply(compensation$Fruit, compensation$Grazing, mean)\n\n  Grazed Ungrazed \n 67.9405  50.8805 \n\naoc.1 <- lm(Fruit~Root * Grazing, data = compensation)\nsummary.aov(aoc.1)\n\n             Df Sum Sq Mean Sq F value   Pr(>F)    \nRoot          1  16795   16795 359.968  < 2e-16 ***\nGrazing       1   5264    5264 112.832 1.21e-12 ***\nRoot:Grazing  1      5       5   0.103     0.75    \nResiduals    36   1680      47                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\naoc.2 <- lm(Fruit~Grazing * Root, data = compensation)\nsummary.aov(aoc.2)\n\n             Df Sum Sq Mean Sq F value   Pr(>F)    \nGrazing       1   2910    2910  62.380 2.26e-09 ***\nRoot          1  19149   19149 410.420  < 2e-16 ***\nGrazing:Root  1      5       5   0.103     0.75    \nResiduals    36   1680      47                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\naoc.3 <- lm(Fruit~Grazing + Root, data = compensation)\nsummary.lm(aoc.3)\n\n\nCall:\nlm(formula = Fruit ~ Grazing + Root, data = compensation)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17.1920  -2.8224   0.3223   3.9144  17.3290 \n\nCoefficients:\n                Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     -127.829      9.664  -13.23 1.35e-15 ***\nGrazingUngrazed   36.103      3.357   10.75 6.11e-13 ***\nRoot              23.560      1.149   20.51  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.747 on 37 degrees of freedom\nMultiple R-squared:  0.9291,    Adjusted R-squared:  0.9252 \nF-statistic: 242.3 on 2 and 37 DF,  p-value: < 2.2e-16\n\n# Plotten der Ergebnisse\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n✔ tibble  3.1.8     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.1\n✔ readr   2.1.2     ✔ forcats 0.5.2\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\n\n\nggplot(compensation, aes(Root, Fruit, color = Grazing)) +\n  geom_point() + theme_classic()\n\n\n\n# Ploten mit base R\nplot(Fruit~Root, pch = 16, col = Grazing, data = compensation)\nlegend(\"topleft\", c(\"grazed\", \"ungrazed\"), col = c(\"black\",\"red\"), pch = 16)"
  },
  {
    "objectID": "stat1-4/Statistik3_Demo.html#section",
    "href": "stat1-4/Statistik3_Demo.html#section",
    "title": "Stat3: Demo",
    "section": "",
    "text": "e <- c(20, 19, 25, 10, 8, 15, 13, 18, 11, 14, 25, 39, 38, 28, 24)\nf <- c(12, 15, 10, 7, 2, 10, 12, 11, 13, 10, 9, 2, 4, 7, 13)\n\nlm.1 <- lm(f~e)\nlm.quad <- lm(f~e + I(e^2))\n\nsummary(lm.1)\n\n\nCall:\nlm(formula = f ~ e)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.0549 -1.7015  0.5654  2.0617  5.6406 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  12.2879     2.4472   5.021 0.000234 ***\ne            -0.1541     0.1092  -1.412 0.181538    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.863 on 13 degrees of freedom\nMultiple R-squared:  0.1329,    Adjusted R-squared:  0.06622 \nF-statistic: 1.993 on 1 and 13 DF,  p-value: 0.1815\n\nsummary(lm.quad)\n\n\nCall:\nlm(formula = f ~ e + I(e^2))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.3866 -1.1018 -0.2027  1.3831  4.4211 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)   \n(Intercept) -2.239308   3.811746  -0.587  0.56777   \ne            1.330933   0.360105   3.696  0.00306 **\nI(e^2)      -0.031587   0.007504  -4.209  0.00121 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.555 on 12 degrees of freedom\nMultiple R-squared:  0.6499,    Adjusted R-squared:  0.5915 \nF-statistic: 11.14 on 2 and 12 DF,  p-value: 0.001842\n\npar(mfrow = c(1, 1))\n\n# 1. lineares Modell\nplot(f~e, xlim = c(0, 40), ylim = c(0, 20))\nabline(lm(f~e), col = \"blue\")\n\n\n\n# 2. quadratisches Modell\nxv <- seq(0, 40, 0.1)\nplot(f~e, xlim = c(0, 40), ylim = c(0, 20))\nyv2 <- predict(lm.quad, list(e = xv))\nlines(xv, yv2, col = \"red\")\n\n\n\n# Residualplots\npar(mfrow = c(2, 2))\nplot(lm.1)\n\n\n\nplot(lm.quad)"
  },
  {
    "objectID": "stat1-4/Statistik3_Demo.html#multiple-lineare-regression",
    "href": "stat1-4/Statistik3_Demo.html#multiple-lineare-regression",
    "title": "Stat3: Demo",
    "section": "Multiple lineare Regression",
    "text": "Multiple lineare Regression\n\nSimulation Overfitting\n\ntest <- data.frame(\"x\" = c(1, 2, 3, 4, 5, 6), \"y\" = c(34, 21, 70, 47, 23, 45))\n\npar(mfrow=c(1,1))\nplot(y~x, data = test)\n\nlm.0 <- lm(y~1, data = test)\nlm.1 <- lm(y~x, data = test)\nlm.2 <- lm(y~x+ I(x^2), data = test)\nlm.3 <- lm(y~x+ I(x^2) + I(x^3), data = test)\nlm.4 <- lm(y~x+ I(x^2) + I(x^3) + I(x^4), data = test)\nlm.5 <- lm(y~x+ I(x^2) + I(x^3) + I(x^4) + I(x^5), data = test)\nlm.6 <- lm(y~x+ I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6), data = test)\nsummary(lm.0)\nsummary(lm.1)\nsummary(lm.2)\nsummary(lm.3)\nsummary(lm.4)\nsummary(lm.5)\n\nxv <- seq(from = 0, to = 10, by = 0.1)\n\nplot(y~x, cex = 2, col = \"black\", lwd = 3, data = test)\nyv <- predict(lm.1, list(x = xv))\nlines(xv, yv, col = \"red\", lwd = 3)\nyv <- predict(lm.2, list(x = xv))\nlines(xv, yv, col = \"blue\", lwd = 3)\nyv<-predict(lm.3, list(x = xv))\nlines(xv, yv, col = \"green\", lwd =3)\nyv <- predict(lm.4, list(x = xv))\nlines(xv, yv, col = \"orange\", lwd = 3)\nyv <- predict(lm.5, list(x = xv))\nlines(xv, yv, col = \"black\", lwd = 3)\n\n\n\nMultiple lineare Regression basierend auf Logan, Beispiel 9A\n\nloyn <- read.delim(here(\"data\",\"loyn.csv\"), sep = \",\")\nsummary(loyn)\n\n       X             ABUND            AREA            YR.ISOL    \n Min.   : 1.00   Min.   : 1.50   Min.   :   0.10   Min.   :1890  \n 1st Qu.:14.75   1st Qu.:12.40   1st Qu.:   2.00   1st Qu.:1928  \n Median :28.50   Median :21.05   Median :   7.50   Median :1962  \n Mean   :28.50   Mean   :19.51   Mean   :  69.27   Mean   :1950  \n 3rd Qu.:42.25   3rd Qu.:28.30   3rd Qu.:  29.75   3rd Qu.:1966  \n Max.   :56.00   Max.   :39.60   Max.   :1771.00   Max.   :1976  \n      DIST            LDIST            GRAZE            ALT       \n Min.   :  26.0   Min.   :  26.0   Min.   :1.000   Min.   : 60.0  \n 1st Qu.:  93.0   1st Qu.: 158.2   1st Qu.:2.000   1st Qu.:120.0  \n Median : 234.0   Median : 338.5   Median :3.000   Median :140.0  \n Mean   : 240.4   Mean   : 733.3   Mean   :2.982   Mean   :146.2  \n 3rd Qu.: 333.2   3rd Qu.: 913.8   3rd Qu.:4.000   3rd Qu.:182.5  \n Max.   :1427.0   Max.   :4426.0   Max.   :5.000   Max.   :260.0  \n\n\n\n\nKorrelation zwischen den Prädiktoren\n\ncor <- cor(loyn[, 2:7])\nprint(cor, digits = 2)\n\n         ABUND    AREA YR.ISOL  DIST  LDIST  GRAZE\nABUND    1.000  0.2560  0.5034  0.24  0.087 -0.683\nAREA     0.256  1.0000 -0.0015  0.11  0.035 -0.310\nYR.ISOL  0.503 -0.0015  1.0000  0.11 -0.083 -0.636\nDIST     0.236  0.1083  0.1132  1.00  0.317 -0.256\nLDIST    0.087  0.0346 -0.0833  0.32  1.000 -0.028\nGRAZE   -0.683 -0.3104 -0.6356 -0.26 -0.028  1.000\n\ncor[abs(cor)<0.6] <- 0\ncor\n\n             ABUND AREA    YR.ISOL DIST LDIST      GRAZE\nABUND    1.0000000    0  0.0000000    0     0 -0.6825114\nAREA     0.0000000    1  0.0000000    0     0  0.0000000\nYR.ISOL  0.0000000    0  1.0000000    0     0 -0.6355671\nDIST     0.0000000    0  0.0000000    1     0  0.0000000\nLDIST    0.0000000    0  0.0000000    0     1  0.0000000\nGRAZE   -0.6825114    0 -0.6355671    0     0  1.0000000\n\nprint(cor, digits = 3)\n\n         ABUND AREA YR.ISOL DIST LDIST  GRAZE\nABUND    1.000    0   0.000    0     0 -0.683\nAREA     0.000    1   0.000    0     0  0.000\nYR.ISOL  0.000    0   1.000    0     0 -0.636\nDIST     0.000    0   0.000    1     0  0.000\nLDIST    0.000    0   0.000    0     1  0.000\nGRAZE   -0.683    0  -0.636    0     0  1.000\n\nlm.1 <- lm (ABUND ~ YR.ISOL + ALT + GRAZE, data = loyn)\nif(!require(car)){install.packages(\"car\")} \n\nLoading required package: car\n\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nlibrary(car)\nvif(lm.1)\n\n YR.ISOL      ALT    GRAZE \n1.679995 1.200372 1.904799 \n\ninfluence.measures(lm.1)\n\nInfluence measures of\n     lm(formula = ABUND ~ YR.ISOL + ALT + GRAZE, data = loyn) :\n\n      dfb.1_  dfb.YR.I   dfb.ALT  dfb.GRAZ     dffit cov.r   cook.d    hat inf\n1   0.128900 -0.136701 -2.25e-02  8.68e-02 -0.455383 0.663 4.64e-02 0.0286   *\n2  -0.046388  0.041396  1.50e-01 -2.15e-02 -0.222873 1.159 1.26e-02 0.0996    \n3  -0.178685  0.184085 -5.40e-02 -4.08e-02 -0.298379 1.108 2.23e-02 0.0901    \n4   0.054207 -0.053864 -2.43e-02 -4.06e-02 -0.085906 1.099 1.87e-03 0.0331    \n5   0.032249 -0.035235  3.34e-02  6.56e-02  0.138294 1.123 4.85e-03 0.0597    \n6   0.072550 -0.075381  3.68e-02 -3.40e-02 -0.129304 1.072 4.22e-03 0.0315    \n7   0.153153 -0.155477  8.56e-02 -1.78e-01 -0.263831 1.139 1.75e-02 0.0978    \n8  -0.044533  0.039741  1.44e-01 -2.07e-02 -0.213965 1.162 1.16e-02 0.0996    \n9   0.305330 -0.305810  1.16e-02 -2.93e-01 -0.412610 0.935 4.12e-02 0.0593    \n10 -0.134119  0.136978 -1.50e-02 -2.15e-02 -0.217402 1.140 1.19e-02 0.0876    \n11  0.145761 -0.154644  2.14e-01 -2.21e-02  0.300565 1.103 2.26e-02 0.0883    \n12 -0.246939  0.255702 -1.22e-01 -2.33e-02 -0.369735 1.161 3.42e-02 0.1318    \n13  0.071832 -0.068266 -1.34e-01 -3.53e-02 -0.191283 1.110 9.23e-03 0.0653    \n14  0.019281 -0.016626 -3.08e-01  1.90e-01 -0.597735 0.810 8.32e-02 0.0692    \n15  0.000311 -0.000315 -2.26e-05  2.96e-05  0.000496 1.184 6.27e-08 0.0874    \n16 -0.131537  0.136111 -5.26e-02 -3.46e-02 -0.223973 1.146 1.27e-02 0.0923    \n17 -0.098856  0.108184 -1.60e-01  1.44e-02  0.266285 0.984 1.75e-02 0.0393    \n18  0.238014 -0.243468  3.85e-02 -1.36e-01 -0.397451 0.753 3.65e-02 0.0293   *\n19 -0.031350  0.029292  5.78e-02  3.66e-02  0.081711 1.121 1.70e-03 0.0460    \n20 -0.024122  0.019709  7.59e-02  5.75e-02 -0.093805 1.170 2.24e-03 0.0829    \n21  0.036050 -0.033748 -7.79e-02 -2.15e-02 -0.102357 1.162 2.66e-03 0.0786    \n22 -0.015768  0.016959  4.26e-03 -6.28e-02 -0.127636 1.116 4.13e-03 0.0531    \n23  0.050368 -0.052333  2.55e-02 -2.36e-02 -0.089769 1.095 2.04e-03 0.0315    \n24 -0.012264  0.008841  5.20e-02  5.07e-02 -0.071851 1.209 1.31e-03 0.1091    \n25  0.145637 -0.146703  2.41e-02 -7.94e-02  0.157322 1.319 6.30e-03 0.1876   *\n26 -0.007372  0.007451  1.67e-03  5.11e-03  0.015793 1.106 6.36e-05 0.0232    \n27  0.043873 -0.045585  2.22e-02 -2.05e-02 -0.078194 1.100 1.55e-03 0.0315    \n28 -0.018037  0.016743  2.82e-02  2.63e-02  0.036688 1.224 3.43e-04 0.1175    \n29 -0.131935  0.133012 -2.20e-02  1.11e-01  0.164334 1.152 6.84e-03 0.0836    \n30  0.094249 -0.092478 -8.47e-02  1.81e-02  0.210983 1.127 1.12e-02 0.0790    \n31  0.118899 -0.130120  1.93e-01 -1.73e-02 -0.320276 0.928 2.49e-02 0.0393    \n32 -0.103130  0.098781  9.40e-02  1.33e-01  0.170699 1.126 7.37e-03 0.0690    \n33 -0.284839  0.290760 -1.33e-01  2.50e-01  0.433995 0.919 4.54e-02 0.0602    \n34 -0.213008  0.199453  2.95e-01  3.01e-01  0.408017 1.071 4.12e-02 0.1008    \n35  0.068874 -0.066760 -1.35e-01 -3.57e-03 -0.246916 1.008 1.51e-02 0.0407    \n36 -0.151383  0.159324 -1.23e-01  5.71e-02  0.283014 0.959 1.96e-02 0.0376    \n37  0.022901 -0.022520 -3.21e-02  3.25e-02  0.103312 1.136 2.71e-03 0.0605    \n38 -0.001488  0.001427  3.83e-03 -1.89e-04  0.006929 1.125 1.22e-05 0.0393    \n39 -0.299662  0.296262  7.86e-02  2.86e-01  0.365529 1.060 3.31e-02 0.0860    \n40  0.045779 -0.044212 -7.15e-02  3.70e-02  0.168859 1.126 7.21e-03 0.0685    \n41 -0.043463  0.037744  6.26e-02  1.22e-01 -0.153196 1.126 5.94e-03 0.0653    \n42 -0.067499  0.070133 -3.42e-02  3.16e-02  0.120302 1.078 3.66e-03 0.0315    \n43  0.002552 -0.002850 -1.05e-02  1.52e-02 -0.036428 1.143 3.38e-04 0.0558    \n44  0.011473 -0.009053 -3.51e-02 -3.92e-02  0.052676 1.192 7.07e-04 0.0953    \n45  0.002848  0.003165 -8.61e-02 -6.95e-02  0.137899 1.092 4.81e-03 0.0427    \n46 -0.116776  0.109111  2.15e-01  1.36e-01  0.304366 0.977 2.28e-02 0.0460    \n47  0.445830 -0.431209 -2.69e-01 -3.41e-01  0.629701 0.642 8.76e-02 0.0483   *\n48 -0.000133  0.004718  4.46e-02 -1.58e-01  0.302736 1.002 2.26e-02 0.0520    \n49  0.008724 -0.006876 -2.00e-02 -3.60e-02  0.048292 1.150 5.94e-04 0.0626    \n50  0.019369 -0.017688 -5.80e-03 -5.14e-02  0.069197 1.136 1.22e-03 0.0548    \n51 -0.122055  0.122805  7.02e-02  2.13e-02  0.231107 1.022 1.33e-02 0.0408    \n52  0.020580 -0.015671 -8.25e-02 -6.78e-02  0.099679 1.298 2.53e-03 0.1704   *\n53  0.014674 -0.013095 -8.87e-03 -4.28e-02  0.057249 1.139 8.35e-04 0.0549    \n54  0.138452 -0.137403  3.82e-02 -1.54e-01  0.204365 1.168 1.06e-02 0.1011    \n55 -0.000650  0.000535 -4.05e-03  6.97e-03 -0.014242 1.144 5.17e-05 0.0555    \n56  0.021139 -0.021938  2.56e-02 -1.62e-02  0.039541 1.363 3.98e-04 0.2077   *\n\n\n\n\nModellvereinfachung\n\nlm.1 <- lm(ABUND ~ YR.ISOL + ALT + GRAZE, data = loyn)\nsummary(lm.1)\n\n\nCall:\nlm(formula = ABUND ~ YR.ISOL + ALT + GRAZE, data = loyn)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-19.5498  -4.8951   0.6504   4.7798  20.2384 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -73.58185  107.24995  -0.686 0.495712    \nYR.ISOL       0.05143    0.05393   0.954 0.344719    \nALT           0.03285    0.02679   1.226 0.225618    \nGRAZE        -4.01692    0.99881  -4.022 0.000188 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.894 on 52 degrees of freedom\nMultiple R-squared:  0.4887,    Adjusted R-squared:  0.4592 \nF-statistic: 16.57 on 3 and 52 DF,  p-value: 1.106e-07\n\nlm.2 <- update(lm.1,~.-YR.ISOL)\nanova(lm.1, lm.2)\n\nAnalysis of Variance Table\n\nModel 1: ABUND ~ YR.ISOL + ALT + GRAZE\nModel 2: ABUND ~ ALT + GRAZE\n  Res.Df    RSS Df Sum of Sq      F Pr(>F)\n1     52 3240.4                           \n2     53 3297.1 -1   -56.662 0.9093 0.3447\n\nsummary(lm.2)\n\n\nCall:\nlm(formula = ABUND ~ ALT + GRAZE, data = loyn)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-19.1677  -4.8261   0.0266   4.6944  19.1054 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 28.55582    5.43245   5.257 2.67e-06 ***\nALT          0.03191    0.02675   1.193    0.238    \nGRAZE       -4.59679    0.79167  -5.806 3.67e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.887 on 53 degrees of freedom\nMultiple R-squared:  0.4798,    Adjusted R-squared:  0.4602 \nF-statistic: 24.44 on 2 and 53 DF,  p-value: 3.011e-08\n\nlm.3 <- update(lm.2,~.-ALT)\nanova(lm.2, lm.3)\n\nAnalysis of Variance Table\n\nModel 1: ABUND ~ ALT + GRAZE\nModel 2: ABUND ~ GRAZE\n  Res.Df    RSS Df Sum of Sq      F Pr(>F)\n1     53 3297.1                           \n2     54 3385.6 -1   -88.519 1.4229 0.2382\n\nsummary(lm.3)\n\n\nCall:\nlm(formula = ABUND ~ GRAZE, data = loyn)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-19.1066  -5.4097   0.0934   4.4856  18.2747 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  34.3692     2.4095  14.264  < 2e-16 ***\nGRAZE        -4.9813     0.7259  -6.862  6.9e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.918 on 54 degrees of freedom\nMultiple R-squared:  0.4658,    Adjusted R-squared:  0.4559 \nF-statistic: 47.09 on 1 and 54 DF,  p-value: 6.897e-09\n\npar(mfrow = c(2, 2))\nplot(lm.1)\n\n\n\n\n\n\nHierarchical partitioning\n\nif(!require(hier.part)){install.packages(\"hier.part\")}\n\nLoading required package: hier.part\n\nlibrary(hier.part)\n\nloyn.preds <-with(loyn, data.frame(YR.ISOL, ALT, GRAZE))\nhier.part(loyn$ABUND, loyn.preds, gof = \"Rsqu\")\n\n\n\n\n$gfs\n[1] 0.0000000 0.2533690 0.1488696 0.4658218 0.3297010 0.4739432 0.4797883\n[8] 0.4887284\n\n$IJ\n                 I          J     Total\nYR.ISOL 0.11892853 0.13444049 0.2533690\nALT     0.06960132 0.07926823 0.1488696\nGRAZE   0.30019854 0.16562324 0.4658218\n\n$I.perc\n        ind.exp.var\nYR.ISOL    24.33428\nALT        14.24131\nGRAZE      61.42441\n\n$params\n$params$full.model\n[1] \"y ~ YR.ISOL + ALT + GRAZE\"\n\n$params$family\n[1] \"gaussian\"\n\n$params$link\n[1] \"default\"\n\n$params$gof\n[1] \"Rsqu\"\n\n\n\n\nPartial regressions\n\navPlots(lm.1, ask = F)"
  },
  {
    "objectID": "stat1-4/Statistik3_Demo.html#multimodel-inference",
    "href": "stat1-4/Statistik3_Demo.html#multimodel-inference",
    "title": "Stat3: Demo",
    "section": "Multimodel inference",
    "text": "Multimodel inference\n\nif(!require(MuMIn)){install.packages(\"MuMIn\")}\n\nLoading required package: MuMIn\n\nlibrary(MuMIn)\n\nglobal.model <- lm(ABUND ~ YR.ISOL + ALT + GRAZE, data = loyn)\noptions(na.action = \"na.fail\")\n\nallmodels <- dredge(global.model)\n\nFixed term is \"(Intercept)\"\n\nallmodels\n\nGlobal model call: lm(formula = ABUND ~ YR.ISOL + ALT + GRAZE, data = loyn)\n---\nModel selection table \n     (Int)     ALT    GRA  YR.ISO df   logLik  AICc delta weight\n3   34.370         -4.981          3 -194.315 395.1  0.00  0.407\n4   28.560 0.03191 -4.597          4 -193.573 395.9  0.84  0.267\n7  -62.750         -4.440 0.04898  4 -193.886 396.6  1.46  0.196\n8  -73.580 0.03285 -4.017 0.05143  5 -193.087 397.4  2.28  0.130\n6 -348.500 0.07006        0.18350  4 -200.670 410.1 15.03  0.000\n5 -392.300                0.21120  3 -203.690 413.8 18.75  0.000\n2    5.598 0.09515                 3 -207.358 421.2 26.09  0.000\n1   19.510                         2 -211.871 428.0 32.88  0.000\nModels ranked by AICc(x) \n\nimportance(allmodels)\n\nWarning: 'importance' is deprecated.\nUse 'sw' instead.\nSee help(\"Deprecated\")\n\n\nfunction (x) \nUseMethod(\"sw\")\n<bytecode: 0x5612ecc750c8>\n<environment: namespace:MuMIn>\n\navgmodel <- model.avg(allmodels, subset = TRUE)\nsummary(avgmodel)\n\n\nCall:\nmodel.avg(object = allmodels, subset = TRUE)\n\nComponent model call: \nlm(formula = ABUND ~ <8 unique rhs>, data = loyn)\n\nComponent models: \n       df  logLik   AICc delta weight\n2       3 -194.31 395.09  0.00   0.41\n12      4 -193.57 395.93  0.84   0.27\n23      4 -193.89 396.56  1.46   0.20\n123     5 -193.09 397.37  2.28   0.13\n13      4 -200.67 410.13 15.03   0.00\n3       3 -203.69 413.84 18.75   0.00\n1       3 -207.36 421.18 26.09   0.00\n(Null)  2 -211.87 427.97 32.88   0.00\n\nTerm codes: \n    ALT   GRAZE YR.ISOL \n      1       2       3 \n\nModel-averaged coefficients:  \n(full average) \n            Estimate Std. Error Adjusted SE z value Pr(>|z|)    \n(Intercept) -0.29874   77.23966    78.39113   0.004    0.997    \nGRAZE       -4.64605    0.89257     0.91048   5.103    3e-07 ***\nALT          0.01282    0.02311     0.02340   0.548    0.584    \nYR.ISOL      0.01631    0.03883     0.03941   0.414    0.679    \n \n(conditional average) \n            Estimate Std. Error Adjusted SE z value Pr(>|z|)    \n(Intercept) -0.29874   77.23966    78.39113   0.004    0.997    \nGRAZE       -4.64724    0.88957     0.90755   5.121    3e-07 ***\nALT          0.03224    0.02678     0.02741   1.176    0.240    \nYR.ISOL      0.05007    0.05421     0.05548   0.902    0.367    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "stat1-4/Statistik3_Loesung.html#lösung-übung-3.1",
    "href": "stat1-4/Statistik3_Loesung.html#lösung-übung-3.1",
    "title": "Stat3: Lösung",
    "section": "Lösung Übung 3.1",
    "text": "Lösung Übung 3.1\nSchon vor dem Einlesen kürzt man am besten bereits in Excel die Variablennamen so ab, dass sie noch eindeutig, aber nicht unnötig lang sind, etwa indem man die Einheiten wegstreicht\n\n# Aus der Excel-Tabelle wurde das relevante Arbeitsblatt als csv gespeichert\nukraine <- read.delim(here(\"data\",\"Ukraine_bearbeitet.csv\"), sep = \",\")\n\n\nukraine\n\n\nstr(ukraine)\n\n'data.frame':   199 obs. of  24 variables:\n $ X                : int  1 2 3 4 5 6 7 8 9 10 ...\n $ PlotID           : chr  \"UA01NW\" \"UA01SE\" \"UA02NW\" \"UA02SE\" ...\n $ Species_richness : int  44 53 48 50 53 40 46 56 30 35 ...\n $ Altitude         : int  179 178 188 183 162 165 153 158 192 197 ...\n $ Inclination      : int  24 17 27 33 7 33 30 32 25 18 ...\n $ Heat_index       : num  -0.42 -0.3 -0.51 -0.65 -0.09 -0.42 0 -0.59 0.46 0.32 ...\n $ Microrelief      : num  5 2.5 2 2 3 4 16 15 5 3 ...\n $ Grazing_intensity: int  0 0 0 0 0 0 1 1 0 0 ...\n $ Litter           : int  12 10 0 4 15 30 5 6 10 20 ...\n $ Stones_and_rocks : num  0 0 0 0 0 0 40 10 0 0 ...\n $ Gravel           : num  0 0 0 0 0 0 0 0 0 0 ...\n $ Fine_soil        : num  2 5 0 7 0 0 2 5 5 2 ...\n $ Sand             : int  45 30 20 20 55 30 10 30 10 5 ...\n $ Silt             : int  40 35 60 60 10 35 60 35 60 90 ...\n $ Clay             : int  15 35 20 20 35 35 30 35 30 5 ...\n $ pH               : num  7.32 6.91 6.72 6.44 6.1 6.23 6.79 6.43 7.19 7 ...\n $ Conductivity     : int  90 115 126 90 73 76 163 119 151 69 ...\n $ CaCO3            : num  0.0754 0.1271 0.0723 0.0771 0.0829 ...\n $ N_total          : num  0.14 0.17 0.24 0.26 0.29 0.2 0.34 0.29 0.18 0.2 ...\n $ C_org            : num  1.54 1.97 2.99 3.22 3.77 2.5 4.59 3.67 2.16 2.38 ...\n $ CN_ratio         : num  10.8 11.5 12.5 12.6 12.8 ...\n $ Temperature      : int  79 79 80 80 82 82 82 82 79 83 ...\n $ Temperature_range: int  330 330 329 329 328 328 328 328 326 327 ...\n $ Precipitation    : int  608 608 603 603 594 594 594 594 600 586 ...\n\nsummary(ukraine)\n\n       X            PlotID          Species_richness    Altitude    \n Min.   :  1.0   Length:199         Min.   :14.00    Min.   : 73.0  \n 1st Qu.: 50.5   Class :character   1st Qu.:34.00    1st Qu.:140.0  \n Median :100.0   Mode  :character   Median :40.00    Median :166.0  \n Mean   :100.0                      Mean   :40.23    Mean   :161.7  \n 3rd Qu.:149.5                      3rd Qu.:47.50    3rd Qu.:188.0  \n Max.   :199.0                      Max.   :67.00    Max.   :251.0  \n                                                                    \n  Inclination      Heat_index        Microrelief      Grazing_intensity\n Min.   : 1.00   Min.   :-0.94000   Min.   :  0.000   Min.   :0.0000   \n 1st Qu.:12.00   1st Qu.:-0.15500   1st Qu.:  2.500   1st Qu.:0.0000   \n Median :19.00   Median : 0.01000   Median :  5.000   Median :1.0000   \n Mean   :19.28   Mean   : 0.01603   Mean   :  7.126   Mean   :0.9296   \n 3rd Qu.:25.00   3rd Qu.: 0.21500   3rd Qu.:  7.000   3rd Qu.:2.0000   \n Max.   :48.00   Max.   : 0.85000   Max.   :100.000   Max.   :3.0000   \n                                                                       \n     Litter      Stones_and_rocks     Gravel         Fine_soil    \n Min.   : 0.00   Min.   : 0.000   Min.   : 0.000   Min.   : 0.00  \n 1st Qu.: 3.50   1st Qu.: 0.000   1st Qu.: 0.000   1st Qu.: 2.00  \n Median : 7.00   Median : 0.500   Median : 0.000   Median : 5.00  \n Mean   :12.16   Mean   : 3.994   Mean   : 2.984   Mean   : 7.02  \n 3rd Qu.:13.50   3rd Qu.: 4.000   3rd Qu.: 3.000   3rd Qu.:10.00  \n Max.   :90.00   Max.   :68.000   Max.   :40.000   Max.   :38.00  \n                                                                  \n      Sand            Silt            Clay             pH       \n Min.   : 5.00   Min.   : 5.00   Min.   : 5.00   Min.   :4.890  \n 1st Qu.:20.00   1st Qu.:20.00   1st Qu.:20.00   1st Qu.:7.240  \n Median :30.00   Median :35.00   Median :20.00   Median :7.420  \n Mean   :35.81   Mean   :40.43   Mean   :23.74   Mean   :7.286  \n 3rd Qu.:55.00   3rd Qu.:60.00   3rd Qu.:35.00   3rd Qu.:7.545  \n Max.   :80.00   Max.   :90.00   Max.   :55.00   Max.   :7.790  \n NA's   :1       NA's   :1       NA's   :1                      \n  Conductivity       CaCO3            N_total           C_org       \n Min.   : 40.0   Min.   : 0.0042   Min.   :0.0700   Min.   : 1.040  \n 1st Qu.:148.5   1st Qu.: 0.4306   1st Qu.:0.2000   1st Qu.: 2.850  \n Median :171.0   Median : 4.6578   Median :0.2700   Median : 3.560  \n Mean   :162.3   Mean   : 7.4757   Mean   :0.2788   Mean   : 3.689  \n 3rd Qu.:189.5   3rd Qu.:13.0002   3rd Qu.:0.3300   3rd Qu.: 4.400  \n Max.   :232.0   Max.   :35.2992   Max.   :0.9500   Max.   :11.300  \n                                                                    \n    CN_ratio      Temperature    Temperature_range Precipitation  \n Min.   : 6.04   Min.   :78.00   Min.   :326.0     Min.   :577.0  \n 1st Qu.:12.24   1st Qu.:82.00   1st Qu.:328.0     1st Qu.:583.0  \n Median :12.95   Median :84.00   Median :329.0     Median :592.0  \n Mean   :13.48   Mean   :84.82   Mean   :328.6     Mean   :596.4  \n 3rd Qu.:14.02   3rd Qu.:88.00   3rd Qu.:330.0     3rd Qu.:602.5  \n Max.   :27.42   Max.   :92.00   Max.   :331.0     Max.   :630.0  \n                                                                  \n\n\nMan erkennt, dass alle Spalten bis auf die erste mit der Plot ID numerisch (num oder int) und dass die abhängige Variable in Spalte 2 sowie die Prediktorvariablen in den Spalten 3 bis 23 stehen.\n\n#Explorative Datenanalyse der abhängigen Variablen\nboxplot(ukraine$Species_richness)\n\nDer Boxplot sieht sehr gut symmetrisch aus. Insofern gibt es keinen Anlass über eine Transformation nachzudenken. (Da es sich bei Artenzahlen um Zähldaten handelt, müsste man theoretisch ein glm mit Poisson-Verteilung rechnen; bei einem Mittelwert, der hinreichend von Null verschieden ist (hier: ca. 40), ist eine Poisson-Verteilung aber praktisch nicht von einer Normalverteilung zu unterscheiden und wir können uns den Aufwand auch sparen).\n\ncor <- cor(ukraine[,3:23])\ncor\ncor[abs(cor)<0.7] <- 0\ncor\n\nDie Korrelationsanalyse dient dazu, zu entscheiden, ob die Prädiktorvariablen hinreichend voneinander unabhängig sind, um alle in das globale Modell hinein zu nehmen. Bei Pearson’s Korrelationskoeffizienten r, die betragsmässig grösser als 0.7 sind, würde es problematisch. Alternativ hätten wir auch den VIF (Variance Inflation Factor) als Kriterium für den möglichen Ausschluss von Variablen aus dem globalen Modell nehmen können. Diese initiale Korrelationsanalyse zeigt uns aber, dass unsere Daten noch ein anderes Problem haben: für die drei Korngrössenfraktionen des Bodens (Sand, Silt, Clay) stehen lauter NA’s. Um herauszufinden, was das Problem ist, geben wir ein:\n\nsummary(ukraine$Sand)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   5.00   20.00   30.00   35.81   55.00   80.00       1 \n\nukraine[!complete.cases(ukraine), ] # Zeigt zeilen mit NAs ein\n\n    X PlotID Species_richness Altitude Inclination Heat_index Microrelief\n85 85 UAR061               23      159          48        0.1         100\n   Grazing_intensity Litter Stones_and_rocks Gravel Fine_soil Sand Silt Clay\n85                 0      1               68      0         1   NA   NA   NA\n     pH Conductivity   CaCO3 N_total C_org CN_ratio Temperature\n85 7.53          203 10.9638    0.95  11.3    11.86          82\n   Temperature_range Precipitation\n85               327           599\n\n\nDa gibt es offensichtlich je ein NA in jeder dieser Zeilen. Jetzt können wir entscheiden, entweder auf die drei Variablen oder auf die eine Beobachtung zu verzichten. Da wir eh schon eher mehr unabhängige Variablen haben als wir händeln können, entscheide ich pragmatisch für ersteres. Wir rechnen die Korrelation also noch einmal ohne diese drei Spalten (es sind die Nummern 12:14, wie wir aus der anfänglichen Variablenbetrachtung oben wissen).\n\ncor <- cor(ukraine[, c(3:11, 15:23)])\ncor[abs(cor)<0.7] <- 0\ncor\n\nWenn man auf cor nun doppel-clickt und es in einem separaten Fenster öffnet, sieht man, wo es problematische Korrelationen zwischen Variablenpaaren gibt. Es sind dies Altitude vs. Temperature und N.total vs. C.org. Wir müssen aus jedem dieser Paare jetzt eine Variable rauswerfen, am besten jene, die weniger gut interpretierbar ist. Ich entscheide mich dafür Temperature statt Altitude (weil das der direktere ökologische Wirkfaktor ist) und C.org statt N.total zu behalten (weil es in der Literatur mehr Daten zum Humusgehalt als zum N-Gehalt gibt, damit eine bessere Vergleichbarkeit erzielt wird). Die Aussagen, die wir für die beibehaltene Variable erzielen, stehen aber +/- auch für die entfernte. Das Problem ist aber, dass wir immer noch 16 Variablen haben, was einen sehr leistungsfähigen Rechner oder sehr lange Rechenzeit erfordern würde. Wir sollten also unter 15 Variablen kommen. Wir könnten uns jetzt überlegen, welche uns ökologisch am wichtigsten sind, oder ein noch strengeres Kriterium bei r verwenden, etwa 0.6\n\ncor <- cor(ukraine[,c(3:11, 15:23)])\ncor[abs(cor)<0.6] <- 0\ncor\n\n                  Species_richness   Altitude Inclination Heat_index\nSpecies_richness                 1  0.0000000           0          0\nAltitude                         0  1.0000000           0          0\nInclination                      0  0.0000000           1          0\nHeat_index                       0  0.0000000           0          1\nMicrorelief                      0  0.0000000           0          0\nGrazing_intensity                0  0.0000000           0          0\nLitter                           0  0.0000000           0          0\nStones_and_rocks                 0  0.0000000           0          0\nGravel                           0  0.0000000           0          0\nClay                            NA         NA          NA         NA\npH                               0  0.0000000           0          0\nConductivity                     0  0.0000000           0          0\nCaCO3                            0  0.0000000           0          0\nN_total                          0  0.0000000           0          0\nC_org                            0  0.0000000           0          0\nCN_ratio                         0  0.0000000           0          0\nTemperature                      0 -0.8309559           0          0\nTemperature_range                0 -0.6794514           0          0\n                  Microrelief Grazing_intensity Litter Stones_and_rocks Gravel\nSpecies_richness            0                 0      0                0      0\nAltitude                    0                 0      0                0      0\nInclination                 0                 0      0                0      0\nHeat_index                  0                 0      0                0      0\nMicrorelief                 1                 0      0                0      0\nGrazing_intensity           0                 1      0                0      0\nLitter                      0                 0      1                0      0\nStones_and_rocks            0                 0      0                1      0\nGravel                      0                 0      0                0      1\nClay                       NA                NA     NA               NA     NA\npH                          0                 0      0                0      0\nConductivity                0                 0      0                0      0\nCaCO3                       0                 0      0                0      0\nN_total                     0                 0      0                0      0\nC_org                       0                 0      0                0      0\nCN_ratio                    0                 0      0                0      0\nTemperature                 0                 0      0                0      0\nTemperature_range           0                 0      0                0      0\n                  Clay       pH Conductivity CaCO3   N_total     C_org CN_ratio\nSpecies_richness    NA 0.000000     0.000000     0 0.0000000 0.0000000        0\nAltitude            NA 0.000000     0.000000     0 0.0000000 0.0000000        0\nInclination         NA 0.000000     0.000000     0 0.0000000 0.0000000        0\nHeat_index          NA 0.000000     0.000000     0 0.0000000 0.0000000        0\nMicrorelief         NA 0.000000     0.000000     0 0.0000000 0.0000000        0\nGrazing_intensity   NA 0.000000     0.000000     0 0.0000000 0.0000000        0\nLitter              NA 0.000000     0.000000     0 0.0000000 0.0000000        0\nStones_and_rocks    NA 0.000000     0.000000     0 0.0000000 0.0000000        0\nGravel              NA 0.000000     0.000000     0 0.0000000 0.0000000        0\nClay                 1       NA           NA    NA        NA        NA       NA\npH                  NA 1.000000     0.674678     0 0.0000000 0.0000000        0\nConductivity        NA 0.674678     1.000000     0 0.0000000 0.0000000        0\nCaCO3               NA 0.000000     0.000000     1 0.0000000 0.0000000        0\nN_total             NA 0.000000     0.000000     0 1.0000000 0.9551133        0\nC_org               NA 0.000000     0.000000     0 0.9551133 1.0000000        0\nCN_ratio            NA 0.000000     0.000000     0 0.0000000 0.0000000        1\nTemperature         NA 0.000000     0.000000     0 0.0000000 0.0000000        0\nTemperature_range   NA 0.000000     0.000000     0 0.0000000 0.0000000        0\n                  Temperature Temperature_range\nSpecies_richness    0.0000000         0.0000000\nAltitude           -0.8309559        -0.6794514\nInclination         0.0000000         0.0000000\nHeat_index          0.0000000         0.0000000\nMicrorelief         0.0000000         0.0000000\nGrazing_intensity   0.0000000         0.0000000\nLitter              0.0000000         0.0000000\nStones_and_rocks    0.0000000         0.0000000\nGravel              0.0000000         0.0000000\nClay                       NA                NA\npH                  0.0000000         0.0000000\nConductivity        0.0000000         0.0000000\nCaCO3               0.0000000         0.0000000\nN_total             0.0000000         0.0000000\nC_org               0.0000000         0.0000000\nCN_ratio            0.0000000         0.0000000\nTemperature         1.0000000         0.6900784\nTemperature_range   0.6900784         1.0000000\n\n\nEntsprechend „werfen“ wir auch noch die folgenden Variablen „raus“: Temperature.range (positiv mit Temperature), Precipitation (negativ mit Temperature) sowie Conductivity (positiv mit pH).\nNun können wir das globale Modell definieren, indem wir alle verbleibenden Variablen aufnehmen, das sind 13. (Wenn das nicht eh schon so viele wären, dass es uns an die Grenze der Rechenleistung bringt, hätten wir auch noch darüber nachdenken können, einzelne quadratische Terme oder Interaktionsterme zu berücksichtigen).\n\nglobal.model <- lm(Species_richness ~ Inclination + Heat_index + Microrelief + Grazing_intensity +\n                    Litter + Stones_and_rocks + Gravel + Fine_soil + pH + CaCO3 + C_org + CN_ratio + Temperature, data = ukraine)\n\nNun gibt es im Prinzip zwei Möglichkeiten, vom globalen (vollen) Modell zu einem minimal adäquaten Modell zu kommen. (1) Der Ansatz der „frequentist statistic“, in dem man aus dem vollen Modell so lange schrittweise Variablen entfernt, bis nur noch signifikante Variablen verbleiben. (2) Den informationstheoretischen Ansatz, bei dem alle denkbaren Modelle berechnet und verglichen werden (also alle möglichen Kombinationen von 13,12,…, 1, 0 Parametern). Diese Lösung stelle ich im Folgenden vor:\n\n# Multimodel inference\nif(!require(MuMIn)){install.packages(\"MuMIn\")}\n\nLoading required package: MuMIn\n\nlibrary(MuMIn)\n\noptions(na.action = \"na.fail\")\nallmodels <- dredge(global.model)\n\nFixed term is \"(Intercept)\"\n\n\n\nallmodels\n\nJetzt bekommen wir die besten der insgesamt 8192 möglichen Modelle gelistet mit ihren Parameterschätzungen und ihrem AICc.\nDas beste Modell umfasst 5 Parameter (CaCO3, CN.ratio, Grazing.intensity. Heat.index, Litter). Allerdings ist das nächstbeste Modell (mit 6 Parametern) nur wenig schlechter (delta AICc = 0.71), was sich in fast gleichen (und zudem sehr niedrigen) Akaike weights bemerkbar macht. Nach dem Verständnis des Information theoretician approach, sollte man in einer solchen Situation nicht das eine „beste“ Modell benennen, sondern eine Aussage über die Gruppe der insgesamt brauchbaren Modelle treffen. Hierzu kann man (a) Importance der Parameter über alle Modelle hinweg berechnen (= Summe der Akaike weights aller Modelle, die den betreffenden Parameter enthalten) und/oder (b) ein nach Akaike weights gemitteltes Modell berechnen.\n\n# Importance values der Variablen\nimportance(allmodels)\n\nWarning: 'importance' is deprecated.\nUse 'sw' instead.\nSee help(\"Deprecated\")\n\n\nfunction (x) \nUseMethod(\"sw\")\n<bytecode: 0x560cc730b1e0>\n<environment: namespace:MuMIn>\n\n\nDemnach ist Heat.index die wichtigste Variable (in 100% aller relevanten Modelle), während ferner Litter, CaCO3, CN_ratio und Grazing_intensity in mehr als 50% der relevanten Modelle enthalten sind.\n\n# Modelaveraging (Achtung: dauert mit 13 Variablen einige Minuten)\nsummary(model.avg(allmodels, rank = \"AICc\"), subset = TRUE)\n\nAus dem gemittelten Modell können wir die Richtung der Beziehung (positiv oder negativ) und ggf. die Effektgrössen (wie verändert sich die Artenzahl, wenn die Prädiktorvariable um eine Einheit zunimmt?) ermitteln.\n\n# Modelldiagnostik nicht vergessen\npar(mfrow = c(2, 2))\nplot(global.model)\n\n\n\nplot(lm(Species_richness ~ Heat_index + Litter + CaCO3 + CN_ratio + Grazing_intensity, data = ukraine))\n\n\n\nsummary(global.model)\n\n\nCall:\nlm(formula = Species_richness ~ Inclination + Heat_index + Microrelief + \n    Grazing_intensity + Litter + Stones_and_rocks + Gravel + \n    Fine_soil + pH + CaCO3 + C_org + CN_ratio + Temperature, \n    data = ukraine)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-25.1317  -5.8226   0.5007   5.9982  21.4941 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(>|t|)    \n(Intercept)        29.35672   17.93590   1.637  0.10338    \nInclination         0.01179    0.08581   0.137  0.89084    \nHeat_index        -12.17483    2.41802  -5.035 1.13e-06 ***\nMicrorelief         0.07488    0.07312   1.024  0.30716    \nGrazing_intensity   1.23000    0.67730   1.816  0.07098 .  \nLitter             -0.12338    0.04309  -2.864  0.00467 ** \nStones_and_rocks   -0.14803    0.08840  -1.675  0.09570 .  \nGravel             -0.03114    0.12924  -0.241  0.80988    \nFine_soil          -0.08720    0.10181  -0.856  0.39286    \npH                 -0.21774    1.70826  -0.127  0.89871    \nCaCO3               0.22638    0.10597   2.136  0.03397 *  \nC_org               0.31994    0.55929   0.572  0.56798    \nCN_ratio           -0.75167    0.33393  -2.251  0.02556 *  \nTemperature         0.24527    0.21510   1.140  0.25566    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.149 on 185 degrees of freedom\nMultiple R-squared:  0.2349,    Adjusted R-squared:  0.1812 \nF-statistic:  4.37 on 13 and 185 DF,  p-value: 2.027e-06\n\n\nWie immer kommt am Ende die Modelldiagnostik. Wir können uns entweder das globale Modell oder das Modell mit den 5 Variablen mit importance > 50% anschauen. Das Bild sieht fast identisch aus und zeigt keinerlei problematische Abweichungen, d. h. links oben weder ein Keil, noch eine Banane, rechts oben eine nahezu perfekte Gerade."
  },
  {
    "objectID": "stat1-4/Statistik3_Uebung.html",
    "href": "stat1-4/Statistik3_Uebung.html",
    "title": "Stat3: Übung",
    "section": "",
    "text": "Datensatz Ukraine_bearbeitet.xlsx Ukraine_bearbeitet.xlsx Datensatz Ukraine_bearbeitet.csv Ukraine_bearbeitet.csv\n\nÜbung 3: Multiple Regression\n\nBereiten Sie den Datensatz Ukraine.xlsx für das Einlesen in R vor und lesen Sie ihn dann ein. Dieser enthält Pflanzenartenzahlen (Species.richness) von 199 10 m² grossen Plots (Vegetationsaufnahmen) von Steppenrasen in der Ukraine sowie zahlreiche Umweltvariablen, deren Bedeutung und Einheiten im Kopf der ExcelTabelle angegeben sind.\nErmitteln Sie ein minimal adäquates Modell, das den Artenreichtum in den Plots durch die Umweltvariablen erklärt.\nBitte erklären und begründen Sie die einzelnen Schritte, die Sie unternehmen, um zu diesem Ergebnis zu kommen. Dazu erstellen Sie bitte ein Word-Dokument, in das Sie Schritt für Schritt den verwendeten R-Code, die dazu gehörigen Ausgaben von R, Ihre Interpretation derselben und die sich ergebenden Schlussfolgerungen für das weitere Vorgehen dokumentieren.\nDieser Ablauf sollte insbesondere beinhalten:\n\nÜberprüfen der Datenstruktur nach dem Einlesen: welches sind die abhängige(n) und welches die unabängige(n) Variablen, sind alle Variablen für die Analyse geeignet?\nExplorative Datenanalyse, um zu sehen, ob die abhängige Variable in der vorliegenden Form für die Analyse geeignet ist\nDefinition eines globalen Modelles und dessen Reduktion zu einem minimal adäquaten Modell\nDurchführen der Modelldiagnostik für dieses\nGenerieren aller Zahlen, Statistiken und Tabellen, die für eine wiss. Ergebnisdarstellung benötigt werden\nFormulieren Sie abschliessend einen Methoden- und Ergebnisteil (ggf. incl. adäquaten Abbildungen) zu dieser Untersuchung in der Form einer wissenschaftlichen Arbeit (ausformulierte schriftliche Zusammenfassung, mit je einem Absatz von ca. 60-100 Worten, resp. 3-8 Sätzen für den Methoden- und Ergebnisteil). D. h. alle wichtigen Informationen sollten enthalten sein, unnötige Redundanz dagegen vermieden werden.\nAbzugeben sind am Ende (a) Ein lauffähiges R-Skript; (b) begründeter Lösungsweg (Kombination aus R-Code, R Output und dessen Interpretation) und (c) ausformulierter Methoden- und Ergebnisteil (für eine wiss. Arbeit)."
  },
  {
    "objectID": "stat1-4/Statistik4_Demo.html#von-lms-zu-glms",
    "href": "stat1-4/Statistik4_Demo.html#von-lms-zu-glms",
    "title": "Stat4: Demo",
    "section": "von LMs zu GLMs",
    "text": "von LMs zu GLMs\n\ntemp <- c(10, 12 ,16, 20, 24, 25, 30, 33, 37)\nbesucher <- c(40, 12, 50, 500, 400, 900, 1500, 900, 2000)\nstrand <- data.frame(\"Temperatur\" = temp, \"Besucher\" = besucher)\n\nplot(besucher~temp, data = strand)\n\n\n\nlm.strand <- lm(Besucher~Temperatur, data = strand)\nsummary(lm.strand)\n\n\nCall:\nlm(formula = Besucher ~ Temperatur, data = strand)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-476.41 -176.89   55.59  218.82  353.11 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -855.01     290.54  -2.943 0.021625 *  \nTemperatur     67.62      11.80   5.732 0.000712 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 311.7 on 7 degrees of freedom\nMultiple R-squared:  0.8244,    Adjusted R-squared:  0.7993 \nF-statistic: 32.86 on 1 and 7 DF,  p-value: 0.0007115\n\npar(mfrow = c(2, 2))\nplot(lm.strand)\n\n\n\npar(mfrow = c(1 ,1))\nxv <- seq(0, 40, by = .1)\nyv <- predict(lm.strand, list(Temperatur = xv))\nplot(strand$Temperatur, strand$Besucher, xlim = c(0, 40))\nlines(xv, yv, lwd = 3, col=  \"blue\")\n\n\n\nglm.gaussian <- glm(Besucher~Temperatur, family = gaussian, data = strand)\nglm.poisson <- glm(Besucher~Temperatur, family = poisson, data = strand)\n\nsummary(glm.gaussian)\n\n\nCall:\nglm(formula = Besucher ~ Temperatur, family = gaussian, data = strand)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-476.41  -176.89    55.59   218.82   353.11  \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -855.01     290.54  -2.943 0.021625 *  \nTemperatur     67.62      11.80   5.732 0.000712 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 97138.03)\n\n    Null deviance: 3871444  on 8  degrees of freedom\nResidual deviance:  679966  on 7  degrees of freedom\nAIC: 132.63\n\nNumber of Fisher Scoring iterations: 2\n\nsummary(glm.poisson)\n\n\nCall:\nglm(formula = Besucher ~ Temperatur, family = poisson, data = strand)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-13.577  -12.787   -4.491    9.515   15.488  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept) 3.500301   0.056920   61.49   <2e-16 ***\nTemperatur  0.112817   0.001821   61.97   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 6011.8  on 8  degrees of freedom\nResidual deviance: 1113.7  on 7  degrees of freedom\nAIC: 1185.1\n\nNumber of Fisher Scoring iterations: 5\n\n\nRücktranformation der Werte auf die orginale Skale (Hier Exponentialfunktion da family=possion als Link-Funktion den natürlichen Logarithmus (log) verwendet) Besucher = exp(3.50 + 0.11 Temperatur/°C)\n\nexp(3.500301) # Anzahl besucher bei 0°C\n\n[1] 33.12542\n\nexp(glm.poisson$coefficients[1]) # Werte aus Modell\n\n(Intercept) \n   33.12542 \n\nexp(3.500301 + 30*0.112817) # Anzahl besucher bei 30°C\n\n[1] 977.3169\n\nexp(glm.poisson$coeff[1] * glm.poisson$coeff[2]) #coefficients kann mit coeff abgekürzt werden\n\n(Intercept) \n   1.484225 \n\n# Test Overdispersion\nif(!require(AER)){install.packages(\"AER\")}\n\nLoading required package: AER\n\n\nLoading required package: car\n\n\nLoading required package: carData\n\n\nLoading required package: lmtest\n\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n\nLoading required package: sandwich\n\n\nLoading required package: survival\n\nlibrary(AER)\ndispersiontest(glm.poisson)\n\n\n    Overdispersion test\n\ndata:  glm.poisson\nz = 3.8576, p-value = 5.726e-05\nalternative hypothesis: true dispersion is greater than 1\nsample estimates:\ndispersion \n  116.5467 \n\nglm.quasi <- glm(Besucher~Temperatur, family = quasipoisson, data = strand)\nsummary(glm.quasi)\n\n\nCall:\nglm(formula = Besucher ~ Temperatur, family = quasipoisson, data = strand)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-13.577  -12.787   -4.491    9.515   15.488  \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)  3.50030    0.69639   5.026  0.00152 **\nTemperatur   0.11282    0.02227   5.065  0.00146 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasipoisson family taken to be 149.6826)\n\n    Null deviance: 6011.8  on 8  degrees of freedom\nResidual deviance: 1113.7  on 7  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 5\n\npar(mfrow = c(2,2))\nplot(glm.gaussian)\n\n\n\nplot(glm.poisson)\n\n\n\nplot(glm.quasi)\n\n\n\npar(mfrow = c(1, 1))\nplot(strand$Temperatur, strand$Besucher, xlim=c(0, 40))\nxv <- seq(0, 40, by = .1)\n\nyv <- predict(lm.strand, list(Temperatur = xv))\nlines(xv, yv, lwd = 3, col = \"blue\")\n\nyv2 <- predict(glm.poisson, list(Temperatur = xv))\nlines(xv, exp(yv2), lwd = 3, col = \"red\")\n\nyv3 <- predict(glm.quasi, list(Temperatur = xv))\nlines(xv, exp(yv3), lwd = 3, col = \"green\")"
  },
  {
    "objectID": "stat1-4/Statistik4_Demo.html#logistische-regression",
    "href": "stat1-4/Statistik4_Demo.html#logistische-regression",
    "title": "Stat4: Demo",
    "section": "Logistische Regression",
    "text": "Logistische Regression\n\nbathing <- data.frame(\n  \"temperature\" = c(1, 2, 5, 9, 14, 14, 15, 19, 22, 24, 25, 26, 27, 28, 29),\n  \"bathing\" = c(0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1))\nplot(bathing~temperature, data = bathing)\n\n\n\nglm.1<-glm(bathing~temperature, family = \"binomial\", data = bathing)\nsummary(glm.1)\n\n\nCall:\nglm(formula = bathing ~ temperature, family = \"binomial\", data = bathing)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.7408  -0.4723  -0.1057   0.5123   1.8615  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)  \n(Intercept)  -5.4652     2.8501  -1.918   0.0552 .\ntemperature   0.2805     0.1350   2.077   0.0378 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 20.728  on 14  degrees of freedom\nResidual deviance: 10.829  on 13  degrees of freedom\nAIC: 14.829\n\nNumber of Fisher Scoring iterations: 6\n\n# Modeldiagnostik (wenn nicht signifikant, dann OK)\n1 - pchisq (glm.1$deviance, glm.1$df.resid)\n\n[1] 0.6251679\n\n# Modellgüte (pseudo-R²)\n1 - (glm.1$dev / glm.1$null)\n\n[1] 0.4775749\n\n# Steilheit der Beziehung (relative Änderung der odds bei x + 1 vs. x)\nexp(glm.1$coefficients[2])\n\ntemperature \n   1.323807 \n\n# LD50 (also hier: Temperatur, bei der 50% der Touristen baden)\n-glm.1$coefficients[1] / glm.1$coefficients[2]\n\n(Intercept) \n   19.48311 \n\n# Vorhersagen\npredicted <- predict(glm.1, type = \"response\")\n\n# Konfusionsmatrix\nkm <- table(bathing$bathing, predicted > 0.5)\nkm\n\n   \n    FALSE TRUE\n  0     7    1\n  1     1    6\n\n# Missklassifizierungsrate\n1 - sum(diag(km) / sum(km))\n\n[1] 0.1333333\n\n#Plotting\nxs <- seq(0, 30, l = 1000)\nmodel.predict <- predict(glm.1, type = \"response\", se = T, \n                         newdata = data.frame(temperature = xs))\n\nplot(bathing~temperature, xlab = \"Temperature (°C)\", \n     ylab = \"% Bathing\", pch = 16, col = \"red\", data = bathing)\npoints(model.predict$fit ~ xs, type=\"l\")\nlines(model.predict$fit+model.predict$se.fit ~ xs, type = \"l\", lty = 2)\nlines(model.predict$fit-model.predict$se.fit ~ xs, type = \"l\", lty = 2)"
  },
  {
    "objectID": "stat1-4/Statistik4_Demo.html#nicht-lineare-regression",
    "href": "stat1-4/Statistik4_Demo.html#nicht-lineare-regression",
    "title": "Stat4: Demo",
    "section": "Nicht-lineare Regression",
    "text": "Nicht-lineare Regression\n\nif(!require(AICcmodavg)){install.packages(\"AICcmodavg\")}\n\nLoading required package: AICcmodavg\n\nif(!require(nlstools)){install.packages(\"nlstools\")}\n\nLoading required package: nlstools\n\n\n\n'nlstools' has been loaded.\n\n\nIMPORTANT NOTICE: Most nonlinear regression models and data set examples\n\n\nrelated to predictive microbiolgy have been moved to the package 'nlsMicrobio'\n\nlibrary(AICcmodavg)\nlibrary(nlstools)\n\nloyn <- read.delim(here(\"data\",\"loyn.csv\"), sep = \",\") # Verzeichnis muss dort gesetzt sein wo Daten sind\n\n#Selbstdefinierte Funktion, hier Potenzfunktion\npower.model <- nls(ABUND~c*AREA^z, start = (list(c = 1, z = 0)), data = loyn)\nsummary(power.model)\n\n\nFormula: ABUND ~ c * AREA^z\n\nParameters:\n  Estimate Std. Error t value Pr(>|t|)    \nc 13.39418    1.30721  10.246 2.87e-14 ***\nz  0.16010    0.02438   6.566 2.09e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.995 on 54 degrees of freedom\n\nNumber of iterations to convergence: 12 \nAchieved convergence tolerance: 7.122e-06\n\nAICc(power.model)\n\n[1] 396.1723\n\n#Modeldiagnostik (in nlstools)\nplot(nlsResiduals(power.model))\n\n\n\n#Vordefinierte \"Selbststartfunktionen\"#\n?selfStart\nlogistic.model <- nls(ABUND~SSlogis(AREA, Asym, xmid, scal), data = loyn)\nsummary(logistic.model)\n\n\nFormula: ABUND ~ SSlogis(AREA, Asym, xmid, scal)\n\nParameters:\n     Estimate Std. Error t value Pr(>|t|)    \nAsym   31.306      2.207  14.182  < 2e-16 ***\nxmid    6.501      2.278   2.854  0.00614 ** \nscal    9.880      3.152   3.135  0.00280 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.274 on 53 degrees of freedom\n\nNumber of iterations to convergence: 8 \nAchieved convergence tolerance: 4.371e-06\n\nAICc(logistic.model)\n\n[1] 386.8643\n\n#Modeldiagnostik (in nlstools)\nplot(nlsResiduals(logistic.model))\n\n\n\n#Visualisierung\nplot(ABUND~AREA, data = loyn)\npar(mfrow = c(1, 1))\nxv <- seq(0, 2000, 0.01)\n\n# 1. Potenzfunktion\nyv1 <- predict(power.model, list(AREA = xv))\nlines(xv, yv1, col = \"green\")\n\n# 2. Logistische Funktion\nyv2 <- predict(logistic.model, list(AREA = xv))\nlines(xv, yv2, col = \"blue\")\n\n\n\n#Visualisierung II\nplot(ABUND~log10(AREA), data = loyn)\npar(mfrow = c(1, 1))\n\n# 1. Potenzfunktion\nyv1 <- predict(power.model, list(AREA = xv))\nlines(log10(xv), yv1, col = \"green\")\n\n# 2. Logistische Funktion\nyv2 <- predict(logistic.model, list(AREA = xv))\nlines(log10(xv), yv2, col = \"blue\")\n\n\n\n#Model seletkion zwischen den nicht-lineraen Modelen\ncand.models<-list()\ncand.models[[1]] <- power.model\ncand.models[[2]] <- logistic.model\n\nModnames <- c(\"Power\", \"Logistic\")\n\naictab(cand.set = cand.models, modnames = Modnames)\n\n\nModel selection based on AICc:\n\n         K   AICc Delta_AICc AICcWt Cum.Wt      LL\nLogistic 4 386.86       0.00   0.99   0.99 -189.04\nPower    3 396.17       9.31   0.01   1.00 -194.86"
  },
  {
    "objectID": "stat1-4/Statistik4_Demo.html#smoother",
    "href": "stat1-4/Statistik4_Demo.html#smoother",
    "title": "Stat4: Demo",
    "section": "Smoother",
    "text": "Smoother\n\nloyn$log_AREA<-log10(loyn$AREA)       \nplot(ABUND~log_AREA, data = loyn)\nlines(lowess(loyn$log_AREA, loyn$ABUND, f = 0.25), lwd = 2, col = \"red\")\nlines(lowess(loyn$log_AREA, loyn$ABUND, f = 0.5), lwd = 2, col = \"blue\")\nlines(lowess(loyn$log_AREA, loyn$ABUND, f = 1), lwd = 2, col = \"green\")"
  },
  {
    "objectID": "stat1-4/Statistik4_Demo.html#gams",
    "href": "stat1-4/Statistik4_Demo.html#gams",
    "title": "Stat4: Demo",
    "section": "GAMs",
    "text": "GAMs\n\nif(!require(mgcv)){install.packages(\"mgcv\")}\n\nLoading required package: mgcv\n\n\nLoading required package: nlme\n\n\nThis is mgcv 1.8-40. For overview type 'help(\"mgcv-package\")'.\n\nlibrary(mgcv)\n\ngam.1 <- gam(ABUND~s(log_AREA), data = loyn)\ngam.1\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nABUND ~ s(log_AREA)\n\nEstimated degrees of freedom:\n2.88  total = 3.88 \n\nGCV score: 52.145     \n\nsummary(gam.1)\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nABUND ~ s(log_AREA)\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  19.5143     0.9309   20.96   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n              edf Ref.df     F p-value    \ns(log_AREA) 2.884  3.628 21.14  <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.579   Deviance explained = 60.1%\nGCV = 52.145  Scale est. = 48.529    n = 56\n\nplot(loyn$log_AREA, loyn$ABUND, pch = 16)\nxv <- seq(-1,4, by = 0.1)\nyv <- predict(gam.1, list(log_AREA = xv))\nlines(xv, yv, lwd = 2, col = \"red\")\n\n\n\nAICc(gam.1)\n\n[1] 383.2109\n\nsummary(gam.1)\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nABUND ~ s(log_AREA)\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  19.5143     0.9309   20.96   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n              edf Ref.df     F p-value    \ns(log_AREA) 2.884  3.628 21.14  <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.579   Deviance explained = 60.1%\nGCV = 52.145  Scale est. = 48.529    n = 56"
  },
  {
    "objectID": "stat1-4/Statistik4_Loesung_1.html#musterlösung-übung-4.1-nicht-lineare-regression",
    "href": "stat1-4/Statistik4_Loesung_1.html#musterlösung-übung-4.1-nicht-lineare-regression",
    "title": "Stat4: Lösung",
    "section": "Musterlösung Übung 4.1: Nicht-lineare Regression",
    "text": "Musterlösung Übung 4.1: Nicht-lineare Regression\n\nR-Code als Download\nLoesungstext Beispiel\n\nÜbungsaufgabe (hier so ausführlich formuliert, wie dies auch in der Klausur der Fall sein wird)\n\nLaden Sie den Datensatz Curonia_spit.xlsx. Dieser enthält gemittelte\nPflanzenartenzahlen (Species.richness) von geschachtelten Plots (Vegetationsaufnahmen) der Pflanzengesellschaft Lolio-Cynosuretum im Nationalpark Kurische Nehrung (Russland) auf Flächengrössen (Area) von 0.0001 bis 900 m².\nErmitteln Sie den funktionellen Zusammenhang, der die Zunahme der Artenzahl mit der Flächengrösse am besten beschreibt. Berücksichtigen Sie dabei mindestens die Potenzfunktion (power function), die logarithmische Funktion (logarithmic function) und eine Funktion mit Sättigung (saturation, asymptote) Ihrer Wahl\nBitte erklären und begründen Sie die einzelnen Schritte, die Sie unternehmen, um zu diesem Ergebnis zu kommen. Dazu erstellen Sie bitte ein Word-Dokument, in das Sie Schritt für Schritt den verwendeten R-Code, die dazu gehörigen Ausgaben von R, Ihre Interpretation derselben und die sich ergebenden Schlussfolgerungen für das weitere Vorgehen dokumentieren.\nDieser Ablauf sollte insbesondere beinhalten:\n\nÜberprüfen der Datenstruktur nach dem Einlesen, welches sind die abhängige(n) und welches die unabängige(n) Variablen\nExplorative Datenanalyse, um zu sehen, ob eine nicht-lineare Regression überhaupt nötig ist und ob evtl. Dateneingabefehler vorliegen vorgenommen werden sollten\nDefinition von mindestens drei nicht-linearen Regressionsmodellen\nSelektion des/der besten Models/Modelle\nDurchführen der Modelldiagnostik für die Modelle in der engeren Auswahl, um zu entscheiden, ob das gewählte Vorgehen korrekt war oder ggf. angepasst werden muss\nGenerieren aller Zahlen, Statistiken und Tabellen, die für eine wiss. Ergebnisdarstellung benötigt werden\n\nFormulieren Sie abschliessend einen Methoden- und Ergebnisteil (ggf. incl. adäquaten Abbildungen) zu dieser Untersuchung in der Form einer wissenschaftlichen Arbeit (ausformulierte schriftliche Zusammenfassung, mit je einem Absatz von ca. 60-100 Worten, resp. 3-8 Sätzen für den Methoden- und Ergebnisteil). D. h. alle wichtigen Informationen sollten enthalten sein, unnötige Redundanz dagegen vermieden werden.\nAbzugeben sind am Ende (a) Ein lauffähiges R-Skript; (b) begründeter Lösungsweg (Kombination aus R-Code, R Output und dessen Interpretation) und (c) ausformulierter Methoden- und Ergebnisteil (für eine wiss. Arbeit).\n\n\nMusterlösung Übung 4.1 - Nicht-lineare Regression\nAus der Excel-Tabelle wurde das relevante Arbeitsblatt als csv gespeichert\n\ncuronian <- read.delim(here(\"data\",\"Curonian_spit.csv\"), sep=\",\")\nstr(curonian)\n\n'data.frame':   16 obs. of  3 variables:\n $ X               : int  1 2 3 4 5 6 7 8 9 10 ...\n $ Area            : num  0.0001 0.0025 0.01 0.0625 0.25 1 4 9 16 25 ...\n $ Species.richness: num  2.1 9.1 14.3 23.1 30.1 37.4 48.5 54.5 58 59.9 ...\n\nsummary(curonian)\n\n       X              Area          Species.richness\n Min.   : 1.00   Min.   :  0.0001   Min.   : 2.10   \n 1st Qu.: 4.75   1st Qu.:  0.2031   1st Qu.:28.35   \n Median : 8.50   Median : 12.5000   Median :56.25   \n Mean   : 8.50   Mean   :147.1453   Mean   :50.09   \n 3rd Qu.:12.25   3rd Qu.:131.2500   3rd Qu.:69.95   \n Max.   :16.00   Max.   :900.0000   Max.   :92.40   \n\n # Explorative Datenanalyse\nplot(Species.richness~Area, data = curonian)\n\n\n\n\nEs liegt in der Tat ein nicht-linearer Zusammenhang vor, der sich gut mit nls analysieren lässt. Die Daten beinhalten keine erkennbaren Fehler, da der Artenreichtum der geschachtelten Plots mit der Fläche ansteigt.\n\n# Potenzfunktion selbst definiert\nif(!require(nlstools)){install.packages(\"nlstools\")}\n\nLoading required package: nlstools\n\n\n\n'nlstools' has been loaded.\n\n\nIMPORTANT NOTICE: Most nonlinear regression models and data set examples\n\n\nrelated to predictive microbiolgy have been moved to the package 'nlsMicrobio'\n\nlibrary(nlstools)\n# power.model <- nls(Species.richness~c*Area^z, data = curonian)\n# summary(power.model)\n\nFalls die Funktion so keine Ergebnisse liefert, oder das Ergebnis unsinnig aussieht, wenn man es später plottet, müsste man hier geeignete Startwerte angeben, die man aus der Betrachtung der Daten oder aus Erfahrungen mit der Funktion für ähnliche Datensets gewinnt,etwa so:\n\npower.model <- nls(Species.richness~c * Area^z, start = (list(c = 1, z = 0.2)), data = curonian)\nsummary(power.model)\n\n\nFormula: Species.richness ~ c * Area^z\n\nParameters:\n   Estimate Std. Error t value Pr(>|t|)    \nc 36.168960   1.408966   25.67 3.56e-13 ***\nz  0.138941   0.007472   18.60 2.88e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.142 on 14 degrees of freedom\n\nNumber of iterations to convergence: 9 \nAchieved convergence tolerance: 8.143e-06\n\n\nDas Ergebnis ist identisch\n\n#logarithmische Funktion selbst definiert\nlogarithmic.model <- nls(Species.richness~b0 + b1 * log10(Area), data = curonian)\n\nWarning in nls(Species.richness ~ b0 + b1 * log10(Area), data = curonian): No starting values specified for some parameters.\nInitializing 'b0', 'b1' to '1.'.\nConsider specifying 'start' or using a selfStart model\n\nsummary(logarithmic.model)\n\n\nFormula: Species.richness ~ b0 + b1 * log10(Area)\n\nParameters:\n   Estimate Std. Error t value Pr(>|t|)    \nb0   43.333      1.358   31.91 1.78e-14 ***\nb1   13.281      0.654   20.31 8.75e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.265 on 14 degrees of freedom\n\nNumber of iterations to convergence: 1 \nAchieved convergence tolerance: 5.568e-09\n\n\nZu den verschiedenen Funktionen mit Sättigungswert (Asymptote) gehören Michaelis-Menten, das aymptotische Modell durch den Ursprung und die logistische Funktion. Die meisten gibt es in R als selbststartende Funktionen, was meist besser funktioniert als wenn man sich selbst Gedanken über Startwerte usw. machen muss. Man kann sie aber auch selbst definieren\nIm Folgenden habe ich ein paar unterschiedliche Sättigungsfunktionen mit verschiedenen Einstellungen durchprobiert, um zu zeigen, was alles passieren kann…\n\nmicmen.1 <- nls(Species.richness~SSmicmen(Area, Vm, K), data = curonian)\nsummary(micmen.1)\n\n\nFormula: Species.richness ~ SSmicmen(Area, Vm, K)\n\nParameters:\n   Estimate Std. Error t value Pr(>|t|)    \nVm  72.0108     4.2708  16.861 1.07e-10 ***\nK    0.8477     0.4371   1.939   0.0729 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.96 on 14 degrees of freedom\n\nNumber of iterations to convergence: 0 \nAchieved convergence tolerance: 3.383e-06\n\n# Dasselbe selbst definiert (mit default-Startwerten)\nmicmen.2 <- nls(Species.richness~Vm*Area/(K+Area), data = curonian)\n\nWarning in nls(Species.richness ~ Vm * Area/(K + Area), data = curonian): No starting values specified for some parameters.\nInitializing 'Vm', 'K' to '1.'.\nConsider specifying 'start' or using a selfStart model\n\nsummary(micmen.2)\n\n\nFormula: Species.richness ~ Vm * Area/(K + Area)\n\nParameters:\n   Estimate Std. Error t value Pr(>|t|)    \nVm  46.7020     9.6748   4.827 0.000268 ***\nK   -2.1532     0.5852  -3.679 0.002477 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 35.14 on 14 degrees of freedom\n\nNumber of iterations to convergence: 23 \nAchieved convergence tolerance: 9.097e-06\n\n\nHier ist das Ergebnis deutlich verschieden, ein Phänomen, das einem bei nicht-linearen Regressionen anders als bei linearen Regressionen immer wieder begegnen kann, da der Iterationsalgorithmus in lokalen Optima hängen bleiben kann. Oftmals dürfte die eingebaute Selbststartfunktion bessere Ergebnisse liefern, aber das werden wir unten sehen.\n\n# Dasselbe selbst definiert (mit sinnvollen Startwerten, basierend auf dem Plot)\nmicmen.3 <- nls(Species.richness~Vm*Area/(K+Area), start = list(Vm = 100, K = 1), data = curonian)\nsummary(micmen.3)\n\n\nFormula: Species.richness ~ Vm * Area/(K + Area)\n\nParameters:\n   Estimate Std. Error t value Pr(>|t|)    \nVm  72.0111     4.2708  16.861 1.07e-10 ***\nK    0.8477     0.4371   1.939   0.0729 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.96 on 14 degrees of freedom\n\nNumber of iterations to convergence: 22 \nAchieved convergence tolerance: 7.003e-06\n\n\nWenn man sinnvollere Startwerte als die default-Werte (1 für alle Parameter) eingibt, hier etwas einen mutmasslichen Asymptoten-Wert (aus der Grafik) von Vm = ca. 100, dann bekommt man das gleiche Ergebnis wie bei der Selbsstartfunktion\n\n# Eine asymptotische Funktion durch den Ursprung (mit implementierter Selbststartfunktion)\nasym.model <- nls(Species.richness~SSasympOrig(Area, Asym, lrc), data = curonian)\nsummary(asym.model)\n\n\nFormula: Species.richness ~ SSasympOrig(Area, Asym, lrc)\n\nParameters:\n     Estimate Std. Error t value Pr(>|t|)    \nAsym  68.5066     4.4278  15.472 3.38e-10 ***\nlrc    0.1184     0.4864   0.244    0.811    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.88 on 14 degrees of freedom\n\nNumber of iterations to convergence: 0 \nAchieved convergence tolerance: 2.867e-06\n\n\n\nlogistic.model <- nls(Species.richness~SSlogis(Area, asym, xmid, scal), data = curonian)\nsummary(logistic.model)\n\nError in nls(y ~ 1/(1 + exp((xmid - x)/scal)), data = xy, start = list(xmid= aux[1L], : Iterationenzahl überschritt Maximum 50\nDas ist etwas, was einem bei nls immer wieder passieren kann. Die Iteration ist nach der eingestellten max. Iterationszahl noch nicht zu einem Ergebnis konvergiert. Um ein Ergebnis für diese Funktion zu bekommen, müsste man mit den Einstellungen von nls „herumspielen“, etwas bei den Startwerten oder den max. Um das effizient zu machen, braucht man aber etwas Erfahrung Interationszahlen (man kann z. B. manuell die Maximalzahl der Iterationen erhöhen, indem man in den Funktionsaufruf etwa maxiter =100 als zusätzliches Argument reinschreibtn).\n#Logistische Regression mit Startwerten\n\nlogistic.model.2 <- nls(Species.richness~asym/(1 + exp((xmid-Area) / scal)), \n                      control = nls.control(maxiter = 100), \n                      start = (list(xmid = 1, scal = 0.2, asym = 100)), data = curonian)\nsummary(logistic.model.2)\n\n\nFormula: Species.richness ~ asym/(1 + exp((xmid - Area)/scal))\n\nParameters:\n     Estimate Std. Error t value Pr(>|t|)    \nxmid    3.970      1.608   2.469   0.0282 *  \nscal    4.112      1.676   2.453   0.0290 *  \nasym   73.634      4.507  16.339 4.79e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.11 on 13 degrees of freedom\n\nNumber of iterations to convergence: 59 \nAchieved convergence tolerance: 9.676e-06\n\n\n\n# Vergleich der Modellgüte mittels AICc\nlibrary(AICcmodavg)\ncand.models <- list()\ncand.models[[1]] <- power.model\ncand.models[[2]] <- logarithmic.model\ncand.models[[3]] <- micmen.1\ncand.models[[4]] <- micmen.2\ncand.models[[5]] <- asym.model\ncand.models[[6]] <- logistic.model.2\n\nModnames<-c(\"Power\", \"Logarithmic\", \"Michaelis-Menten (SS)\", \"Michaelis-Menten\", \n            \"Asymptotic through origin\", \"Logistische Regression\")\naictab(cand.set=cand.models, modnames=Modnames)\n\n\nModel selection based on AICc:\n\n                          K   AICc Delta_AICc AICcWt Cum.Wt     LL\nPower                     3  96.75       0.00   0.98   0.98 -44.38\nLogarithmic               3 104.43       7.68   0.02   1.00 -48.21\nMichaelis-Menten (SS)     3 130.67      33.92   0.00   1.00 -61.34\nLogistische Regression    4 133.53      36.78   0.00   1.00 -60.95\nAsymptotic through origin 3 135.44      38.69   0.00   1.00 -63.72\nMichaelis-Menten          3 165.17      68.42   0.00   1.00 -78.58\n\n\nDiese Ergebnistabelle vergleicht die Modellgüte zwischen den fünf Modellen, die wir in unsere Auswahl reingesteckt haben. Alle haben drei geschätzte Parameter (K), also zwei Funktionsparameter und die Varianz. Das beste Modell (niedrigster AICc bzw. Delta = 0) hat das Potenzmodell (power). Das zweitbeste Modell (logarithmic) hat bereits einen Delta-AICc von mehr als 4, ist daher statistisch nicht relevant. Das zeigt sich auch am Akaike weight, das für das zweite Modell nur noch 2 % ist. Die verschiedenen Modelle mit oberem Grenzwert (3-5) sind komplett ungeeignet.\n\n# Modelldiagnostik für das beste Modell\nlibrary(nlstools)\nplot(nlsResiduals(power.model))\n\n\n\n\nLinks oben sieht man zwar ein Muster (liegt daran, dass in diesem Fall die Plots geschachtelt, und nicht unabhängig waren), aber jedenfalls keinen problematischen Fall wie einen Bogen oder einen Keil. Der QQ-Plot rechts unten ist völlig OK. Somit haben wir auch keine problematische Abweichung von der Normalverteilung der Residuen. Da es sich bei den einzelnen Punkten allerdings bereits um arithmetische Mittelwerte aus je 8 Beobachtungen handelt, hätte man sich auch einfach auf das Central Limit Theorem beziehen können, das sagt, dass Mittelwerte automatisch einer Normalverteilung folgen.\n\n# Ergebnisplot\nplot(Species.richness~Area, pch = 16, xlab = \"Fläche [m²]\", ylab = \"Artenreichtum\", data = curonian)\nxv <- seq(0, 1000, by = 0.1)\nyv <- predict(power.model, list(Area = xv))\nlines(xv, yv, lwd = 2, col = \"red\")\nyv2 <- predict(micmen.1, list(Area = xv))\nlines(xv, yv2, lwd = 2, col = \"blue\")\n\n\n\n\nDas ist der Ergebnisplot für das beste Modell. Wichtig ist, dass man die Achsen korrekt beschriftet und nicht einfach die mehr oder weniger kryptischen Spaltennamen aus R nimmt.\nIm Weiteren habe ich noch eine Sättigungsfunktion (Michaelis-Menten mit Selbststarter) zum Vergleich hinzugeplottet\nMan erkennt, dass die Sättigungsfunktion offensichtlich den tatsächlichen Kurvenverlauf sehr schlecht widergibt. Im mittleren Kurvenbereich sind die Schätzwerte zu hoch, für grosse Flächen dann aber systematisch viel zu niedrig. Man kann die Darstellung im doppeltlogarithmischen Raum wiederholen, um die Kurvenanpassung im linken Bereich besser differenzieren zu können:\n\n# Ergebnisplot Double-log\nplot(log10(Species.richness)~log10(Area), pch = 16, xlab = \"log A\", ylab = \"log (S)\", data = curonian)\n\nxv <- seq(0, 1000, by = 0.0001)\n\nyv <- predict(power.model, list(Area = xv))\nlines(log10(xv), log10(yv), lwd = 2, col = \"red\")\n\nyv2 <- predict(micmen.1, list(Area=xv))\nlines(log10(xv), log10(yv2), lwd = 2, col = \"blue\")\n\n\n\n\nAuch hier sieht man, dass die rote Kurve zwar nicht perfekt, aber doch viel besser als die blaue Kurve ist."
  },
  {
    "objectID": "stat1-4/Statistik4_Loesung_2n.html#musterlösung-aufgabe-4.2n-multiple-logistische-regression",
    "href": "stat1-4/Statistik4_Loesung_2n.html#musterlösung-aufgabe-4.2n-multiple-logistische-regression",
    "title": "Stat4: Lösung 2n",
    "section": "Musterlösung Aufgabe 4.2N: Multiple logistische Regression",
    "text": "Musterlösung Aufgabe 4.2N: Multiple logistische Regression\n\nR-Skript als Download\nLoesungstext\n\n\npolis <- read.csv(here(\"data\",\"polis.csv\"))\npolis\n\n    X     ISLAND RATIO PA\n1   1       Bota 15.41  1\n2   2     Cabeza  5.63  1\n3   3    Cerraja 25.92  1\n4   4 Coronadito 15.17  0\n5   5     Flecha 13.04  1\n6   6   Gemelose 18.85  0\n7   7   Gemelosw 30.95  0\n8   8   Jorabado 22.87  0\n9   9     Mitlan 12.01  0\n10 10       Pata 11.60  1\n11 11      Piojo  6.09  1\n12 12      Smith  2.28  1\n13 13    Ventana  4.05  1\n14 14    Bahiaan 59.94  0\n15 15    Bahiaas 63.16  0\n16 16     Blanca 22.76  0\n17 17   Pescador 23.54  0\n18 18   Angeldlg  0.21  1\n19 19      Mejia  2.55  1\n\nstr(polis)\n\n'data.frame':   19 obs. of  4 variables:\n $ X     : int  1 2 3 4 5 6 7 8 9 10 ...\n $ ISLAND: chr  \"Bota\" \"Cabeza\" \"Cerraja\" \"Coronadito\" ...\n $ RATIO : num  15.41 5.63 25.92 15.17 13.04 ...\n $ PA    : int  1 1 1 0 1 0 0 0 0 1 ...\n\nsummary(polis)\n\n       X           ISLAND              RATIO             PA        \n Min.   : 1.0   Length:19          Min.   : 0.21   Min.   :0.0000  \n 1st Qu.: 5.5   Class :character   1st Qu.: 5.86   1st Qu.:0.0000  \n Median :10.0   Mode  :character   Median :15.17   Median :1.0000  \n Mean   :10.0                      Mean   :18.74   Mean   :0.5263  \n 3rd Qu.:14.5                      3rd Qu.:23.20   3rd Qu.:1.0000  \n Max.   :19.0                      Max.   :63.16   Max.   :1.0000  \n\n\nMan erkennt, dass polis 19 Beobachtungen von drei Parametern enthält, wobei ISLAND ein Faktor mit den Inselnamen ist, während RATIO metrisch ist und PA nur 0 oder 1 enthält. Prädiktorvariable ist RATIO, abhängige Variable PA, mithin ist das korrekte statistische Verfahren eine logistische Regression (GLM).\n\n# Explorative Datenanalyse\nboxplot(polis$RATIO)\n\n\n\n\nDer Boxplot zeigt zwei starke Ausreisser, ist also etwas rechtsschief. Da es sich aber um die unabhängige Variable handelt, muss uns das nicht weiter stören.\n\n# Definition des logistischen Modells\nglm.1 <- glm(PA~RATIO, family = \"binomial\", data = polis)\nsummary (glm.1)\n\n\nCall:\nglm(formula = PA ~ RATIO, family = \"binomial\", data = polis)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.6067  -0.6382   0.2368   0.4332   2.0986  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)  \n(Intercept)   3.6061     1.6953   2.127   0.0334 *\nRATIO        -0.2196     0.1005  -2.184   0.0289 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 26.287  on 18  degrees of freedom\nResidual deviance: 14.221  on 17  degrees of freedom\nAIC: 18.221\n\nNumber of Fisher Scoring iterations: 6\n\n\nModell ist signifikant (p-Wert in Zeile RATIO ist < 0.05). Jetzt müssen wir noch prüfen, ob es auch valide ist:\n\n# Modelldiagnostik für das gewählte Modell (wenn nicht signifikant, dann OK)\n1 - pchisq(glm.1$deviance, glm.1$df.resid)\n\n[1] 0.6514215\n\n# Visuelle Inspektion der Linearität\nlibrary(car)\n\nLoading required package: carData\n\ncrPlots(glm.1, ask = F)\n\n\n\n\nBeide Aspekte sind OK, d.h. der Test war nicht signifikant und die pinkfarbene Linie liegt fast auf der theoretischen Linie (blau gestrichelt).\nJetzt brauchen wir noch die Modellgüte (Pseudo-R2):\n\n# Modellgüte (pseudo-R²)\n1 - (glm.1$dev / glm.1$null)\n\n[1] 0.4590197\n\n\nUm zu unser Modell zu interpretieren müssen wir noch in Betracht ziehen, dass wir nicht die Vorkommenswahrscheinlichkeit selbst, sondern logit (Vorkommenswahrscheinlichkeit) modelliert haben. Unser Ergebnis (die beiden Parameterschätzungen von oben, also 3.6061 und -0.2196) muss also zunächst in etwas Interpretierbares übersetzt werden:\n\n# Steilheit der Beziehung in Modellen mit nur einem Parameter\nexp(glm.1$coef[2])\n\n    RATIO \n0.8028734 \n\n\n< 1, d. h. Vorkommenswahrscheinlichkeit sinkt mit zunehmender Isolation.\n\n# LD50 für 1-Parameter-Modelle (hier also x-Werte, bei der 50% der Inseln besiedelt sind)\n-glm.1$coef[1] / glm.1$coef[2]\n\n(Intercept) \n    16.4242 \n\n\nAm besten stellen wir auch unsere Funktionsgleichung dar. Dazu müssen wir das „Rohergebnis“ (mit P = Vorkommenswahrscheinlichkeit)\nln (P/ (1- P)) = 3.606 – 0.220 RATIO\nso umformen, dass wir links nur P stehen haben:\nP = exp (3.606 – 0.220 RATIO) / (1 + exp (3.606 – 0.220 RATIO))\nDas ist also unsere vorhergesagte Regressionsfunktion, die wir in einem letzten Schritt auch noch visualisieren können (und sollten):\n\n# Ergebnisplots\npar(mfrow = c(1, 1))\n\nxs <- seq(0, 70, l = 1000)\nglm.predict <- predict(glm.1, type = \"response\", se = T, newdata = data.frame(RATIO=xs))\n\nplot(PA~RATIO, data = polis, xlab = \"Umfang-Flächen-Verhältnis\", ylab = \"Vorkommenswahrscheinlichkeit\", pch = 16, col = \"red\")\n     points(glm.predict$fit ~ xs,type=\"l\")\n     lines(glm.predict$fit + glm.predict$se.fit ~ xs, type = \"l\", lty = 2)\n     lines(glm.predict$fit - glm.predict$se.fit ~ xs, type = \"l\", lty = 2)"
  },
  {
    "objectID": "stat1-4/Statistik4_Loesung_2s.html#kommentierter-loesungsweg",
    "href": "stat1-4/Statistik4_Loesung_2s.html#kommentierter-loesungsweg",
    "title": "Stat4: Lösung 2s",
    "section": "kommentierter Loesungsweg",
    "text": "kommentierter Loesungsweg\n\n\n\n\n# Genereiert eine Dummyvariable: Fleisch 1, kein Fleisch 0\ndf <- nova_survey %>%  # kopiert originaler Datensatz\n  rename(umwelteinstellung = tho_2) %>% # änderung name der variable\n  mutate(umwelteinstellung = case_when(umwelteinstellung == 4 ~ 1,\n                                       umwelteinstellung == 3 ~ 1,\n                                       umwelteinstellung == 2 ~ 0, \n                                       umwelteinstellung == 1 ~ 0)) %>% \n  # lasse die kleine Gruppe mit x weg\n  dplyr::filter(!str_detect(string = \"x\", gender)) %>% \n  # lasse die kleine Gruppe \"andere\" weg\n  dplyr::filter(!str_detect(string = \"Andere\", member)) %>%  \n  # wähle nur die relevanten variablen aus\n  dplyr::select(mensa, age_groups, gender, member, umwelteinstellung, meat) \n\n# Schaut euch die Missings an in der Kriteriumsvariable \"mensa\"\nsum(is.na(df$mensa))\n\n[1] 0\n\n# schaut euch die Missings an in den Prädiktorvariablen \"Alter\", \"Geschlecht\", \"Hochschulzugehörigkeit\", \"Umwelteinstellung\"\n\nAmelia::missmap(df) \n\nWarning: Unknown or uninitialised column: `arguments`.\nUnknown or uninitialised column: `arguments`.\n\n\nWarning: Unknown or uninitialised column: `imputations`.\n\n\n\n\n# vieles deutet darauf hin, dass die missings (fehlende Werte) \n# zufällig zustande gekommen sind (sog. MCAR); für mehr Informationen: https://uvastatlab.github.io/2019/05/01/getting-started-with-multiple-imputation-in-r/\n\n# bester Weg wäre, die wenigen fehlenden Werte zu imputieren; \n# einfachheitshalber löschen wir sie aber :)\ndf %<>%\n  drop_na()\n\n#  sieht euch die Verteilung zwischen Mensagänger und Selbstverpfleger an\n# sind nicht gleichmässig verteilt, bei der Vorhersage müssen wir das berücksichtigen\ntable(df$mensa) \n\n\n  0   1 \n282 786 \n\ndf %>% count(mensa) # alternativ\n\n# A tibble: 2 × 2\n  mensa     n\n  <dbl> <int>\n1     0   282\n2     1   786\n\n# definiert das logistische Modell und wendet es auf den Datensatz an\n\nmod0 <-glm(mensa ~ gender + member + age_groups + meat + umwelteinstellung, \n           data = df, binomial(\"logit\"))\nsummary.lm(mod0) # Umwelteinstellung scheint keinen Einfluss auf die \n\n\nCall:\nglm(formula = mensa ~ gender + member + age_groups + meat + umwelteinstellung, \n    family = binomial(\"logit\"), data = df)\n\nWeighted Residuals:\n    Min      1Q  Median      3Q     Max \n-5.6740 -0.8078  0.3712  0.5867  1.2379 \n\nCoefficients:\n                             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                  -0.18889    0.40225  -0.470 0.638750    \ngenderMann                    0.71017    0.16018   4.434 1.02e-05 ***\nmemberStudent/in             -0.63072    0.29442  -2.142 0.032404 *  \nage_groups26- bis 34-jaehrig  1.09429    0.19574   5.591 2.88e-08 ***\nage_groups35- bis 49-jaehrig  1.75379    0.45968   3.815 0.000144 ***\nage_groups50- bis 64-jaehrig  2.43530    0.78923   3.086 0.002083 ** \nmeat                          0.19945    0.05055   3.945 8.49e-05 ***\numwelteinstellung             0.19334    0.18688   1.035 0.301107    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.009 on 1060 degrees of freedom\nMultiple R-squared:  0.004042,  Adjusted R-squared:  -0.002536 \nF-statistic: 0.6145 on 7 and 1060 DF,  p-value: 0.7443\n\n# Verpflegung zu haben, gegeben die Daten\n\n# neues Modell ohne Umwelteinstellung\nmod1 <- update(mod0, ~. -umwelteinstellung)\nsummary.lm(mod1)\n\n\nCall:\nglm(formula = mensa ~ gender + member + age_groups + meat, family = binomial(\"logit\"), \n    data = df)\n\nWeighted Residuals:\n    Min      1Q  Median      3Q     Max \n-6.0117 -0.8060  0.3584  0.6100  1.2407 \n\nCoefficients:\n                             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                   0.03212    0.34053   0.094 0.924860    \ngenderMann                    0.69697    0.15951   4.369 1.37e-05 ***\nmemberStudent/in             -0.64418    0.29426  -2.189 0.028806 *  \nage_groups26- bis 34-jaehrig  1.11651    0.19458   5.738 1.25e-08 ***\nage_groups35- bis 49-jaehrig  1.77409    0.45947   3.861 0.000120 ***\nage_groups50- bis 64-jaehrig  2.44683    0.78953   3.099 0.001992 ** \nmeat                          0.18070    0.04709   3.837 0.000132 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.01 on 1061 degrees of freedom\nMultiple R-squared:  0.003998,  Adjusted R-squared:  -0.001635 \nF-statistic: 0.7098 on 6 and 1061 DF,  p-value: 0.6418\n\n# Modeldiagnostik (wenn nicht signifikant, dann OK)\n1 - pchisq(mod1$deviance, mod1$df.resid) # Ok\n\n[1] 0.4509591\n\n#Modellgüte (pseudo-R²)\n1 - (mod1$dev / mod1$null) # eher kleines pseudo-R2, deckt sich mit dem R-Squared aus dem obigen output summary.lm()\n\n[1] 0.1354244\n\n# Konfusionsmatrix vom  Datensatz\n# Model Vorhersage\n# hier ein anderes Beispiel: \npredicted <- predict(mod1, df, type = \"response\")\n\n# erzeugt eine Tabelle mit den beobachteten\n# Mensagänger/Selbstverpfleger und den Vorhersagen des Modells\nkm <- table(predicted > 0.5, df$mensa) \n# alles was höher ist als 50% ist \n# kommt in die Kategorie Mensagänger\n\n# anpassung der namen\ndimnames(km) <- list(\n  c(\"Modell Selbst\", \"Modell Mensa\"),\n  c(\"Daten Selbst\", \"Daten Mensa\"))\nkm\n\n              Daten Selbst Daten Mensa\nModell Selbst           87          59\nModell Mensa           195         727\n\n#############\n### reminder: https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62\n#############\n\n#TP = true positive: you predicted positive and it’s true; hier vorhersage \n# mensagänger stimmt also (727)\n\n#TN = true negative: you predicted negative and it’s true, hier vorhersage der \n# selbstverpfleger stimmt (87)\n\n#FP = false positive (fehler 1. art, auch spezifizität genannt) you predicted \n# and it’s false. hier modell sagt mensagänger vorher \n# (obwohl in realität selbstverpfleger) (195)\n\n#FN = false negative (fehler 2. art, auch sensitivität genannt), \n# you predicted negative and it’s false. hier modell sagt selbtverpfleger vorher \n# (obwohl in realität mensagänger) (59)\n\n\n# es scheint, dass das Modell häufig einen alpha Fehler zu machen, d.h. es \n# das Modell weist keine hohe Spezifizität auf: konkret werden viele Mensagänger als \n# Selbstverpfleger vorhergesagt resp. klassifiziert. Dafür gibt es mehere Gründe: \n\n#1) die Kriteriumsvariable ist sehr ungleich verteilt, d.h. es gibt weniger\n# Selbstverpfleger als Mensgänger im Datensatz \n \n#2) nicht adäquates Modell z.B. link mit probit zeigt besserer fit\n\n#3) Overfitting: wurde hier nicht berücksichtigt, in einem Paper/Arbeit \n# müsste noch einen Validierungstest gemacht werden z.B. test-train \n# Cross-Validation oder k fold Cross-Validation \n\n# kalkuliert die Missklassifizierungsrate \nmf <- 1-sum(diag(km)/sum(km)) # ist mit knapp 23 %  eher hoch\nmf\n\n[1] 0.2378277\n\n# kleiner exkurs: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2636062/\n# col wise proportion, da diese die \"realität\" ist\nkm_prop <- prop.table(km,2)\n\n# specificity = a / (a+c) => ability of a test to correctly \n# classify an individual as disease-free is called the test′s specificity\nspec = km_prop[1] / (km_prop[1] + km_prop[2])\nspec\n\n[1] 0.3085106\n\n# sensitivity = d / (b+d) => Sensitivity is the ability of a \n# test to correctly classify an individual as ′diseased′\nsens = km_prop[4] / (km_prop[3] + km_prop[4])\nsens\n\n[1] 0.9249364"
  },
  {
    "objectID": "stat1-4/Statistik4_Loesung_2s.html#methode",
    "href": "stat1-4/Statistik4_Loesung_2s.html#methode",
    "title": "Stat4: Lösung 2s",
    "section": "Methode",
    "text": "Methode\nIn der Aufgabe war es das Ziel zu schauen, ob wir einen potenziellen Besuch eines Mensagasts vorhersagen können und zwar in Abhängigkeit von den sozioökonimischen Variablen, wahrgenommene Fleischkonsum und der Umwelteinstellung. Die Kriteriumsvariable “Mensa” weist eine binäre Verteilung auf: Deshalb rechnen wir eine multiple logistische Regression mit den Prädiktoren “Alter”, “Geschlecht”, “Hochschulzugehörigkeit”, “Fleischkonsum” und “Umwelteinstellung. Mehr Informatinen zu den logistischen Regressinen findet ihr im Buch von Crawley (2015) oder auch im Buch von Gareth (2016), Kapitel 4.3; passendes Video hier."
  },
  {
    "objectID": "stat1-4/Statistik4_Loesung_2s.html#ergebnisse",
    "href": "stat1-4/Statistik4_Loesung_2s.html#ergebnisse",
    "title": "Stat4: Lösung 2s",
    "section": "Ergebnisse",
    "text": "Ergebnisse\n\n\n\n\n\n\nDaten Selbst\nDaten Mensa\n\n\n\n\nModell Selbst\n87\n59\n\n\nModell Mensa\n195\n727\n\n\n\nKonfusionsmatrix\n\n\nDer Output des logistischen Models mit der Linkfunktin “logit” sagt und, dass das Modell nicht gut zu den Daten passt, d.h. mit dem Modell (gegeben die Daten) können wir nur schlecht vorhersagen, ob eine Person zukünftig sich in der Mensa verpflegt oder ihr Mittagessen selber mitnimmt. Hinweise dafür geben das kleine pseudo-R2 (14%) als auch die hohe Missklassifizierungsrate (24%): bei genauerer Betrachtung fällt auf, dass das Modell häufig einen alpha-Fehler begeht, d.h. unser Modell sagt zu viele Mensagänger vorher, obwohl diese in Realität Selbstverpfleger sind. Es gibt verschiedene Gründe für diesen schlechten Modelfit:\n\ndie Kriteriumsvariable ist sehr ungleich verteilt, d.h. es gibt weniger Selbstverpfleger als Mensgänger im Datensatz (26% vs. 74%)\ndie Prädiktorvariablen sind alle entweder kategorial oder ordinal: dies kann dazu führen, dass das Model keinen guten fit zu den Daten erzielt\n\nFazit: Es sollte nach einem weiteren adäquateren Modell gesucht werden: insbesondere ein Modell, welches einen mit ordinalen Prädiktorvariablen umgehen kann:\n\neine bessere Link-Funktion für das GLM suchen z.B. probit\npolynomiale Kontraste\nSmooth Splines hier\nmultinomiale Regression z.M. nnet::mulitom()hier"
  },
  {
    "objectID": "stat1-4/Statistik4_Uebung.html",
    "href": "stat1-4/Statistik4_Uebung.html",
    "title": "Stat4: Übung",
    "section": "",
    "text": "Aufgabe 4.1: Nicht-lineare Regression\nDatensatz Curonian_Spit.csv\nDieser enthält gemittelte Pflanzenartenzahlen (Species.richness) von geschachtelten Plots (Vegetationsaufnahmen) der Pflanzengesellschaft LolioCynosuretum im Nationalpark Kurische Nehrung (Russland) auf Flächengrössen (Area) von 0.0001 bis 900 m².\nErmittelt den funktionellen Zusammenhang (das beste Modell), der die Zunahme der Artenzahl mit der Flächengrösse am besten beschreibt.Berücksichtigt dabei mindestens die Potenzfunktion (power function, die logarithmische Funktion (logarithmic function,und eine Funktion mit Sättigung (saturation, asymptote) eurer Wahl.\n\n\nAufgabe 4.2N: Logistische Regression (NatWis)\nDatensatz polis.csv\nDer Datensatz polis.csv beschreibt für 19 Inseln im Golf von Kalifornien, ob Eidechsen der Gattung Uta vorkommen (presence/absence: PA) in Abhängigkeit von der Form der Inseln (Verhältnis Umfang zu Fläche: RATIO).\nBitte prüft mit einer logistischen Regression, ob und ggf. wie die Inselform die Präsenz der Eidechsen beinflusst\n\n\nAufgabe 4.2S: Multiple logistische Regression (SozWis)\nFührt mit dem Datensatz der Gästebefragung eine logistische Regression durch. Kann der Mensabesuch druch die sozioökonomischen Variablen (Alter, Geschlecht, Hochschulzugehörigkeit), wahrgenommener Fleischkonsum und Einstellung zu ?? vorhergesagt werden?\nHinweise:\n\nDas Item tho_2 (“Ich mache mir allgemein Gedanken über die Folgen meiner Ernährungsgewohnheiten für die Umwelt.”) müsst ihr in einem ersten Schritt zu einer Dummy-Variable umcodieren: die Antwortkategorien «stimme eher zu» (=3) und «stimme zu» (=4) müsst ihr eine 1 zuweisen, den anderen zwei Kategorien eine 0. Hinweis dafür könnt ihr die Funktion dpylr::case_when() oder dpylr::if_else() verwenden.\nFehlende Werten könnt ihr weglassen (z.B. dpylr::drop_na())\nDefiniert das Modell und wendet es auf den Datensatz an\nBerechnet eine Vorhersage des Modells mit predict()\nEruiert den Modellfit und die Modellgenauigkeit\nFür Motivierte: Berechnet eine Konfusionsmatrix und zieht euer Fazit daraus\nStellt eure Ergebnisse angemessen dar (Text und/oder Tabelle)"
  },
  {
    "objectID": "stat5-8/Statistik5_Demo.html#split-plot-anova",
    "href": "stat5-8/Statistik5_Demo.html#split-plot-anova",
    "title": "Stat5: Demo",
    "section": "Split-plot ANOVA",
    "text": "Split-plot ANOVA\nBased on Logan (2010), Chapter 14\n\nspf <- read.delim(here(\"data\",\"spf.csv\"), sep = \";\") \nspf.aov <- aov(Reaktion~Signal * Messung + Error(VP), data = spf)\nsummary(spf.aov)\n\ninteraction.plot(spf$Messung, spf$Signal, spf$Reaktion)\n\n# nun als LMM\nif(!require(nlme)){install.packages(\"nlme\")}\nlibrary(nlme)\n\n# mit random intercept (VP) und random slope (Messung)\nspf.lme.1 <- lme(Reaktion~Signal * Messung, random = ~Messung | VP, data = spf)\n# nur random intercept\nspf.lme.2 <- lme(Reaktion~Signal * Messung, random = ~1 | VP, data = spf)\n\nanova(spf.lme.1)\nanova(spf.lme.2)\n\nsummary(spf.lme.1)\nsummary(spf.lme.2)"
  },
  {
    "objectID": "stat5-8/Statistik5_Demo.html#glmm",
    "href": "stat5-8/Statistik5_Demo.html#glmm",
    "title": "Stat5: Demo",
    "section": "GLMM",
    "text": "GLMM\nBased on Zuur et al. (2009), chapter 13\n\nDeerEcervi <- read.delim(here(\"data\",\"DeerEcervi.txt\"), sep = \"\", stringsAsFactors = T)\n\n\n# Anzahl Larven hier in Presence/Absence übersetzt\nDeerEcervi$Ecervi.01 <- DeerEcervi$Ecervi\nDeerEcervi$Ecervi.01[DeerEcervi$Ecervi>0] <- 1\n\n#Numerische Geschlechtscodierung als Factor\nDeerEcervi$fSex <- as.factor(DeerEcervi$Sex)\n\nHirschlänge hier standardisiert, sonst würde der Achsenabschnitt im Modell für einen Hirsch der Länge 0 modelliert, was schlecht interpretierbar ist, jetzt ist der Achsenabschnitt für einen durschnittlich langen Hirsch\n\nDeerEcervi$CLength <- DeerEcervi$Length - mean(DeerEcervi$Length)\n\n# Zunächst als GLM\n# Interaktionen mit fFarm nicht berücksichtigt, da zu viele Freiheitsgrade verbraucht würden\nDE.glm <- glm(Ecervi.01 ~ CLength * fSex + Farm, family = binomial, data = DeerEcervi)\n\ndrop1(DE.glm, test = \"Chi\")\nsummary(DE.glm)\nanova(DE.glm)\n\n\n# Response curves für die einzelnen Farmen (Weibliche Tiere: fSex = \"1\" )\nplot(DeerEcervi$CLength, DeerEcervi$Ecervi.01,\n     xlab = \"Length\", ylab = \"Probability of \\\n     presence of E. cervi L1\")\n\nI <- order(DeerEcervi$CLength)\nAllFarms <- unique(DeerEcervi$Farm)\nfor (j in AllFarms){\n  mydata <- data.frame(CLength=DeerEcervi$CLength, fSex = \"1\",\n                       Farm = j)\n  n <- dim(mydata)[1]\n  if (n>10){\n    P.DE2 <- predict(DE.glm, mydata, type = \"response\")\n    lines(mydata$CLength[I], P.DE2[I])\n  }}"
  },
  {
    "objectID": "stat5-8/Statistik5_Demo.html#glmm-1",
    "href": "stat5-8/Statistik5_Demo.html#glmm-1",
    "title": "Stat5: Demo",
    "section": "GLMM",
    "text": "GLMM\n\nif(!require(MASS)){install.packages(\"MASS\")}\nlibrary(MASS)\nDE.PQL <- glmmPQL(Ecervi.01 ~ CLength * fSex,\n                random = ~ 1 | Farm, family = binomial, data = DeerEcervi)\nsummary(DE.PQL)\n\n\ng <- 0.8883697 + 0.0378608 * DeerEcervi$CLength\np.averageFarm1 <- exp(g)/(1 + exp(g))\nI <- order(DeerEcervi$CLength)  #Avoid spaghetti plot\nplot(DeerEcervi$CLength, DeerEcervi$Ecervi.01, xlab=\"Length\",\n     ylab = \"Probability of presence of E. cervi L1\")\nlines(DeerEcervi$CLength[I], p.averageFarm1[I],lwd = 3)\np.Upp <- exp(g + 1.96 * 1.462108)/(1 + exp(g + 1.96 * 1.462108))\np.Low <- exp(g - 1.96 * 1.462108)/(1 + exp(g - 1.96 * 1.462108))\nlines(DeerEcervi$CLength[I], p.Upp[I])\nlines(DeerEcervi$CLength[I], p.Low[I])\n\n\nif(!require(lme4)){install.packages(\"lme4\")}\nlibrary(lme4)\nDE.lme4 <- glmer(Ecervi.01 ~ CLength * fSex + (1|Farm), \n                 family = binomial, data = DeerEcervi)\nsummary(DE.lme4)\n\nif(!require(glmmML)){install.packages(\"glmmML\")}\nlibrary(glmmML)\nDE.glmmML <- glmmML(Ecervi.01 ~ CLength * fSex,\n                  cluster = Farm, family = binomial, data = DeerEcervi)\nsummary(DE.glmmML)"
  },
  {
    "objectID": "stat5-8/Statistik5_Loesung_1.html#musterloesung-aufgabe-5.1-split-plot-anova",
    "href": "stat5-8/Statistik5_Loesung_1.html#musterloesung-aufgabe-5.1-split-plot-anova",
    "title": "Stat5: Lösung 1",
    "section": "Musterloesung Aufgabe 5.1: Split-plot ANOVA",
    "text": "Musterloesung Aufgabe 5.1: Split-plot ANOVA\nÜbungsaufgabe (hier so ausführlich formuliert, wie dies auch in der Klausur der Fall sein wird)\n\nLadet den Datensatz splityield.csv. Dieser enthält Versuchsergebnisse eines Experiments zum Ernteertrag (yield) einer Kulturpflanze in Abhängigkeit der drei Faktoren Bewässerung (irrigated vs. control), Düngung (N, NP, P) und Aussaatdichten (low, medium, high). Es gab vier ganze Felder (block), die zwei Hälften mit den beiden Bewässerungstreatments (irrigation), diese wiederum drei Drittel für die drei Saatdichten (density) und diese schliesslich je drei Drittel für die drei Düngertreatments (fertilizer) hatten.\nErmittelt das minimal adäquate statistische Modell, das den Ernteertrag in Abhängigkeit von den angegebenen Faktoren beschreibt.\nBitte erklärt und begründet die einzelnen Schritte, die ihr unternehmt, um zu diesem Ergebnis zu kommen. Dazu erstellt bitte ein Word-Dokument, in das ihr Schritt für Schritt den verwendeten R-Code, die dazu gehörigen Ausgaben von R, eure Interpretation derselben und die sich ergebenden Schlussfolgerungen für das weitere Vorgehen dokumentieren.\nDieser Ablauf sollte insbesondere beinhalten:\n\nÜberprüfen der Datenstruktur nach dem Einlesen, welches sind die abhängige(n) und welches die unabängige(n) Variablen\nExplorative Datenanalyse, um zu sehen, ob evtl. Dateneingabefehler vorliegen oder Datentransformationen vorgenommen werden sollten\nAuswahl und Begründung eines statistischen Verfahrens\nBestimmung des vollständigen/maximalen Models\nSelektion des/der besten Models/Modelle\nGenerieren aller Zahlen, Statistiken und Tabellen, die für eine wiss\n(Ergebnisdarstellung benötigt werden)\nFormuliert abschliessend einen Methoden- und Ergebnisteil (ggf. incl. adäquaten Abbildungen) zu dieser Untersuchung in der Form einer wissenschaftlichen Arbeit (ausformulierte schriftliche Zusammenfassung, mit je einem Absatz von ca. 60-100 Worten, resp. 3-8 Sätzen für den Methoden- und Ergebnisteil). D. h. alle wichtigen Informationen sollten enthalten sein, unnötige Redundanz dagegen vermieden werden.\nAbzugeben sind am Ende (a) Ein lauffähiges R-Skript; (b) begründeter Lösungsweg (Kombination aus R-Code, R Output und dessen Interpretation) und (c) ausformulierter Methoden- und Ergebnisteil (für eine wiss. Arbeit).\n\nR-Skript als Download\nLoesungstext\n\nkommentierter Lösungsweg\n\nsplityield <- read.delim(here(\"data\",\"splityield.csv\"), sep = \",\", stringsAsFactors = T)\n\n\n# Checken der eingelesenen Daten\nsplityield\n\n    X yield block irrigation density fertilizer\n1   1    90     A    control     low          N\n2   2    95     A    control     low          P\n3   3   107     A    control     low         NP\n4   4    92     A    control  medium          N\n5   5    89     A    control  medium          P\n6   6    92     A    control  medium         NP\n7   7    81     A    control    high          N\n8   8    92     A    control    high          P\n9   9    93     A    control    high         NP\n10 10    80     A  irrigated     low          N\n11 11    87     A  irrigated     low          P\n12 12   100     A  irrigated     low         NP\n13 13   121     A  irrigated  medium          N\n14 14   110     A  irrigated  medium          P\n15 15   119     A  irrigated  medium         NP\n16 16    78     A  irrigated    high          N\n17 17    98     A  irrigated    high          P\n18 18   122     A  irrigated    high         NP\n19 19    83     B    control     low          N\n20 20    80     B    control     low          P\n21 21    95     B    control     low         NP\n22 22    98     B    control  medium          N\n23 23    98     B    control  medium          P\n24 24   106     B    control  medium         NP\n25 25    74     B    control    high          N\n26 26    81     B    control    high          P\n27 27    74     B    control    high         NP\n28 28   102     B  irrigated     low          N\n29 29   109     B  irrigated     low          P\n30 30   105     B  irrigated     low         NP\n31 31    99     B  irrigated  medium          N\n32 32    94     B  irrigated  medium          P\n33 33   123     B  irrigated  medium         NP\n34 34   136     B  irrigated    high          N\n35 35   133     B  irrigated    high          P\n36 36   132     B  irrigated    high         NP\n37 37    85     C    control     low          N\n38 38    88     C    control     low          P\n39 39    88     C    control     low         NP\n40 40   112     C    control  medium          N\n41 41   104     C    control  medium          P\n42 42    91     C    control  medium         NP\n43 43    82     C    control    high          N\n44 44    78     C    control    high          P\n45 45    94     C    control    high         NP\n46 46    60     C  irrigated     low          N\n47 47   104     C  irrigated     low          P\n48 48   114     C  irrigated     low         NP\n49 49    90     C  irrigated  medium          N\n50 50   118     C  irrigated  medium          P\n51 51   113     C  irrigated  medium         NP\n52 52   119     C  irrigated    high          N\n53 53   122     C  irrigated    high          P\n54 54   136     C  irrigated    high         NP\n55 55    86     D    control     low          N\n56 56    78     D    control     low          P\n57 57    89     D    control     low         NP\n58 58    79     D    control  medium          N\n59 59    86     D    control  medium          P\n60 60    87     D    control  medium         NP\n61 61    85     D    control    high          N\n62 62    89     D    control    high          P\n63 63    83     D    control    high         NP\n64 64    73     D  irrigated     low          N\n65 65   114     D  irrigated     low          P\n66 66   114     D  irrigated     low         NP\n67 67   109     D  irrigated  medium          N\n68 68   131     D  irrigated  medium          P\n69 69   126     D  irrigated  medium         NP\n70 70   116     D  irrigated    high          N\n71 71   136     D  irrigated    high          P\n72 72   133     D  irrigated    high         NP\n\n\nMan sieht, dass das Design vollkommen balanciert ist, d.h. jede Kombination irrigation  density  fertilizer kommt genau 4x vor (in jedem der vier Blöcke A-D einmal).\n\nstr(splityield)\n\n'data.frame':   72 obs. of  6 variables:\n $ X         : int  1 2 3 4 5 6 7 8 9 10 ...\n $ yield     : int  90 95 107 92 89 92 81 92 93 80 ...\n $ block     : Factor w/ 4 levels \"A\",\"B\",\"C\",\"D\": 1 1 1 1 1 1 1 1 1 1 ...\n $ irrigation: Factor w/ 2 levels \"control\",\"irrigated\": 1 1 1 1 1 1 1 1 1 2 ...\n $ density   : Factor w/ 3 levels \"high\",\"low\",\"medium\": 2 2 2 3 3 3 1 1 1 2 ...\n $ fertilizer: Factor w/ 3 levels \"N\",\"NP\",\"P\": 1 3 2 1 3 2 1 3 2 1 ...\n\nsummary(splityield)\n\n       X             yield        block      irrigation   density   fertilizer\n Min.   : 1.00   Min.   : 60.00   A:18   control  :36   high  :24   N :24     \n 1st Qu.:18.75   1st Qu.: 86.00   B:18   irrigated:36   low   :24   NP:24     \n Median :36.50   Median : 95.00   C:18                  medium:24   P :24     \n Mean   :36.50   Mean   : 99.72   D:18                                        \n 3rd Qu.:54.25   3rd Qu.:114.00                                               \n Max.   :72.00   Max.   :136.00                                               \n\nsplityield$density <- ordered(splityield$density, levels = c(\"low\", \"medium\", \"high\"))\nsplityield$density\n\n [1] low    low    low    medium medium medium high   high   high   low   \n[11] low    low    medium medium medium high   high   high   low    low   \n[21] low    medium medium medium high   high   high   low    low    low   \n[31] medium medium medium high   high   high   low    low    low    medium\n[41] medium medium high   high   high   low    low    low    medium medium\n[51] medium high   high   high   low    low    low    medium medium medium\n[61] high   high   high   low    low    low    medium medium medium high  \n[71] high   high  \nLevels: low < medium < high\n\n\nMan sieht, dass die Variable yield metrisch ist, während die vier anderen Variablen schon korrekt als kategoriale Variablen (factors) kodiert sind\n\n# Explorative Datenanalyse (auf Normalverteilung, Varianzhomogenität)\nboxplot(yield~fertilizer, data = splityield)\n\n\n\nboxplot(yield~irrigation, data = splityield)\n\n\n\nboxplot(yield~density, data = splityield)\n\n\n\nboxplot(yield~irrigation * density * fertilizer, data = splityield)\n\n\n\n\nDie Boxplots sind generell hinreichend symmetrisch, so dass man davon ausgehen kann, dass keine problematische Abweichung von der Normalverteilung vorliegt. Die Varianzhomogenität sieht für den Gesamtboxplot sowie für fertilizer und density bestens aus, für irrigation und für die 3-fach-Interaktion deuten sich aber gewisse Varianzheterogenitäten an, d. h. die Boxen (Interquartil-Spannen) sind deutlich unterschiedlich lang. Da das Design aber vollkommen „balanciert“ war, wie wir von oben wissen, sind selbst relativ stark divergierende Varianzen nicht besonders problematisch. Der Boxplot der Dreifachinteraktion zeigt zudem, dass grössere Varianzen (~Boxen) mal bei kleinen, mal bei grossen Mittelwerten vorkommen, womit wir bedenkenlos weitermachen können (Wenn die grossen Varianzen immer bei grossen Mittelwerten aufgetreten wären, hätten wir eine log- oder Wurzeltransformation von yield in Betracht ziehen müssen).\n\nboxplot(log10(yield)~irrigation * density * fertilizer, data = splityield)# bringt keine Verbesserung\n\n\n\naov.1 <- aov(yield~irrigation * density * fertilizer + Error(block/irrigation/density), data = splityield)\n\nDas schwierigste an der Analyse ist hier die Definition des Splitt-Plot ANOVA-Modells. Hier machen wir es mit der einfachsten Möglichkeit, dem aov-Befehl. Um diesen richtig zu spezifieren, muss man verstanden haben, welches der „random“-Faktor war und wie die „fixed“ factors ineinander geschachtelt waren. In diesem Fall ist block der random Faktor, in den zunächst irrigation und dann density geschachtelt sind (die unterste Ebene fertilizer muss man nicht mehr angeben, da diese in der nächsthöheren nicht repliziert ist).\n(Übrigens: das simple 3-faktorielle ANOVA-Modell aov(yield~irrigationdensityfertilizer,data=splityield) würde unterstellen, dass alle 72 subplot unabhängig von allen anderen angeordnet sind, also nicht in Blöcken. Man kann ausprobieren, wie sich das Ergebnis mit dieser Einstellung unterscheidet)\n\nsummary(aov.1)\n\n\nError: block\n          Df Sum Sq Mean Sq F value Pr(>F)\nResiduals  3  194.4   64.81               \n\nError: block:irrigation\n           Df Sum Sq Mean Sq F value Pr(>F)  \nirrigation  1   8278    8278   17.59 0.0247 *\nResiduals   3   1412     471                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nError: block:irrigation:density\n                   Df Sum Sq Mean Sq F value Pr(>F)  \ndensity             2   1758   879.2   3.784 0.0532 .\nirrigation:density  2   2747  1373.5   5.912 0.0163 *\nResiduals          12   2788   232.3                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nError: Within\n                              Df Sum Sq Mean Sq F value   Pr(>F)    \nfertilizer                     2 1977.4   988.7  11.449 0.000142 ***\nirrigation:fertilizer          2  953.4   476.7   5.520 0.008108 ** \ndensity:fertilizer             4  304.9    76.2   0.883 0.484053    \nirrigation:density:fertilizer  4  234.7    58.7   0.680 0.610667    \nResiduals                     36 3108.8    86.4                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWir bekommen p-Werte für die drei Einzeltreatments, die drei 2-fach-Interaktionen und die 3- fach Interaktion. Keinen p-Wert gibt es dagegen für block, da dieser als „random“ Faktor spezifiziert wurde. Signifikant sind für sich genommen irrigation und fertilizer sowie die Interaktionen irrigation:density und irrigation:fertilizer.\n\n# Modelvereinfachung\naov.2 <- aov(yield ~ irrigation + density + fertilizer + irrigation:density + \n               irrigation:fertilizer + density:fertilizer + Error(block/irrigation/density), data = splityield)\nsummary(aov.2)\n\n\nError: block\n          Df Sum Sq Mean Sq F value Pr(>F)\nResiduals  3  194.4   64.81               \n\nError: block:irrigation\n           Df Sum Sq Mean Sq F value Pr(>F)  \nirrigation  1   8278    8278   17.59 0.0247 *\nResiduals   3   1412     471                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nError: block:irrigation:density\n                   Df Sum Sq Mean Sq F value Pr(>F)  \ndensity             2   1758   879.2   3.784 0.0532 .\nirrigation:density  2   2747  1373.5   5.912 0.0163 *\nResiduals          12   2788   232.3                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nError: Within\n                      Df Sum Sq Mean Sq F value   Pr(>F)    \nfertilizer             2   1977   988.7  11.828 9.21e-05 ***\nirrigation:fertilizer  2    953   476.7   5.703  0.00662 ** \ndensity:fertilizer     4    305    76.2   0.912  0.46639    \nResiduals             40   3344    83.6                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\naov.3 <- aov(yield ~ irrigation + density + fertilizer + irrigation:density + \n               irrigation:fertilizer+ Error(block/irrigation/density), data = splityield)\nsummary(aov.3)\n\n\nError: block\n          Df Sum Sq Mean Sq F value Pr(>F)\nResiduals  3  194.4   64.81               \n\nError: block:irrigation\n           Df Sum Sq Mean Sq F value Pr(>F)  \nirrigation  1   8278    8278   17.59 0.0247 *\nResiduals   3   1412     471                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nError: block:irrigation:density\n                   Df Sum Sq Mean Sq F value Pr(>F)  \ndensity             2   1758   879.2   3.784 0.0532 .\nirrigation:density  2   2747  1373.5   5.912 0.0163 *\nResiduals          12   2788   232.3                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nError: Within\n                      Df Sum Sq Mean Sq F value   Pr(>F)    \nfertilizer             2   1977   988.7  11.924 7.28e-05 ***\nirrigation:fertilizer  2    953   476.7   5.749  0.00605 ** \nResiduals             44   3648    82.9                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nJetzt muss man nur noch herausfinden, wie irrigation und fertilizer wirken und wie die Interaktionen aussehen. Bei multiplen ANOVAs macht man das am besten visuell:\n\n# Visualisierung der Ergebnisse\nboxplot(yield~fertilizer, data = splityield)\n\n\n\nboxplot(yield~irrigation, data = splityield)\n\n\n\ninteraction.plot(splityield$fertilizer, splityield$irrigation, splityield$yield, \n                 xlab= \"fertilizer\", ylab = \"mean of splityield\", trace.label = \"irrigation\")\n\n\n\ninteraction.plot(splityield$density, splityield$irrigation, splityield$yield, \n                 xlab= \"fertilizer\", ylab = \"mean of splityield\", trace.label = \"irrigation\")"
  },
  {
    "objectID": "stat5-8/Statistik5_Loesung_2.html#kommentierter-lösungsweg",
    "href": "stat5-8/Statistik5_Loesung_2.html#kommentierter-lösungsweg",
    "title": "Stat5: Lösung 2",
    "section": "kommentierter Lösungsweg",
    "text": "kommentierter Lösungsweg\n\n\n\n\ndf <- nova # kopiert originaler Datensatz\n\n# Genereiert eine Dummyvariable: Fleisch 1, kein Fleisch 0\ndf %>%\n  # entfernt Personen die sich ein Buffet Teller gekauft\n  filter(label_content != \"Hot and Cold\") %>%\n  # ihr könnt keine Angabe vernachlässigen, sind (nur) 54 Personen\n  filter(age_group != \"keine Angaben\") %>% \n  mutate(label_content = str_replace_all(.$label_content, c(\"Fisch|Geflügel\"),\"Fleisch\")) %>%   \n  mutate(meat = if_else(.$label_content == \"Fleisch\", 1, 0)) %>%\n  # setzt andere Reihenfolge für die Hochschulzugehörigkeit, nur für die Interpretation\n  # nützlich: neu Referenzkategorie Studierende (vorher Mitarbeitende)\n  mutate(member = factor(.$member, levels = c(\"Studierende\", \"Mitarbeitende\")))  \n\n# wie viele NA's hat es dirn (uns interessiert v.a. die responsevariable: meat)\nsum(is.na(df$meat)) #Amelia::missmap(df_)\n\n# sieht euch die Verteilung zwischen Fleisch und  kein Fleisch an, \n# beide kategorien kommen nicht gleich häufig vor, aber nicht super tragisch\nprop.table(table(df$meat)) # gibt die prozente an\ntable(df$meat) # gibt die absoluten werte an\n\n# definiert das logistische Modell mit ccrs als random intercept und \n# wendet es auf den Datensatz an\n\n#check out ICC: https://www.datanovia.com/en/lessons/intraclass-correlation-coefficient-in-r/\n#however data needs to be wide format\n#not working yet\n#df %>% dplyr::select(ccrs, label_content) %>% tidyr::pivot_wider(names_from = \"ccrs\", values_fill = \"label_content\") -> tt\n\nlibrary(lme4)\n#dauert ein paar sekunden\nmod0 <- glmer(meat ~ gender + member + age_group + (1|ccrs),\n              data = df, binomial(\"logit\")) \n\n# lasst euch das Modell anzeigen: sieht so aus, als ob v.a. Geschlecht eine \n# Rolle spielt\n# wahrnmeldung kann vernachlässigt werden (aufgrund der unicode resp. \n# umlaute in den variablen)\nsummary(mod0) \n\n## erste Interpretation: Geschlecht (Mann) und Alter (junge Personen) scheinen den Fleischkonsum positiv zu beeinflussen + Hochschulzugehörigkeit spielt keien Rolle\n# d.h. könnte man vernachlässigen. Ich lasse aus inhaltlichen Gründen aber im Modell drin\n\n# Pseudo R^2\nlibrary(MuMIn)\nr.squaredGLMM(mod0) \n# das marginale R^2 (r2m) gibt uns die erklärte Varianz der fixen Effekte: hier 4% (das ist sehr wenig)\n# das conditionale R^2 (r2c) gibt uns die erklärte Varianz für das ganze Modell \n# (mit fixen und variablen Effekten): hier 27% (ganz ok, aber auch nicht super mega)\n# für weitere Informationen: https://rdrr.io/cran/MuMIn/man/r.squaredGLMM.html \n\n# zusätzliche Informationen, welche für die Interpretation gut sein kann\n# berechnet den Standardfehler (mehr infos: https://www.youtube.com/watch?v=r-txC-dpI-E oder hier: https://mgimond.github.io/Stats-in-R/CI.html)\n# weitere info: https://stats.stackexchange.com/questions/26650/how-do-i-reference-a-regression-models-coefficients-standard-errors\nse <- sqrt(diag(vcov(mod0)))\n\n# zeigt eine Tabelle der Schätzer mit 95% Konfidenzintervall (KI)\n# => Faustregel: falls 0 im KI enthalten ist, dann ist der Unterschied statistisch NICHT signifikant\ntab1 <- cbind(Est = fixef(mod0), LL = fixef(mod0) - 1.96 * se,\n              UL = fixef(mod0) + 1.96 * se)\n\n# erzeugt die Odds Ratios\ntab2 <- exp(tab1)"
  },
  {
    "objectID": "stat5-8/Statistik5_Loesung_2.html#methoden",
    "href": "stat5-8/Statistik5_Loesung_2.html#methoden",
    "title": "Stat5: Lösung 2",
    "section": "Methoden",
    "text": "Methoden\nDie Responsevariable “Fleischkonsum” ist eine binäre Variable. Demnach wird eine multiple logistische Regression mit den Prädiktoren “Alter (Gruppen)”, “Geschlecht” und “Hochschulzugehörigkeit” gerechnet. Da in den Daten gewisse Individuen mehrmals vorkommen, wird das Individuum (Variable ccrs) als variabler Effekt in das Modell aufgenommen."
  },
  {
    "objectID": "stat5-8/Statistik5_Loesung_2.html#ergebnisse",
    "href": "stat5-8/Statistik5_Loesung_2.html#ergebnisse",
    "title": "Stat5: Lösung 2",
    "section": "Ergebnisse",
    "text": "Ergebnisse\nDas Geschlecht und das Alter nehmen einen signifikanten Einfluss auf den Fleischkonsum (siehe Table 1): Männer kaufen signifikant häufiger ein fleischhaltiges Gericht als Frauen; junge Personen (15 bis 25-jährig) kaufen signifikant häufiger ein fleischhaltiges Gericht in der Mensa. Es sieht so aus, als ob die Hochschulzugehörigkeit auf den ersten Blick keinen Einfluss nimmt. Aber man müsste auch die Interaktion zwischen Geschlecht und Hochschulzugehörigkeit berücksichtigen, um ein abschliessendes Bild zu bekommen. Das kleine marginale Pseudo-R^2 zeigt auf, dass es nicht das “beste” Modell ist. Insbesondere die tiefe Varianzaufklärung für die randomisierte Variable (r2c; ccrs) scheint mit (nur) 4% sehr gering. Das sind Hinweise dafür, dass das Modell ggf. noch weitere Ebenen haben könnte (z.B. Standort Mensa).\n\n\n\nDie Chance, dass Männer ein fleischhaltiges Gericht kaufen ist 2.36mal (+136%) höher als bei Frauen (siehe Table 2). Die Chance, dass 26 bis 34-jährige Personen ein fleischhaltiges Gericht kaufen ist kleiner (-21%) als bei den 15 bis 25-jährigen Personen."
  },
  {
    "objectID": "stat5-8/Statistik5_Uebung.html",
    "href": "stat5-8/Statistik5_Uebung.html",
    "title": "Stat5: Übung",
    "section": "",
    "text": "Aufgabe 5.1: Split-plot ANOVA\nDatensatz splityield.csv\nVersuch zum Ernteertrag (yield) einer Kulturpflanze in Abhängigkeit der drei Faktoren Bewässerung (irrigated vs. control), Düngung (N, NP, P) und Aussaatdichten (low, medium, high). Es gab vier ganze Felder (block), die zwei Hälften mit den beiden Bewässerungstreatments, diese wiederum drei Drittel für die drei Saatdichten und diese schliesslich je drei Drittel für die drei Düngertreatments hatten.\nAufgaben\n\nBestimmt das minimal adäquate Modell\nStellt die Ergebnisse da\n\n\n\nAufgabe 5.2: GLMM\nFührt mit dem Datensatz novanimal.csv eine logistische Regression durch, wobei ihr die einzelnen Käufer (single campus_card holder) als weitere randomisierte Variable mitberücksichtigt. Kann der Fleischkonsum durch das Geschlecht, die Hochschulzugehörigkeit und das Alter erklärt werden? Vergleich die Ergebnisse mit der eurem multiplen logistische Modell von Aufgabe 4.2\nKann der Fleischkonsum nun besser durch das Geschlecht, die Hochschulzugehörigkeit und das Alter erklärt werden?\nAufgaben\n\nBestimmt das minimal adäquate Modell\nStellt die Ergebnisse dar\n\nÄhnliches Vorgehen wie bei der Übung 4.2S:\n\nGeneriert eine neue Variable “Fleisch” (0 = kein Fleisch, 1 = Fleisch)\nEntfernt fehlende Werte aus der Variable “Fleisch”\nLasst für die Analyse den Menüinhalt “Buffet” weg"
  },
  {
    "objectID": "stat5-8/Statistik6_Demo.html#pca",
    "href": "stat5-8/Statistik6_Demo.html#pca",
    "title": "Stat6: Demo",
    "section": "PCA",
    "text": "PCA\n\nif(!require(labdsv)){install.packages(\"labdsv\")}\n\nLoading required package: labdsv\n\n\nLoading required package: mgcv\n\n\nLoading required package: nlme\n\n\nThis is mgcv 1.8-40. For overview type 'help(\"mgcv-package\")'.\n\n\nThis is labdsv 2.0-1\nconvert existing ordinations with as.dsvord()\n\n\n\nAttaching package: 'labdsv'\n\n\nThe following object is masked from 'package:stats':\n\n    density\n\nlibrary(labdsv)\n\n# Für Ordinationen benötigen wir Matrizen, nicht Data.frames\n# Generieren von Daten\nraw <- matrix(c(1, 2, 2.5, 2.5, 1, 0.5, 0, 1, 2, 4, 3, 1), nrow = 6)\ncolnames(raw) <- c(\"spec.1\", \"spec.2\")\nrownames(raw) <- c(\"r1\", \"r2\", \"r3\", \"r4\", \"r5\", \"r6\")\nraw\n\n   spec.1 spec.2\nr1    1.0      0\nr2    2.0      1\nr3    2.5      2\nr4    2.5      4\nr5    1.0      3\nr6    0.5      1\n\n# Originale Daten im zweidimensionalen Raum\nx1 <- raw[,1]\ny1 <- raw[,2]\nz <- c(rep(1:6))\n\n\n# Plot Abhängigkeit der Arten vom Umweltgradienten\nplot(c(x1, y1)~c(z, z), type = \"n\", axes = T, bty = \"l\", \n     las = 1, xlim = c(1,6), ylim = c(0,5),\n     xlab = \"Umweltgradient\", ylab = \"Deckung der Arten\")\npoints(x1~z, pch = 21, type = \"b\")\npoints(y1~z, pch = 16, type = \"b\")\n\n\n\n# zentrierte Daten\ncent <- scale(raw, scale = F)\nx2 <- cent[,1]\ny2 <- cent[,2]\n\n# rotierte Daten\no.pca <- pca(raw)\nx3 <- o.pca$scores[,1]\ny3 <- o.pca$scores[,2]\n\n# Visualisierung der Schritte im Ordinationsraum\nplot(c(y1, y2, y3)~c(x1, x2, x3), type = \"n\", axes = T, bty = \"l\", las = 1,\n     xlim = c(-4, 4), ylim = c(-4, 4), xlab = \"Art 1\", ylab=  \"Art 2\")\npoints(y1~x1, pch = 21, type = \"b\", col = \"green\", lwd = 2)\npoints(y2~x2, pch = 16, type = \"b\",col = \"red\", lwd = 2)\npoints(y3~x3, pch = 17, type = \"b\", col = \"blue\", lwd = 2)\n\n\n\n# Durchführung der PCA\no.pca <- pca(raw)\n\n# Koordinaten im Ordinationsraum\no.pca$scores\n\n          PC1         PC2\nr1 -1.9216223 -0.09357697\nr2 -0.6353776 -0.68143293\nr3  0.4762699 -0.80076373\nr4  2.3503705 -0.10237502\nr5  0.8895287  0.95400610\nr6 -1.1591692  0.72414255\n\n# Korrelationen der Variablen mit den Ordinationsachsen\no.pca$loadings\n\n             PC1        PC2\nspec.1 0.3491944 -0.9370503\nspec.2 0.9370503  0.3491944\n\n#Erklärte Varianz der Achsen\nE <- o.pca$sdev^2 / o.pca$totdev * 100\nE\n\n[1] 82.40009 17.59991\n\n# mit prcomp\npca.2 <- prcomp(raw, scale = F)\nsummary(pca.2)\n\nImportance of components:\n                         PC1    PC2\nStandard deviation     1.548 0.7154\nProportion of Variance 0.824 0.1760\nCumulative Proportion  0.824 1.0000\n\nplot(pca.2)\n\n\n\nbiplot(pca.2)\n\n\n\n# mit vegan\nif(!require(vegan)){install.packages(\"vegan\")}\n\nLoading required package: vegan\n\n\nLoading required package: permute\n\n\nLoading required package: lattice\n\n\nThis is vegan 2.6-2\n\nlibrary(\"vegan\")\n# Die Funktion rda führt ein PCA aus an wenn nicht Artdaten UND Umweltdaten definiert werden\npca.3 <- rda(raw, scale = FALSE)\n#scores(pca.3, display = c(\"sites\"))\n#scores(pca.3, display = c(\"species\"))\nsummary(pca.3, axes = 0)\n\n\nCall:\nrda(X = raw, scale = FALSE) \n\nPartitioning of variance:\n              Inertia Proportion\nTotal           2.908          1\nUnconstrained   2.908          1\n\nEigenvalues, and their contribution to the variance \n\nImportance of components:\n                        PC1    PC2\nEigenvalue            2.396 0.5119\nProportion Explained  0.824 0.1760\nCumulative Proportion 0.824 1.0000\n\nScaling 2 for species and site scores\n* Species are scaled proportional to eigenvalues\n* Sites are unscaled: weighted dispersion equal on all dimensions\n* General scaling constant of scores:  \n\nbiplot(pca.3)\n\n\n\n# Mit Beispieldaten aus Wildi\nif(!require(dave)){install.packages(\"dave\")}\n\nLoading required package: dave\n\n\nLoading required package: cluster\n\n\nLoading required package: nnet\n\n\n\nAttaching package: 'nnet'\n\n\nThe following object is masked from 'package:mgcv':\n\n    multinom\n\n\nLoading required package: tree\n\nlibrary(dave)\ndata(sveg)\n\n\nstr(sveg)\nsummary(sveg)\nnames(sveg)\n\n\n# PCA: Deckungen Wurzeltransformiert, cor=T erzwingt Nutzung der Korrelationsmatrix\npca.5 <- pca(sveg^0.25, cor = T)\n\n\n# Koordinaten im Ordinationsraum\npca.5$scores\n\n# Korrelationen der Variablen mit den Ordinationsachsen\npca.5$loadings\n\n\n# Erklärte Varianz der Achsen in Prozent (sdev ist die Wurzel daraus)\nE <- pca.5$sdev^2 / pca.5$totdev * 100\nE\n\n [1] 2.061885e+01 8.098205e+00 6.070537e+00 3.666650e+00 3.322363e+00\n [6] 3.128942e+00 3.003875e+00 2.634636e+00 2.605558e+00 2.449637e+00\n[11] 2.339344e+00 2.265430e+00 2.116464e+00 2.046578e+00 1.969912e+00\n[16] 1.871020e+00 1.777063e+00 1.693483e+00 1.524015e+00 1.503332e+00\n[21] 1.434245e+00 1.378271e+00 1.329404e+00 1.291336e+00 1.251895e+00\n[26] 1.186157e+00 1.109340e+00 1.068661e+00 1.044385e+00 9.891552e-01\n[31] 9.764586e-01 8.869747e-01 8.451212e-01 8.049318e-01 7.603242e-01\n[36] 7.311274e-01 6.945830e-01 6.339064e-01 6.063542e-01 5.502527e-01\n[41] 5.411059e-01 4.956931e-01 4.795188e-01 4.601244e-01 3.936176e-01\n[46] 3.477631e-01 3.402128e-01 3.165971e-01 2.951856e-01 2.728882e-01\n[51] 2.635725e-01 2.233500e-01 2.125542e-01 1.989449e-01 1.681852e-01\n[56] 1.555571e-01 1.485298e-01 1.271079e-01 9.164615e-02 7.880113e-02\n[61] 5.913306e-02 5.113452e-02 4.066351e-30\n\nE[1:5]\n\n[1] 20.618848  8.098205  6.070537  3.666650  3.322363\n\n# PCA-Plot der Lage der Beobachtungen im Ordinationsraum\nplot(pca.5$scores[,1], pca.5$scores[,2], type = \"n\", asp = 1, xlab = \"PC1\", ylab = \"PC2\")\npoints(pca.5$scores[,1], pca.5$scores[,2], pch = 18)\n\n\n\n# Subjektive Auswahl von Arten zur Darstellung\nsel.sp <- c(3, 11, 23, 39, 46, 72, 77, 96)\nsnames <- names(sveg[,sel.sp])\nsnames\n\n[1] \"Vaccinium.oxycoccos\" \"Carex.echinata\"      \"Arnica.montana\"     \n[4] \"Carex.pulicaris\"     \"Sphagnum.recurvum\"   \"Viola.palustris\"    \n[7] \"Galium.uliginosum\"   \"Stachys.officinalis\"\n\n# PCA-Plot der Korrelationen der Variablen (hier Arten) mit den Achsen (h)\nx <- pca.5$loadings[,1]\ny <- pca.5$loadings[,2]\nplot(x, y, type = \"n\", asp = 1)\narrows(0,0, x[sel.sp], y[sel.sp], length = 0.08)\ntext(x[sel.sp], y[sel.sp], snames, pos = 1, cex = 0.6)\n\n\n\n# Mit vegan\npca.6 <- rda(sveg^0.25, scale = TRUE)\n# Erklärte Varianz der Achsen\nsummary(pca.6, axes = 0)\n\n\nCall:\nrda(X = sveg^0.25, scale = TRUE) \n\nPartitioning of correlations:\n              Inertia Proportion\nTotal             119          1\nUnconstrained     119          1\n\nEigenvalues, and their contribution to the correlations \n\nImportance of components:\n                          PC1     PC2     PC3     PC4     PC5     PC6     PC7\nEigenvalue            24.5364 9.63686 7.22394 4.36331 3.95361 3.72344 3.57461\nProportion Explained   0.2062 0.08098 0.06071 0.03667 0.03322 0.03129 0.03004\nCumulative Proportion  0.2062 0.28717 0.34788 0.38454 0.41777 0.44906 0.47909\n                          PC8     PC9   PC10    PC11    PC12    PC13    PC14\nEigenvalue            3.13522 3.10061 2.9151 2.78382 2.69586 2.51859 2.43543\nProportion Explained  0.02635 0.02606 0.0245 0.02339 0.02265 0.02116 0.02047\nCumulative Proportion 0.50544 0.53150 0.5560 0.57939 0.60204 0.62320 0.64367\n                        PC15    PC16    PC17    PC18    PC19    PC20    PC21\nEigenvalue            2.3442 2.22651 2.11470 2.01524 1.81358 1.78896 1.70675\nProportion Explained  0.0197 0.01871 0.01777 0.01693 0.01524 0.01503 0.01434\nCumulative Proportion 0.6634 0.68208 0.69985 0.71679 0.73203 0.74706 0.76140\n                         PC22    PC23    PC24    PC25    PC26    PC27    PC28\nEigenvalue            1.64014 1.58199 1.53669 1.48976 1.41153 1.32011 1.27171\nProportion Explained  0.01378 0.01329 0.01291 0.01252 0.01186 0.01109 0.01069\nCumulative Proportion 0.77518 0.78848 0.80139 0.81391 0.82577 0.83687 0.84755\n                         PC29     PC30     PC31    PC32     PC33     PC34\nEigenvalue            1.24282 1.177095 1.161986 1.05550 1.005694 0.957869\nProportion Explained  0.01044 0.009892 0.009765 0.00887 0.008451 0.008049\nCumulative Proportion 0.85800 0.867887 0.877652 0.88652 0.894973 0.903022\n                          PC35     PC36     PC37     PC38     PC39     PC40\nEigenvalue            0.904786 0.870042 0.826554 0.754349 0.721562 0.654801\nProportion Explained  0.007603 0.007311 0.006946 0.006339 0.006064 0.005503\nCumulative Proportion 0.910626 0.917937 0.924883 0.931222 0.937285 0.942788\n                          PC41     PC42     PC43     PC44     PC45     PC46\nEigenvalue            0.643916 0.589875 0.570627 0.547548 0.468405 0.413838\nProportion Explained  0.005411 0.004957 0.004795 0.004601 0.003936 0.003478\nCumulative Proportion 0.948199 0.953156 0.957951 0.962552 0.966488 0.969966\n                          PC47     PC48     PC49     PC50     PC51     PC52\nEigenvalue            0.404853 0.376750 0.351271 0.324737 0.313651 0.265787\nProportion Explained  0.003402 0.003166 0.002952 0.002729 0.002636 0.002234\nCumulative Proportion 0.973368 0.976534 0.979486 0.982215 0.984851 0.987084\n                          PC53     PC54     PC55     PC56     PC57     PC58\nEigenvalue            0.252939 0.236744 0.200140 0.185113 0.176750 0.151258\nProportion Explained  0.002126 0.001989 0.001682 0.001556 0.001485 0.001271\nCumulative Proportion 0.989210 0.991199 0.992881 0.994436 0.995922 0.997193\n                           PC59     PC60      PC61      PC62\nEigenvalue            0.1090589 0.093773 0.0703683 0.0608501\nProportion Explained  0.0009165 0.000788 0.0005913 0.0005113\nCumulative Proportion 0.9981093 0.998897 0.9994887 1.0000000\n\nScaling 2 for species and site scores\n* Species are scaled proportional to eigenvalues\n* Sites are unscaled: weighted dispersion equal on all dimensions\n* General scaling constant of scores:  \n\n# PCA-Plot der Lage der Beobachtungen im Ordinationsraum\nbiplot(pca.6, display = \"sites\", type = \"points\", scaling = 1)\n\n\n\n# Subjektive Auswahl von Arten zur Darstellung\nsel.sp <- c(3, 11, 23, 39, 46, 72, 77, 96)\nsnames <- names(sveg[,sel.sp])\nsnames\n\n[1] \"Vaccinium.oxycoccos\" \"Carex.echinata\"      \"Arnica.montana\"     \n[4] \"Carex.pulicaris\"     \"Sphagnum.recurvum\"   \"Viola.palustris\"    \n[7] \"Galium.uliginosum\"   \"Stachys.officinalis\"\n\n# PCA-Plot der Korrelationen der Variablen (hier Arten) mit den Achsen (h)\nscores <- scores(pca.6, display = \"species\")\nx <- scores[,1]\ny <- scores[,2]\nplot(x, y, type = \"n\", asp = 1)\narrows(0,0, x[sel.sp], y[sel.sp], length = 0.08)\ntext(x[sel.sp], y[sel.sp], snames, pos = 1, cex = 0.6)\n\n\n\n# Mit angepassten Achsen\nplot(x, y, type = \"n\", asp = 1, xlim = c(-1, 1), ylim = c(-0.6, 0.6))\narrows(0,0, x[sel.sp], y[sel.sp], length = 0.08)\ntext(x[sel.sp], y[sel.sp], snames, pos = 1, cex = 0.6)"
  },
  {
    "objectID": "stat5-8/Statistik6_Demo.html#ca",
    "href": "stat5-8/Statistik6_Demo.html#ca",
    "title": "Stat6: Demo",
    "section": "CA",
    "text": "CA\n\nca.1 <- cca(sveg^0.5)\n# Arten (o) und Communities (+) plotten\nplot(ca.1)\n\n\n\n# Nur Arten plotten\nplot(ca.1, display = \"species\", type = \"points\")\n\n\n\n# Anteilige Varianz, die durch die ersten beiden Achsen erklärt wird\nca.1$CA$eig[1:2] / sum(ca.1$CA$eig)\n\n      CA1       CA2 \n0.1938717 0.0784178 \n\nsummary(eigenvals(ca.1))\n\nImportance of components:\n                         CA1     CA2     CA3     CA4     CA5     CA6     CA7\nEigenvalue            0.4248 0.17182 0.12995 0.09102 0.07954 0.07274 0.06705\nProportion Explained  0.1939 0.07842 0.05931 0.04154 0.03630 0.03320 0.03060\nCumulative Proportion 0.1939 0.27229 0.33160 0.37314 0.40944 0.44264 0.47324\n                          CA8     CA9    CA10    CA11    CA12    CA13   CA14\nEigenvalue            0.06245 0.05811 0.05348 0.05261 0.05133 0.04868 0.0480\nProportion Explained  0.02850 0.02652 0.02441 0.02401 0.02343 0.02222 0.0219\nCumulative Proportion 0.50174 0.52826 0.55267 0.57668 0.60010 0.62232 0.6442\n                         CA15    CA16    CA17    CA18    CA19    CA20    CA21\nEigenvalue            0.04421 0.04279 0.03913 0.03752 0.03699 0.03412 0.03309\nProportion Explained  0.02018 0.01953 0.01786 0.01712 0.01688 0.01557 0.01510\nCumulative Proportion 0.66440 0.68393 0.70179 0.71892 0.73580 0.75137 0.76647\n                         CA22    CA23    CA24    CA25    CA26    CA27    CA28\nEigenvalue            0.03253 0.03033 0.02963 0.02718 0.02621 0.02486 0.02372\nProportion Explained  0.01485 0.01384 0.01352 0.01241 0.01196 0.01135 0.01083\nCumulative Proportion 0.78132 0.79516 0.80869 0.82109 0.83305 0.84440 0.85523\n                         CA29     CA30     CA31     CA32     CA33     CA34\nEigenvalue            0.02262 0.021397 0.020274 0.018805 0.018216 0.017737\nProportion Explained  0.01032 0.009765 0.009253 0.008582 0.008314 0.008095\nCumulative Proportion 0.86555 0.875318 0.884571 0.893153 0.901467 0.909561\n                          CA35    CA36     CA37     CA38     CA39     CA40\nEigenvalue            0.016855 0.01422 0.014044 0.013002 0.011367 0.011185\nProportion Explained  0.007693 0.00649 0.006409 0.005934 0.005188 0.005105\nCumulative Proportion 0.917254 0.92374 0.930153 0.936087 0.941275 0.946379\n                          CA41     CA42     CA43     CA44     CA45     CA46\nEigenvalue            0.010417 0.010172 0.009513 0.009183 0.008162 0.007993\nProportion Explained  0.004754 0.004643 0.004342 0.004191 0.003725 0.003648\nCumulative Proportion 0.951133 0.955776 0.960118 0.964308 0.968033 0.971681\n                          CA47     CA48     CA49     CA50    CA51     CA52\nEigenvalue            0.006900 0.006684 0.006108 0.005493 0.00515 0.004995\nProportion Explained  0.003149 0.003051 0.002788 0.002507 0.00235 0.002279\nCumulative Proportion 0.974830 0.977881 0.980668 0.983176 0.98553 0.987805\n                          CA53     CA54     CA55     CA56     CA57     CA58\nEigenvalue            0.004426 0.004011 0.003517 0.003455 0.003059 0.002279\nProportion Explained  0.002020 0.001830 0.001605 0.001577 0.001396 0.001040\nCumulative Proportion 0.989825 0.991656 0.993261 0.994837 0.996233 0.997274\n                           CA59      CA60      CA61      CA62\nEigenvalue            0.0019296 0.0017784 0.0011904 0.0010752\nProportion Explained  0.0008807 0.0008116 0.0005433 0.0004907\nCumulative Proportion 0.9981544 0.9989660 0.9995093 1.0000000"
  },
  {
    "objectID": "stat5-8/Statistik6_Demo.html#dca",
    "href": "stat5-8/Statistik6_Demo.html#dca",
    "title": "Stat6: Demo",
    "section": "DCA",
    "text": "DCA\n\nlibrary(vegan)\ndca.1 <- decorana(sveg, mk = 10)\nplot(dca.1, display = \"sites\", type = \"point\")\n\n\n\ndca.2 <- decorana(sveg, mk = 100)\nplot(dca.2, display = \"sites\", type = \"point\")"
  },
  {
    "objectID": "stat5-8/Statistik6_Demo.html#nmds",
    "href": "stat5-8/Statistik6_Demo.html#nmds",
    "title": "Stat6: Demo",
    "section": "NMDS",
    "text": "NMDS\n\n# Distanzmatrix als Start erzeugen\nmde <- vegdist(sveg, method = \"euclidean\")\n\n# Alternative mit einem für Vegetationsdaten häufig verwendeten Dissimilarity-index\nmde <- vegdist(sveg, method = \"bray\")\n\n#Z wei verschiedene NMDS-Methoden\nif(!require(MASS)){install.packages(\"MASS\")}\n\nLoading required package: MASS\n\nlibrary(MASS)\nset.seed(1) # macht man, wenn man bei einer Wiederholung exakt die gleichen Ergebnisse will\nimds <- isoMDS(mde, k = 2)\n\ninitial  value 16.524491 \niter   5 value 12.518681\niter  10 value 12.025808\niter  10 value 12.020751\niter  10 value 12.020751\nfinal  value 12.020751 \nconverged\n\nset.seed(1)\nmmds <- metaMDS(mde, k = 2)\n\nRun 0 stress 0.1179909 \nRun 1 stress 0.1179909 \n... Procrustes: rmse 1.11122e-05  max resid 4.697213e-05 \n... Similar to previous best\nRun 2 stress 0.170918 \nRun 3 stress 0.1529993 \nRun 4 stress 0.1179909 \n... Procrustes: rmse 2.030179e-06  max resid 1.189668e-05 \n... Similar to previous best\nRun 5 stress 0.1252011 \nRun 6 stress 0.1583424 \nRun 7 stress 0.1181212 \n... Procrustes: rmse 0.006525662  max resid 0.04396629 \nRun 8 stress 0.1596312 \nRun 9 stress 0.1630026 \nRun 10 stress 0.1179909 \n... New best solution\n... Procrustes: rmse 3.475821e-06  max resid 2.360888e-05 \n... Similar to previous best\nRun 11 stress 0.1538119 \nRun 12 stress 0.1252011 \nRun 13 stress 0.1500845 \nRun 14 stress 0.1251634 \nRun 15 stress 0.1251634 \nRun 16 stress 0.1179909 \n... Procrustes: rmse 5.655652e-06  max resid 1.960818e-05 \n... Similar to previous best\nRun 17 stress 0.1179909 \n... Procrustes: rmse 7.036899e-06  max resid 2.755273e-05 \n... Similar to previous best\nRun 18 stress 0.1179909 \n... Procrustes: rmse 1.0129e-05  max resid 3.793497e-05 \n... Similar to previous best\nRun 19 stress 0.1251572 \nRun 20 stress 0.1179909 \n... Procrustes: rmse 5.011736e-06  max resid 2.261906e-05 \n... Similar to previous best\n*** Solution reached\n\nplot(imds$points)\n\n\n\nplot(mmds$points)\n\n\n\n#Stress = S² = Abweichung der zweidimensionalen NMDS-Lösung von der originalen Distanzmatrix\nstressplot(imds, mde)\n\n\n\nstressplot(mmds, mde)"
  },
  {
    "objectID": "stat5-8/Statistik6_Loesung.html",
    "href": "stat5-8/Statistik6_Loesung.html",
    "title": "Stat6: Lösung",
    "section": "",
    "text": "R-Skript als Download\nLoesungstext\n\n\nload(here(\"data\",\"Doubs.RData\"))\nsummary(env)\n\n      dfs              ele             slo              dis       \n Min.   :  0.30   Min.   :172.0   Min.   : 0.200   Min.   : 0.84  \n 1st Qu.: 54.45   1st Qu.:248.0   1st Qu.: 0.525   1st Qu.: 4.20  \n Median :175.20   Median :395.0   Median : 1.200   Median :22.10  \n Mean   :188.23   Mean   :481.6   Mean   : 3.497   Mean   :22.20  \n 3rd Qu.:301.73   3rd Qu.:782.0   3rd Qu.: 2.875   3rd Qu.:28.57  \n Max.   :453.00   Max.   :934.0   Max.   :48.000   Max.   :69.00  \n       pH             har              pho              nit       \n Min.   :7.700   Min.   : 40.00   Min.   :0.0100   Min.   :0.150  \n 1st Qu.:7.925   1st Qu.: 84.25   1st Qu.:0.1250   1st Qu.:0.505  \n Median :8.000   Median : 89.00   Median :0.2850   Median :1.600  \n Mean   :8.050   Mean   : 86.10   Mean   :0.5577   Mean   :1.654  \n 3rd Qu.:8.100   3rd Qu.: 96.75   3rd Qu.:0.5600   3rd Qu.:2.425  \n Max.   :8.600   Max.   :110.00   Max.   :4.2200   Max.   :6.200  \n      amm              oxy              bod        \n Min.   :0.0000   Min.   : 4.100   Min.   : 1.300  \n 1st Qu.:0.0000   1st Qu.: 8.025   1st Qu.: 2.725  \n Median :0.1000   Median :10.200   Median : 4.150  \n Mean   :0.2093   Mean   : 9.390   Mean   : 5.117  \n 3rd Qu.:0.2000   3rd Qu.:10.900   3rd Qu.: 5.275  \n Max.   :1.8000   Max.   :12.400   Max.   :16.700  \n\nsummary(spe)\n\n      Cogo           Satr           Phph            Babl            Thth     \n Min.   :0.00   Min.   :0.00   Min.   :0.000   Min.   :0.000   Min.   :0.00  \n 1st Qu.:0.00   1st Qu.:0.00   1st Qu.:0.000   1st Qu.:1.000   1st Qu.:0.00  \n Median :0.00   Median :1.00   Median :3.000   Median :2.000   Median :0.00  \n Mean   :0.50   Mean   :1.90   Mean   :2.267   Mean   :2.433   Mean   :0.50  \n 3rd Qu.:0.75   3rd Qu.:3.75   3rd Qu.:4.000   3rd Qu.:4.000   3rd Qu.:0.75  \n Max.   :3.00   Max.   :5.00   Max.   :5.000   Max.   :5.000   Max.   :4.00  \n      Teso             Chna          Pato             Lele      \n Min.   :0.0000   Min.   :0.0   Min.   :0.0000   Min.   :0.000  \n 1st Qu.:0.0000   1st Qu.:0.0   1st Qu.:0.0000   1st Qu.:0.000  \n Median :0.0000   Median :0.0   Median :0.0000   Median :1.000  \n Mean   :0.6333   Mean   :0.6   Mean   :0.8667   Mean   :1.433  \n 3rd Qu.:0.7500   3rd Qu.:1.0   3rd Qu.:2.0000   3rd Qu.:2.000  \n Max.   :5.0000   Max.   :3.0   Max.   :4.0000   Max.   :5.000  \n      Sqce            Baba            Albi          Gogo            Eslu      \n Min.   :0.000   Min.   :0.000   Min.   :0.0   Min.   :0.000   Min.   :0.000  \n 1st Qu.:1.000   1st Qu.:0.000   1st Qu.:0.0   1st Qu.:0.000   1st Qu.:0.000  \n Median :2.000   Median :0.000   Median :0.0   Median :1.000   Median :1.000  \n Mean   :1.867   Mean   :1.433   Mean   :0.9   Mean   :1.833   Mean   :1.333  \n 3rd Qu.:3.000   3rd Qu.:3.000   3rd Qu.:1.0   3rd Qu.:3.750   3rd Qu.:2.000  \n Max.   :5.000   Max.   :5.000   Max.   :5.0   Max.   :5.000   Max.   :5.000  \n      Pefl          Rham          Legi             Scer          Cyca       \n Min.   :0.0   Min.   :0.0   Min.   :0.0000   Min.   :0.0   Min.   :0.0000  \n 1st Qu.:0.0   1st Qu.:0.0   1st Qu.:0.0000   1st Qu.:0.0   1st Qu.:0.0000  \n Median :0.5   Median :0.0   Median :0.0000   Median :0.0   Median :0.0000  \n Mean   :1.2   Mean   :1.1   Mean   :0.9667   Mean   :0.7   Mean   :0.8333  \n 3rd Qu.:2.0   3rd Qu.:2.0   3rd Qu.:1.7500   3rd Qu.:1.0   3rd Qu.:1.0000  \n Max.   :5.0   Max.   :5.0   Max.   :5.0000   Max.   :5.0   Max.   :5.0000  \n      Titi          Abbr             Icme          Gyce            Ruru    \n Min.   :0.0   Min.   :0.0000   Min.   :0.0   Min.   :0.000   Min.   :0.0  \n 1st Qu.:0.0   1st Qu.:0.0000   1st Qu.:0.0   1st Qu.:0.000   1st Qu.:0.0  \n Median :1.0   Median :0.0000   Median :0.0   Median :0.000   Median :1.0  \n Mean   :1.5   Mean   :0.8667   Mean   :0.6   Mean   :1.267   Mean   :2.1  \n 3rd Qu.:3.0   3rd Qu.:1.0000   3rd Qu.:0.0   3rd Qu.:2.000   3rd Qu.:5.0  \n Max.   :5.0   Max.   :5.0000   Max.   :5.0   Max.   :5.000   Max.   :5.0  \n      Blbj            Alal          Anan     \n Min.   :0.000   Min.   :0.0   Min.   :0.00  \n 1st Qu.:0.000   1st Qu.:0.0   1st Qu.:0.00  \n Median :0.000   Median :0.0   Median :0.00  \n Mean   :1.033   Mean   :1.9   Mean   :0.90  \n 3rd Qu.:1.750   3rd Qu.:5.0   3rd Qu.:1.75  \n Max.   :5.000   Max.   :5.0   Max.   :5.00  \n\n# Die Dataframes env und spe enthalten die Umwelt- respective die Artdaten\n\nif(!require(vegan)){install.packages(\"vegan\")}\n\nLoading required package: vegan\n\n\nLoading required package: permute\n\n\nLoading required package: lattice\n\n\nThis is vegan 2.6-2\n\nlibrary(\"vegan\")\n\nDie PCA wird im Package vegan mit dem Befehl rda ausgeführt, wobei in diesem scale = TRUE gesetzt werden muss, da die Umweltdaten mit ganz unterschiedlichen Einheiten und Wertebereichen daherkommen\n\nenv.pca <- rda(env, scale = TRUE)\nenv.pca\n\nCall: rda(X = env, scale = TRUE)\n\n              Inertia Rank\nTotal              11     \nUnconstrained      11   11\nInertia is correlations \n\nEigenvalues for unconstrained axes:\n  PC1   PC2   PC3   PC4   PC5   PC6   PC7   PC8   PC9  PC10  PC11 \n5.969 2.164 1.065 0.739 0.400 0.336 0.173 0.108 0.024 0.017 0.006 \n\n# In env.pca sieht man, dass es bei 11 Umweltvariablen logischerweise 11 orthogonale Principle Components gibt\n\nsummary(env.pca, axes = 0)\n\n\nCall:\nrda(X = env, scale = TRUE) \n\nPartitioning of correlations:\n              Inertia Proportion\nTotal              11          1\nUnconstrained      11          1\n\nEigenvalues, and their contribution to the correlations \n\nImportance of components:\n                         PC1    PC2     PC3     PC4     PC5     PC6     PC7\nEigenvalue            5.9687 2.1639 1.06517 0.73875 0.40019 0.33563 0.17263\nProportion Explained  0.5426 0.1967 0.09683 0.06716 0.03638 0.03051 0.01569\nCumulative Proportion 0.5426 0.7393 0.83616 0.90332 0.93970 0.97022 0.98591\n                           PC8      PC9     PC10     PC11\nEigenvalue            0.108228 0.023701 0.017083 0.005983\nProportion Explained  0.009839 0.002155 0.001553 0.000544\nCumulative Proportion 0.995748 0.997903 0.999456 1.000000\n\nScaling 2 for species and site scores\n* Species are scaled proportional to eigenvalues\n* Sites are unscaled: weighted dispersion equal on all dimensions\n* General scaling constant of scores:  \n\n# Hier sieht man auch die Übersetzung der Eigenvalues in erklärte Varianzen der einzelnen Principle Components\n\nsummary(env.pca)\n\n\nCall:\nrda(X = env, scale = TRUE) \n\nPartitioning of correlations:\n              Inertia Proportion\nTotal              11          1\nUnconstrained      11          1\n\nEigenvalues, and their contribution to the correlations \n\nImportance of components:\n                         PC1    PC2     PC3     PC4     PC5     PC6     PC7\nEigenvalue            5.9687 2.1639 1.06517 0.73875 0.40019 0.33563 0.17263\nProportion Explained  0.5426 0.1967 0.09683 0.06716 0.03638 0.03051 0.01569\nCumulative Proportion 0.5426 0.7393 0.83616 0.90332 0.93970 0.97022 0.98591\n                           PC8      PC9     PC10     PC11\nEigenvalue            0.108228 0.023701 0.017083 0.005983\nProportion Explained  0.009839 0.002155 0.001553 0.000544\nCumulative Proportion 0.995748 0.997903 0.999456 1.000000\n\nScaling 2 for species and site scores\n* Species are scaled proportional to eigenvalues\n* Sites are unscaled: weighted dispersion equal on all dimensions\n* General scaling constant of scores:  4.226177 \n\n\nSpecies scores\n\n         PC1     PC2      PC3      PC4      PC5      PC6\ndfs  1.08657  0.5342 -0.27333 -0.13477  0.07336 -0.22566\nele -1.04396 -0.6148  0.20712  0.12854  0.14610 -0.02111\nslo -0.57703 -0.4893 -0.63490 -0.71684  0.33349  0.11782\ndis  0.95843  0.6608 -0.32456 -0.16183  0.11542 -0.13935\npH  -0.06364  0.4629  1.01317 -0.58606  0.17094 -0.07360\nhar  0.90118  0.5850  0.06449  0.25696  0.30995  0.53390\npho  1.05821 -0.6014  0.13866 -0.17883 -0.11125  0.13751\nnit  1.15013 -0.1005 -0.05167 -0.24537 -0.35105 -0.02145\namm  1.00679 -0.6969  0.14077 -0.14684 -0.19200  0.11904\noxy -0.97459  0.4991 -0.09017 -0.31040 -0.38066  0.36500\nbod  0.97315 -0.7148  0.15145  0.07193  0.23633  0.05540\n\n\nSite scores (weighted sums of species scores)\n\n        PC1      PC2      PC3      PC4       PC5       PC6\n1  -1.41274 -1.40098 -2.03484 -2.67759  1.117150  0.184951\n2  -1.03725 -0.77955  0.24400  0.25635 -1.192043 -1.849810\n3  -0.94507 -0.46765  1.25042 -0.49330 -0.234194 -1.319198\n4  -0.87371 -0.26988  0.19304  0.51979 -0.494639  0.116092\n5  -0.42088 -0.66944  0.83191  0.71729  0.867751  0.112219\n6  -0.77224 -0.72067 -0.07357  0.77902 -0.386130 -0.654273\n7  -0.77466 -0.08103  0.39630  0.19224  0.416470  1.026304\n8  -0.28840 -0.60589  0.83822  1.01440  1.707316  0.295861\n9  -0.28305 -0.47710  0.39908  1.13075  0.882098  0.002961\n10 -0.48714 -0.41860 -1.27555  0.90267  0.013704  0.542270\n11 -0.26940  0.45384  0.09119 -0.15127 -0.233814  1.157483\n12 -0.43834  0.36049 -0.52352  0.57279 -0.650095  0.817673\n13 -0.37794  0.70379  0.10339  0.06127 -0.101571  1.376623\n14 -0.23878  0.75522  0.83648 -0.55822 -0.011527  1.221217\n15 -0.30425  0.95026  1.80274 -1.48211  0.135021  0.031795\n16 -0.13354  0.33951 -0.23252  0.19177 -0.667112  0.227348\n17  0.10111  0.32379 -0.20380  0.18495 -0.676546  0.364915\n18  0.06913  0.37913 -0.25881  0.06998 -0.851379  0.289054\n19  0.05746  0.43915  0.04566 -0.32171 -0.899449 -0.090759\n20  0.17478  0.39927 -0.36244 -0.15647 -1.300718 -0.093396\n21  0.16944  0.35608 -0.73929  0.42751 -0.509249 -0.653892\n22  0.14898  0.55339 -0.08008 -0.04972  0.196636 -0.621753\n23  1.39778 -1.19102  0.66424 -0.46178  0.252908  0.573369\n24  0.99357 -0.52036  0.07186  0.48088  1.068785 -0.373991\n25  2.22002 -2.03168  0.17940 -0.52606 -1.148014  0.786506\n26  0.89388 -0.10410 -0.61440  0.42034  0.343649 -0.800522\n27  0.64866  0.41296 -0.17444 -0.26105  0.274443 -1.259099\n28  0.77100  0.82592  0.43387 -1.00092 -0.001674 -0.703378\n29  0.66413  1.11562 -1.58043  0.65099  0.650327 -0.020001\n30  0.74743  1.36955 -0.22810 -0.43281  1.431895 -0.686570\n\n# Hier das ausführliche Summary mit den Art- und Umweltkorrelationen auf den ersten sechs Achsen\n\nscreeplot(env.pca, bstick = TRUE, npcs = length(env.pca$CA$eig))\n\n\n\n# Visualisierung der Anteile erklärter Varianz, auch im Vergleich zu einem Broken-Stick-Modell\n\n\nDie Anteile fallen steil ab. Nur die ersten vier Achsen erklären jeweils mehr als 5 % (und zusammen über 90 %)\nDas Broken-stick-Modell würde sogar nur die ersten beiden Achsen als relevant vorschlagen\nDa die Relevanz für das Datenmuster in den Umweltdaten nicht notwendig die Relevanz für die Erklärung der Artenzahlen ist, nehmen wir ins globale Modell grosszügig die ersten vier Achsen rein (PC1-PC4) Die Bedeutung der Achsen (benötigt man später für die Interpretation!) findet man in den “species scores” (da so, wie wir die PCA hier gerechnet haben, die Umweltdaten die Arten sind. Zusätzlich oder alternative kann man sich die ersten vier Achsen auch visualisieren, indem man PC2 vs. PC1 (ohne choices), PC3 vs. PC1 oder PC4 vs. PC1 plottet.\n\n\npar(mfrow = c(2, 2))\nbiplot(env.pca, scaling = 1)\nbiplot(env.pca, choices = c(1, 3), scaling = 1)\nbiplot(env.pca, choices = c(1, 4), scaling = 1)\n\n\n\n\n\nPC1 steht v.a. für Nitrat (positiv), Sauerstoff (negativ)\nPC2 steht v.a. für pH (positiv)\nPC3 steht v.a. für pH (positiv) und slo (negativ)\nPC4 steht v.a. für pH (negativ) und slo (negativ)\n\n\n#Wir extrahieren nun die ersten vier PC-Scores aller Aufnahmeflächen\n\nscores <- scores(env.pca, c(1:4), display = c(\"sites\"))\nscores\n\n           PC1         PC2         PC3         PC4\n1  -1.41273883 -1.40097880 -2.03483870 -2.67758838\n2  -1.03724733 -0.77955354  0.24400009  0.25634696\n3  -0.94506998 -0.46765361  1.25042488 -0.49329701\n4  -0.87371164 -0.26988488  0.19304045  0.51979381\n5  -0.42087585 -0.66943957  0.83190665  0.71729089\n6  -0.77223581 -0.72066623 -0.07357441  0.77902331\n7  -0.77466085 -0.08103491  0.39629959  0.19223674\n8  -0.28839689 -0.60588978  0.83822295  1.01439781\n9  -0.28305399 -0.47710013  0.39908190  1.13074537\n10 -0.48714448 -0.41859737 -1.27554791  0.90267450\n11 -0.26940072  0.45383527  0.09118967 -0.15126579\n12 -0.43834427  0.36049094 -0.52351661  0.57279309\n13 -0.37793587  0.70379486  0.10338604  0.06127189\n14 -0.23878321  0.75521955  0.83648481 -0.55822243\n15 -0.30424687  0.95025522  1.80274307 -1.48210897\n16 -0.13353523  0.33951332 -0.23252035  0.19177453\n17  0.10111086  0.32378989 -0.20379779  0.18495051\n18  0.06913015  0.37912929 -0.25881042  0.06998196\n19  0.05745832  0.43915445  0.04566423 -0.32171096\n20  0.17478169  0.39926644 -0.36244421 -0.15647276\n21  0.16944233  0.35608121 -0.73929343  0.42751253\n22  0.14898095  0.55338567 -0.08008399 -0.04971692\n23  1.39778235 -1.19101965  0.66424125 -0.46178368\n24  0.99357418 -0.52036211  0.07185912  0.48087634\n25  2.22001746 -2.03168135  0.17940028 -0.52606378\n26  0.89388246 -0.10410321 -0.61440303  0.42034205\n27  0.64865976  0.41296206 -0.17444493 -0.26104930\n28  0.77099833  0.82591720  0.43386950 -1.00091524\n29  0.66413124  1.11561620 -1.58043410  0.65099411\n30  0.74743174  1.36955357 -0.22810459 -0.43281119\nattr(,\"const\")\n[1] 4.226177\n\n#Berechnung der Artenzahl mittels specnumber; Artenzahl und Scores werden zum Dataframe für die Regressionsanalyse hinzugefügt\ndoubs <- data.frame(env, scores, species_richness = specnumber(spe))\ndoubs\n\n     dfs ele  slo   dis  pH har  pho  nit  amm  oxy  bod         PC1\n1    0.3 934 48.0  0.84 7.9  45 0.01 0.20 0.00 12.2  2.7 -1.41273883\n2    2.2 932  3.0  1.00 8.0  40 0.02 0.20 0.10 10.3  1.9 -1.03724733\n3   10.2 914  3.7  1.80 8.3  52 0.05 0.22 0.05 10.5  3.5 -0.94506998\n4   18.5 854  3.2  2.53 8.0  72 0.10 0.21 0.00 11.0  1.3 -0.87371164\n5   21.5 849  2.3  2.64 8.1  84 0.38 0.52 0.20  8.0  6.2 -0.42087585\n6   32.4 846  3.2  2.86 7.9  60 0.20 0.15 0.00 10.2  5.3 -0.77223581\n7   36.8 841  6.6  4.00 8.1  88 0.07 0.15 0.00 11.1  2.2 -0.77466085\n8   49.1 792  2.5  1.30 8.1  94 0.20 0.41 0.12  7.0  8.1 -0.28839689\n9   70.5 752  1.2  4.80 8.0  90 0.30 0.82 0.12  7.2  5.2 -0.28305399\n10  99.0 617  9.9 10.00 7.7  82 0.06 0.75 0.01 10.0  4.3 -0.48714448\n11 123.4 483  4.1 19.90 8.1  96 0.30 1.60 0.00 11.5  2.7 -0.26940072\n12 132.4 477  1.6 20.00 7.9  86 0.04 0.50 0.00 12.2  3.0 -0.43834427\n13 143.6 450  2.1 21.10 8.1  98 0.06 0.52 0.00 12.4  2.4 -0.37793587\n14 152.2 434  1.2 21.20 8.3  98 0.27 1.23 0.00 12.3  3.8 -0.23878321\n15 164.5 415  0.5 23.00 8.6  86 0.40 1.00 0.00 11.7  2.1 -0.30424687\n16 185.9 375  2.0 16.10 8.0  88 0.20 2.00 0.05 10.3  2.7 -0.13353523\n17 198.5 349  0.5 24.30 8.0  92 0.20 2.50 0.20 10.2  4.6  0.10111086\n18 211.0 333  0.8 25.00 8.0  90 0.50 2.20 0.20 10.3  2.8  0.06913015\n19 224.6 310  0.5 25.90 8.1  84 0.60 2.20 0.15 10.6  3.3  0.05745832\n20 247.7 286  0.8 26.80 8.0  86 0.30 3.00 0.30 10.3  2.8  0.17478169\n21 282.1 262  1.0 27.20 7.9  85 0.20 2.20 0.10  9.0  4.1  0.16944233\n22 294.0 254  1.4 27.90 8.1  88 0.20 1.62 0.07  9.1  4.8  0.14898095\n23 304.3 246  1.2 28.80 8.1  97 2.60 3.50 1.15  6.3 16.4  1.39778235\n24 314.7 241  0.3 29.76 8.0  99 1.40 2.50 0.60  5.2 12.3  0.99357418\n25 327.8 231  0.5 38.70 7.9 100 4.22 6.20 1.80  4.1 16.7  2.22001746\n26 356.9 214  0.5 39.10 7.9  94 1.43 3.00 0.30  6.2  8.9  0.89388246\n27 373.2 206  1.2 39.60 8.1  90 0.58 3.00 0.26  7.2  6.3  0.64865976\n28 394.7 195  0.3 43.20 8.3 100 0.74 4.00 0.30  8.1  4.5  0.77099833\n29 422.0 183  0.6 67.70 7.8 110 0.45 1.62 0.10  9.0  4.2  0.66413124\n30 453.0 172  0.2 69.00 8.2 109 0.65 1.60 0.10  8.2  4.4  0.74743174\n           PC2         PC3         PC4 species_richness\n1  -1.40097880 -2.03483870 -2.67758838                1\n2  -0.77955354  0.24400009  0.25634696                3\n3  -0.46765361  1.25042488 -0.49329701                4\n4  -0.26988488  0.19304045  0.51979381                8\n5  -0.66943957  0.83190665  0.71729089               11\n6  -0.72066623 -0.07357441  0.77902331               10\n7  -0.08103491  0.39629959  0.19223674                5\n8  -0.60588978  0.83822295  1.01439781                0\n9  -0.47710013  0.39908190  1.13074537                5\n10 -0.41859737 -1.27554791  0.90267450                6\n11  0.45383527  0.09118967 -0.15126579                6\n12  0.36049094 -0.52351661  0.57279309                6\n13  0.70379486  0.10338604  0.06127189                6\n14  0.75521955  0.83648481 -0.55822243               10\n15  0.95025522  1.80274307 -1.48210897               11\n16  0.33951332 -0.23252035  0.19177453               17\n17  0.32378989 -0.20379779  0.18495051               22\n18  0.37912929 -0.25881042  0.06998196               23\n19  0.43915445  0.04566423 -0.32171096               23\n20  0.39926644 -0.36244421 -0.15647276               22\n21  0.35608121 -0.73929343  0.42751253               23\n22  0.55338567 -0.08008399 -0.04971692               22\n23 -1.19101965  0.66424125 -0.46178368                3\n24 -0.52036211  0.07185912  0.48087634                8\n25 -2.03168135  0.17940028 -0.52606378                8\n26 -0.10410321 -0.61440303  0.42034205               21\n27  0.41296206 -0.17444493 -0.26104930               22\n28  0.82591720  0.43386950 -1.00091524               22\n29  1.11561620 -1.58043410  0.65099411               26\n30  1.36955357 -0.22810459 -0.43281119               21\n\nstr(doubs)\n\n'data.frame':   30 obs. of  16 variables:\n $ dfs             : num  0.3 2.2 10.2 18.5 21.5 32.4 36.8 49.1 70.5 99 ...\n $ ele             : int  934 932 914 854 849 846 841 792 752 617 ...\n $ slo             : num  48 3 3.7 3.2 2.3 3.2 6.6 2.5 1.2 9.9 ...\n $ dis             : num  0.84 1 1.8 2.53 2.64 2.86 4 1.3 4.8 10 ...\n $ pH              : num  7.9 8 8.3 8 8.1 7.9 8.1 8.1 8 7.7 ...\n $ har             : int  45 40 52 72 84 60 88 94 90 82 ...\n $ pho             : num  0.01 0.02 0.05 0.1 0.38 0.2 0.07 0.2 0.3 0.06 ...\n $ nit             : num  0.2 0.2 0.22 0.21 0.52 0.15 0.15 0.41 0.82 0.75 ...\n $ amm             : num  0 0.1 0.05 0 0.2 0 0 0.12 0.12 0.01 ...\n $ oxy             : num  12.2 10.3 10.5 11 8 10.2 11.1 7 7.2 10 ...\n $ bod             : num  2.7 1.9 3.5 1.3 6.2 5.3 2.2 8.1 5.2 4.3 ...\n $ PC1             : num  -1.413 -1.037 -0.945 -0.874 -0.421 ...\n $ PC2             : num  -1.401 -0.78 -0.468 -0.27 -0.669 ...\n $ PC3             : num  -2.035 0.244 1.25 0.193 0.832 ...\n $ PC4             : num  -2.678 0.256 -0.493 0.52 0.717 ...\n $ species_richness: int  1 3 4 8 11 10 5 0 5 6 ...\n\n##Lösung mit lm (alternativ ginge Poisson-glm) und frequentist approach (alternativ ginge Multimodelinference mit AICc)\nlm.pc.0 <- lm(species_richness ~ PC1 + PC2 + PC3 + PC4, data = doubs)\nsummary(lm.pc.0)\n\n\nCall:\nlm(formula = species_richness ~ PC1 + PC2 + PC3 + PC4, data = doubs)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.2359 -4.3792  0.4256  4.5453  7.5058 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  12.5000     0.9869  12.667 2.24e-12 ***\nPC1           4.4035     1.2790   3.443  0.00204 ** \nPC2           6.6729     1.2790   5.217 2.13e-05 ***\nPC3          -2.9645     1.2790  -2.318  0.02893 *  \nPC4           0.1674     1.2790   0.131  0.89694    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.405 on 25 degrees of freedom\nMultiple R-squared:  0.6401,    Adjusted R-squared:  0.5825 \nF-statistic: 11.12 on 4 and 25 DF,  p-value: 2.55e-05\n\n# Modellvereinfachung: PC4 ist nicht signifikant und wird entfernt\nlm.pc.1 <- lm(species_richness ~ PC1 + PC2 + PC3, data = doubs)\nsummary(lm.pc.1) # jetzt sind alle Achsen signifikant und werden in das minimal adäquate Modell aufgenommen\n\n\nCall:\nlm(formula = species_richness ~ PC1 + PC2 + PC3, data = doubs)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-9.226 -4.457  0.403  4.545  7.452 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   12.500      0.968  12.913 8.11e-13 ***\nPC1            4.404      1.255   3.510  0.00165 ** \nPC2            6.673      1.255   5.319 1.45e-05 ***\nPC3           -2.965      1.255  -2.363  0.02589 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.302 on 26 degrees of freedom\nMultiple R-squared:  0.6399,    Adjusted R-squared:  0.5983 \nF-statistic:  15.4 on 3 and 26 DF,  p-value: 5.853e-06\n\n# Modelldiagnostik/Modellvalidierung\npar(mfrow = c(2, 2))\nplot(lm.pc.1) \n\n\n\n\nNicht besonders toll, ginge aber gerade noch. Da wir aber ohnehin Zähldaten haben, können wir es mit einem Poisson-GLM versuchen\n#Alternativ mit glm\n\nglm.pc.1 <- glm(species_richness ~ PC1 + PC2 + PC3 + PC4, family = \"poisson\", data = doubs)\nsummary(glm.pc.1)\n\n\nCall:\nglm(formula = species_richness ~ PC1 + PC2 + PC3 + PC4, family = \"poisson\", \n    data = doubs)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-3.6063  -1.4535  -0.1915   1.3852   2.2701  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  2.38447    0.05889  40.491  < 2e-16 ***\nPC1          0.39601    0.08240   4.806 1.54e-06 ***\nPC2          0.54840    0.07550   7.263 3.78e-13 ***\nPC3         -0.15174    0.08345  -1.818    0.069 .  \nPC4          0.06053    0.09661   0.627    0.531    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 179.812  on 29  degrees of freedom\nResidual deviance:  76.312  on 25  degrees of freedom\nAIC: 206.97\n\nNumber of Fisher Scoring iterations: 5\n\nglm.pc.2 <- glm(species_richness ~ PC1 + PC2 + PC3, family = \"poisson\", data = doubs)\nsummary(glm.pc.2)\n\n\nCall:\nglm(formula = species_richness ~ PC1 + PC2 + PC3, family = \"poisson\", \n    data = doubs)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-3.4821  -1.3539  -0.2734   1.4039   2.2096  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  2.38670    0.05858  40.742  < 2e-16 ***\nPC1          0.38609    0.07941   4.862 1.16e-06 ***\nPC2          0.53665    0.07161   7.494 6.70e-14 ***\nPC3         -0.17669    0.07106  -2.486   0.0129 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 179.812  on 29  degrees of freedom\nResidual deviance:  76.722  on 26  degrees of freedom\nAIC: 205.38\n\nNumber of Fisher Scoring iterations: 5\n\npar(mfrow = c(2, 2))\nplot(glm.pc.2) # sieht nicht besser aus als LM, die Normalverteilung ist sogar schlechter\n\n\n\n\nLM oder GLM sind für die Analyse möglich, Modellwahl nach Gusto. Man muss jetzt noch die Ergebnisse adäquat aus all den erzielten Outputs zusammenstellen (siehe Ergebnistext). In dieser Aufgabe haben wir ja die PC-Achsen als Alternative zur direkten Modellierung mit den originalen Umweltvariablen ausprobiert. Deshalb (war nicht Teil der Aufgabe), kommt hier noch eine Lösung, wie wir es bisher gemacht hätten.\nZum Vergleich die Modellierung mit den Originaldaten\n\n# Korrelationen zwischen Prädiktoren\ncor <- cor(doubs[, 1:11])\ncor[abs(cor)<.7] <- 0\ncor \n\nDie Korrelationsmatrix betrachtet man am besten in Excel. Es zeigt sich, dass es zwei grosse Gruppen von untereinander hochkorrelierten Variablen gibt: zum einen dfs-ele-dis-har-nit, zum anderen pho-nit-amm-oxy-bod, während slo und pH mit jeweils keiner anderen Variablen hochkorreliert sind. Insofern behalten wir eine aus der ersten Gruppe (ele), eine aus der zweiten Gruppe (pho) und die beiden «unabhängigen».\n\n# Globalmodell (als hinreichend unabhängige Variablen werden ele, slo, pH und pho aufgenommen)\nlm.orig.1 <- lm(species_richness ~ ele + slo + pH + pho, data = doubs)\nsummary(lm.orig.1)\n\n\nCall:\nlm(formula = species_richness ~ ele + slo + pH + pho, data = doubs)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-9.784 -3.265  1.869  3.375  7.664 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 74.19236   47.29223   1.569  0.12926    \nele         -0.02645    0.00441  -5.997 2.91e-06 ***\nslo         -0.09597    0.12988  -0.739  0.46684    \npH          -5.75643    5.84799  -0.984  0.33438    \npho         -4.09089    1.25657  -3.256  0.00324 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.285 on 25 degrees of freedom\nMultiple R-squared:  0.6559,    Adjusted R-squared:  0.6009 \nF-statistic: 11.92 on 4 and 25 DF,  p-value: 1.485e-05\n\n# Modellvereinfachung: slo als am wenigsten signifikante Variable gestrichen\nlm.orig.2 <- lm(species_richness ~ ele + pH + pho, data = doubs)\nsummary(lm.orig.2)\n\n\nCall:\nlm(formula = species_richness ~ ele + pH + pho, data = doubs)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-9.446 -3.323  1.485  3.562  8.209 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 66.416530  45.702262   1.453  0.15812    \nele         -0.027744   0.004011  -6.917 2.41e-07 ***\npH          -4.756146   5.639266  -0.843  0.40670    \npho         -4.068860   1.245202  -3.268  0.00305 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.239 on 26 degrees of freedom\nMultiple R-squared:  0.6484,    Adjusted R-squared:  0.6079 \nF-statistic: 15.98 on 3 and 26 DF,  p-value: 4.305e-06\n\n# Modellvereinfachung: pH ist immer noch nicht signifikant und wird gestrichen\nlm.orig.3 <- lm(species_richness ~ ele + pho, data = doubs)\nsummary(lm.orig.3)\n\n\nCall:\nlm(formula = species_richness ~ ele + pho, data = doubs)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-9.334 -4.548  1.058  3.717  7.889 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 27.929234   2.490667  11.214 1.15e-11 ***\nele         -0.027463   0.003976  -6.908 2.01e-07 ***\npho         -3.951980   1.230833  -3.211  0.00341 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.211 on 27 degrees of freedom\nMultiple R-squared:  0.6388,    Adjusted R-squared:  0.6121 \nF-statistic: 23.88 on 2 and 27 DF,  p-value: 1.07e-06\n\n# Modelldiagnostik\npar(mfrow = c(2, 2))\nplot(lm.orig.3) # nicht so gut, besonders die Bananenform in der linken obereren Abbildung\n\n\n\n# Nach Modellvereinfachung bleiben zwei signifikante Variablen, ele und pho.\n\n# Da das nicht so gut aussieht, versuchen wir es mit dem theoretisch angemesseneren Modell, einem Poisson-GLM.\n\n#Versuch mit glm\nglm.orig.1 <- glm(species_richness ~ ele + pho + pH + slo, family = \"poisson\", data = doubs)\nsummary(glm.orig.1)\n\n\nCall:\nglm(formula = species_richness ~ ele + pho + pH + slo, family = \"poisson\", \n    data = doubs)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-3.3800  -0.7094   0.1622   0.8079   2.4435  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  7.855498   2.550786   3.080  0.00207 ** \nele         -0.002277   0.000321  -7.094  1.3e-12 ***\npho         -0.362280   0.082384  -4.397  1.1e-05 ***\npH          -0.506330   0.316934  -1.598  0.11013    \nslo         -0.054200   0.027685  -1.958  0.05026 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 179.812  on 29  degrees of freedom\nResidual deviance:  55.128  on 25  degrees of freedom\nAIC: 185.79\n\nNumber of Fisher Scoring iterations: 5\n\nglm.orig.2 <- glm(species_richness ~ ele + pho + slo, family = \"poisson\", data = doubs)\nsummary(glm.orig.2)\n\n\nCall:\nglm(formula = species_richness ~ ele + pho + slo, family = \"poisson\", \n    data = doubs)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-3.3902  -0.8159   0.2153   0.8648   2.4389  \n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  3.7819203  0.1356022  27.890  < 2e-16 ***\nele         -0.0023363  0.0003167  -7.377 1.62e-13 ***\npho         -0.3563681  0.0835094  -4.267 1.98e-05 ***\nslo         -0.0446686  0.0246618  -1.811   0.0701 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 179.812  on 29  degrees of freedom\nResidual deviance:  57.752  on 26  degrees of freedom\nAIC: 186.41\n\nNumber of Fisher Scoring iterations: 5\n\nglm.orig.3 <- glm(species_richness ~ ele + pho, family = \"poisson\", data = doubs)\nsummary(glm.orig.3)\n\n\nCall:\nglm(formula = species_richness ~ ele + pho, family = \"poisson\", \n    data = doubs)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-3.1946  -0.9256   0.0642   0.8567   2.8093  \n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  3.8381921  0.1342437  28.591  < 2e-16 ***\nele         -0.0026994  0.0002795  -9.658  < 2e-16 ***\npho         -0.3525967  0.0829766  -4.249 2.14e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 179.812  on 29  degrees of freedom\nResidual deviance:  63.336  on 27  degrees of freedom\nAIC: 190\n\nNumber of Fisher Scoring iterations: 5\n\nplot(glm.orig.3)\n\n\n\n# Das sieht deutlich besser aus als beim LM. Wir müssen aber noch prüfen, ob evtl. Overdispersion vorliegt.\n\nif(!require(AER)){install.packages(\"AER\")}\n\nLoading required package: AER\n\n\nLoading required package: car\n\n\nLoading required package: carData\n\n\nLoading required package: lmtest\n\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n\nLoading required package: sandwich\n\n\nLoading required package: survival\n\nlibrary(AER)\ndispersiontest(glm.orig.3) #signifikante Überdispersion\n\n\n    Overdispersion test\n\ndata:  glm.orig.3\nz = 2.1816, p-value = 0.01457\nalternative hypothesis: true dispersion is greater than 1\nsample estimates:\ndispersion \n  1.967504 \n\n# Ja, es gibt signifikante Overdispersion (obwohl der Dispersionparameter sogar unter 2 ist, also nicht extrem). Wir können nun entweder quasipoisson oder negativebinomial nehmen.\n\nglmq.orig.3 <- glm(species_richness ~ ele + pho, family = \"quasipoisson\", data = doubs)\nsummary(glmq.orig.3)\n\n\nCall:\nglm(formula = species_richness ~ ele + pho, family = \"quasipoisson\", \n    data = doubs)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-3.1946  -0.9256   0.0642   0.8567   2.8093  \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.8381921  0.1996453  19.225  < 2e-16 ***\nele         -0.0026994  0.0004157  -6.494 5.81e-07 ***\npho         -0.3525967  0.1234016  -2.857  0.00813 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasipoisson family taken to be 2.211722)\n\n    Null deviance: 179.812  on 29  degrees of freedom\nResidual deviance:  63.336  on 27  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 5\n\n# Parameterschätzung bleiben gleich, aber Signifikanzen sind niedriger als beim GLM ohne Overdispersion.\nplot(glmq.orig.3)\n\n\n\n\nSieht gut aus, wir hätten hier also unser finales Modell.\nIm Vergleich der beiden Vorgehensweisen (PC-Achsen vs. Umweltdaten direkt) scheint in diesem Fall die direkte Modellierung der Umweltachsen informativer: Man kommt mit zwei Prädiktoren aus, die jeweils direkt für eine der Hauptvariablen stehen – Meereshöhe und Phosphor – zugleich aber jeweils eine grössere Gruppe von Variablen mit hohen Korrelationen inkludieren, im ersten Fall Variablen, die sich im Flusslauf von oben nach unten systematisch ändern, im zweiten Masse der Nährstoffbelastung des Gewässers. Bei der PCA-Lösung kamen drei signifikante Komponenten heraus, die allerdings nicht so leicht zu interpretieren sind. Dies insbesondere, weil in diesem Fall auf der Ebene PC2 vs. PC1 die Mehrzahl der Umweltparameter ungefähr in 45-Grad-Winkeln angeordnet sind. Im allgemeinen Fall kann aber die Nutzung von PC-Achsen durchaus eine gute Lösung sein."
  },
  {
    "objectID": "stat5-8/Statistik6_Uebung.html",
    "href": "stat5-8/Statistik6_Uebung.html",
    "title": "Stat6: Übung",
    "section": "",
    "text": "Übung 6.1: PCA (naturwissenschaftlich)\nDatensatz Doubs.RData\nLädt den Datensatz Doubs.RData mit dem folgenden Befehl ins R: load(“Doubs.RData”)\nDie Umweltvariablen findet ihr im data.frame env die Abundanzen im data.frame spe. Im data.frame fishtrait findet ihr die Vollständigen Namen der Fische\nDer Datensatz enthält Daten zum Vorkommen von Fischarten und den zugehörigen Umweltvariablen im Fluss Doubs (Jura). Es gibt 30 Probestellen (sites), an denen jeweils die Abundanzen von 27 Fischarten (auf einer Skalen von 0 bis 5) sowie 11 Umweltvariablen erhoben wurden:\ndfs = Distance from source (km) ele = Elevation (m a.s.l.) slo = Slope (‰) dis = Mean annual discharge (m3 s-1) pH = pH of water har = Hardness (Ca concentration) (mg L-1) pho = Phosphate concentration (mg L-1) nit = Nitrate concentration (mg L-1) amm = Ammonium concentration (mg L-1) oxy = Dissolved oxygen (mg L-1) bod = Biological oxygen demand (mg L-1)\nEure Aufgabe ist nun, in einem ersten Schritt eine PCA für die 11 Umweltvariablen zu rechnen. Da die einzelnen Variablen auf ganz unterschiedlichen Skalen gemessen wurden, ist dazu eine Standardisierung nötig (pca mit der Funktion rda, scale=TRUE). Überlegt, wie viele Achsen wichtig sind und für was sie jeweils stehen.\nIn einem zweiten Schritt sollen dann die vollständig unkorrelierten PCA-Achsen als Prädiktoren einer multiplen Regression zur Erklärung der Fischartenzahl (Anzahl kann z.B. kann mit dem Befehl specnumber(spe) ermittel werden) verwendet werden (wahlweise lm oder glm). Gebt das minimal adäquate Modell an und interpretiert dieses (wahlweise im frequentist oder information theoretician approach). (Wer noch mehr probieren möchte, kann zum Vergleich noch eine multiple Regression mit den Originaldaten rechnen)."
  },
  {
    "objectID": "stat5-8/Statistik7_Demo.html#ordinationen-ii",
    "href": "stat5-8/Statistik7_Demo.html#ordinationen-ii",
    "title": "Stat7: Demo",
    "section": "Ordinationen II",
    "text": "Ordinationen II\n\nDemoscript als Download\nDatensatz Doubs.RData\nFunktion triplot.rda.R\n\n\nInterpretation von Ordinationen\nWildi pp. 96 et seq.\n\n## Plot Arten\nif(!require(dave)){install.packages(\"dave\")}\n\nLoading required package: dave\n\n\nLoading required package: cluster\n\n\nLoading required package: labdsv\n\n\nLoading required package: mgcv\n\n\nLoading required package: nlme\n\n\nThis is mgcv 1.8-40. For overview type 'help(\"mgcv-package\")'.\n\n\nThis is labdsv 2.0-1\nconvert existing ordinations with as.dsvord()\n\n\n\nAttaching package: 'labdsv'\n\n\nThe following object is masked from 'package:stats':\n\n    density\n\n\nLoading required package: vegan\n\n\nLoading required package: permute\n\n\nLoading required package: lattice\n\n\nThis is vegan 2.6-2\n\n\nLoading required package: nnet\n\n\n\nAttaching package: 'nnet'\n\n\nThe following object is masked from 'package:mgcv':\n\n    multinom\n\n\nLoading required package: tree\n\nlibrary(dave)\nca <- cca(sveg^0.5)\n\n## Plot mit ausgewählten Arten\nsel.spec <- c(3, 11, 23, 31, 39, 46, 72, 77, 96)\nsnames <- names(sveg[,sel.spec])\nsnames\n\n[1] \"Vaccinium.oxycoccos\" \"Carex.echinata\"      \"Arnica.montana\"     \n[4] \"Festuca.rubra\"       \"Carex.pulicaris\"     \"Sphagnum.recurvum\"  \n[7] \"Viola.palustris\"     \"Galium.uliginosum\"   \"Stachys.officinalis\"\n\nscores <- scores(ca, display = \"species\", scaling = \"sites\")\nsx <- scores[sel.spec, 1]\nsy <- scores[sel.spec, 2]\nplot(ca, display = \"sites\", type = \"point\")\npoints(sx, sy, pch = 16)\nsnames <- make.cepnames(snames)\ntext(sx, sy, snames, pos = c(1,2,1,1,3,2,4,3,1), cex = 0.8)\n\n\n\n## Plot \"response surfaces\" in der CA\nplot(ca, display = \"sites\", type = \"point\")\nordisurf(ca, ssit$pH.peat, add = T)\n\n\n\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\ny ~ s(x1, x2, k = 10, bs = \"tp\", fx = FALSE)\n\nEstimated degrees of freedom:\n4.63  total = 5.63 \n\nREML score: 28.14791     \n\nplot(ca, display = \"sites\", type = \"points\")\nordisurf(ca, ssit$Waterlev.av, add = T, col = \"blue\")\n\n\n\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\ny ~ s(x1, x2, k = 10, bs = \"tp\", fx = FALSE)\n\nEstimated degrees of freedom:\n5.07  total = 6.07 \n\nREML score: 161.492     \n\n## Das gleiche für die DCA\ndca <- decorana(sveg)\nplot(dca, display = \"sites\", type = \"points\")\nordisurf(dca, ssit$pH.peat, add = T)\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\ny ~ s(x1, x2, k = 10, bs = \"tp\", fx = FALSE)\n\nEstimated degrees of freedom:\n2.61  total = 3.61 \n\nREML score: 29.47878     \n\nordisurf(dca, ssit$Waterlev.av, add = T, col = \"blue\")\n\n\n\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\ny ~ s(x1, x2, k = 10, bs = \"tp\", fx = FALSE)\n\nEstimated degrees of freedom:\n6.23  total = 7.23 \n\nREML score: 161.1293     \n\n## Das gleiche mit NMDS\nmde <- vegdist(sveg, method = \"euclidean\")\nmmds <- metaMDS(mde)\n\nRun 0 stress 0.1478603 \nRun 1 stress 0.1972602 \nRun 2 stress 0.1472019 \n... New best solution\n... Procrustes: rmse 0.0115021  max resid 0.06899205 \nRun 3 stress 0.1757188 \nRun 4 stress 0.1489369 \nRun 5 stress 0.2063934 \nRun 6 stress 0.1478582 \nRun 7 stress 0.1514736 \nRun 8 stress 0.1992033 \nRun 9 stress 0.1471444 \n... New best solution\n... Procrustes: rmse 0.003999956  max resid 0.02131571 \nRun 10 stress 0.1489369 \nRun 11 stress 0.1471305 \n... New best solution\n... Procrustes: rmse 0.00215906  max resid 0.0131935 \nRun 12 stress 0.1471847 \n... Procrustes: rmse 0.004001129  max resid 0.02135821 \nRun 13 stress 0.1478603 \nRun 14 stress 0.1462813 \n... New best solution\n... Procrustes: rmse 0.03020837  max resid 0.1480365 \nRun 15 stress 0.1472019 \nRun 16 stress 0.1954235 \nRun 17 stress 0.177165 \nRun 18 stress 0.1950714 \nRun 19 stress 0.1462959 \n... Procrustes: rmse 0.002066783  max resid 0.01268207 \nRun 20 stress 0.1646404 \n*** No convergence -- monoMDS stopping criteria:\n     2: no. of iterations >= maxit\n    12: stress ratio > sratmax\n     6: scale factor of the gradient < sfgrmin\n\nif(!require(MASS)){install.packages(\"MASS\")}\n\nLoading required package: MASS\n\nlibrary(MASS)\nimds <- isoMDS(mde)\n\ninitial  value 21.981028 \niter   5 value 15.595142\niter  10 value 15.269201\nfinal  value 15.229997 \nconverged\n\nplot(mmds$points)\nordisurf(mmds, ssit$pH.peat, add = T)\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\ny ~ s(x1, x2, k = 10, bs = \"tp\", fx = FALSE)\n\nEstimated degrees of freedom:\n5.99  total = 6.99 \n\nREML score: 41.84457     \n\nordisurf(mmds, ssit$Waterlev.av,add = T, col = \"blue\")\n\n\n\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\ny ~ s(x1, x2, k = 10, bs = \"tp\", fx = FALSE)\n\nEstimated degrees of freedom:\n6.32  total = 7.32 \n\nREML score: 168.9823     \n\nplot(imds$points)\nordisurf(imds, ssit$pH.peat, add = T)\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\ny ~ s(x1, x2, k = 10, bs = \"tp\", fx = FALSE)\n\nEstimated degrees of freedom:\n7.06  total = 8.06 \n\nREML score: 37.68641     \n\nordisurf(imds, ssit$Waterlev.av, add = T, col = \"blue\")\n\n\n\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\ny ~ s(x1, x2, k = 10, bs = \"tp\", fx = FALSE)\n\nEstimated degrees of freedom:\n6.01  total = 7.01 \n\nREML score: 167.6801     \n\n\n\n\nConstrained ordination\n\n## 5 Umweltvariablen gewählt, durch die die Ordination constrained werden soll\nssit\nsummary(ssit)\ns5 <- c(\"pH.peat\", \"P.peat\", \"Waterlev.av\", \"CEC.peat\", \"Acidity.peat\")\nssit5 <- ssit[s5]\n\ndata(sveg)\nsummary(sveg)\n\n\n## RDA = constrained PCA\nrda <- rda(sveg~., ssit5)\nplot(rda)\n\n## CCA = constrained CA\ncca <- cca(sveg~., ssit5)\nplot(cca)\n\n## Unconstrained and constrained variance\ntot <- cca$tot.chi\nconstr <- cca$CCA$tot.chi\nconstr / tot\n\n\n\nRedundancy analysis (RDA)\nMehr Details zu RDA aus Borcard et al. (Numerical ecology with R)\n\n## Datensatz Doubs\n## Doubs Datensatz in den workspace laden\nload(here(\"data\",\"Doubs.RData\"))  \n\n\nsummary(spe)\nsummary(env)\nsummary(spa)\n\n\n## Entfernen der Untersuchungsfläche ohne Arten\nspe <- spe[-8, ]\nenv <- env[-8, ]\nspa <- spa[-8, ]\n\n## Karten für 4 Fischarten\npar(mfrow = c(2, 2))\nplot(spa, asp = 1, col = \"brown\", cex = spe$Satr, xlab = \"x (km)\", ylab = \"y (km)\", main = \"Brown trout\")\nlines(spa, col = \"light blue\")\nplot(spa, asp = 1, col = \"brown\", cex = spe$Thth, xlab = \"x (km)\", ylab = \"y (km)\", main = \"Grayling\")\nlines(spa, col = \"light blue\")\nplot(spa, asp = 1, col = \"brown\", cex = spe$Alal, xlab = \"x (km)\", ylab = \"y (km)\", main = \"Bleak\")\nlines(spa, col = \"light blue\")\nplot(spa, asp = 1, col = \"brown\", cex = spe$Titi, xlab = \"x (km)\", ylab = \"y (km)\", main = \"Tench\")\nlines(spa, col = \"light blue\")\n\n\n\n## Set aside the variable 'dfs' (distance from the source) for \n## later use\ndfs <- env[, 1]\n## Remove the 'dfs' variable from the env data frame\nenv2 <- env[, -1]\n\n## Recode the slope variable (slo) into a factor (qualitative) \n## variable to show how these are handled in the ordinations\nslo2 <- rep(\".very_steep\", nrow(env))\nslo2[env$slo <= quantile(env$slo)[4]] <- \".steep\"\nslo2[env$slo <= quantile(env$slo)[3]] <- \".moderate\"\nslo2[env$slo <= quantile(env$slo)[2]] <- \".low\"\nslo2 <- factor(slo2, levels = c(\".low\", \".moderate\", \".steep\", \".very_steep\"))\ntable(slo2)\n\nslo2\n       .low   .moderate      .steep .very_steep \n          8           8           6           7 \n\n## Create an env3 data frame with slope as a qualitative variable\nenv3 <- env2\nenv3$slo <- slo2\n\n## Create two subsets of explanatory variables\n## Physiography (upstream-downstream gradient)\nenvtopo <- env2[, c(1 : 3)]\nnames(envtopo)\n\n[1] \"ele\" \"slo\" \"dis\"\n\n## Water quality\nenvchem <- env2[, c(4 : 10)]\nnames(envchem)\n\n[1] \"pH\"  \"har\" \"pho\" \"nit\" \"amm\" \"oxy\" \"bod\"\n\n## Hellinger-transform the species dataset\nlibrary(vegan)\nspe.hel <- decostand(spe, \"hellinger\")\n\n\nspe.hel\n\n\n## Redundancy analysis (RDA)\n### RDA of the Hellinger-transformed fish species data, constrained\n### by all the environmental variables contained in env3\nspe.rda <- rda(spe.hel ~ ., env3) # Observe the shortcut formula\n\n\nspe.rda\nsummary(spe.rda)    # Scaling 2 (default)\n\n\n## Canonical coefficients from the rda object\ncoef(spe.rda)\n\n\n## Unadjusted R^2 und Adjusted R^2\n(R2 <- RsquareAdj(spe.rda))\n\n### Triplots of the rda results (lc scores)\n### Site scores as linear combinations of the environmental variables\n## dev.new(title = \"RDA scaling 1 and 2 + lc\", width = 12, height = 6, noRStudioGD = TRUE)\npar(mfrow = c(1, 2))\n## Scaling 1\nplot(spe.rda,scaling = 1, display = c(\"sp\", \"lc\", \"cn\"), main = \"Triplot RDA spe.hel ~ env3 - scaling 1 - lc scores\")\nspe.sc1 <- scores(spe.rda, choices = 1:2, scaling = 1, display = \"sp\")\narrows(0, 0, spe.sc1[, 1] * 0.92, spe.sc1[, 2] * 0.92, length = 0, lty = 1, col = \"red\")\ntext(-0.75, 0.7, \"a\", cex = 1.5)\n## Scaling 2\nplot(spe.rda, display = c(\"sp\", \"lc\", \"cn\"), main = \"Triplot RDA spe.hel ~ env3 - scaling 2 - lc scores\")\nspe.sc2 <- scores(spe.rda, choices = 1:2, display = \"sp\")\narrows(0, 0, spe.sc2[, 1] * 0.92, spe.sc2[, 2] * 0.92,length = 0, lty = 1, col = \"red\")\ntext(-0.82, 0.55, \"b\", cex = 1.5)\n\n\n### Triplots of the rda results (wa scores)\n### Site scores as weighted averages (vegan's default)\n## Scaling 1 :  distance triplot\n##dev.new(title = \"RDA plot\", width = 12, height = 6, noRStudioGD = TRUE)\npar(mfrow = c(1, 2))\nplot(spe.rda, scaling = 1, main = \"Triplot RDA spe.hel ~ env3 - scaling 1 - wa scores\")\narrows(0, 0, spe.sc1[, 1] * 0.92, spe.sc1[, 2] * 0.92, length = 0, lty = 1, col = \"red\")\n## Scaling 2 (default) :  correlation triplot\nplot(spe.rda, main = \"Triplot RDA spe.hel ~ env3 - scaling 2 - wa scores\")\narrows(0, 0, spe.sc2[, 1] * 0.92, spe.sc2[, 2] * 0.92, length = 0, lty = 1, col = \"red\")\n\n## Select species with goodness-of-fit at least 0.6 in the \n## ordination plane formed by axes 1 and 2\nspe.good <- goodness(spe.rda)\nsel.sp <- which(spe.good[, 2] >= 0.6)\nsel.sp\n\n## Triplots with homemade function triplot.rda(), scalings 1 and 2\nsource(\"triplot.rda.R\")\n##dev.new(title = \"RDA plot with triplot.rda\", width = 12, height = 6, noRStudioGD = TRUE)\npar(mfrow = c(1, 2))\ntriplot.rda(spe.rda, site.sc = \"lc\", scaling = 1, cex.char2 = 0.7, pos.env = 3, \n            pos.centr = 1, mult.arrow = 1.1, mar.percent = 0.05, select.spe = sel.sp)\ntext(-0.92, 0.72, \"a\", cex = 2)\ntriplot.rda(spe.rda, site.sc = \"lc\", scaling = 2, cex.char2 = 0.7, pos.env = 3, \n            pos.centr = 1, mult.arrow = 1.1, mar.percent = 0.05, select.spe = sel.sp)\ntext(-2.82, 2, \"b\", cex = 2)\n\n## Global test of the RDA result\nanova(spe.rda, permutations = how(nperm = 999))\n## Tests of all canonical axes\nanova(spe.rda, by = \"axis\", permutations = how(nperm = 999))\n\n### Partial RDA: effect of water chemistry, holding physiography\n### constant\n\n## Simple syntax; X and W may be in separate tables of quantitative \n## variables\n(spechem.physio <- rda(spe.hel, envchem, envtopo))\n\n\nsummary(spechem.physio)\n\n\n## Formula interface; X and W variables must be in the same \n## data frame\n(spechem.physio2 <- rda(spe.hel ~ pH + har + pho + nit + amm + oxy + bod \n        + Condition(ele + slo + dis), data = env2))\n\n## Test of the partial RDA, using the results with the formula \n## interface to allow the tests of the axes to be run\nanova(spechem.physio2, permutations = how(nperm = 999))\nanova(spechem.physio2, permutations = how(nperm = 999), by = \"axis\")\n\n## Partial RDA triplots (with fitted site scores) \n## with function triplot.rda\n## Scaling 1\n##dev.new(title = \"Partial RDA\",width = 12, height = 6, noRStudioGD = TRUE)\npar(mfrow = c(1, 2))\ntriplot.rda(spechem.physio, site.sc = \"lc\", scaling = 1, \n            cex.char2 = 0.8, pos.env = 3, mar.percent = 0)\ntext(-0.58, 0.64, \"a\", cex = 2)\n\n## Scaling 2\ntriplot.rda(spechem.physio, site.sc = \"lc\", scaling = 2, cex.char2 = 0.8, \n            pos.env = 3, mult.spe = 1.1, mar.percent = 0.04)\ntext(-3.34, 3.64, \"b\", cex = 2)\n\n\n\nVariation partioning\n\n### Variation partitioning with two sets of explanatory variables\n\n## Explanation of fraction labels (two, three and four explanatory \n## matrices) with optional colours\npar(mfrow = c(1, 3), mar = c(1, 1, 1, 1))\nshowvarparts(2, bg = c(\"red\", \"blue\"))\nshowvarparts(3, bg = c(\"red\", \"blue\", \"yellow\"))\nshowvarparts(4, bg = c(\"red\", \"blue\", \"yellow\", \"green\"))\n\n### 1. Variation partitioning with all explanatory variables\n###    (except dfs)\n(spe.part.all <- varpart(spe.hel, envchem, envtopo))\n\n## Plot of the partitioning results\npar(mfrow = c(1, 1))\nplot(spe.part.all, digits = 2, bg = c(\"red\", \"blue\"),\n     Xnames = c(\"Chemistry\", \"Physiography\"), \n     id.size = 0.7)"
  },
  {
    "objectID": "stat5-8/Statistik7_Loesung.html#musterloesung-aufgabe-7.1-rda",
    "href": "stat5-8/Statistik7_Loesung.html#musterloesung-aufgabe-7.1-rda",
    "title": "Stat7: Lösung",
    "section": "Musterloesung Aufgabe 7.1: RDA",
    "text": "Musterloesung Aufgabe 7.1: RDA\n\nR-Skript als Download\nLoesungstext\nÜbungsaufgabe (hier so ausführlich formuliert, wie dies auch in der Klausur der Fall sein wird)\nLadet die library dave, welche den Moordatensatz enthält. sveg beinhaltet presenceabsence-Daten aller untersuchten Arten in den Plots; ssit beinhaltet 18 metrische Umweltdaten sowie Koordinaten der Plots\nFührt eine RDA und eine Varianzpartizionierung in die Variablengruppen Physiographie (Waterlev.max, Waterlev.av, Waterlev.min, log.peat.lev, log slope.deg) und Chemie (alle übrigen) durch.\nFormuliert abschliessend einen Methoden- und Ergebnisteil (ggf. incl. adäquaten Abbildungen) zu dieser Untersuchung in der Form einer wissenschaftlichen Arbeit (ausformulierte schriftliche Zusammenfassung, mit je einem Absatz von ca. 60-100 Worten, resp. 3-8 Sätzen für den Methoden- und Ergebnisteil). D. h. alle wichtigen Informationen sollten enthalten sein, unnötige Redundanz dagegen vermieden werden.\nWährend im Text normalerweise die Variablen ausgeschrieben werden solltet, genügt es hier (da ihr die entsprechenden Infos nicht bekommen habt und nur raten könntet), wenn ihr die Abkürzungen aus dem dataframe nehmt.\n\nMoordatensatz laden\n\nif(!require(dave)){install.packages(\"dave\")}\n\nLoading required package: dave\n\n\nLoading required package: cluster\n\n\nLoading required package: labdsv\n\n\nLoading required package: mgcv\n\n\nLoading required package: nlme\n\n\nThis is mgcv 1.8-40. For overview type 'help(\"mgcv-package\")'.\n\n\nThis is labdsv 2.0-1\nconvert existing ordinations with as.dsvord()\n\n\n\nAttaching package: 'labdsv'\n\n\nThe following object is masked from 'package:stats':\n\n    density\n\n\nLoading required package: vegan\n\n\nLoading required package: permute\n\n\nLoading required package: lattice\n\n\nThis is vegan 2.6-2\n\n\nLoading required package: nnet\n\n\n\nAttaching package: 'nnet'\n\n\nThe following object is masked from 'package:mgcv':\n\n    multinom\n\n\nLoading required package: tree\n\nlibrary(dave)\ndata(sveg)\ndata(ssit)\n\n\nsummary(sveg)\nsummary(ssit)\nstr(ssit)\n\n\n# x.axis and y.axis vom data frame data frame ssit entfernen\nenv2 <- ssit[, -c(19, 20)]\n\nBetrachtung der Daten zeigt, dass die Koordinaten in Spalten 19 und 20 sind, die daraufhin entfernt werden.\n\n# Generiere zwei subset der erklärenden Variablen\n# Physiografie (upstream-downstream-Gradient)\nenvtopo <- env2[, c(11 : 15)]\nnames(envtopo)\n\n[1] \"Waterlev.max\"  \"Waterlev.av\"   \"Waterlev.min\"  \"log.peat.lev\" \n[5] \"log.slope.deg\"\n\n# Chemie\nenvchem <- env2[, c(1:10, 16:18)]\nnames(envchem)\n\n [1] \"pH.peat\"        \"log.ash.perc\"   \"Ca_peat\"        \"Mg_peat\"       \n [5] \"Na_peat\"        \"K_peat\"         \"Acidity.peat\"   \"CEC.peat\"      \n [9] \"Base.sat.perc\"  \"P.peat\"         \"pH.water\"       \"log.cond.water\"\n[13] \"log.Ca.water\"  \n\n# Hellinger-transform the species dataset\nlibrary(vegan)\nspe.hel <- decostand(sveg, \"hellinger\")\n\nVorstehend wurden die Variablen in die zwei Gruppen Chemistry und Physiography aufgteilt. Die Hellilnger-Transformation wird gemeinhin empfohlen (wobei dahingestellt sei, ob sie auch bei presence-absence-Daten nötig ist). Die weiteren Analysen führen wir mit der default-Einstellung „Scaling 2“ durch. (Je nach Bedarf bzw. persönlichen Vorlieben könnte auch Scaling 1 genommen werden).\nRedundancy analysis (RDA)\nRDA of the Hellinger-transformed mire species data, constrained by all the environmental variables contained in env2\n\n## RDA der Hellinger-transformireten Moorarten-Daten, constrained\n## mit allen Umweltvarialben die in env2 enthalten sind\n(spe.rda <- rda(spe.hel ~ ., env2)) # Observe the shortcut formula\n\nCall: rda(formula = spe.hel ~ pH.peat + log.ash.perc + Ca_peat +\nMg_peat + Na_peat + K_peat + Acidity.peat + CEC.peat + Base.sat.perc +\nP.peat + Waterlev.max + Waterlev.av + Waterlev.min + log.peat.lev +\nlog.slope.deg + pH.water + log.cond.water + log.Ca.water, data = env2)\n\n              Inertia Proportion Rank\nTotal          0.4979     1.0000     \nConstrained    0.2773     0.5569   18\nUnconstrained  0.2206     0.4431   44\nInertia is variance \n\nEigenvalues for constrained axes:\n   RDA1    RDA2    RDA3    RDA4    RDA5    RDA6    RDA7    RDA8    RDA9   RDA10 \n0.13693 0.03784 0.01860 0.01110 0.00950 0.00882 0.00735 0.00670 0.00603 0.00582 \n  RDA11   RDA12   RDA13   RDA14   RDA15   RDA16   RDA17   RDA18 \n0.00498 0.00478 0.00411 0.00394 0.00310 0.00294 0.00271 0.00205 \n\nEigenvalues for unconstrained axes:\n     PC1      PC2      PC3      PC4      PC5      PC6      PC7      PC8 \n0.018768 0.015312 0.013231 0.012074 0.011420 0.009385 0.008904 0.008639 \n(Showing 8 of 44 unconstrained eigenvalues)\n\n\n\nsummary(spe.rda)    # Skalierung 2 (default)\n\n\n# Canonical coefficients from the rda object\ncoef(spe.rda)\n\n                        RDA1          RDA2         RDA3         RDA4\npH.peat         4.531329e-02  0.0828973389  0.010965288 -0.155085533\nlog.ash.perc   -1.949986e-02 -0.0359844539 -0.701312936  0.034317746\nCa_peat        -1.337537e-03 -0.0288554798  0.027228621  0.068880266\nMg_peat        -3.936852e-02  0.0901067458 -0.141325150 -0.070256979\nNa_peat        -9.247087e-02  0.4384377252  0.088851211 -0.248295480\nK_peat          6.296120e-02 -0.0699716499  0.015460922  0.314006668\nAcidity.peat    4.708024e-05 -0.0277547066  0.005448542  0.043747599\nCEC.peat        3.744129e-03  0.0297812515 -0.010532610 -0.040816301\nBase.sat.perc   1.129368e-03 -0.0041436746 -0.013166371 -0.005538586\nP.peat         -1.097201e-02 -0.0352965867 -0.063285184 -0.019908698\nWaterlev.max   -3.179331e-03 -0.0006509661 -0.015533249  0.038929542\nWaterlev.av     8.236051e-04 -0.0049269233  0.027915286 -0.018711058\nWaterlev.min    3.830259e-04 -0.0009284990  0.002283015  0.002325086\nlog.peat.lev   -1.168763e-01  0.1415162776  0.002413566 -0.271470076\nlog.slope.deg  -3.383155e-02  0.0016520826 -0.236646952  0.076539930\npH.water        6.367244e-02  0.0538977579  0.079825940  0.179498186\nlog.cond.water  6.110612e-03 -0.2600161375 -0.162560478  0.093945551\nlog.Ca.water    1.317034e-02 -0.0622061756 -0.023932814 -0.597437862\n                        RDA5          RDA6         RDA7         RDA8\npH.peat        -0.1892255621 -0.2118068056  0.071909611 -0.027538260\nlog.ash.perc    0.1802037523 -0.2669540104  0.724618282 -0.633888225\nCa_peat        -0.0007103952  0.0253977768  0.029045044 -0.021223056\nMg_peat         0.0994092273 -0.1176642458  0.100095923  0.381181043\nNa_peat         2.2386515374  0.7455515343  0.664234477 -0.718624356\nK_peat          0.1287939324 -0.2046202165 -0.441055492 -0.417918725\nAcidity.peat    0.0519802264  0.0270485440  0.071957387 -0.032035431\nCEC.peat       -0.0472321525 -0.0329326428 -0.053476411  0.032372963\nBase.sat.perc   0.0276382747  0.0003004136  0.012777988  0.002256490\nP.peat         -0.0401765601  0.0112976551  0.068766100  0.003460905\nWaterlev.max   -0.0456458872 -0.1345249731  0.097859198 -0.022480057\nWaterlev.av     0.0355444546  0.0722625146 -0.090790927  0.036951069\nWaterlev.min   -0.0105108162 -0.0136267137  0.017986436 -0.006832735\nlog.peat.lev   -0.5267908795 -0.2162043424 -0.274818126 -0.338891940\nlog.slope.deg  -0.2184244227  0.1500482029 -0.105292557  0.095365402\npH.water       -0.0425049325  0.1782004196 -0.059654515 -0.114988275\nlog.cond.water  0.0168002458  0.1815625052 -0.007997259 -0.464849052\nlog.Ca.water   -0.1033058867 -0.3899181259  0.078901455  0.315891985\n                       RDA9        RDA10       RDA11        RDA12        RDA13\npH.peat        -0.012382751 -0.017468114 -0.06519173 -0.027412762 -0.104975398\nlog.ash.perc   -0.965668438 -0.028793591  0.44461671  0.129536979  0.377398995\nCa_peat        -0.012619772 -0.107438326 -0.04460841 -0.103406142  0.114429353\nMg_peat        -0.158294076 -0.280166029  0.05634171 -0.070201766  0.268732885\nNa_peat         1.104751868  1.544769984 -1.39741894 -0.039845605  1.780098125\nK_peat         -0.534198927 -0.076296838 -0.22435445 -0.163742387 -0.123997869\nAcidity.peat   -0.068974997 -0.087677482 -0.06530877 -0.046256867  0.033805349\nCEC.peat        0.047560024  0.083210188  0.06665024  0.077515801 -0.068579244\nBase.sat.perc  -0.020829815  0.009830788  0.00910173  0.022643234 -0.034283054\nP.peat          0.012695344 -0.003138906  0.03352924 -0.008483557  0.043199360\nWaterlev.max    0.012606335 -0.048317756  0.09623737 -0.013544370 -0.032439161\nWaterlev.av     0.004208039  0.047229561 -0.08062624 -0.005265601  0.023482438\nWaterlev.min   -0.001054475 -0.005577274  0.01298239  0.007069788  0.002347459\nlog.peat.lev    0.003928806 -0.238217788  0.30310954  0.563613618  0.160866568\nlog.slope.deg  -0.057527637  0.291653459 -0.22134679  0.391384381  0.236877948\npH.water        0.212751287  0.111459569 -0.01131132 -0.090296584  0.120070232\nlog.cond.water  0.668937274 -0.008842423  0.03594579 -0.809188635  0.224126901\nlog.Ca.water   -1.090714519 -0.887533762 -0.45040221  0.927233456 -0.402746243\n                      RDA14        RDA15         RDA16       RDA17\npH.peat        -0.067972158  0.018701068  0.1485174317 -0.13349865\nlog.ash.perc   -0.194930772  0.919823359 -0.0460514789  1.35532652\nCa_peat         0.021190482 -0.045772935 -0.0586146947 -0.14093897\nMg_peat        -0.350138790  0.113874369  0.0939318308 -0.04980116\nNa_peat         0.278280645 -0.213777421 -0.8329460427  1.09568980\nK_peat         -0.105394480 -0.062226421 -0.0750622607  0.05144200\nAcidity.peat   -0.096815006  0.023817495 -0.0702792017 -0.09633424\nCEC.peat        0.054680669 -0.005738998  0.0663748635  0.11471933\nBase.sat.perc  -0.039890903  0.034786341 -0.0054775951  0.00801054\nP.peat          0.071393270  0.052202136  0.0478902318 -0.02965954\nWaterlev.max    0.061094560 -0.047338261  0.0032500492 -0.07021013\nWaterlev.av    -0.037716665  0.061946296 -0.0002491098  0.05976381\nWaterlev.min    0.007026893 -0.017849338  0.0015009609 -0.01493811\nlog.peat.lev   -0.263156404  0.415682834 -0.5691321204 -0.16806725\nlog.slope.deg  -0.219914297 -0.271490962 -0.4115760623  0.06629741\npH.water       -0.141903633 -0.018249431 -0.0414434431  0.04361964\nlog.cond.water -0.817975382 -0.634193576  0.1319661957  0.28691611\nlog.Ca.water    1.358028534  0.192611956 -0.2582206194  0.02520368\n                       RDA18\npH.peat         0.1701622100\nlog.ash.perc    0.4171590020\nCa_peat         0.1568432425\nMg_peat        -0.2684650074\nNa_peat        -0.3478060087\nK_peat          0.1752018988\nAcidity.peat    0.0918662915\nCEC.peat       -0.0870338687\nBase.sat.perc  -0.0140300438\nP.peat          0.0140083088\nWaterlev.max   -0.0017944793\nWaterlev.av    -0.0001508213\nWaterlev.min    0.0034494643\nlog.peat.lev   -0.1448115821\nlog.slope.deg  -0.0655661508\npH.water       -0.1695726179\nlog.cond.water  0.3588429780\nlog.Ca.water   -0.2342020703\n\n# Unadjusted R^2 retrieved from the rda object\n(R2 <- RsquareAdj(spe.rda)$r.squared)\n\n[1] 0.5569395\n\n# Adjusted R^2 retrieved from the rda object\n(R2adj <- RsquareAdj(spe.rda)$adj.r.squared)\n\n[1] 0.3756874\n\n\nMan erhält R²adj. = 0.376 Jetzt kann man den Triplot erstellen\n\n## Triplots of the rda results (lc scores)\n## Site scores as linear combinations of the environmental variables\ndev.new(title = \"RDA scaling 1 and 2 + lc\", width = 15, height = 6, noRStudioGD = TRUE)\npar(mfrow = c(1, 2))\n\n# 1 und 2 Achse\nplot(spe.rda, display = c(\"sp\", \"lc\", \"cn\"), \n     main = \"Triplot RDA spe.hel ~ env2 - scaling 2 - lc scores\")\nspe.sc2 <- scores(spe.rda, choices = 1:2, display = \"sp\")\narrows(0, 0, spe.sc2[, 1] * 0.92, spe.sc2[, 2] * 0.92,length = 0,\n       lty = 1,col = \"red\")\ntext(-0.82, 0.55, \"b\", cex = 1.5)\n\n# 1 und 3 Achse\nplot(spe.rda, display = c(\"sp\", \"lc\", \"cn\"), choices = c(1,3),\n     main = \"Triplot RDA spe.hel ~ env2 - scaling 2 - lc scores\")\nspe.sc2 <- scores(spe.rda, choices = c(1,3), display = \"sp\")\narrows(0, 0, spe.sc2[, 1] * 0.92, spe.sc2[, 2] * 0.92,length = 0,\n       lty = 1,col = \"red\")\ntext(-0.82, 0.55, \"b\", cex = 1.5)\n\n\n## Triplots of the rda results (wa scores)\n## Site scores as weighted averages (vegan's default)\n# Scaling 1 :  distance triplot\ndev.new(title = \"RDA scaling 2 + wa\",width = 7, height = 6, noRStudioGD = TRUE)\n\n# Scaling 2 (default) :  correlation triplot\nplot(spe.rda, main = \"Triplot RDA spe.hel ~ env3 - scaling 2 - wa scores\")\narrows(0, 0, spe.sc2[, 1] * 0.92, spe.sc2[, 2] * 0.92, length = 0, lty = 1, col = \"red\")\n\nAuswahl der höchstkorrelierten Arten (Grenzwert kann subjektiv nach Bedarf gesetzt werden, hier 0.5).\n\n# Select species with goodness-of-fit at least 0.6 in the \n# ordination plane formed by axes 1 and 2\nspe.good <- goodness(spe.rda)\nsel.sp <- which(spe.good[, 2] >= 0.6)\nsel.sp\n\n# Triplots with homemade function triplot.rda()\nsource(\"triplot.rda.R\")\n\ndev.new(title = \"RDA plot with triplot.rda\", width = 7, height = 6, noRStudioGD = TRUE)\n\ntriplot.rda(spe.rda, site.sc = \"lc\", cex.char2 = 0.7, pos.env = 3, \n            pos.centr = 1, mult.arrow = 1.1, mar.percent = 0.05, select.spe = sel.sp)\n\n\n# Global test of the RDA result\nanova(spe.rda, permutations = how(nperm = 999))\n# Tests of all canonical axes\nanova(spe.rda, permutations = how(nperm = 999))\nanova(spe.rda, by = \"axis\", permutations = how(nperm = 999))\n\nDie ersten drei RDA-Achsen sind also signifikant. Man könnte also auch noch eine Visualisierung von RDA 3 vs. RDA 1 machen.\n\nPartielle RDA\nSimple syntax; X and W may be in separate tables of quantitative\nvariables\n\n\nspechem.physio <- rda(spe.hel, envchem, envtopo)\nsummary(spechem.physio)\n\n# Formula interface; X and W variables must be in the same \n# data frame\n(spechem.physio2 <- \n    rda(spe.hel ~ pH.peat + log.ash.perc + Ca_peat + Mg_peat + Na_peat\n       + K_peat + Acidity.peat + CEC.peat + Base.sat.perc + P.peat\n       + pH.water + log.cond.water + log.Ca.water\n       + Condition(Waterlev.max + Waterlev.av + Waterlev.min + log.peat.lev\n       + log.slope.deg), data = env2))\n\n# Test of the partial RDA, using the results with the formula \n# interface to allow the tests of the axes to be run\nanova(spechem.physio2, permutations = how(nperm = 999))\nanova(spechem.physio2, permutations = how(nperm = 999), by = \"axis\")\n\n# Partial RDA triplots (with fitted site scores) \n# with function triplot.rda\ndev.new(title = \"Partial RDA\", width = 7, height = 6, noRStudioGD = TRUE)\n\ntriplot.rda(spechem.physio, site.sc = \"lc\", scaling = 2, \n            cex.char2 = 0.8, pos.env = 3, mult.spe = 1.1, mar.percent = 0.04)\ntext(-3.34, 3.64, \"b\", cex = 2)\n\nVarianzpartitionierung\n\n## 1. Variation partitioning with all explanatory variables\n(spe.part.all <- varpart(spe.hel, envchem, envtopo))\n\n\nPartition of variance in RDA \n\nCall: varpart(Y = spe.hel, X = envchem, envtopo)\n\nExplanatory tables:\nX1:  envchem\nX2:  envtopo \n\nNo. of explanatory tables: 2 \nTotal variation (SS): 30.873 \n            Variance: 0.49795 \nNo. of observations: 63 \n\nPartition table:\n                     Df R.squared Adj.R.squared Testable\n[a+b] = X1           13   0.47805       0.33958     TRUE\n[b+c] = X2            5   0.24779       0.18181     TRUE\n[a+b+c] = X1+X2      18   0.55694       0.37569     TRUE\nIndividual fractions                                    \n[a] = X1|X2          13                 0.19388     TRUE\n[b]                   0                 0.14570    FALSE\n[c] = X2|X1           5                 0.03611     TRUE\n[d] = Residuals                         0.62431    FALSE\n---\nUse function 'rda' to test significance of fractions of interest\n\n# Plot of the partitioning results\ndev.new(title = \"Variation partitioning\", width = 7, height = 7, noRStudioGD = TRUE)\n\nplot(spe.part.all, digits = 2, bg = c(\"red\", \"blue\"),\n     Xnames = c(\"Chemistry\", \"Physiography\"), id.size = 0.7)\n\nDie durch die erhobenen Umweltvariablen insgesamt erklärte Varianz (37.6%, s.o.) entfällt zu 19.4% auf chemische Variablen, 3.6% auf physiographische Variablen und zu 14.6% auf gemeinsame Erklärung."
  },
  {
    "objectID": "stat5-8/Statistik7_Uebung.html",
    "href": "stat5-8/Statistik7_Uebung.html",
    "title": "Stat7: Übung",
    "section": "",
    "text": "Übung 7.1: RDA (naturwissenschaftlich)\n\nFunktion triplot.rda.R\n\nMoordatensatz in library(dave) :\n\nsveg (Vegetationsdaten)\nssit (Umweltdaten)\n\nFührt eine RDA mit allen in der Vorlesung gezeigten Schritten durch und interpretiert die Ergebnisse.\nVon den Umweltvariablen entfallen x.axis & y.axis\nFür die partielle RDA und die Varianzpartitionierung bildet zwei Gruppen:\n\nPhysiographie (Waterlev.max, Waterlev.av, Waterlev.min, log.peat.lev, log slope.deg)\nChemie (alle übrigen)"
  },
  {
    "objectID": "stat5-8/Statistik8_Demo.html#k-means-clustering",
    "href": "stat5-8/Statistik8_Demo.html#k-means-clustering",
    "title": "Stat8: Demo",
    "section": "k-means clustering",
    "text": "k-means clustering\n\n# das Moordatenset aus Wildi...\nif(!require(dave)){install.packages(\"dave\")}\n\nLoading required package: dave\n\n\nLoading required package: cluster\n\n\nLoading required package: labdsv\n\n\nLoading required package: mgcv\n\n\nLoading required package: nlme\n\n\nThis is mgcv 1.8-40. For overview type 'help(\"mgcv-package\")'.\n\n\nThis is labdsv 2.0-1\nconvert existing ordinations with as.dsvord()\n\n\n\nAttaching package: 'labdsv'\n\n\nThe following object is masked from 'package:stats':\n\n    density\n\n\nLoading required package: vegan\n\n\nLoading required package: permute\n\n\nLoading required package: lattice\n\n\nThis is vegan 2.6-2\n\n\nLoading required package: nnet\n\n\n\nAttaching package: 'nnet'\n\n\nThe following object is masked from 'package:mgcv':\n\n    multinom\n\n\nLoading required package: tree\n\nlibrary(dave)\npca <- rda(sveg^0.25, scale = TRUE)\nca <- cca(sveg^0.5)\n\nkmeans.1 <- kmeans(sveg, 4)\n\n\nkmeans.1\n\n\nplot(ca, type = \"n\")\npoints(ca, display = \"sites\", col = kmeans.1[[1]])\n\n\n\nkmeans.2 <- kmeans(sveg, 3)\nplot(pca, type = \"n\")\npoints(pca, display = \"sites\", pch=19, col = kmeans.2[[1]])\n\n\n\nplot(pca, choices = c(1, 3), type = \"n\")\npoints(pca, choices = c(1, 3), display = \"sites\", pch = 19, col=kmeans.2[[1]])\n\n\n\n# k-means partitioning, 2 to 10 groups\nKM.cascade <- cascadeKM(sveg,  inf.gr = 2, sup.gr = 10, iter = 100, criterion = \"ssi\")\nsummary(KM.cascade)\n\n          Length Class  Mode     \npartition 567    -none- numeric  \nresults    18    -none- numeric  \ncriterion   1    -none- character\nsize       90    -none- numeric  \n\nKM.cascade$results\n\n      2 groups     3 groups     4 groups     5 groups     6 groups     7 groups\nSSE 1840.13571 1629.4399038 1488.2961538 1378.3369048 1286.5005411 1214.3219697\nssi    0.26103    0.2752258    0.3467853    0.3033231    0.4041437    0.4328301\n        8 groups     9 groups    10 groups\nSSE 1156.7314935 1101.5523810 1053.1476190\nssi    0.3716024    0.5261008    0.4702227\n\nKM.cascade$partition\n\n    2 groups 3 groups 4 groups 5 groups 6 groups 7 groups 8 groups 9 groups\n501        2        1        2        1        5        3        6        7\n502        2        1        4        3        6        4        3        5\n503        2        1        2        1        5        3        6        7\n504        2        1        2        1        5        3        6        7\n505        2        1        4        3        6        4        3        5\n506        2        1        2        1        6        4        3        5\n507        2        1        2        1        5        3        6        7\n508        2        1        2        1        5        3        6        7\n509        2        1        2        1        5        3        6        7\n510        2        1        4        3        3        7        8        2\n511        2        1        2        1        5        3        6        7\n512        2        1        4        3        6        4        3        5\n513        2        1        4        3        3        7        8        4\n514        2        1        4        3        6        4        3        5\n515        2        1        4        3        6        4        3        5\n516        2        1        4        3        6        4        3        5\n517        2        1        4        3        3        7        8        2\n518        1        2        3        4        2        2        7        1\n519        2        1        4        3        6        4        3        5\n520        2        1        4        3        3        7        8        2\n521        2        1        2        1        5        3        6        2\n522        2        1        4        3        6        4        3        5\n523        1        2        3        4        2        2        7        1\n524        2        1        2        1        5        3        6        2\n525        2        1        2        1        5        3        6        2\n526        1        3        1        4        2        2        7        1\n527        2        1        4        3        3        7        8        4\n528        2        1        4        3        3        7        8        4\n529        2        1        4        3        3        7        8        4\n530        2        1        4        3        3        7        8        2\n531        1        2        3        5        4        5        5        6\n532        2        1        4        3        3        7        8        4\n533        2        1        4        3        3        7        8        2\n534        2        2        3        5        4        5        5        6\n535        1        2        3        5        4        5        5        6\n536        2        1        4        3        3        7        8        4\n537        2        1        4        3        3        7        8        4\n538        1        2        3        4        2        2        7        1\n539        1        3        1        2        1        6        2        8\n540        2        1        2        1        5        3        6        7\n541        1        3        1        2        1        6        1        9\n542        1        3        1        2        1        6        2        8\n543        1        3        1        2        1        6        1        9\n544        2        2        3        5        4        5        5        6\n545        1        2        3        4        2        2        7        1\n546        1        2        3        5        4        5        5        6\n547        2        2        3        5        4        5        5        4\n548        1        3        1        2        1        6        1        9\n549        1        2        3        5        4        5        5        6\n550        1        2        3        5        4        5        5        6\n551        1        3        1        4        2        1        4        3\n552        1        3        1        2        1        6        1        9\n553        1        3        1        2        1        6        4        3\n554        1        3        1        4        2        1        4        3\n555        1        3        1        4        2        1        4        3\n556        1        3        1        2        1        6        1        9\n557        1        3        1        4        2        1        4        3\n558        1        3        1        4        2        1        4        3\n559        1        3        1        4        2        2        7        1\n560        1        3        1        4        2        2        7        1\n561        1        2        3        4        2        2        7        1\n562        1        3        1        2        1        6        2        8\n563        1        3        1        4        2        1        4        3\n    10 groups\n501         5\n502         6\n503         5\n504         5\n505         6\n506         6\n507         5\n508         5\n509         5\n510        10\n511         8\n512         6\n513         2\n514         6\n515         6\n516         6\n517        10\n518         8\n519         6\n520        10\n521        10\n522         6\n523         1\n524        10\n525        10\n526         1\n527         2\n528         2\n529         2\n530        10\n531         9\n532         2\n533        10\n534         9\n535         9\n536         2\n537         2\n538         8\n539         3\n540         5\n541         7\n542         3\n543         7\n544         9\n545         1\n546         9\n547         2\n548         7\n549         9\n550         9\n551         4\n552         7\n553         4\n554         4\n555         4\n556         7\n557         4\n558         4\n559         1\n560         1\n561         1\n562         3\n563         4\n\n# k-means visualisation\nplot(KM.cascade, sortg = TRUE)"
  },
  {
    "objectID": "stat5-8/Statistik8_Demo.html#agglomarative-clusteranalyse",
    "href": "stat5-8/Statistik8_Demo.html#agglomarative-clusteranalyse",
    "title": "Stat8: Demo",
    "section": "Agglomarative Clusteranalyse",
    "text": "Agglomarative Clusteranalyse\nmit Daten und Skripten aus Borcard et al. (2018)\n\nload(here(\"data\",\"Doubs.RData\"))  \n\n\n# Remove empty site 8\nspe <- spe[-8, ]\nenv <- env[-8, ]\nspa <- spa[-8, ]\nlatlong <- latlong[-8, ]"
  },
  {
    "objectID": "stat5-8/Statistik8_Demo.html#dendogramme-berechnen-und-ploten",
    "href": "stat5-8/Statistik8_Demo.html#dendogramme-berechnen-und-ploten",
    "title": "Stat8: Demo",
    "section": "Dendogramme berechnen und ploten",
    "text": "Dendogramme berechnen und ploten\n\n## Hierarchical agglomerative clustering of the species abundance \n\n# Compute matrix of chord distance among sites\nspe.norm <- decostand(spe, \"normalize\")\nspe.ch <- vegdist(spe.norm, \"euc\")\n\n# Attach site names to object of class 'dist'\nattr(spe.ch, \"Labels\") <- rownames(spe)\n\npar(mfrow = c(1, 1))\n\n# Compute single linkage agglomerative clustering\nspe.ch.single <- hclust(spe.ch, method = \"single\")\n# Plot a dendrogram using the default options\nplot(spe.ch.single, labels = rownames(spe), main = \"Chord - Single linkage\")\n\n\n\n# Compute complete-linkage agglomerative clustering\nspe.ch.complete <- hclust(spe.ch, method = \"complete\")\nplot(spe.ch.complete, labels = rownames(spe), main = \"Chord - Complete linkage\")\n\n\n\n# Compute UPGMA agglomerative clustering\nspe.ch.UPGMA <- hclust(spe.ch, method = \"average\")\nplot(spe.ch.UPGMA, labels = rownames(spe), main = \"Chord - UPGMA\")\n\n\n\n# Compute centroid clustering\nspe.ch.centroid <- hclust(spe.ch, method = \"centroid\")\nplot(spe.ch.centroid, labels = rownames(spe),  main = \"Chord - Centroid\")\n\n\n\n# Compute Ward's minimum variance clustering\nspe.ch.ward <-hclust(spe.ch, method = \"ward.D2\")\nplot(spe.ch.ward, labels = rownames(spe),  main = \"Chord - Ward\")\n\n\n\n# Compute beta-flexible clustering using cluster::agnes()\n# beta = -0.1\nspe.ch.beta1 <- agnes(spe.ch, method = \"flexible\", par.method = 0.55)\n# beta = -0.25\nspe.ch.beta2 <- agnes(spe.ch, method = \"flexible\", par.method = 0.625)\n# beta = -0.5\nspe.ch.beta3 <- agnes(spe.ch, method = \"flexible\", par.method = 0.75)\n# Change the class of agnes objects\nclass(spe.ch.beta1)\n\n[1] \"agnes\" \"twins\"\n\nspe.ch.beta1 <- as.hclust(spe.ch.beta1)\nclass(spe.ch.beta1)\n\n[1] \"hclust\"\n\nspe.ch.beta2 <- as.hclust(spe.ch.beta2)\nspe.ch.beta3 <- as.hclust(spe.ch.beta3)\n\npar(mfrow = c(2, 2))\nplot(spe.ch.beta1, labels = rownames(spe), main = \"Chord - Beta-flexible (beta=-0.1)\")\nplot(spe.ch.beta2, labels = rownames(spe), main = \"Chord - Beta-flexible (beta=-0.25)\")\nplot(spe.ch.beta3,  labels = rownames(spe),  main = \"Chord - Beta-flexible (beta=-0.5)\")\n\n# Compute Ward's minimum variance clustering\nspe.ch.ward <- hclust(spe.ch, method = \"ward.D2\")\nplot(spe.ch.ward, labels = rownames(spe), main = \"Chord - Ward\")"
  },
  {
    "objectID": "stat5-8/Statistik8_Demo.html#cophenetic-correlations",
    "href": "stat5-8/Statistik8_Demo.html#cophenetic-correlations",
    "title": "Stat8: Demo",
    "section": "Cophenetic correlations",
    "text": "Cophenetic correlations\n\n# Single linkage clustering\nspe.ch.single.coph <- cophenetic(spe.ch.single)\ncor(spe.ch, spe.ch.single.coph)\n\n[1] 0.5015116\n\n# Complete linkage clustering\nspe.ch.comp.coph <- cophenetic(spe.ch.complete)\ncor(spe.ch, spe.ch.comp.coph)\n\n[1] 0.7567998\n\n# Average clustering\nspe.ch.UPGMA.coph <- cophenetic(spe.ch.UPGMA)\ncor(spe.ch, spe.ch.UPGMA.coph)\n\n[1] 0.8537529\n\n# Ward clustering\nspe.ch.ward.coph <- cophenetic(spe.ch.ward)\ncor(spe.ch, spe.ch.ward.coph)\n\n[1] 0.7821555\n\n# Shepard-like diagrams\npar(mfrow = c(2, 2))\nplot(spe.ch, spe.ch.single.coph,\n  xlab = \"Chord distance\", ylab = \"Cophenetic distance\",\n  asp = 1, xlim = c(0, sqrt(2)), ylim = c(0, sqrt(2)),\n  main = c(\"Single linkage\", paste(\"Cophenetic correlation =\",\n                                   round(cor(spe.ch, spe.ch.single.coph), 3))))\nabline(0, 1)\nlines(lowess(spe.ch, spe.ch.single.coph), col = \"red\")\n\nplot(spe.ch, spe.ch.comp.coph,\n  xlab = \"Chord distance\", ylab = \"Cophenetic distance\",\n  asp = 1, xlim = c(0, sqrt(2)), ylim = c(0, sqrt(2)),\n  main = c(\"Complete linkage\", paste(\"Cophenetic correlation =\",\n                                     round(cor(spe.ch, spe.ch.comp.coph), 3))))\nabline(0, 1)\nlines(lowess(spe.ch, spe.ch.comp.coph), col = \"red\")\n\nplot(spe.ch, spe.ch.UPGMA.coph,\n  xlab = \"Chord distance\", ylab = \"Cophenetic distance\",\n  asp = 1, xlim = c(0, sqrt(2)), ylim = c(0, sqrt(2)),\n  main = c(\"UPGMA\", paste(\"Cophenetic correlation =\",\n                          round( cor(spe.ch, spe.ch.UPGMA.coph), 3))))\nabline(0, 1)\nlines(lowess(spe.ch, spe.ch.UPGMA.coph), col = \"red\")\n\nplot(spe.ch, spe.ch.ward.coph,\n  xlab = \"Chord distance\", ylab = \"Cophenetic distance\",\n  asp = 1, xlim = c(0, sqrt(2)), ylim = c(0, max(spe.ch.ward$height)),\n  main = c(\"Ward\", paste(\"Cophenetic correlation =\", \n                         round(cor(spe.ch, spe.ch.ward.coph), 3))))\nabline(0, 1)\nlines(lowess(spe.ch, spe.ch.ward.coph), col = \"red\")"
  },
  {
    "objectID": "stat5-8/Statistik8_Demo.html#optimale-anzahl-cluster",
    "href": "stat5-8/Statistik8_Demo.html#optimale-anzahl-cluster",
    "title": "Stat8: Demo",
    "section": "Optimale Anzahl Cluster",
    "text": "Optimale Anzahl Cluster\n\n## Select a dendrogram (Ward/chord) and apply three criteria\n## to choose the optimal number of clusters\n\n# Choose and rename the dendrogram (\"hclust\" object)\nhc <- spe.ch.ward\n# hc <- spe.ch.beta2\n# hc <- spe.ch.complete\n\npar(mfrow = c(1, 2))\n\n# Average silhouette widths (Rousseeuw quality index)\nSi <- numeric(nrow(spe))\nfor (k in 2:(nrow(spe) - 1))\n{\n  sil <- silhouette(cutree(hc, k = k), spe.ch)\n  Si[k] <- summary(sil)$avg.width\n}\n\nk.best <- which.max(Si)\nplot(1:nrow(spe), Si, type = \"h\",\n  main = \"Silhouette-optimal number of clusters\",\n  xlab = \"k (number of clusters)\", ylab = \"Average silhouette width\")\naxis(1, k.best,paste(\"optimum\", k.best, sep = \"\\n\"), col = \"red\", \n     font = 2, col.axis = \"red\")\npoints(k.best,max(Si), pch = 16, col = \"red\",cex = 1.5)\n\n# Optimal number of clusters according to matrix correlation \n# statistic (Pearson)\n\n# Homemade function grpdist from Borcard et al. (2018)\ngrpdist <- function(X)\n{\n  require(cluster)\n  veg <- as.data.frame(as.factor(X))\n  distgr <- daisy(veg, \"gower\")\n  distgr\n} \n\nkt <- data.frame(k = 1:nrow(spe), r = 0)\nfor (i in 2:(nrow(spe) - 1)) \n{\n  gr <- cutree(hc, i)\n  distgr <- grpdist(gr)\n  mt <- cor(spe.ch, distgr, method = \"pearson\")\n  kt[i, 2] <- mt\n}\n\nk.best <- which.max(kt$r)\nplot(kt$k,kt$r, type = \"h\",\n  main = \"Matrix correlation-optimal number of clusters\",\n  xlab = \"k (number of clusters)\", ylab = \"Pearson's correlation\")\naxis(1, k.best, paste(\"optimum\", k.best, sep = \"\\n\"),\n  col = \"red\", font = 2, col.axis = \"red\")\npoints(k.best, max(kt$r), pch = 16, col = \"red\", cex = 1.5)\n\n\n\n# Optimal number of clusters according as per indicator species\n# analysis (IndVal, Dufrene-Legendre; package: labdsv)\nIndVal <- numeric(nrow(spe))\nng <- numeric(nrow(spe))\nfor (k in 2:(nrow(spe) - 1))\n{\n  iva <- indval(spe, cutree(hc, k = k), numitr = 1000)\n  gr <- factor(iva$maxcls[iva$pval <= 0.05])\n  ng[k] <- length(levels(gr)) / k\n  iv <- iva$indcls[iva$pval <= 0.05]\n  IndVal[k] <- sum(iv)\n}\n\nk.best <- which.max(IndVal[ng == 1]) + 1\ncol3 <- rep(1, nrow(spe))\ncol3[ng == 1] <- 3\n\npar(mfrow = c(1, 2))\nplot(1:nrow(spe), IndVal, type = \"h\",\n  main = \"IndVal-optimal number of clusters\",\n  xlab = \"k (number of clusters)\", ylab = \"IndVal sum\", col = col3)\naxis(1,k.best,paste(\"optimum\", k.best, sep = \"\\n\"),\n  col = \"red\", font = 2, col.axis = \"red\")\n\npoints(which.max(IndVal),max(IndVal),pch = 16,col = \"red\",cex = 1.5)\ntext(28, 15.7, \"a\", cex = 1.8)\n\nplot(1:nrow(spe),ng,\n  type = \"h\",\n  xlab = \"k (number of clusters)\",\n  ylab = \"Ratio\",\n  main = \"Proportion of clusters with significant indicator species\",\n  col = col3)\naxis(1,k.best,paste(\"optimum\", k.best, sep = \"\\n\"),\n     col = \"red\", font = 2, col.axis = \"red\")\npoints(k.best,max(ng), pch = 16, col = \"red\", cex = 1.5)\ntext(28, 0.98, \"b\", cex = 1.8)"
  },
  {
    "objectID": "stat5-8/Statistik8_Demo.html#final-dendrogram-with-the-selected-clusters",
    "href": "stat5-8/Statistik8_Demo.html#final-dendrogram-with-the-selected-clusters",
    "title": "Stat8: Demo",
    "section": "Final dendrogram with the selected clusters",
    "text": "Final dendrogram with the selected clusters\n\n# Choose the number of clusters\nk <- 4\n# Silhouette plot of the final partition\nspech.ward.g <- cutree(spe.ch.ward, k = k)\nsil <- silhouette(spech.ward.g, spe.ch)\nrownames(sil) <- row.names(spe)\n\nplot(sil, main = \"Silhouette plot - Chord - Ward\", cex.names = 0.8, col = 2:(k + 1), nmax = 100)\n\n\n\n# Reorder clusters\nif(!require(gclus)){install.packages(\"gclus\")}\n\nLoading required package: gclus\n\n\nRegistered S3 method overwritten by 'gclus':\n  method         from \n  reorder.hclust vegan\n\nlibrary(\"gclus\")\nspe.chwo <- reorder.hclust(spe.ch.ward, spe.ch)\n\n# Plot reordered dendrogram with group labels\npar(mfrow = c(1, 1))\nplot(spe.chwo, hang = -1, xlab = \"4 groups\", ylab = \"Height\", sub = \"\",\n  main = \"Chord - Ward (reordered)\", labels = cutree(spe.chwo, k = k))\nrect.hclust(spe.chwo, k = k)\n\n\n\n# Plot the final dendrogram with group colors (RGBCMY...)\n# Fast method using the additional hcoplot() function:\n# Usage:\n# hcoplot(tree = hclust.object,\n#   diss = dissimilarity.matrix,\n#   lab = object labels (default NULL),\n#   k = nb.clusters,\n#   title = paste(\"Reordered dendrogram from\",deparse(tree$call),\n#   sep=\"\\n\"))\nsource(here(\"stat5-8\",\"hcoplot.R\"))\nhcoplot(spe.ch.ward, spe.ch, lab = rownames(spe), k = 4)\n\n\n\n# Plot the Ward clusters on a map of the Doubs River\n# (see Chapter 2)\nsource(here(\"stat5-8\",\"drawmap.R\"))\ndrawmap(xy = spa, clusters = spech.ward.g, main = \"Four Ward clusters along the Doubs River\")"
  },
  {
    "objectID": "stat5-8/Statistik8_Demo.html#miscellaneous-graphical-outputs",
    "href": "stat5-8/Statistik8_Demo.html#miscellaneous-graphical-outputs",
    "title": "Stat8: Demo",
    "section": "Miscellaneous graphical outputs",
    "text": "Miscellaneous graphical outputs\n\n# konvertieren von \"hclust\" Objekt in ein Dendogram Objekt\ndend <- as.dendrogram(spe.ch.ward)\n\n# Heat map of the dissimilarity matrix ordered with the dendrogram\nheatmap(as.matrix(spe.ch), Rowv = dend, symm = TRUE, margin = c(3, 3))\n\n\n\n# Ordered community table\n# Species are ordered by their weighted averages on site scores.\n# Dots represent absences.\nlibrary(vegan)\nor <- vegemite(spe, spe.chwo)\n\n                                    \n      32222222222  111111     1111  \n      098762105439598765064732213481\n Icme 5432121.......................\n Abbr 54332431.....1................\n Blbj 54542432.1...1................\n Anan 54432222.....111..............\n Gyce 5555443212...11...............\n Scer 522112221...21................\n Cyca 53421321.....1111.............\n Rham 55432333.....221..............\n Legi 35432322.1...1111.............\n Alal 55555555352..322..............\n Chna 12111322.1...211..............\n Titi 53453444...1321111.21.........\n Ruru 55554555121455221..1..........\n Albi 53111123.....2341.............\n Baba 35342544.....23322.........1..\n Eslu 453423321...41111..12.1....1..\n Gogo 5544355421..242122111......1..\n Pefl 54211432....41321..12.........\n Pato 2211.222.....3344.............\n Sqce 3443242312152132232211..11.1..\n Lele 332213221...52235321.1........\n Babl .1111112...32534554555534124..\n Teso .1...........11254........23..\n Phph .1....11...13334344454544455..\n Cogo ..............1123......2123..\n Satr .1..........2.12341345555355.3\n Thth .1............11.2......2134..\n30 sites, 27 species"
  },
  {
    "objectID": "stat5-8/Statistik8_Loesung.html#musterloesung-aufgabe-8.1-clusteranalysen",
    "href": "stat5-8/Statistik8_Loesung.html#musterloesung-aufgabe-8.1-clusteranalysen",
    "title": "Stat8: Lösung",
    "section": "Musterloesung Aufgabe 8.1: Clusteranalysen",
    "text": "Musterloesung Aufgabe 8.1: Clusteranalysen\n\nR-Skript als Download\nLoesungstext"
  },
  {
    "objectID": "stat5-8/Statistik8_Loesung.html#übungsaufgabe",
    "href": "stat5-8/Statistik8_Loesung.html#übungsaufgabe",
    "title": "Stat8: Lösung",
    "section": "Übungsaufgabe",
    "text": "Übungsaufgabe\n(hier so ausführlich formuliert, wie dies auch in der Klausur der Fall sein wird)\n\nLadet den Datensatz crime2.csv. Dieser enthält Raten von 7 Kriminatlitätsformen pro 100000 Einwohner und Jahr für die Bundesstaaten der USA.\nFührt eine k-means- und eine agglomerative Clusteranalyse eurer Wahl durch. Bitte beachet, dass wegen der sehr ungleichen Varianzen in jedem Fall eine Standardisierung stattfinden muss, damit die Distanzen zwischen den verschiedenen Kriminalitätsraten sinnvoll berechnet werden können.\nÜberlegt in beiden Fällen, wie viele Cluster sinnvoll sind (k-means: z. B. visuelle Betrachtung einer PCA, agglomertive Clusteranalyse: z. B. Silhoutte-Plot).\nEntscheidet euch dann für eine der beiden Clusterungen und vergleicht dann die erhaltenen Cluster bezüglich der Kriminalitätsformen und interpretiert die Cluster entsprechend.\nBitte erklärt und begründet die einzelnen Schritte, die ihr unternehmt, um zu diesem Ergebnis zu kommen. Dazu erstellt bitte ein Word-Dokument, in das ihr Schritt für Schritt den verwendeten R-Code, die dazu gehörigen Ausgaben von R, eure Interpretation derselben und die sich ergebenden Schlussfolgerungen für das weitere Vorgehen dokumentieren.\nFormuliert abschliessend einen Methoden- und Ergebnisteil (ggf. incl. adäquaten Abbildungen) zu dieser Untersuchung in der Form einer wissenschaftlichen Arbeit (ausformulierte schriftliche Zusammenfassung, mit je einem Absatz von ca. 60-100 Worten, resp. 3-8 Sätzen für den Methoden- und Ergebnisteil). D. h. alle wichtigen Informationen sollten enthalten sein, unnötige Redundanz dagegen vermieden werden.\nAbzugeben sind am Ende (a) Ein lauffähiges R-Skript; (b) begründeter Lösungsweg (Kombination aus R-Code, R Output und dessen Interpretation) und (c) ausformulierter Methoden- und Ergebnisteil (für eine wiss. Arbeit).\n\n\nUebung 8.1 - Clusteranalysen – Loesung\n\ncrime <- read.csv(here(\"data\",\"crime2.csv\"), sep = \";\")\n\n\ncrime\n\nIm mitgelieferten R-Skript habe ich die folgenden Analysen zunächst mit untransformierten, dann mit standardisierten Kriminalitätsraten berechnet. Ihr könnt die Ergebnisse vergleichen und seht, dass sie sehr unterschiedlich ausfallen.\n\ncrimez <- crime\ncrimez[,c(2:8)] <- lapply(crime[, c(2:8)], scale)\ncrimez\n\n„scale“ führt eine Standardisierung (z-Transformation) durch, so dass alle Variablen anschiessen einen Mittelwert von 0 und eine SD von 1 haben, ausgenommen natürlich die 1. Spalte mit den Kürzeln der Bundesstaaten. Anschliessend wird das SSI-Kriterium getestet und zwar für Partitionierungen von 2 bis 6 Gruppen (wie viele Gruppen man maximal haben will, muss man pragmatisch nach der jeweiligen Fragestelltung entscheiden).\n\nlibrary(vegan)\ncrimez.KM.cascade <- cascadeKM(crimez[,c(2:8)],\n                        inf.gr = 2, sup.gr = 6, iter = 100, criterion = \"ssi\")\nsummary(crimez.KM.cascade)\n\ncrimez.KM.cascade$results\ncrimez.KM.cascade$partition\n\n# k-means visualisation\nlibrary(cclust)\nplot(crimez.KM.cascade, sortg = TRUE)\n\nNach SSI ist die 4-Gruppenlösung die beste, mit dieser wird also weitergerechnet.\n\n# 4 Kategorien sind nach SSI offensichtlich besonders gut\nmodelz <- kmeans(crimez[,c(2:8)], 4)\nmodelz\n\n#File für ANOVA (Originaldaten der Vorfälle, nicht die ztransformierten)\ncrime.KM4 <- data.frame(crime,modelz[1])\ncrime.KM4$cluster <- as.factor(crime.KM4$cluster)\ncrime.KM4\nstr(crime.KM4)\n\nVon den agglomerativen Clusterverfahren habe ich mich für Ward’s minimum variance clustering entschieden, da dieses allgemein als besonders geeignet gilt.\nVor der Berechnung von crime.norm und crime.ch muss man die Spalte mit den Bundesstaatenkürzeln entfern.\n\n# Agglomerative Clusteranalyse\ncrime2 <- crime[,-1]\ncrime.norm <- decostand(crime2, \"normalize\")\ncrime.ch <- vegdist(crime.norm, \"euc\")\n# Attach site names to object of class 'dist'\nattr(crime.ch, \"Labels\") <- crime[,1]\n\n# Ward's minimum variance clustering\ncrime.ch.ward <- hclust(crime.ch, method = \"ward.D2\")\npar(mfrow = c(1, 1))\nplot(crime.ch.ward, labels = crime[,1], main = \"Chord - Ward\")\n\n# Choose and rename the dendrogram (\"hclust\" object)\nhc <- crime.ch.ward\n# hc <- spe.ch.beta2\n# hc <- spe.ch.complete\ndev.new(title = \"Optimal number of clusters\", width = 12, height = 8, noRStudioGD = TRUE)\ndev.off()\npar(mfrow = c(1, 2))\n\n\n# Average silhouette widths (Rousseeuw quality index)\nlibrary(cluster)\nSi <- numeric(nrow(crime))\nfor (k in 2:(nrow(crime) - 1))\n{\n sil <- silhouette(cutree(hc, k = k), crime.ch)\n Si[k] <- summary(sil)$avg.width\n}\nk.best <- which.max(Si)\nplot( 1:nrow(crime), Si, type = \"h\",\n main = \"Silhouette-optimal number of clusters\",\n xlab = \"k (number of clusters)\", ylab = \"Average silhouette width\")\n\naxis(1, k.best, paste(\"optimum\", k.best, sep = \"\\n\"), col = \"red\",\n font = 2, col.axis = \"red\")\npoints(k.best, max(Si), pch = 16, col = \"red\", cex = 1.5)\n\nDemnach wären beim Ward’s-Clustering nur zwei Gruppen die optimale Lösung.\nFür die Vergleiche der Bundesstaatengruppen habe ich mich im Folgenden für die k-means Clusterung mit 4 Gruppen entschieden.\nDamit die Boxplots und die ANOVA direkt interpretierbar sind, werden für diese, anders als für die Clusterung, die untransformierten Incidenz-Werte verwendet (also crime statt crimez). Die Spalte mit der Clusterzugehörigkeit im Fall von k-means mit 4 Clustern hängt man als Spalte an (Achtung: muss als Faktor definiert werden!).\nAnschliessend kann man die 7 ANOVAs rechnen, die Posthoc-Vergleiche durchführen und die zugehörigen Boxplots mit Buchstaben für die homogenen Gruppen erzeugen. Sinnvollerweise gruppiert man die Abbildungen gleich, z. B. je 2 x 2. Das Skript ist hier simple für jede Verbrechensart wiederholt. Erfahrenere R-Nutzer können das Ganze hier natürlich durch eine Schleife abkürzen.\n\nlibrary(multcomp)\nif(!require(multcomp)){install.packages(\"multcomp\")}\nlibrary(multcomp)\npar(mfrow = c(3,3))\n\nANOVA.Murder <- aov(Murder~cluster, data = crime.KM4)\nsummary(ANOVA.Murder)\nletters <- cld(glht(ANOVA.Murder, linfct = mcp(cluster = \"Tukey\")))\nboxplot(Murder~cluster, xlab = \"Cluster\", ylab = \"Murder\", data = crime.KM4)\nmtext(letters$mcletters$Letters, at = 1:6)\n\nANOVA.Rape <- aov(Rape~cluster,data = crime.KM4)\nsummary(ANOVA.Rape)\nletters <- cld(glht(ANOVA.Rape, linfct = mcp(cluster = \"Tukey\")))\nboxplot(Rape~cluster, xlab = \"Cluster\", ylab = \"Rape\", data = crime.KM4)\nmtext(letters$mcletters$Letters, at = 1:6)\n\nANOVA.Robbery <- aov(Robbery~cluster, data = crime.KM4)\nsummary(ANOVA.Robbery)\nletters <- cld(glht(ANOVA.Robbery, linfct = mcp(cluster = \"Tukey\")))\nboxplot(Robbery~cluster, xlab = \"Cluster\", ylab = \"Robbery\", data = crime.KM4)\nmtext(letters$mcletters$Letters, at = 1:6)\n\nANOVA.Assault <- aov(Assault~cluster, data = crime.KM4)\nsummary(ANOVA.Assault)\nletters <- cld(glht(ANOVA.Assault, linfct = mcp(cluster = \"Tukey\")))\nboxplot(Assault~cluster, xlab = \"Cluster\", ylab = \"Assault\",  data = crime.KM4)\nmtext(letters$mcletters$Letters, at = 1:6)\n\nANOVA.Burglary <- aov(Burglary~cluster, data = crime.KM4)\nsummary(ANOVA.Burglary)\nletters <- cld(glht(ANOVA.Burglary, linfct=mcp(cluster = \"Tukey\")))\nboxplot(Burglary~cluster, data = crime.KM4, xlab = \"Cluster\", ylab = \"Burglary\")\nmtext(letters$mcletters$Letters, at=1:6)\n\nANOVA.Theft <- aov(Theft~cluster, data = crime.KM4)\nsummary(ANOVA.Theft)\nletters <- cld(glht(ANOVA.Theft, linfct = mcp(cluster = \"Tukey\")))\nboxplot(Theft~cluster, xlab = \"Cluster\", ylab = \"Theft\", data = crime.KM4)\nmtext(letters$mcletters$Letters, at = 1:6)\n\nANOVA.Vehicle <- aov(Vehicle~cluster, data = crime.KM4)\nsummary(ANOVA.Vehicle)\nletters <- cld(glht(ANOVA.Vehicle, linfct = mcp(cluster = \"Tukey\")))\nboxplot(Vehicle~cluster, data = crime.KM4, xlab = \"Cluster\", ylab = \"Vehicle\")\nmtext(letters$mcletters$Letters, at = 1:6)\n\nDie Boxplots erlauben jetzt auch eine Beurteilung der Modelldiagnostik: sind die Residuen hinreichen normalverteilt (symmetrisch) und sind die Varianzen zwischen den Kategorien einigermassen ähnlich. Mit der Symmetrie/Normalverteilung sieht es OK aus. Die Varianzhomogenität ist nicht optimal – meist deutlich grössere Varianz bei höheren Mittelwerten. Eine log-Transformation hätte das verbessert und könnte hier gut begründet werden. Da die p-Werte sehr niedrig waren und die Varianzheterogenität noch nicht extrem war, habe ich aber von einer Transformation abgesehen, da jede Transformation die Interpretation der Ergebnisse erschwert. Jetzt muss man nur noch herausfinden, welche Bundesstaaten überhaupt zu welchem der vier Cluster gehören, sonst ist das ganze schöne Ergebnis nutzlos. Z. B. kann man in R auf den Dataframe clicken und ihn nach cluster sortieren."
  },
  {
    "objectID": "stat5-8/Statistik8_Uebung.html#uebung-8.1-clusteranalyse-sozioökonomisch",
    "href": "stat5-8/Statistik8_Uebung.html#uebung-8.1-clusteranalyse-sozioökonomisch",
    "title": "Stat8: Übung",
    "section": "Uebung 8.1: Clusteranalyse (sozioökonomisch)",
    "text": "Uebung 8.1: Clusteranalyse (sozioökonomisch)\n\nDatensatz crime2.csv\n\nRaten von 7 Kriminalitätsformen pro 100000 Einwohner und Jahr für die Bundesstaaten der USA\n(a) Führt eine k-means- und eine agglomerative Clusteranalyse eurer Wahl durch.\n(b) Überlegt in beiden Fällen, wie viele Cluster sinnvoll sind (k-means z. B.visuelle Betrachtung einer PCA, agglomerative Clusteranalyse z. B. SilhouettePlot).\n(c) Abschliessend entscheidet euch für eine Clusterung und vergleicht die erhaltenen Cluster bezüglich der Kriminalitätsformen mittels ANOVA und interpretiert die Cluster entsprechend.\nHinweis:\nWegen der sehr ungleichen Varianzen muss auf jeden Fall eine Standardisierung stattfinden, damit Distanzen zwischen den verschiedenen Kriminalitätsraten sinnvoll berechnet werden können"
  },
  {
    "objectID": "statKons/StatKons1_Demo_assoziationen.html#möglicher-text-für-ergebnisse",
    "href": "statKons/StatKons1_Demo_assoziationen.html#möglicher-text-für-ergebnisse",
    "title": "StatKons1: Demo 1",
    "section": "möglicher Text für Ergebnisse",
    "text": "möglicher Text für Ergebnisse\nDer \\(\\chi^2\\)-Test sagt uns, dass das Art des Motors und Art des Fahrwerks statistisch nicht zusammenhängen. Es gibt keine signifikante Unterscheide zwischen den Variablen “VS” und “AM - Transmission” (\\(\\chi^2\\)(1) = 0.348, p = 0.556. Der Fisher exacter Test bestätigt diesen Befund. Die Odds Ratio (OR) sagt uns hingegen - unter der Prämisse, dass “normale” Motoren eher mit automatischen und V-Motoren eher mit handgeschalteten Fahrwerken ausgestattet sind - dass die Chance doppelt so hoch ist, dass ein Auto mit “normalem” Motor automatisch geschaltet ist, als dies bei einem Auto mit V-Motor der Fall wäre\n\n#define dataset\ncars <- mtcars\n\n#neue kategoriale variable\ncars %<>% \n  as_tibble() %>% # da \"nur\" data frame kann glimplse nichts damit anfangen \n  mutate(vs_cat = if_else(.$vs == 0, \"normal\", \"v-type\")) %>% \n  mutate(am_cat = if_else(am == 0, \"automatic\", \"manual\"))\n\n# bei t-Test immer zuerst visualisieren: in diesem Fall Boxplot mit Variablen Getriebe (v- vs. s-motor) und Anzahl Pferdestärke\nggplot2::ggplot(cars, aes(y = hp, x = vs_cat)) +\n  stat_boxplot(geom ='errorbar', width = .25) +\n  geom_boxplot() +\n  # geom_violin()+\n  labs(x = \"\\nBauform Motor\", y = \"Pferdestärke (PS)\\n\") +\n  mytheme\n\n\n\n  \n#alternativ     \nboxplot(cars$hp ~ cars$vs_cat) # sieht ganz ok aus, jedoch weist die variable \"normale Motoren\" deutlich eine grössere Streuung aus -> siehe aov.1 und deren Modelgüte-Plots\n\n\n\n\n# Definiere Model: t-Test, wobei die AV metrisch (in unserem Fall eine Zählvariable) sein muss\nttest <- t.test(cars$hp ~ cars$vs_cat)\naov.1 <- aov(cars$hp ~ cars$vs_cat)\n\n#schaue Modellgüte an\npar(mfrow = c(2,2))\nplot(aov.1)\n\n\n\n\n#zeige resultate\nttest\n## \n##  Welch Two Sample t-test\n## \n## data:  cars$hp by cars$vs_cat\n## t = 6.2908, df = 23.561, p-value = 1.82e-06\n## alternative hypothesis: true difference in means between group normal and group v-type is not equal to 0\n## 95 percent confidence interval:\n##   66.06161 130.66854\n## sample estimates:\n## mean in group normal mean in group v-type \n##            189.72222             91.35714\nsummary.lm(aov.1)\n## \n## Call:\n## aov(formula = cars$hp ~ cars$vs_cat)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -98.72 -25.61  -4.04  22.55 145.28 \n## \n## Coefficients:\n##                   Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)         189.72      11.35  16.720  < 2e-16 ***\n## cars$vs_catv-type   -98.37      17.16  -5.734 2.94e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 48.14 on 30 degrees of freedom\n## Multiple R-squared:  0.5229, Adjusted R-squared:  0.507 \n## F-statistic: 32.88 on 1 and 30 DF,  p-value: 2.941e-06\n\n#wie würdet ihr nun die Ergebnisse darstellen?\n\n\n# für mehr infos here: https://cran.r-project.org/web/packages/datasauRus/vignettes/Datasaurus.html\n\nlibrary(datasauRus)\nif(requireNamespace(\"dplyr\")){\n  suppressPackageStartupMessages(library(dplyr))\n  dt <- datasaurus_dozen %>% \n    group_by(dataset) %>% \n    summarize(\n      mean_x    = mean(x),\n      mean_y    = mean(y),\n      std_dev_x = sd(x),\n      std_dev_y = sd(y),\n      corr_x_y  = cor(x, y)\n    )\n}\n\n# check data structure\nglimpse(dt)\n\n# plot two examples  \nif(requireNamespace(\"ggplot2\")){\n  library(ggplot2)\n  \n  dt = filter(datasaurus_dozen, dataset == \"dino\" | dataset == \"slant_up\")\n  \n  ggplot(dt, aes(x=x, y=y, colour=dataset))+\n    geom_point()+\n    theme_bw() +\n    theme(legend.position = \"none\") +\n    facet_wrap(~dataset) +\n    geom_smooth(method = \"lm\", se = FALSE)\n  \n}"
  },
  {
    "objectID": "statKons/StatKons1_Demo_open_datasets.html",
    "href": "statKons/StatKons1_Demo_open_datasets.html",
    "title": "StatKons1: Datensätze",
    "section": "",
    "text": "In diesem Dokument findet ihr verschiedene Wege und Quellen, um an Datensätze zu gelangen."
  },
  {
    "objectID": "statKons/StatKons1_Demo_open_datasets.html#in-r",
    "href": "statKons/StatKons1_Demo_open_datasets.html#in-r",
    "title": "StatKons1: Datensätze",
    "section": "in R",
    "text": "in R\nIn R gibt es vordefinierte Datensätze, welche gut abrufbar sind. Beispiele sind:\n\nsleep\nUSAccDeaths\nUSArrests\n…\n\n\ndata() # erzeugt eine Liste mit den Datensätzen, welche in R verfügbaren sind\nhead(chickwts)\n\n  weight      feed\n1    179 horsebean\n2    160 horsebean\n3    136 horsebean\n4    227 horsebean\n5    217 horsebean\n6    168 horsebean\n\nstr(chickwts)\n\n'data.frame':   71 obs. of  2 variables:\n $ weight: num  179 160 136 227 217 168 108 124 143 140 ...\n $ feed  : Factor w/ 6 levels \"casein\",\"horsebean\",..: 2 2 2 2 2 2 2 2 2 2 ..."
  },
  {
    "objectID": "statKons/StatKons1_Demo_open_datasets.html#kaggle",
    "href": "statKons/StatKons1_Demo_open_datasets.html#kaggle",
    "title": "StatKons1: Datensätze",
    "section": "Kaggle",
    "text": "Kaggle\nAuf Kaggle findet ihr öffentlich zugängliche Datensätze. Einzig was ihr tun müsst, ist euch registrieren. Beispiele sind:\n\n911\nfoodPreferences\nS.F. salaries\n…\n\n\n# Load packages and data\ndata_911 <- read_delim(here(\"data\",\"911.csv\"), delim = \",\")\n\nRows: 99492 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): desc, title, twp, addr\ndbl  (4): lat, lng, zip, e\ndttm (1): timeStamp\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nstr(data_911)\n\nspec_tbl_df [99,492 × 9] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ lat      : num [1:99492] 40.3 40.3 40.1 40.1 40.3 ...\n $ lng      : num [1:99492] -75.6 -75.3 -75.4 -75.3 -75.6 ...\n $ desc     : chr [1:99492] \"REINDEER CT & DEAD END;  NEW HANOVER; Station 332; 2015-12-10 @ 17:10:52;\" \"BRIAR PATH & WHITEMARSH LN;  HATFIELD TOWNSHIP; Station 345; 2015-12-10 @ 17:29:21;\" \"HAWS AVE; NORRISTOWN; 2015-12-10 @ 14:39:21-Station:STA27;\" \"AIRY ST & SWEDE ST;  NORRISTOWN; Station 308A; 2015-12-10 @ 16:47:36;\" ...\n $ zip      : num [1:99492] 19525 19446 19401 19401 NA ...\n $ title    : chr [1:99492] \"EMS: BACK PAINS/INJURY\" \"EMS: DIABETIC EMERGENCY\" \"Fire: GAS-ODOR/LEAK\" \"EMS: CARDIAC EMERGENCY\" ...\n $ timeStamp: POSIXct[1:99492], format: \"2015-12-10 17:40:00\" \"2015-12-10 17:40:00\" ...\n $ twp      : chr [1:99492] \"NEW HANOVER\" \"HATFIELD TOWNSHIP\" \"NORRISTOWN\" \"NORRISTOWN\" ...\n $ addr     : chr [1:99492] \"REINDEER CT & DEAD END\" \"BRIAR PATH & WHITEMARSH LN\" \"HAWS AVE\" \"AIRY ST & SWEDE ST\" ...\n $ e        : num [1:99492] 1 1 1 1 1 1 1 1 1 1 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   lat = col_double(),\n  ..   lng = col_double(),\n  ..   desc = col_character(),\n  ..   zip = col_double(),\n  ..   title = col_character(),\n  ..   timeStamp = col_datetime(format = \"\"),\n  ..   twp = col_character(),\n  ..   addr = col_character(),\n  ..   e = col_double()\n  .. )\n - attr(*, \"problems\")=<externalptr>"
  },
  {
    "objectID": "statKons/StatKons1_Demo_open_datasets.html#tidytuesday",
    "href": "statKons/StatKons1_Demo_open_datasets.html#tidytuesday",
    "title": "StatKons1: Datensätze",
    "section": "Tidytuesday",
    "text": "Tidytuesday\nTidytuesday ist eine Plattform, in der wöchentlich - jeden Dienstag - einen öffentlich zugänglichen Datensatz publiziert. Dieses Projekt ist aus der R4DS Online Learning Community und dem R for Data Science Lehrbuch hervorgegangen. Beispiele sind:\n\nWomen in the Workplace\nDairy production & Consumption\nStar Wars Survey\nGlobal Coffee Chains\nMalaria Deaths\n…\n\nDownload via Github - 1. Möglichkeit\n\nGeht zum File, welches ihr herunterladen wollt\nKlickt auf das File (.csv, .xlsx etc.), um den Inhalt innerhalb der GitHub Benutzeroberfläche anzuzeigen\n\n\n\nKlickt mit der rechten Maustaste auf den Knopf “raw”\n\n\n\n(Ziel) Speichern unter…\n\nDownload via Github - 2. Möglichkeit\n\n# Beachtet dabei, dass ihr die URL zum originalen (raw) Datensatz habt \nstar_wars <- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2018/2018-05-14/week7_starwars.csv\", locale = readr::locale(encoding = \"latin1\")) #not working yet \n\nNew names:\nRows: 1187 Columns: 38\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(37): Have you seen any of the 6 films in the Star Wars franchise?, Do y... dbl\n(1): RespondentID\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -> `...5`\n• `` -> `...6`\n• `` -> `...7`\n• `` -> `...8`\n• `` -> `...9`\n• `` -> `...11`\n• `` -> `...12`\n• `` -> `...13`\n• `` -> `...14`\n• `` -> `...15`\n• `` -> `...17`\n• `` -> `...18`\n• `` -> `...19`\n• `` -> `...20`\n• `` -> `...21`\n• `` -> `...22`\n• `` -> `...23`\n• `` -> `...24`\n• `` -> `...25`\n• `` -> `...26`\n• `` -> `...27`\n• `` -> `...28`\n• `` -> `...29`"
  },
  {
    "objectID": "statKons/StatKons1_Demo_open_datasets.html#opendata.swiss",
    "href": "statKons/StatKons1_Demo_open_datasets.html#opendata.swiss",
    "title": "StatKons1: Datensätze",
    "section": "opendata.swiss",
    "text": "opendata.swiss\nAuf opendata.swiss sind offene, frei verfügbare Daten der Schweizerischen Behörden zu finden. opendata.swiss ist ein gemeinsames Projekt von Bund, Kantonen, Gemeinden und weiteren Organisationen mit einem staatlichen Auftrag. Beispiele sind:\n\nStatistik der Schweizer Städte\nVerpflegungsbetriebe nach Jahr und Stadtquartier\nAltpapiermengen\n…"
  },
  {
    "objectID": "statKons/StatKons1_Demo_open_datasets.html#open-data-katalog-stadt-zürich",
    "href": "statKons/StatKons1_Demo_open_datasets.html#open-data-katalog-stadt-zürich",
    "title": "StatKons1: Datensätze",
    "section": "Open Data Katalog Stadt Zürich",
    "text": "Open Data Katalog Stadt Zürich\nAuf der Seite der Stadt Zürich Open Data findet ihr verschiedene Datensätze der Stadt Zürich. Spannend daran ist, dass die veröffentlichten Daten kostenlos und zur freien - auch kommerziellen - Weiterverwendung zur Verfügung. Beispiele sind:\n\nBevölkerung nach Bildungsstand, Jahr, Alter und Geschlecht seit 1970\nLuftqualitätsmessungen\nHäufigste Hauptsprachen\n…\n\n\n# lade die Datei \"Häufigste Sprachen\"\nurlfile = \"https://data.stadt-zuerich.ch/dataset/bfs_ste_bev_hauptsprachen_top50_od3011/download/BEV301OD3011.csv\"\n\ndat_lang <- read_delim(url(urlfile), delim = \",\", col_names = T)\n\nRows: 50 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Sprache\ndbl (6): AntBev, AnzBev, untAntBevKI, obAntBevKI, untAnzBevKI, obAnzBevKI\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(dat_lang)\n\n# A tibble: 6 × 7\n  Sprache        AntBev AnzBev untAntBevKI obAntBevKI untAnzBevKI obAnzBevKI\n  <chr>           <dbl>  <dbl>       <dbl>      <dbl>       <dbl>      <dbl>\n1 Deutsch          74.9 259670        74.4       75.3      257970     261380\n2 Englisch         13.5  46970        13.2       13.9       45630      48320\n3 Italienisch       6.2  21390         5.9        6.4       20470      22310\n4 Französisch       4.9  17060         4.7        5.2       16230      17890\n5 Spanisch          4.3  14940         4.1        4.5       14140      15740\n6 Serbokroatisch    3.1  10900         2.9        3.3       10220      11590"
  },
  {
    "objectID": "statKons/StatKons1_Demo_suggest_datasets.html",
    "href": "statKons/StatKons1_Demo_suggest_datasets.html",
    "title": "StatKons1: Datensätze",
    "section": "",
    "text": "R data sets\n\nchickwts\niris\nTitanic (achtung hat das “table” Datenformat)\nstarwars (dplyr package)\n\nResearch Methods data sets\n\nNOVANIMAL (Kassendaten oder Gästebefragung)\nUkraine (Demoskript 3)\nIpomopsis (Demoskript 3)"
  },
  {
    "objectID": "statKons/StatKons2_Demo_PCA.html#pca-mit-mtcars",
    "href": "statKons/StatKons2_Demo_PCA.html#pca-mit-mtcars",
    "title": "StatKons2: Demo",
    "section": "PCA mit mtcars",
    "text": "PCA mit mtcars\n\n#Beispiel inspiriert von Luke Hayden: https://www.datacamp.com/community/tutorials/pca-analysis-r\n\n#Ausgangslage: viel zusammenhängende Variablen\n#Ziel: Reduktion der Variablenkomplexität\n#WICHTIG hier: Datenformat muss Wide sein! Damit die Matrixmultiplikation gemacht werden kann\n\n# lade Datei\ncars <- mtcars\n\n# Korrelationen\ncor<- cor(cars[,c(1:7,10,11)])\ncor[abs(cor)<.7] <- 0\ncor\n\n#definiere Datei für PCA\ncars <- mtcars[,c(1:7,10,11)]\n\n# pca\n# achtung unterschiedliche messeinheiten, wichtig es muss noch einheitlich transfomiert werden\nlibrary(FactoMineR) # siehe Beispiel hier: https://www.youtube.com/watch?v=vP4korRby0Q\no.pca <- PCA(cars, scale.unit = TRUE) # entweder korrelations oder covarianzmatrix\n\n# schaue output an\nsummary(o.pca) # generiert auch automatische plots\n\n\n# plotte das ganze\nlibrary(devtools)\ninstall_github(\"vqv/ggbiplot\")\n\n\nlibrary(ggbiplot)\nggbiplot(o.pca,choices = c(1,2))\n\n\n\n\n# nehme noch die autonamen hinzu\nggbiplot(o.pca, labels=rownames(mtcars), choices = c(1,2)) # (+ mytheme) # choice gibt die axen an"
  },
  {
    "objectID": "statKons/StatKons2_Demo_PCA.html#ca-mit-mtcars",
    "href": "statKons/StatKons2_Demo_PCA.html#ca-mit-mtcars",
    "title": "StatKons2: Demo",
    "section": "CA mit mtcars",
    "text": "CA mit mtcars\n\nlibrary(vegan)\n\n# ebenfalls mit transformierten daten\no.ca<-vegan::cca(cars)\no.ca1 <- FactoMineR::CA(cars) #blau: auots, rot: variablen\n\n\n\n\n# plotten (schwarz: autos, rot: variablen)\nplot(o.ca)\n\n\n\nsummary(o.ca)\nsummary(o.ca1)\n\n#Nur autos plotten; wieso?\nx<-o.ca$CA$u[,1]\ny<-o.ca$CA$u[,2]\nplot(x,y)\n\n\n\n\n#Anteilige Varianz, die durch die ersten beiden Achsen erklaert wird\no.ca$CA$eig[1:8]/sum(o.ca$CA$eig)"
  },
  {
    "objectID": "statKons/StatKons2_Demo_PCA.html#nmds-mit-mtcars",
    "href": "statKons/StatKons2_Demo_PCA.html#nmds-mit-mtcars",
    "title": "StatKons2: Demo",
    "section": "NMDS mit mtcars",
    "text": "NMDS mit mtcars\n\n#Distanzmatrix als Start erzeugen\nlibrary(MASS)\n\nmde <-vegan::vegdist(cars,method=\"euclidean\")\nmdm <-vegan::vegdist(cars,method=\"manhattan\")\n\n#Zwei verschiedene NMDS-Methoden\nset.seed(1) #macht man, wenn man bei einer Wiederholung exakt die gleichen Ergebnisse will\no.mde.mass <- MASS::isoMDS(mde, k=2) # mit K = Dimensionen\no.mdm.mass <- MASS::isoMDS(mdm)\n\nset.seed(1)\no.mde.vegan <- vegan::metaMDS(mde,k=1) # scheint nicht mit 2 Dimensionen zu konvergieren\no.mdm.vegan <- vegan::metaMDS(mdm, k = 2)\n\n#plot euclidean distance\nplot(o.mde.mass$points)\n\n\n\nplot(o.mde.vegan$points)\n\n\n\n\n#plot manhattan distance\nplot(o.mdm.mass$points)\n\n\n\nplot(o.mdm.vegan$points)\n\n\n\n\n\n#Stress =  Abweichung der zweidimensionalen NMDS-Loesung von der originalen Distanzmatrix\nvegan::stressplot(o.mde.vegan, mde)\n\n\n\nvegan::stressplot(o.mde.mass, mde)"
  },
  {
    "objectID": "statKons/StatKons2_Demo_PCA.html#pca-mit-sveg",
    "href": "statKons/StatKons2_Demo_PCA.html#pca-mit-sveg",
    "title": "StatKons2: Demo",
    "section": "PCA mit sveg",
    "text": "PCA mit sveg\n\n#Mit Beispieldaten aus Wildi (2013, 2017)\nlibrary(labdsv)\nlibrary(dave) # lade package für Daten sveg\nhead(sveg)\nstr(sveg)\n#View(sveg)\n\n#PCA-----------\n#Deckungen Wurzeltransformiert, cor=T erzwingt Nutzung der Korrelationsmatrix\no.pca <- labdsv::pca(sveg^0.25,cor=T)\no.pca2 <- stats::prcomp(sveg^0.25)\n\n#Koordinaten im Ordinationsraum => Y\nhead(o.pca$scores)\nhead(o.pca2$x)\n\n#Korrelationen der Variablen mit den Ordinationsachsen\nhead(o.pca$loadings)\nhead(o.pca2$rotation)\n\n#Erklaerte Varianz der Achsen (sdev ist die Wurzel daraus)\n# früher gabs den Befehl summary()\n# jetzt von hand: standardabweichung im quadrat/totale varianz * 100 (um prozentwerte zu bekommen)\nE<-o.pca$sdev^2/o.pca$totdev*100\nE[1:5] # erste fünf PCA\n\n#package stats funktioniert summary()\nsummary(o.pca2)\n\n#PCA-Plot der Lage der Beobachtungen im Ordinationsraum\nplot(o.pca$scores[,1],o.pca$scores[,2], type=\"n\", asp=1, xlab=\"PC1\", ylab=\"PC2\")\npoints(o.pca$scores[,1],o.pca$scores[,2],pch=18)\n\n\n\n\nplot(o.pca$scores[,1],o.pca$scores[,3],type=\"n\", asp=1, xlab=\"PC1\", ylab=\"PC3\")\npoints(o.pca$scores[,1],o.pca$scores[,3],pch=18)\n\n\n\n\n#Subjektive Auswahl von Arten zur Darstellung\nsel.sp <- c(3,11,23,39,46,72,77,96, 101, 119)\nsnames <- names(sveg[ , sel.sp])\nsnames\n\n#PCA-Plot der Korrelationen der Variablen (hier Arten) mit den Achsen\n#(hier reduction der observationen)\nx <- o.pca$loadings[,1]\ny <- o.pca$loadings[,2]\nplot(x,y,type=\"n\",asp=1)\narrows(0,0,x[sel.sp],y[sel.sp],length=0.08)\ntext(x[sel.sp],y[sel.sp],snames,pos=1,cex=0.6)\n\n\n\n\n# hier gehts noch zu weiteren Beispielen zu PCA's:\n# https://stats.stackexchange.com/questions/102882/steps-done-in-factor-analysis-compared-to-steps-done-in-pca/102999#102999\n# https://stats.stackexchange.com/questions/222/what-are-principal-component-scores\n# https://stats.stackexchange.com/questions/102882/steps-done-in-factor-analysis-compared-to-steps-done-in-pca/102999#102999"
  },
  {
    "objectID": "statKons/StatKons2_Demo_PCA.html#pca-mit-beispiel-aus-vorlesung",
    "href": "statKons/StatKons2_Demo_PCA.html#pca-mit-beispiel-aus-vorlesung",
    "title": "StatKons2: Demo",
    "section": "PCA mit Beispiel aus Vorlesung",
    "text": "PCA mit Beispiel aus Vorlesung\n\n#Idee von Ordinationen aus Wildi p. 73-74\n\n#Für Ordinationen benötigen wir Matrizen, nicht Data.frames\n#Generieren von Daten\nraw <- matrix(c(1,2,2.5,2.5,1,0.5,0,1,2,4,3,1), nrow=6)\ncolnames(raw) <- c(\"spec.1\", \"spec.2\")\nrownames(raw) <- c(\"r1\",\"r2\",\"r3\",\"r4\",\"r5\",\"r6\")\nraw\n\n#originale Daten im zweidimensionalen Raum\nx1 <- raw[,1]\ny1 <- raw[,2]\nz <- c(rep(1:6))\n\n#Plot Abhängigkeit der Arten vom Umweltgradienten\nplot(c(x1, y1)~c(z,z), type=\"n\", axes=T, bty=\"l\", las=1, xlim=c(1,6), ylim=c(0,5),\n     xlab=\"Umweltgradient\",ylab=\"Deckung der Arten\")\npoints(x1~z, pch=21, type=\"b\")\npoints(y1~z, pch=16, type=\"b\")\n\n\n\n\n#zentrierte Daten\ncent <- scale(raw, scale=F)\nx2 <- cent[,1]\ny2 <- cent[,2]\n\n#rotierte Daten\no.pca <- pca(raw)\nx3 <- o.pca$scores[,1]\ny3 <- o.pca$scores[,2]\n\n\n#Visualisierung der Schritte im Ordinationsraum\nplot(c(y1,y2,y3)~c(x1,x2,x3), type=\"n\", axes=T, bty=\"l\", las=1, xlim=c(-4,4), \n     ylim=c(-4,4), xlab=\"Art 1\", ylab=\"Art 2\")\npoints(y1~x1, pch=21, type=\"b\", col=\"green\", lwd=2)\npoints(y2~x2, pch=16, type=\"b\",col=\"red\", lwd=2)\npoints(y3~x3, pch=17, type=\"b\", col=\"blue\", lwd=2)\n\n\n\n\n#zusammengefasst:-------\n\n#Durchführung der PCA\npca <- pca(raw)\n\n#Koordinaten im Ordinationsraum\npca$scores\n\n#Korrelationen der Variablen mit den Ordinationsachsen\npca$loadings\n\n#Erklärte Varianz der Achsen in Prozent\nE <- pca$sdev^2/pca$totdev*100\nE\n\n### excurs für weitere r-packages####\n\n#mit prcomp, ein weiteres Package für Ordinationen\npca.2 <- stats::prcomp(raw, scale=F)\nsummary(pca.2)\nplot(pca.2)\n\n\n\nbiplot(pca.2)\n\n\n\n\n#mit vegan, ein anderes Package für Ordinationen\npca.3 <- vegan::rda(raw, scale=FALSE) #Die Funktion rda führt ein PCA aus an wenn nicht Umwelt- und Artdaten definiert werden\n#scores(pca.3,display=c(\"sites\"))\n#scores(pca.3,display=c(\"species\"))\nsummary(pca.3, axes=0)\nbiplot(pca.3, scaling=2)\nbiplot(pca.3, scaling=\"species\")#scaling=species macht das selbe wie scaling=2"
  },
  {
    "objectID": "statKons/StatKons2_Demo_PCA.html#ca-mit-sveg",
    "href": "statKons/StatKons2_Demo_PCA.html#ca-mit-sveg",
    "title": "StatKons2: Demo",
    "section": "CA mit sveg",
    "text": "CA mit sveg\n\nlibrary(vegan)\nlibrary(dave) #for the dataset sveg\nlibrary(FactoMineR)# siehe Beispiel hier: https://www.youtube.com/watch?v=vP4korRby0Q\n\n# ebenfalls mit transformierten daten\no.ca<-cca(sveg^0.5) #package vegan\no.ca1 <- CA(sveg^0.5) #package FactoMineR\n\n\n\n\n#Arten (o) und Communities (+) plotten\nplot(o.ca)\n\n\n\nsummary(o.ca1)\n\n#Nur Arten plotten\nx<-o.ca$CA$u[,1]\ny<-o.ca$CA$u[,2]\nplot(x,y)\n\n\n\n\n#Anteilige Varianz, die durch die ersten beiden Achsen erklaert wird\no.ca$CA$eig[1:63]/sum(o.ca$CA$eig)"
  },
  {
    "objectID": "statKons/StatKons2_Demo_PCA.html#nmds-mit-sveg",
    "href": "statKons/StatKons2_Demo_PCA.html#nmds-mit-sveg",
    "title": "StatKons2: Demo",
    "section": "NMDS mit sveg",
    "text": "NMDS mit sveg\n\n#NMDS----------\n\n#Distanzmatrix als Start erzeugen\nlibrary(MASS)\nlibrary(vegan)\n\nmde <-vegdist(sveg,method=\"euclidean\")\nmdm <-vegdist(sveg,method=\"manhattan\")\n\n#Zwei verschiedene NMDS-Methoden\nset.seed(1) #macht man, wenn man bei einer Wiederholung exakt die gleichen Ergebnisse will\no.imds<-isoMDS(mde, k=2) # mit K = Dimensionen\nset.seed(1)\no.mmds<-metaMDS(mde,k=3) # scheint nicht mit 2 Dimensionen zu konvergieren\n\nplot(o.imds$points)\nplot(o.mmds$points)\n\n#Stress =  Abweichung der zweidimensionalen NMDS-Loesung von der originalen Distanzmatrix\nstressplot(o.imds,mde)\nstressplot(o.mmds,mde)"
  },
  {
    "objectID": "statKons/StatKons3_Demo_LM.html#einfaktorielle-anova",
    "href": "statKons/StatKons3_Demo_LM.html#einfaktorielle-anova",
    "title": "StatKons3: Demo",
    "section": "Einfaktorielle ANOVA",
    "text": "Einfaktorielle ANOVA\n\n# für mehr infos\n#https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/mtcars.html\n\ncars <- mtcars %>% \n    mutate(cyl = as.factor(cyl)) %>% \n    slice(-31) # lösch die 31ste Zeile\n\n#Alternativ ginge auch das\ncars[-31,]\n\n# schaue daten zuerst mal an\n#1. Responsevariable\nhist(cars$hp) # nur sinnvoll bei grossem n\nboxplot(cars$hp)\n\n\n#2. Responsevariable ~ Prediktorvariable\ntable(cars$cyl) # mögliches probel, da n's unterschiedlich gross\n\nboxplot(cars$hp ~ cars$cyl) # varianzheterogentität weniger das problem, \n# aber normalverteilung der residuen problematisch\n\n# definiere das modell für eine ein-faktorielle anova\naov.1 <- aov(log10(hp) ~ cyl, data = cars)\n\n#3. Schaue Modelgüte an\npar(mfrow = c(2,2))\nplot(aov.1)\n\n#4. Schaue output an und ordne es ein\nsummary.lm(aov.1)\n\n#5. bei meheren Kategorien wende einen post-hoc Vergleichstest an\nTukeyHSD(aov.1)\n\n#6. Ergebnisse passend darstellen\nlibrary(multcomp)\n\n#erstens die signifikanten Unterschiede mit Buchstaben versehen\nletters <- multcomp::cld(multcomp::glht(aov.1, linfct=multcomp::mcp(cyl=\"Tukey\"))) # Achtung die kategoriale\n#Variable (unsere unabhängige Variable \"cyl\") muss als Faktor\n#definiert sein z.B. as.factor()\n\n#einfachere Variante\nboxplot(hp ~ cyl, data = cars)\nmtext(letters$mcletters$Letters, at=1:3)\n\n#schönere Variante :)\nggplot(cars, aes(x = cyl, y = hp)) +\n    stat_boxplot(geom = \"errorbar\", width = .5) +\n  geom_boxplot(size = 1) +\n    annotate(\"text\", x = 1, y = 350, label = \"a\", size = 7)+\nannotate(\"text\", x = 2, y = 350, label = \"b\", size = 7)+\n  annotate(\"text\", x = 3, y = 350, label = \"c\", size = 7)\n  labs(x = \"\\nAnzahl Zylinder\", y = \"Pferdestärke\")  +\n  mytheme\n\n#Plot exportieren\nggsave(filename = \"distill-preview.png\",\n       device = \"png\") # hier kann man festlegen, was für ein Bildformat\n#exportiert werden möchte\n\n# Sind die Voraussetzungen für eine Anova verletzt, überprüfe alternative \n# nicht-parametische Tests z.B. oneway-Test mit Welch-korrektur für ungleiche\n# Varianzen (Achtung auch dieser Test hat Voraussetzungen -> siehe Skript XY)\nlibrary(rosetta)\nwelch1 <- oneway.test(hp ~ cyl, data = cars, var.equal = FALSE)\nrosetta::posthocTGH(cars$hp, cars$cyl, method = \"games-howell\")"
  },
  {
    "objectID": "statKons/StatKons3_Demo_LM.html#mehrfaktorielle-anova",
    "href": "statKons/StatKons3_Demo_LM.html#mehrfaktorielle-anova",
    "title": "StatKons3: Demo",
    "section": "Mehrfaktorielle ANOVA",
    "text": "Mehrfaktorielle ANOVA"
  },
  {
    "objectID": "statKons/StatKons3_Demo_LM.html#einfache-regression",
    "href": "statKons/StatKons3_Demo_LM.html#einfache-regression",
    "title": "StatKons3: Demo",
    "section": "Einfache Regression",
    "text": "Einfache Regression\n\n# inspiriert von Simon Jackson: http s://drsimonj.svbtle.com/visualising-residuals\ncars <- mtcars %>% \n  #ändere die unabhängige Variable mpg in 100Km/L\n  mutate(kml = (235.214583/mpg)) # mehr Infos hier: https://www.asknumbers.com/mpg-to-L100km.aspx\n  # %>%  # klone data set\n  # slice(-31) # # lösche Maserrati und schaue nochmals Modelfit an\n\n#############\n##1.Daten anschauen\n############\n\n# Zusammenhang mal anschauen\n# Achtung kml = 100km pro Liter \nplot(hp ~ kml, data = cars)\n\n\n\n# Responsevariable anschauen\nboxplot(cars$hp)\n\n\n\n# Korrelationen uv + av anschauen\n# Reihenfolge spielt hier keine Rolle, wieso?\ncor(cars$kml, cars$hp) # hängen stark zusammen\n\n###################\n#2. Modell definieren: einfache regression\n##################\nmodel <- lm(hp ~ kml, data = cars)\nsummary.lm(model)\n\n###############\n#3.Modeldiagnostik und ggf. Anpassungen ans Modell oder ähnliches\n###############\n\n# semi schöne Ergebnisse\nlibrary(ggfortify)\nggplot2::autoplot(model) + mytheme # gitb einige Extremwerte => was tun? (Eingabe/Einlesen \n\n\n\n#überprüfen, Transformation, Extremwerte nur ausschliessen mit guter Begründung)\n\n# erzeuge vorhergesagte Werte und Residualwerte\ncars$predicted <- predict(model)   # bilde neue Variable mit geschätzten y-Werten\ncars$residuals <- residuals(model)\n\n# schaue es dir an, sieht man gut was die Residuen sind\nd <- cars %>%  \n    dplyr::select(hp, kml, predicted, residuals)\n\n# schauen wir es uns an\nhead(d, 4)\n\n#visualisiere residuen\nggplot(d, aes(x = kml, y = hp)) +\n  # verbinde beobachtete werte mit vorausgesagte werte\n  geom_segment(aes(xend = kml, yend = predicted)) + \n  geom_point() + # Plot the actual points\n  geom_point(aes(y = predicted), shape = 4) + # plot geschätzten y-Werten\n  # geom_line(aes(y = predicted), color = \"lightgrey\") # alternativ code\n  geom_smooth(method = \"lm\", se = FALSE, color = \"lightgrey\") +\n  # Farbe wird hier zu den redisuen gemapped, abs(residuals) wegen negativen zahlen  \n  geom_point(aes(color = abs(residuals))) + \n  # Colors to use here (für mehrere farben verwende color_gradient2)\n  scale_color_continuous(low = \"blue\", high = \"red\") +  \n  scale_x_continuous(limits = c(0, 40)) +\n  scale_y_continuous(limits = c(0, 300)) +\n  guides(color = \"none\") +  # Color legende entfernen\n  labs(x = \"\\nVerbraucht in Liter pro 100km\", y = \"Motorleistung in PS\\n\") +\n  mytheme\n\n\n\n##########\n#4. plotte Ergebnis\n##########\nggplot(d, aes(x = kml, y = hp)) +\n    geom_point(size = 4) +\n    # geom_point(aes(y = predicted), shape = 1, size = 4) +\n    # plot regression line\n    geom_smooth(method = \"lm\", se = FALSE, color = \"lightgrey\") +\n    #intercept\n    geom_line(aes(y = mean(hp)), color = \"blue\") +\n    mytheme"
  },
  {
    "objectID": "statKons/StatKons3_Demo_LM.html#multiple-regression",
    "href": "statKons/StatKons3_Demo_LM.html#multiple-regression",
    "title": "StatKons3: Demo",
    "section": "Multiple regression",
    "text": "Multiple regression\n\n# Select data\ncars <- mtcars %>% \n    slice(-31) %>%\n    mutate(kml = (235.214583/mpg)) %>% \n    dplyr::select(kml, hp, wt, disp)\n\n################\n# 1. Multikollinearitüt überprüfen\n# Korrelation zwischen Prädiktoren kleiner .7\ncor <- cor(cars[, -2])\ncor[abs(cor)<0.7] <- 0  \ncor # \n\n##### info zu Variablen\n#wt = gewicht\n#disp = hubraum\n\n###############\n#2. Responsevariable + Kriteriumsvariable anschauen\n##############\n# was würdet ihr tun?\n\n############\n#3. Definiere das Model\n############\nmodel1 <- lm(hp ~ kml + wt + disp, data = cars) \nmodel2 <- lm(hp ~ kml + wt, data = cars)\nmodel3 <- lm(log10(hp) ~ kml + wt, data = cars)\n\n#############\n#4. Modeldiagnostik\n############\n\nlibrary(ggfortify)\nggplot2::autoplot(model1)\n\n\n\nggplot2::autoplot(model2) # besser, immernoch nicht ok => transformation? vgl. model3\n\n\n\nggplot2::autoplot(model3)\n\n\n\n############\n#5. Modellfit vorhersagen: wie gut sagt mein Modell meine Daten vorher\n############\n\n#es gibt 3 Mögliche Wege\n\n# gebe dir predicted values aus für model2 (für vorzeigebeispiel einfacher :)\n# gibts unterschidliche varianten die predicted values zu berechnen\n# 1. default funktion predict(model) verwenden\ncars$predicted <- predict(model2)\n\n# 2. datensatz selber zusammenstellen (nicht empfohlen): wichtig, die \n# prädiktoren müssen denselben\n# namen haben wie im Model\n# besser mit Traindata von Beginn an mehr Infos hier: https://www.r-bloggers.com/using-linear-regression-to-predict-energy-output-of-a-power-plant/\n\nnew.data <- tibble(kml = sample(seq(6.9384, 22.61, .3), 31),\n                   wt = sample(seq(1.513, 5.424, 0.01), 31),\n                   disp = sample(seq(71.1, 472.0, .1), 31)) \ncars$predicted_own <- predict(model2, newdata = new.data)\n\n# 3. train_test_split durchführen (empfohlen) muss jedoch von beginn an bereits \n# gemacht werden - Logik findet ihr hier: https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6 oder https://towardsdatascience.com/6-amateur-mistakes-ive-made-working-with-train-test-splits-916fabb421bb\n# beispiel hier: https://ijlyttle.github.io/model_cv_selection.html\ncars <- mtcars %>% \n  mutate(id = 1:nrow(.)) %>%  # für das mergen der Datensätze\n  mutate(kml = (235.214583/mpg)) %>% \n  dplyr::select(kml, hp, wt, disp, id)\n  \ntrain_data <- cars %>% \n  dplyr::sample_frac(.75) # für das Modellfitting\n\ntest_data  <- dplyr::anti_join(cars, train_data, by = 'id') # für den Test mit predict\n\n# erstelle das Modell und \"trainiere\" es auf den train Datensatz\nmodel2_train <- lm(hp ~ kml + wt, data = train_data)\n\n# mit dem \"neuen\" Datensatz wird das Model überprüft ob guter Modelfit\ntrain_data$predicted_test <- predict(model2_train, newdata = test_data)\n\n# Residuen\ntrain_data$residuals <- residuals(model2_train)\nhead(train_data)\n\n#weiterführende Infos zu \"machine learning\" Idee hier: https://stat-ata-asu.github.io/MachineLearningToolbox/regression-models-fitting-them-and-evaluating-their-performance.html\n#wichtigstes Packet in dieser Hinsicht ist \"caret\": https://topepo.github.io/caret/\n#beste Philosophie ist tidymodels: https://www.tidymodels.org\n\n#----------------\n# Schnelle variante mit broom\nd <- lm(hp ~ kml + wt+ disp, data = cars) %>% \n    broom::augment()\n\nhead(d)\n\nggplot(d, aes(x = kml, y = hp)) +\n    geom_segment(aes(xend = kml, yend = .fitted), alpha = .2) +\n    geom_point(aes(color = .resid)) +\n    scale_color_gradient2(low = \"blue\", mid = \"white\", high = \"red\") +\n    guides(color = \"none\") +\n    geom_point(aes(y = .fitted), shape = 4) +\n    scale_y_continuous(limits = c(0,350)) +\n    geom_smooth(method = \"lm\", se = FALSE, color = \"lightgrey\") +\n    mytheme\n\n\n\n############\n# 6. Modellvereinfachung\n############\n\n# Varianzpartitionierung\nlibrary(hier.part)\ncars <- mtcars %>% \n  mutate(kml = (235.214583/mpg)) %>% \n  select(-mpg)\n\nnames(cars) # finde \"position\" deiner Responsevariable\n\nX = cars[, -3] # definiere all die Prädiktorvariablen im Model (minus Responsevar)\n\n# dauert ein paar sekunden\nhier.part(cars$hp, X, gof = \"Rsqu\")\n\n\n\n# alle Modelle miteinander vergleichen mit dredge Befehl: geht nur bis \n# maximal 15 Variablen\nmodel2 <- lm(hp ~ ., data = cars)\nlibrary(MuMIn)\noptions(na.action = \"na.fail\")\nallmodels <- dredge(model2)\nhead(allmodels)\n\n# Wichtigkeit der Prädiktoren\nMuMIn::importance(allmodels)\n\n# mittleres Model\navgmodel<- MuMIn::model.avg(get.models(allmodels, subset=TRUE))\nsummary(avgmodel)\n\n# adäquatest model gemäss multimodel inference\nmodel_ad <- lm(hp ~ carb + disp + wt, data = mtcars)\nsummary(model_ad)"
  },
  {
    "objectID": "statKons/StatKons4_Demo_GLM.html#poisson-regression",
    "href": "statKons/StatKons4_Demo_GLM.html#poisson-regression",
    "title": "StatKons4: Demo",
    "section": "Poisson Regression",
    "text": "Poisson Regression\n\n############\n# quasipoisson regression\n############\n\ncars <- mtcars %>% \n   mutate(kml = (235.214583/mpg))\n\nglm.poisson <- glm(hp ~ kml, data = cars, family = \"poisson\")\n\nsummary(glm.poisson) # klare overdisperion\n## \n## Call:\n## glm(formula = hp ~ kml, family = \"poisson\", data = cars)\n## \n## Deviance Residuals: \n##    Min      1Q  Median      3Q     Max  \n## -6.438  -2.238  -1.159   2.457  10.576  \n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(>|z|)    \n## (Intercept) 3.894293   0.050262   77.48   <2e-16 ***\n## kml         0.081666   0.003414   23.92   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for poisson family taken to be 1)\n## \n##     Null deviance: 958.27  on 31  degrees of freedom\n## Residual deviance: 426.59  on 30  degrees of freedom\n## AIC: 645.67\n## \n## Number of Fisher Scoring iterations: 4\n\n# deshalb quasipoisson\nglm.quasipoisson <- glm(hp ~ kml, data = cars, family = quasipoisson(link = log))\n\nsummary(glm.quasipoisson)\n## \n## Call:\n## glm(formula = hp ~ kml, family = quasipoisson(link = log), data = cars)\n## \n## Deviance Residuals: \n##    Min      1Q  Median      3Q     Max  \n## -6.438  -2.238  -1.159   2.457  10.576  \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  3.89429    0.19508  19.963  < 2e-16 ***\n## kml          0.08167    0.01325   6.164 8.82e-07 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for quasipoisson family taken to be 15.06438)\n## \n##     Null deviance: 958.27  on 31  degrees of freedom\n## Residual deviance: 426.59  on 30  degrees of freedom\n## AIC: NA\n## \n## Number of Fisher Scoring iterations: 4\n\n# visualisiere\nggplot2::ggplot(cars, aes(x = kml, y = hp)) + \n    geom_point(size = 8) + \n    geom_smooth(method = \"glm\", method.args = list(family = \"poisson\"), se = F,\n                color = \"green\", size = 2) + \n    scale_x_continuous(limits = c(0,35)) + \n    scale_y_continuous(limits = c(0,400)) + \n    theme_classic()\n\n\n\n\n#Rücktransformation meines Outputs für ein besseres Verständnis\nglm.quasi.back <- exp(coef(glm.quasipoisson))\n\n#für ein schönes ergebnis\nglm.quasi.back %>%\n  broom::tidy() %>% \n  knitr::kable(digits = 3)\n## Warning: 'tidy.numeric' is deprecated.\n## See help(\"Deprecated\")\n\n\n\n\nnames\nx\n\n\n\n\n(Intercept)\n49.121\n\n\nkml\n1.085\n\n\n\n\n\n#for more infos, also for posthoc tests\n#here: https://rcompanion.org/handbook/J_01.html"
  },
  {
    "objectID": "statKons/StatKons4_Demo_GLM.html#logistische-regression",
    "href": "statKons/StatKons4_Demo_GLM.html#logistische-regression",
    "title": "StatKons4: Demo",
    "section": "logistische Regression",
    "text": "logistische Regression\n\n############\n# logistische regression\n############\ncars <- mtcars\n\n# erstelle das modell\nglm.binar <- glm(vs ~ hp, data = cars, family = binomial(link = logit)) \n\n#achtung Model gibt Koeffizienten als logit() zurück\nsummary(glm.binar)\n## \n## Call:\n## glm(formula = vs ~ hp, family = binomial(link = logit), data = cars)\n## \n## Deviance Residuals: \n##      Min        1Q    Median        3Q       Max  \n## -2.12148  -0.20302  -0.01598   0.51173   1.20083  \n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(>|z|)   \n## (Intercept)  8.37802    3.21593   2.605  0.00918 **\n## hp          -0.06856    0.02740  -2.502  0.01234 * \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 43.860  on 31  degrees of freedom\n## Residual deviance: 16.838  on 30  degrees of freedom\n## AIC: 20.838\n## \n## Number of Fisher Scoring iterations: 7\n\n# überprüfe das modell\ncars$predicted <- predict(glm.binar, type = \"response\")\n\n# visualisiere\nggplot(cars, aes(x = hp, y = vs)) +    \n    geom_point(size = 8) +\n    geom_point(aes(y = predicted), shape  = 1, size = 6) +\n    guides(color = \"none\") +\n    geom_smooth(method = \"glm\", method.args = list(family = 'binomial'), \n                se = FALSE,\n                size = 2) +\n    # geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n    mytheme\n\n\n\n\n#Modeldiagnostik (wenn nicht signifikant, dann OK)\n1 - pchisq(glm.binar$deviance,glm.binar$df.resid)  \n## [1] 0.9744718\n\n#Modellgüte (pseudo-R²)\n1 - (glm.binar$dev / glm.binar$null)  \n## [1] 0.6161072\n\n#Steilheit der Beziehung (relative Änderung der odds von x + 1 vs. x)\nexp(glm.binar$coefficients[2])\n##        hp \n## 0.9337368\n\n#LD50 (wieso negativ: weil zweiter koeffizient negative steigung hat)\nabs(glm.binar$coefficients[1]/glm.binar$coefficients[2])\n## (Intercept) \n##    122.1986\n\n# kreuztabelle (confusion matrix): fasse die ergebnisse aus predict und \n# \"gegebenheiten, realität\" zusammen\ntab1 <- table(cars$predicted>.5, cars$vs)\ndimnames(tab1) <- list(c(\"M:S-type\",\"M:V-type\"), c(\"T:S-type\", \"T:V-type\"))\ntab1 \n##          T:S-type T:V-type\n## M:S-type       15        2\n## M:V-type        3       12\n\nprop.table(tab1, 2) \n##           T:S-type  T:V-type\n## M:S-type 0.8333333 0.1428571\n## M:V-type 0.1666667 0.8571429\n#was könnt ihr daraus ablesen? Ist unser Modell genau?\n\n# Funktion die die logits in Wahrscheinlichkeiten transformiert\n# mehr infos hier: https://sebastiansauer.github.io/convert_logit2prob/\n# dies ist interessant, falls ihr mal ein kategorialer Prädiktor habt\nlogit2prob <- function(logit){\n  odds <- exp(logit)\n  prob <- odds / (1 + odds)\n  return(prob)\n}"
  },
  {
    "objectID": "statKons/StatKons4_Demo_GLM.html#gams",
    "href": "statKons/StatKons4_Demo_GLM.html#gams",
    "title": "StatKons4: Demo",
    "section": "GAM’s",
    "text": "GAM’s\n\n###########\n# LOESS & GAM\n###########\n\nggplot2::ggplot(mtcars, aes(x = mpg, y = hp)) + \n  geom_point(size = 8) + \n  geom_smooth(method = \"gam\", se = F, color = \"green\", size = 2, formula = y ~ s(x, bs = \"cs\")) + \n  geom_smooth(method = \"loess\", se = F, color = \"red\", size = 2) + \n  geom_smooth(method = \"glm\", size = 2, color = \"blue\", se = F) + \n  scale_x_continuous(limits = c(0,35)) + \n    scale_y_continuous(limits = c(0,400)) + \n    mytheme\n\n\n\n\nggplot2::ggplot(mtcars, aes(x = mpg, y = hp)) + \n  geom_point(size = 8) + \n  geom_smooth(method = \"gam\", se = F, color = \"green\", size = 2, formula = y ~ s(x, bs = \"cs\")) + \n    # geom_smooth(method = \"loess\", se = F, color = \"red\", size = 2) + \n  geom_smooth(method = \"glm\", size = 2, color = \"grey\", se = F) + \n  scale_x_continuous(limits = c(0,35)) + \n  scale_y_continuous(limits = c(0,400)) + \n  mytheme"
  },
  {
    "objectID": "rauman/Rauman1_Uebung_A.html",
    "href": "rauman/Rauman1_Uebung_A.html",
    "title": "Rauman 1: Übung A",
    "section": "",
    "text": "Es gibt bereits eine Vielzahl von Packages um in R mit räumlichen Daten zu arbeiten, die ihrerseits wiederum auf weiteren Packages basieren (Stichwort dependencies). Für Vektordaten dominierte lange das Package sp, welches nun durch sf abgelöst wurde. Wir werden wenn immer möglich mit sf arbeiten und nur in Ausnahmefällen auf andere Packages zurück greifen.\nFür die kommenden Übungen könnt ihr folgende Packages installieren bzw. laden:\n\nlibrary(sf)\n\nLinking to GEOS 3.8.0, GDAL 3.0.4, PROJ 6.3.1; sf_use_s2() is TRUE\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n\n\nAufgabe 1: Vektor Daten runterladen und importieren\nLade zunächst die Datensätze unter folgenden Links herunter:\nEs handelt sich um Geodatensätze im Format Geopackage (“*.gpkg”), eine alternatives Datenformat zum bekannteren Format “Shapefiles”. Importiere die Datensätze wie folgt:\n\nkantone <- read_sf(here(\"data\",\"kantone.gpkg\"))\ngemeinden <- read_sf(here(\"data\",\"gemeinden.gpkg\")) \n\nSchau Dir die importierten Datensätze an. Am meisten Informationen zu sf Objekten bekommst du, wenn du dir den Datensatz in der Konsole anschaust (in dem du den Variabel-Name in der Konsole eintippst). Mit dem RStudio Viewer werden sf Objekte nur sehr langsam geladen und die Metadaten werden nicht angezeigt.\n\n\nAufgabe 2: Daten Visualisieren\nVektordaten (sf Objekte) lassen sich teilweise sehr schön in die bekannten Tidyverse workflows integrieren. Das merkt man schnell, wenn man die Daten visualisieren möchte. In InfoVis 1 & 2 haben wir intensiv mit ggplot2 gearbeitet und dort die Layers geom_point() und geom_line() kennen gelernt. Zusätzlich beinhaltet ggplot die Möglichkeit, mit geom_sf() Vektordaten direkt und sehr einfach zu plotten. Führe die angegebenen R-Befehle aus und studiere die entstehenden Plots. Welche Unterschiede findest Du? Wie erklärst Du diese Unterschiede?\n\nggplot(gemeinden) + \n  geom_sf()\n\nggplot(kantone) + \n  geom_sf()\n\n\n\nInput: Koodinatensysteme\nIn der obigen visualierung fällt folgendes auf:\n\ndie X/Y Achsen weisen zwei ganz unterschiedlichen Zahlenbereiche auf (vergleiche die Achsenbeschriftungen)\nder Umriss der Schweiz sieht in den beiden Datensätzen unterschiedlich aus (kantone ist gegenüber gemeinden gestaucht)\n\nDies hat natürlich damit zu tun, dass die beiden Datensätze in unterschiedlichen Koordinatensystemen erfasst wurden. Koordinatensysteme werden mit CRS (Coordinate Reference System) abgekürzt. Mit st_crs() können die zugewiesenen Koordinatensysteme abgefragt werden.\n\nst_crs(kantone)\n\nCoordinate Reference System: NA\n\nst_crs(gemeinden)\n\nCoordinate Reference System: NA\n\n\nLeider sind in unserem Fall keine Koordinatensysteme zugewiesen. Mit etwas Erfahrung kann man das Koordinatensystem aber erraten, so viele kommen nämlich gar nicht in Frage. Am häufigsten trifft man hierzulande eines der drei folgenden Koordinatensysteme an:\n\nCH1903 LV03: das alte Koordinatensystem der Schweiz\nCH1903+ LV95: das neue Koordinatensystem der Schweiz\nWGS84: ein häufig genutztes weltumspannendes geodätisches Koordinatensystem, sprich die Koordinaten werden in Länge und Breite angegeben (Lat/Lon).\n\nNun gilt es, anhand der Koordinaten die in der Spalte geometry ersichtlich sind das korrekte Koordinatensystem festzustellen. Wenn man sich auf epsg.io/map die Schweiz anschaut, kann man die Koordinaten in verschiedenen Koordinatensystem betrachten.\nBedienungshinweise:\n \n\nKoordinanten (des Fadenkreuzes) werden im ausgewählten Koordinatensystem dargestellt\n\n\n \n\nDas Koordinatensystem, in welchem die Koordinaten dargestellt werden sollen, kann mit “Change” angepasst werden\n\n\n\n\nFür Enthusiasten: Schau Dir die Schweiz in verschiedenen Koordinatensystemen an, in dem Du auf “Reproject Map” klickst\n\n\nWenn man diese Koordinaten mit den Koordinaten unserer Datensätze vergleicht, dann ist schnell klar, dass es sich beim Datensatz kantone um das Koordinatensystem WGS84 handelt und bei gemeinden das Koordinatensystem CH1903+ LV95. Diese Koordinatensyteme weisen wir nun mit st_set_crs() und dem entsprechenden EPSG-Code (siehe die jeweiligen Links) zu.\n\nkantone <- st_set_crs(kantone, 4326)\ngemeinden <- st_set_crs(gemeinden, 2056)\n\n# zuweisen mit st_set_crs(), abfragen mit st_crs()\nst_crs(kantone)\n\nCoordinate Reference System:\n  User input: EPSG:4326 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"unknown\"],\n        AREA[\"World\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]\n\n\nAuch wenn das CRS der Datensätze bekannt ist, nutzt ggplot immer noch EPSG 4326 um die Achsen zu beschriften. Wenn das stört, kann man coord_sf(datum = 2056) in einem weiteren Layer spezifizieren. Oder aber man blendet die Achsenbeschriftung mit theme_void() komplett aus. Versuche beide Varianten.\n\nggplot() + \n  geom_sf(data = kantone) +\n  coord_sf(datum = 2056)\n\n\n\n\n\n\nAufgabe 3: Koordinatensyteme transformieren\nIn der vorherigen Übung haben wir das bestehende Koordinatensystem zugewiesen. Dabei haben wir die bestehenden Koordinaten (in der Spalte geom) nicht manipuliert. Ganz anders ist eine Transformation der Daten von einem Koordinatensystem in das andere. Bei einer Transformation werden die Koordinaten in das neue Koordinatensystem umgerechnet und somit manipuliert. Aus praktischen Gründen wollen  wir all unsere Daten ins neue Schweizer Koordinatensystem CH1903+ LV95 transfomieren. Transformiere den Datensatz kantone mit st_transform()in CH1903+ LV95, nutze dafür den korrekten EPSG-Code.\nVor der Transformation (betrachte die Attribute Bounding box sowie Geodetic CRS):\n\nkantone\n\nSimple feature collection with 51 features and 6 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 5.955902 ymin: 45.81796 xmax: 10.49217 ymax: 47.80845\nGeodetic CRS:  WGS 84\n# A tibble: 51 × 7\n   NAME       KANTON…¹ SEE_F…² KANTO…³ KT_TEIL EINWO…⁴                      geom\n * <chr>         <int>   <dbl>   <dbl> <chr>     <int>             <POLYGON [°]>\n 1 Graubünden       18      NA  710530 0        197888 ((8.877053 46.81291, 8.8…\n 2 Bern              2   11897  595952 1       1031126 ((7.153522 46.98628, 7.1…\n 3 Valais           23    1060  522463 0        341463 ((8.477625 46.52762, 8.4…\n 4 Vaud             22   39097  321201 1        793129 ((6.779825 46.85296, 6.7…\n 5 Ticino           21    7147  281216 0        353709 ((8.477625 46.52762, 8.4…\n 6 St. Gallen       17    7720  202820 1        504686 ((8.808609 47.22009, 8.7…\n 7 Zürich            1    6811  172894 0       1504346 ((8.410084 47.24837, 8.4…\n 8 Fribourg         10    7818  167142 1        315074 ((7.040344 46.97952, 7.0…\n 9 Luzern            3    6438  149352 0        406506 ((8.468167 46.99652, 8.4…\n10 Aargau           19     870  140380 1        670988 ((8.410084 47.24837, 8.4…\n# … with 41 more rows, and abbreviated variable names ¹​KANTONSNUM, ²​SEE_FLAECH,\n#   ³​KANTONSFLA, ⁴​EINWOHNERZ\n\n\n\nkantone <- st_transform(kantone, 2056)\n\nNach der Transformation (betrachte die Attribute Bounding box sowie Projected CRS):\n\nkantone\n\nSimple feature collection with 51 features and 6 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 5.955902 ymin: 45.81796 xmax: 10.49217 ymax: 47.80845\nGeodetic CRS:  WGS 84\n# A tibble: 51 × 7\n   NAME       KANTON…¹ SEE_F…² KANTO…³ KT_TEIL EINWO…⁴                      geom\n * <chr>         <int>   <dbl>   <dbl> <chr>     <int>             <POLYGON [°]>\n 1 Graubünden       18      NA  710530 0        197888 ((8.877053 46.81291, 8.8…\n 2 Bern              2   11897  595952 1       1031126 ((7.153522 46.98628, 7.1…\n 3 Valais           23    1060  522463 0        341463 ((8.477625 46.52762, 8.4…\n 4 Vaud             22   39097  321201 1        793129 ((6.779825 46.85296, 6.7…\n 5 Ticino           21    7147  281216 0        353709 ((8.477625 46.52762, 8.4…\n 6 St. Gallen       17    7720  202820 1        504686 ((8.808609 47.22009, 8.7…\n 7 Zürich            1    6811  172894 0       1504346 ((8.410084 47.24837, 8.4…\n 8 Fribourg         10    7818  167142 1        315074 ((7.040344 46.97952, 7.0…\n 9 Luzern            3    6438  149352 0        406506 ((8.468167 46.99652, 8.4…\n10 Aargau           19     870  140380 1        670988 ((8.410084 47.24837, 8.4…\n# … with 41 more rows, and abbreviated variable names ¹​KANTONSNUM, ²​SEE_FLAECH,\n#   ³​KANTONSFLA, ⁴​EINWOHNERZ\n\n\n\n\nAufgabe 4: Chloroplethen Karte\nNun wollen wir die Gemeinden respektive die Kantone nach ihrer Einwohnerzahl einfärben. Dafür verwenden wir wie gewohnt die Methode aes(fill = ...) von ggplot.\nTips:\n\num die scientific notation (z.B. 3e+03) zu verhindern, könnt ihr den Befehl options(scipen = 999) ausführen\num die Darstellung der Gemeinde- (bzw. Kantons-) Grenzen zu verhindern, könnt ihr im entsprechenden Layer color = NA setzen. Alternativ könnt ihr die Linienbreite mit size = verändern.\n\n\nggplot(kantone, aes(fill = EINWOHNERZ/1e6)) +\n  geom_sf(color= \"white\",size = .05) +\n  labs(title = \"Anzahl Einwohner pro Kanton\",\n       subtitle = \"in Millionen\") +\n  theme(legend.position = \"bottom\",\n        legend.title = element_blank(),\n        legend.key.width = unit(0.15, 'npc'),\n        legend.key.height = unit(0.02, 'npc'),\n        legend.text = element_text(angle = 90,hjust = 0.5),\n        legend.text.align = 1)\n\nggplot(gemeinden,aes(fill = EINWOHNERZ/1e6)) +\n  geom_sf(color= \"white\",size = .05) +\n  scale_fill_continuous(\"Einwohner (in Mio)\") +\n  labs(title = \"Anzahl Einwohner pro Gemeinde\",\n       subtitle = \"in Millionen\") +\n  theme(legend.position = \"bottom\",\n        legend.title = element_blank(),\n        legend.key.width = unit(0.15, 'npc'),\n        legend.key.height = unit(0.02, 'npc'),\n        legend.text = element_text(angle = 90,hjust = 0.5),\n        legend.text.align = 1)\n\n\n\n\nDer Vergleich dieser beiden Darstellungen veranschaulicht die MAUP Problematik sehr deutlich\n\n\n\n\n\n\n\nDer Vergleich dieser beiden Darstellungen veranschaulicht die MAUP Problematik sehr deutlich"
  },
  {
    "objectID": "rauman/Rauman1_Uebung_B.html",
    "href": "rauman/Rauman1_Uebung_B.html",
    "title": "Rauman 1: Übung B",
    "section": "",
    "text": "Für die kommende Übung arbeiten wir mit nachstehendem Datensatz. Lade diesen Herunter und importiere ihn in R.\nZudem brauchen wir die folgenden libraries:"
  },
  {
    "objectID": "rauman/Rauman1_Uebung_B.html#musterlösung",
    "href": "rauman/Rauman1_Uebung_B.html#musterlösung",
    "title": "Rauman 1: Übung B",
    "section": "Musterlösung",
    "text": "Musterlösung"
  },
  {
    "objectID": "rauman/Rauman2_Uebung_A.html#aufgabe-3",
    "href": "rauman/Rauman2_Uebung_A.html#aufgabe-3",
    "title": "Rauman 2: Übung A",
    "section": "Aufgabe 3",
    "text": "Aufgabe 3\nFühre nun die gleichen Schritte mit luftqualitaet durch und vergleiche die ECDF-Plots."
  },
  {
    "objectID": "rauman/Rauman2_Uebung_B.html",
    "href": "rauman/Rauman2_Uebung_B.html",
    "title": "Rauman 2: Übung B",
    "section": "",
    "text": "In dieser Übung geht es darum, zwei verschiedene Interpolationsverfahren in R umzusetzen. Im ersten Interpolationsverfahren verwenden wir die inverse distance weighted interpolation, später verwenden wir die nearest neighbour methode. Dazu braucht ihr die folgenden Packages:\nWeiter benötigt ihr die nachstehenden Datensätze:\nDie Library gstat bietet verschiedene Möglichkeiten, Datenpunkte zu interpolieren, unter anderem auch die inverse distance weighted Methode. Leider ist das Package noch nicht so benutzerfreundlich wie sf: Das Package wird aber aktuell überarbeitet und in mittlerer Zukunft sollte es ebenso einfach zugänglich sein. Damit Ihr Euch nicht mit den Eigenheiten dieser Library umschlagen müsst, haben wir eine Function vorbereitet, die Euch die Verwendung der IDW-Interpolation erleichtern soll.\nWir nehmen Euch damit etwas Komplexität weg und liefern Euch ein pfannenfertiges Werkzeug. Das hat auch Nachteile und wir ermutigen alle, die dafür Kapazität haben, unsere Function eingehend zu studieren und allenfalls ganz auf die Function zu verzichten und stattdessen direkt gstat zu verwenden. Wenn ihr mit unserer Function arbeiten möchtet, müsst ihr den unten stehenden Code in euer Skript kopieren und ausführen.\nNun könnt Ihr mit my_idw() den Datensatz luftqualitaet folgendermassen interpolieren.\nFolgende Parameter stehen Euch zur Verfügung:\nBeim Output handelt sich hier um einen Raster-ähnlichen Datentyp (siehe Vorlesung Spatial DataScience 1). Diesen können wir mit geom_raster mit ggplot visualisieren. Dafür müsst ihr in aes die X und Y Koordinaten angeben, und der interpolierte Wert mit fill einfärben."
  },
  {
    "objectID": "rauman/Rauman2_Uebung_B.html#musterlösung",
    "href": "rauman/Rauman2_Uebung_B.html#musterlösung",
    "title": "Rauman 2: Übung B",
    "section": "Musterlösung",
    "text": "Musterlösung"
  },
  {
    "objectID": "rauman/Rauman2_Uebung_C.html",
    "href": "rauman/Rauman2_Uebung_C.html",
    "title": "Rauman 2: Übung C",
    "section": "",
    "text": "Nun wollen wir für die bereits verwendeten Datensätze luftqualitaet.gpkg und rotmilan.gpkg Dichteschätzungen durchführen. Ladet dafür die notwendigen Package und ladet bei Bedarf die Datensätze herunter."
  },
  {
    "objectID": "rauman/Rauman2_Uebung_C.html#aufgabe-1-rotmilan-bewegungsdaten-visualisieren",
    "href": "rauman/Rauman2_Uebung_C.html#aufgabe-1-rotmilan-bewegungsdaten-visualisieren",
    "title": "Rauman 2: Übung C",
    "section": "Aufgabe 1: Rotmilan Bewegungsdaten visualisieren",
    "text": "Aufgabe 1: Rotmilan Bewegungsdaten visualisieren\nDie erste Frage, die bei solchen Bewegungsstudien typischerweise gestellt wird, lautet: Wo hält sich das Tier hauptsächlich auf? Um diese Frage zu beantworten, kann man als erstes einfach die Datenpunkte in einer einfachen Karte visualisieren. Erstellt zur Beantwortung dieser Frage nachstehende Karte."
  },
  {
    "objectID": "rauman/Rauman2_Uebung_C.html#aufgabe-2-kernel-density-estimation-berechnen",
    "href": "rauman/Rauman2_Uebung_C.html#aufgabe-2-kernel-density-estimation-berechnen",
    "title": "Rauman 2: Übung C",
    "section": "Aufgabe 2: Kernel Density Estimation berechnen",
    "text": "Aufgabe 2: Kernel Density Estimation berechnen\nIn einer ersten Annäherung funktioniert dies, doch wir sehen hier ein klassisches Problem des “Overplotting”. Das heisst, dass wir durch die Überlagerung vieler Punkte in den dichten Regionen nicht abschätzen können, wie viele Punkte dort effektiv liegen und ggf. übereinander liegen. Es gibt hier verschiedene Möglichkeiten, die Punktdichte klarer zu visualisieren. Eine unter Biologen sehr beliebte Methode ist die Dichteverteilung mit einer Kernel Density Estimation (KDE). Dies v.a. darum, weil mit KDE das Habitat (Streifgebiet) eines Tieres abgeschätzt werden kann. Homeranges werden oft mit KDE95 und Core Areas mit KDE50 definiert (Fleming C., Calabrese J., 2016).\nÄhnlich wie beim IDW sind auch die verfügbaren KDE-Funktionen in R etwas kompliziert in der Handhabung. Damit wir dieses Verfahren aber dennoch auf unsere Rotmilan-Daten anwenden können, haben wir eine eigene KDE-Funktion erstellt, die wir Euch zur Verfügung stellen.\nHier gilt das gleiche wie schon bei der Funktion my_idw(): Wir ermutigen alle, die dafür Kapazität haben, unsere Function eingehend zu studieren und allenfalls ganz auf die Funktion zu verzichten und stattdessen direkt MASS zu verwenden. Wenn ihr mit unserer Funktion arbeiten möchtet, müsst ihr den unten stehenden Code in euer Skript kopieren und ausführen.\n\nmy_kde <- function(points,cellsize, bandwith, extent = NULL){\n  library(MASS)\n  library(sf)\n  library(tidyr)\n  if(is.null(extent)){\n    extent_vec <- st_bbox(points)[c(1,3,2,4)]\n  } else{\n    extent_vec <- st_bbox(extent)[c(1,3,2,4)]\n  }\n  \n  n_y <- ceiling((extent_vec[4]-extent_vec[3])/cellsize)\n  n_x <- ceiling((extent_vec[2]-extent_vec[1])/cellsize)\n  \n  extent_vec[2] <- extent_vec[1]+(n_x*cellsize)-cellsize\n  extent_vec[4] <- extent_vec[3]+(n_y*cellsize)-cellsize\n\n  coords <- st_coordinates(points)\n  mat <- kde2d(coords[,1],coords[,2],h = bandwith,n = c(n_x,n_y),lims = extent_vec)\n\n  mydf <- as.data.frame(mat[[3]])\n  \n  colnames(mydf) <- mat[[2]]\n  mydf$X <- mat[[1]]\n  \n  pivot_longer(mydf, -X,names_to = \"Y\",names_transform = list(Y = as.numeric))\n}\n\nDie Parameter der Funktion sollten relativ klar sein:\n\npoints: Ein Punktdatensatz aus der Class sf\ncellsize: Die Zellgrösse des output-Rasters\nbandwith: Der Suchradius für die Dichteberechnung\nextent (optional): Der Perimeter, in dem die Dichteverteilung berechnet werden soll. Wenn kein Perimeter angegeben wird, wird die “bounding box” von points genutzt.\n\nWenn wir nun mit my_kde() die Dichteverteilung berechnen, erhalten wir ein data.frame mit X und Y Koordinaten sowie eine Spalte value zurück. Nutzt diese drei Spalten mit geom_raster() um eure Daten mit ggplot zu visualisieren (aes(x = X, y = Y, fill = value).\n\nrotmilan_kde <- my_kde(points = rotmilan,cellsize = 1000, bandwith = 10000, extent = schweiz)\n\nrotmilan_kde\n\n# A tibble: 77,129 × 3\n          X        Y value\n      <dbl>    <dbl> <dbl>\n 1 2485410. 1075268.     0\n 2 2485410. 1076268.     0\n 3 2485410. 1077268.     0\n 4 2485410. 1078268.     0\n 5 2485410. 1079268.     0\n 6 2485410. 1080268.     0\n 7 2485410. 1081268.     0\n 8 2485410. 1082268.     0\n 9 2485410. 1083268.     0\n10 2485410. 1084268.     0\n# … with 77,119 more rows\n\n\n\n\n\n\n\nDie Kernel Density Estimation ist nun sehr stark von den tiefen Werten dominiert, da die Dichte in den meisten Zellen unseres Untersuchungsgebiets nahe bei Null liegt. Wie erwähnt sind Wissenschaftler häufig nur an den höchsten 95% der Werte interessiert. Folge folgende Schritte um das Resultat etwas besser zu verantschaulichen:\n\nBerechne die 95. Perzentile aller Werte mit der Funktion quantile und benne diesen q25\nErstelle eine neue Spalte in rotmilan_kde, wo alle Werte tiefer als q25 NA entsprechen\n(Optional): Transformiere die Werte mit log10, um einen differenzierteren Farbverlauf zu erhalten\n\nWir können die tiefen Werte ausblenden, indem wir nur die höchsten 5% der Werte darstellen. Dafür berechnen wir mit raster::quantile die 95. Perzentile aller Werte und nutzen diesen Wert als “Grenzwert” für die Darstellung.\nZusätzlich hilft eine logarithmische Transformation der Werte, die Farbskala etwas sichtbarer zu machen.\n\n\nWarning: Removed 73272 rows containing missing values (geom_raster)."
  },
  {
    "objectID": "rauman/Rauman2_Uebung_C.html#aufgabe-3-dichteverteilung-mit-thiessen-polygonen",
    "href": "rauman/Rauman2_Uebung_C.html#aufgabe-3-dichteverteilung-mit-thiessen-polygonen",
    "title": "Rauman 2: Übung C",
    "section": "Aufgabe 3: Dichteverteilung mit Thiessen Polygonen",
    "text": "Aufgabe 3: Dichteverteilung mit Thiessen Polygonen\nThiessen Polygone bieten eine spannende Alternative um Unterschiede in der Dichteverteilung von Punktdatensätzen zu visualisieren. Wir wollen dies nun ausprobieren und konstruieren zum Schluss die Thiessenpolygone für die Rotmilan-Daten für das Untersuchungsgebiet Schweiz. Nutze die Anleitung für das Erstellen von Thiessenpolygonen aus der Übung B um Thiessenpolygone für die Rotmilanpositionen zu erstellen.\n\n\n\n\n\n\n\n\nWenn wir jetzt die Thiessenpolygone (ohne Punkte) darstellen, wird deutlicher, wie die Dichteverteilung im Innern des Clusters aussieht."
  },
  {
    "objectID": "rauman/Rauman3_Uebung_A.html",
    "href": "rauman/Rauman3_Uebung_A.html",
    "title": "Rauman 3: Übung A",
    "section": "",
    "text": "Für die Berechnung von Morans \\(I\\) benutzen wir kein externes Package, sondern erarbeiten uns alles selber, basierend auf der Formel von Moran’s \\(I\\):\n\\[I = \\frac{n}{\\sum_{i=1}^n (y_i - \\bar{y})^2} \\times \\frac{\\sum_{i=1}^n \\sum_{j=1}^n w_{ij}(y_i - \\bar{y})(y_j - \\bar{y})}{\\sum_{i=1}^n \\sum_{j=1}^n w_{ij}}\\]\nDiese sieht sehr beeindruckend aus, aber wenn wir die Formel in ihre Einzelbestandteile aufteilen, sehen wir, dass diese in sich gar nicht so komplex sind.\nAls erster Schritt müssen wir die notwendigen Libraries und Geodaten laden:"
  },
  {
    "objectID": "rauman/Rauman3_Uebung_A.html#aufgabe-1-herleitung-der-formel",
    "href": "rauman/Rauman3_Uebung_A.html#aufgabe-1-herleitung-der-formel",
    "title": "Rauman 3: Übung A",
    "section": "Aufgabe 1: Herleitung der Formel",
    "text": "Aufgabe 1: Herleitung der Formel\nIn der ersten Übung wollen wir Moran’s \\(I\\) für eine gegebene Choroplethenkarte nachrechnen. Dazu nehmen wir die Formel für Moran’s \\(I\\) und zerlegen sie in Einzelteile, die wir dann Schritt für Schritt für unsere Daten berechnen. So teilen wir ein vermeintlich komplexes Problem in überschaubare Einzelteile. Dieses Vorgehen illustriert ausserdem sehr schön ein generelles Data Science Prinzip. Divide and Conquer - Teile und Herrsche: Teile ein komplexes Problem in kleinere, beherrschbare Unterprobleme. Wir beginnen mit dem ersten Bruch und berechnen dabei zuerst den Zähler, dann dem Nenner. So können wir den Bruch auflösen und uns danach dem zweiten Bruch zuwenden:\n\\[I = \\frac{n}{\\sum_{i=1}^n (y_i - \\bar{y})^2} \\times \\frac{\\sum_{i=1}^n \\sum_{j=1}^n w_{ij}(y_i - \\bar{y})(y_j - \\bar{y})}{\\sum_{i=1}^n \\sum_{j=1}^n w_{ij}}\\]\n\nBruch 1\nWidmen wir uns dem ersten Bruch:\n\\[\\frac{n}{\\sum_{i=1}^n (y_i - \\bar{y})^2}\\]\n\nZähler (von Bruch 1)\nBeginnen wir mit dem Zähler, \\(n\\). Dies ist lediglich die Anzahl Messwerte in unserem Datensatz, also die Anzahl Kantone.\n\nn <- nrow(zweitwohnung_kanton)\nn\n## [1] 26\n\n\n\nNenner (von Bruch 1)\nDer Nenner des ersten Bruches (\\({\\sum_{i=1}^n (y_i - \\bar{y})^2}\\)) ist sehr ähnlich der Berechnung der Varianz:\n\nBerechne den Durchschnitt aller Messwerte (\\(\\bar{y}\\))\nBerechne für jeden Messwert die Differenz zum Durchschnitt (\\(y_i - \\bar{y}\\))\nQuadriere diese Werte \\((y_i - \\bar{y})^2\\)\nSummiere die Quadrierten Werte \\(\\sum_{i=1}^n\\)\n\nAlso berechnen wir zuerst diese Differenzwerte (Messwert minus Mittelwert):\n\n# Die Werte aller Kantone:\ny <- zweitwohnung_kanton$ja_in_percent\n\n# Der Durchschnittswert aller Kantone\nybar <- mean(y, na.rm = TRUE)\n\n# von jedem Wert den Durchschnittswert abziehen:\ndy <- y - ybar\n\nWelche dieser Zwischenresultate sind Einzelwerte und welche Vektoren? Nun quadrieren wir die Differenzen:\n\ndy_2 <- dy^2\n\nund summieren die Differenzen:\n\ndy_sum <- sum(dy_2, na.rm = TRUE)\n\n\n\nAuflösung (Bruch 1)\nBeschliessen wir die Bearbeitung des ersten Bruchs indem wir den Zähler durch den Nennen dividieren: n durch dy_sum.\n\nvr <- n/dy_sum\n\n\n\n\nBruch 2\nWenden wir uns nun also dem Bruches der Formel zu.\n\\[\\frac{\\sum_{i=1}^n \\sum_{j=1}^n w_{ij}(y_i - \\bar{y})(y_j - \\bar{y})}{\\sum_{i=1}^n \\sum_{j=1}^n w_{ij}}\\]\nHier berechnen wir die Summe aller Gewichte sowie die gewichteten Covarianzen. Wir betrachten immer Messwertpaare, sprich paarweise Vergleiche zweier Raumeinheiten (hier Kantone). Deshalb haben die zwei Summenzeichen die beiden unterschiedlichen Laufvariablen (\\(i\\) und \\(j\\)). Solche paarweise Vergleiche von Werten mit allen anderen Werten können wir elegant mit Kreuzmatrizen abbilden. In der Kreuzmatrix vergleichen wir jeden Messwert mit allen anderen Messwerten. Dabei gibt es zwei Kreuzmatrizen: (\\(w_{ij}\\) ist die erste Kreuzmatrix, \\((y_i - \\bar{y})(y_j - \\bar{y})\\) ist die zweite Kreuzmatrix).\n\nZähler (Bruch 2)\nDer erste Term, \\(w_{ij}\\), beschreibt die räumlichen Gewichte aller Kantone. Sind die Kantone benachbart, dann gilt ein Gewicht von 1, sind sie nicht benachbart, gilt ein Gewicht von 0. Dies entspricht dem Schalter aus der Vorlesung.\nWie wir “benachbart” definieren ist nicht festgelegt. Denkbar wären zum Beispiel folgende Optionen:\n\nDie Kantone müssen sich berühren (dürfen sich aber nicht überlappen): st_touches()\nDie Kantone müssen innerhalb einer bestimmten Distanz zueinander liegen: st_is_within_distance()\nDie Kantone müssen überlappen: st_overlaps()\n\nEgal für welche Variante Ihr Euch entscheidet, setzt sparse = FALSE damit eine Kreuzmatrix erstellt wird.\n\nw <- st_touches(zweitwohnung_kanton, sparse = FALSE)\n\nw[1:6, 1:6]\n##       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]\n## [1,] FALSE FALSE FALSE FALSE  TRUE FALSE\n## [2,] FALSE FALSE  TRUE  TRUE FALSE  TRUE\n## [3,] FALSE  TRUE FALSE FALSE  TRUE  TRUE\n## [4,] FALSE  TRUE FALSE FALSE  TRUE  TRUE\n## [5,]  TRUE FALSE  TRUE  TRUE FALSE FALSE\n## [6,] FALSE  TRUE  TRUE  TRUE FALSE FALSE\n\n(Lasst Euch nicht davon beirren, dass wir nun TRUE und FALSE statt 1 und 0 haben. In R sind TRUE und 1 äquivalent, sowie auch FALSE und 0).\n\n\n\nZur Überprüfung unserer Operation: Mit w[1,] bekommt ihr ein Vektor, wo bei allen Kantone, die den ersten kanton (Zürich) berühren TRUE steht und bei allen anderen FALSE. Nun können wir überprüfen, ob die räumliche Operation funktioniert hat.\n\nberuehrt_1 <- w[1, ]\n\nggplot(zweitwohnung_kanton[beruehrt_1, ]) +\n  geom_sf(aes(fill = KANTONSNAME)) +\n  labs(title = \"Welche Kanton berühren den Kanton Zürich (st_touches)\")\n\n\n\n\nDer nächste Teil sollte Euch nun bekannt vorkommen. Die Differenz aller Werte vom Mittelwert aller Werte \\((y_i - \\bar{y})\\) kennen wir schon vom ersten Bruch und haben wir auch bereits gelöst. Nun gilt es paarweise das Produkt der Abweichungen vom Mittelwert (die Covarianz) zu berechnen \\((y_i - \\bar{y})(y_j - \\bar{y})\\). DAzu müssen wir das Produkt aller Wertekombinationen berechnen. Dies erreichen wir mit der Funktion tcrossprod():\n\npm <- tcrossprod(dy)\npm[1:6,1:6]\n##               [,1]         [,2]          [,3]         [,4]         [,5]\n## [1,]  0.0008726497  0.001597812 -0.0006495424 -0.003249747 -0.001984912\n## [2,]  0.0015978120  0.002925576 -0.0011893051 -0.005950251 -0.003634352\n## [3,] -0.0006495424 -0.001189305  0.0004834762  0.002418896  0.001477437\n## [4,] -0.0032497469 -0.005950251  0.0024188956  0.012102055  0.007391811\n## [5,] -0.0019849120 -0.003634352  0.0014774366  0.007391811  0.004514842\n## [6,] -0.0023882557 -0.004372870  0.0017776588  0.008893862  0.005432280\n##              [,6]\n## [1,] -0.002388256\n## [2,] -0.004372870\n## [3,]  0.001777659\n## [4,]  0.008893862\n## [5,]  0.005432280\n## [6,]  0.006536145\n\nNun multiplizieren wir die Covarianzen mit den Gewichten \\(w\\) (Schalter), damit wir nur noch die Werte von den Kantonen haben, die auch effektiv benachbart sind (und eliminieren nicht-benachbarte Werte). Beachtet dass wir hier nun eine Matrix mit einer Matrix multiplizieren.\n\npmw <- pm * w\nw[1:6,1:6]\n##       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]\n## [1,] FALSE FALSE FALSE FALSE  TRUE FALSE\n## [2,] FALSE FALSE  TRUE  TRUE FALSE  TRUE\n## [3,] FALSE  TRUE FALSE FALSE  TRUE  TRUE\n## [4,] FALSE  TRUE FALSE FALSE  TRUE  TRUE\n## [5,]  TRUE FALSE  TRUE  TRUE FALSE FALSE\n## [6,] FALSE  TRUE  TRUE  TRUE FALSE FALSE\npmw[1:6,1:6]\n##              [,1]         [,2]         [,3]         [,4]         [,5]\n## [1,]  0.000000000  0.000000000  0.000000000  0.000000000 -0.001984912\n## [2,]  0.000000000  0.000000000 -0.001189305 -0.005950251  0.000000000\n## [3,]  0.000000000 -0.001189305  0.000000000  0.000000000  0.001477437\n## [4,]  0.000000000 -0.005950251  0.000000000  0.000000000  0.007391811\n## [5,] -0.001984912  0.000000000  0.001477437  0.007391811  0.000000000\n## [6,]  0.000000000 -0.004372870  0.001777659  0.008893862  0.000000000\n##              [,6]\n## [1,]  0.000000000\n## [2,] -0.004372870\n## [3,]  0.001777659\n## [4,]  0.008893862\n## [5,]  0.000000000\n## [6,]  0.000000000\n\nDen Zähler des ersten Bruches können wir nun fertig berechnen, indem wir die Summe aller gewichten (sprich eingeschalteten) Werten bilden:\n\nspmw <- sum(pmw, na.rm = TRUE)\nspmw\n## [1] 0.2007517\n\n\n\nNenner (Bruch 2)\nFür den Nenner des zweiten Teils der Formal (des zweiten Bruchs) müssen wir nun nur noch alle Gewichte summieren. Diese Summer entspricht der Anzahl effektiv benachbarter Kantone und kann Anzahl der \\(TRUE\\)-Werte in \\(w\\) bestimmt werden.\n\nsmw <- sum(w, na.rm = TRUE)\n\n\n\nAuflösung (Bruch 2)\nSo können wir den zweiten Bruch auflösen und berechnen:\n\nsw  <- spmw / smw\n\n\n\n\nAuflösung der Formel\nDer allerletzte Schritt besteht darin, die Werte aus den beiden Brüche miteinander zu multiplizieren.\n\nMI <- vr * sw\nMI\n## [1] 0.3148631\n\nDer Global Morans \\(I\\) für die Abstimmungsdaten beträgt auf Kantonsebene also 0.3148631. Wie interpretiert ihr dieses Resultate? Was erwartet ihr für eine Resultat auf Gemeinde- oder Bezirksebene?"
  },
  {
    "objectID": "rauman/Rauman3_Uebung_A.html#aufgabe-2-morans-i-für-gemeinde-oder-bezirke-berechnen",
    "href": "rauman/Rauman3_Uebung_A.html#aufgabe-2-morans-i-für-gemeinde-oder-bezirke-berechnen",
    "title": "Rauman 3: Übung A",
    "section": "Aufgabe 2: Morans I für Gemeinde oder Bezirke berechnen",
    "text": "Aufgabe 2: Morans I für Gemeinde oder Bezirke berechnen\nNun könnt ihr Morans \\(I\\) auf der Ebene der Gemeine oder Bezirke und untersuchen, ob und wie sich Morans \\(I\\) verändert. Wenn ihr einen wenig leistungsfähigen Rechner habt, berechnet verwendet besser die Ebene “Berzirke”. Importiert dazu den Layer bezrik oder gemeinde aus dem Datensatz zweitwohnungsinitiative.gpkg. Visualisiert in einem ersten Schritt die Abstimmungsresultate.\n\nzweitwohnung_gemeinde <- read_sf(here(\"data\",\"zweitwohnungsinitiative.gpkg\"), \"gemeinde\")\n\nggplot(zweitwohnung_gemeinde) +\n  geom_sf(aes(fill = ja_in_percent), colour = \"white\",lwd = 0.2) +\n  scale_fill_gradientn(\"Ja Anteil\",colours = RColorBrewer::brewer.pal(11, \"RdYlGn\"), limits = c(0,1)) +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "rauman/Rauman3_Uebung_A.html#musterlösung",
    "href": "rauman/Rauman3_Uebung_A.html#musterlösung",
    "title": "Rauman 3: Übung A",
    "section": "Musterlösung",
    "text": "Musterlösung"
  },
  {
    "objectID": "rauman/Rauman4_Uebung_A_ahp.html",
    "href": "rauman/Rauman4_Uebung_A_ahp.html",
    "title": "Rauman 4: Übung A",
    "section": "",
    "text": "Now that you have learned the theory, you will carry out concrete example of an Analytical Hierarchy Process (AHP). This is a manual approach to show you the basics of an AHP. If you want to build a more complex AHP you can use specific R AHP packages such as the ahpsurvey package."
  },
  {
    "objectID": "rauman/Rauman4_Uebung_A_ahp.html#exercise-1-define-initial-situation",
    "href": "rauman/Rauman4_Uebung_A_ahp.html#exercise-1-define-initial-situation",
    "title": "Rauman 4: Übung A",
    "section": "Exercise 1: Define initial situation",
    "text": "Exercise 1: Define initial situation\nFirst think of an actual decision you are currently facing or have previously faced (e.g. buying a bicycle or renting an apartment) and define the following points.\n\nA goal for your AHP (e.g. Buy a bike)\n4 criteria on which you want to base your decision (e.g. Price, Distance to Work / School, Size, Scenic Beauty)\n3 different options / alternatives (e.g. 3 different apartments)"
  },
  {
    "objectID": "rauman/Rauman4_Uebung_A_ahp.html#exercise-2-pairwise-comparison-paarweiser-vergleich-1-2",
    "href": "rauman/Rauman4_Uebung_A_ahp.html#exercise-2-pairwise-comparison-paarweiser-vergleich-1-2",
    "title": "Rauman 4: Übung A",
    "section": "Exercise 2: Pairwise comparison (Paarweiser Vergleich 1 & 2)",
    "text": "Exercise 2: Pairwise comparison (Paarweiser Vergleich 1 & 2)\nIn a first step each criterion needs to be compared with another criteria in pairs. Use the following scale for weighting the criteria (see table @ref(tab:ahprating)) .\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\nScale for weighting the criteria.\n\n\nRating\nDefinition\n\n\n\n\n1\nThe two characteristics are equally important\n\n\n3\nCriteria A is slightly more important than criteria B\n\n\n5\nCriteria A is moderately more important than criteria B\n\n\n7\nCriteria A is strongly more important than criteria B\n\n\n9\nCriteria A is absolutely more important than criteria B\n\n\n2, 4, 6, 8\nIntermediate Values\n\n\n\n\n\nYou can use the following code to create your weighting matrix. In the matrix, two criteria are always compared twice, and these two comparisons should be the reciprocal (“Kehrwert”) of each other. To illustrate this, we have added one comparison which reads as follows:\n\nRow 1, column 2: Criteria 1 is slightly more important than Criteria 2\nRow 2, column 1: Criteria 2 is slightly less important than Criteria 1\n\nCreate this matrix comparison matching your criteria, replacing the 0 values with your weights according to table @ref(tab:ahprating). Note that all diagonal values should equal to 1.\n\npairwise_comparison <- c(\n  1,   3, 0, 0,\n  1/3, 1, 0, 0,\n  0,   0, 1, 0,\n  0,   0, 0, 1\n) %>% matrix(ncol = 4, byrow = TRUE) \n\n\n\n\nTip: Add column and row names so your matrix is more readable.\n\ncriterias <- c(\"price\", \"distance\",\"size\", \"beauty\")\n\nrownames(pairwise_comparison) <- criterias\ncolnames(pairwise_comparison) <- criterias"
  },
  {
    "objectID": "rauman/Rauman4_Uebung_A_ahp.html#exercise-3-calculation-of-the-criteria-weights",
    "href": "rauman/Rauman4_Uebung_A_ahp.html#exercise-3-calculation-of-the-criteria-weights",
    "title": "Rauman 4: Übung A",
    "section": "Exercise 3: Calculation of the criteria weights",
    "text": "Exercise 3: Calculation of the criteria weights\n\nExercise 3.1: Normalization of matrix (Berechnung der Kritiriengewichte 1)\nIn the next step the matrix needs to be normalized (see figure @ref(fig:criteria-normalize). You can do this in the following two steps:\n\nCalculate the sum of each column using colSums. Store the output in a variable (e.g. ahp_colsums).\nDivide each value in the matrix by the corresponding column sum. To achieve this, you can use the sweep() function on the matrix, which is very similar to apply (use MARGIN = 2 (columns), STATS = ahp_colsums and FUN = \"/\").\n\n\n\n\n\n\n\nExercise 3.2: Weighting of criteria (Berechnung der Kritiriengewichte 2)\nThis is the final step to calculate the weight of each criteria (see @ref(fig:weights-normalize)). To do so:\n\ncalculate the sum of each row and store the output in a variable (e.g. criteria_sum).\ndivide the criteria_sum by the sum of criteria_sum and store the output in a variable (e.g. criteria_weight).\n\nNote: The sum of criteria_weight should equal to 1"
  },
  {
    "objectID": "rauman/Rauman4_Uebung_A_ahp.html#exercise-4-consistency-analysis-konsistenzanalyse-1-2",
    "href": "rauman/Rauman4_Uebung_A_ahp.html#exercise-4-consistency-analysis-konsistenzanalyse-1-2",
    "title": "Rauman 4: Übung A",
    "section": "Exercise 4: Consistency analysis (Konsistenzanalyse 1 & 2)",
    "text": "Exercise 4: Consistency analysis (Konsistenzanalyse 1 & 2)\nAfter the pairwise comparison is done, a consistency analysis needs to be performed to check whether the pairwise comparisons are consistent or include contradictions. A certain inconsistency is allowed within the framework of an AHP, but it should not be too great.\nTo calculate consistency, you should proceed as explained in Slide 30 (Konsistenzanalyse 1) and the following steps:\n\ndo a matrix multiplication (%*%) between pairwise_comparison and criteria_weight.\n\n\n\n\n\nDivide the result of 1) by criteria_weight\n\n\n\n\n\nCalculate \\(\\lambda_{max}\\) by dividing the sum of the result you obtained in 2) by the number of criteria\n\n\n\n\n\ncalculate \\(CI\\) (\\(CI = \\frac{\\lambda_{max} - n}{n-1}\\)), where n equals to the number of criteria\n\n\n\n\n\nDetermine \\(RI\\) by consulting the table @ref(fig:randomindexbysaaty)\n\n\n\n\n\nCalculate \\(CR\\) (\\(CR = CI / RI\\))\n\n\n\n\n\nIf CR > 0.1, you will need to re-evaluate your pairwise comparisons."
  },
  {
    "objectID": "rauman/Rauman4_Uebung_A_ahp.html#congratulations",
    "href": "rauman/Rauman4_Uebung_A_ahp.html#congratulations",
    "title": "Rauman 4: Übung A",
    "section": "Congratulations!",
    "text": "Congratulations!\nYou now have determined the weights on top of which you can build your decision and have determined if these weights are consistent or not. These next steps are technically very similar to what you did in the exercise above, so we will leave it up to you if you want to complete these steps or not. For sake of completness, the next step would be to:\n\ncompare your options / alternatives with each other in a pairwise comparison (similar as to how you compared the criteria with each other). You do this for every criteria\nnormalize your pairwise comparisons of your options (similar as to how how you normalized the pairwise comparisons of the criteria)\nUse the weights you determined in the exercise above to weigh your results from 2)\nAsses the best decision based on the result from 3)"
  },
  {
    "objectID": "rauman/Rauman4_Uebung_B_raster.html",
    "href": "rauman/Rauman4_Uebung_B_raster.html",
    "title": "Rauman 4: Übung B",
    "section": "",
    "text": "One of the important aspects of the upcoming exercise (Multi-Criteria Evaluation (MCE)) is the use and manipulation of raster datasets. In R, two are the main packages used to handle raster data: terra and raster package. The latter is still heavily used, but inevitably is going to be replaced by the first one.\nBelow we will use terra to demonstrate how we can import a raster dataset. In the link below you can download a tif file, representing the Digital Elevation Model (Digitales Höhenmodell, DHM) of Canton Schwyz in Switzerland. Download the dataset and repeat the code provided.\n\nlibrary(terra)\n\nImport your raster with the function rast\n\ndhm_schwyz <- rast(here(\"data\",\"dhm25m.tif\"))\n\nYou get some important metadata on the raster dataset when you type the variable name in the console.\n\ndhm_schwyz \n\nclass       : SpatRaster \ndimensions  : 1496, 1861, 1  (nrow, ncol, nlyr)\nresolution  : 25, 25  (x, y)\nextent      : 672187.5, 718712.5, 193662.5, 231062.5  (xmin, xmax, ymin, ymax)\ncoord. ref. : CH1903 / LV03 \nsource      : dhm25m.tif \nname        : dhm25m \n\n\nTo get a quick look at the raster dataset, we can simply use either of the following plot() function:\n\nplot(dhm_schwyz)\n\n\n\n\nUnfortunately, adding raster to ggplot is not very straightforward. Since ggplot is a universal plotting framework we quickly reach the limits of what is possible when creating something as specialized as maps. For this reason, we will introduce a new plotting framework which is specialized on maps and was built in a very similar design as ggplot: tmap. Install and load this package now.\n\nlibrary(tmap)\n\nJust as ggplot, tmap is based on the idea of “layers” that are joined using a +. Each layer has two components:\n\na dataset component which is always tm_shape(dataset) (replace dataset with your variable)\na geometry component which describes how the preceeding tm_shape() should be visualized. This can be tm_dots() for points, tm_polygons() for polygons, tm_lines() for lines etc. For single band raster (which is the case for dhm_schwyz), it is tm_raster()\n\n\ntm_shape(dhm_schwyz) + \n  tm_raster() \n\nstars object downsampled to 1115 by 896 cells. See tm_shape manual (argument raster.downsample)\n\n\n\n\n\nNote that tm_shape() and tm_raster() (in this case) belong together, one cannot live without the other.\nIf you consult the help of ?tm_raster you will see a multitude of options with which to change the visualisation of your data. For example, the default style of tm_raster() is to create “bins” of the data with a descrete colour scale. We can override this using style = \"cont\"\n\ntm_shape(dhm_schwyz) + \n  tm_raster(style = \"cont\") \n\nstars object downsampled to 1115 by 896 cells. See tm_shape manual (argument raster.downsample)\n\n\n\n\n\nThis already looks pretty awesome, but maby we want to change the default colour palette. Fortunately, this is much simpler in tmap than in ggplot2. To look at the available palettes, type tmaptools::palette_explorer() or RColorBrewer::display.brewer.all() in the console (the former might require you to install additional packages, e.g. shinyjs).\n\ntm_shape(dhm_schwyz) + \n  tm_raster(style = \"cont\", palette = \"Spectral\") \n\nstars object downsampled to 1115 by 896 cells. See tm_shape manual (argument raster.downsample)\n\n\n\n\n\nYou can make layout adjustments using tm_layout(), check ?tm_layout to see all of the options available!\n\n\ntm_shape(dhm_schwyz) + \n  tm_raster(style = \"cont\", palette = \"Spectral\", legend.is.portrait = FALSE, title = \"\") +\n  tm_layout(legend.outside = TRUE, legend.outside.position = \"bottom\",frame = FALSE)\n\nstars object downsampled to 1115 by 896 cells. See tm_shape manual (argument raster.downsample)"
  },
  {
    "objectID": "rauman/Rauman5_Uebung_mce.html",
    "href": "rauman/Rauman5_Uebung_mce.html",
    "title": "Rauman 5: Übung",
    "section": "",
    "text": "The following exercise may seem familiar to some of you, if you have attended the bachelor program. This MCE was already conducted in the bachelor module “GIS” by using ArcGIS Pro and its ModelBuilder (see figure @ref(fig:figmce)).\nThe goal of this exercise is to do the same MCE by just using R. We will mainly use functions from the R packages sf and raster. Please have a look at the process model (see figure @ref(fig:figmce)) that was created in ArcGIS Pro and try to figure out which functions in the two R packages correspond to the ones in the model. You will see that there are some similar functions available, but for some calculations a different approach is needed."
  },
  {
    "objectID": "rauman/Rauman5_Uebung_mce.html#exercises-1-load-and-view-data",
    "href": "rauman/Rauman5_Uebung_mce.html#exercises-1-load-and-view-data",
    "title": "Rauman 5: Übung",
    "section": "Exercises 1: Load and view data",
    "text": "Exercises 1: Load and view data\n\nAttaching package: 'dplyr'\nThe following object is masked from 'package:kableExtra':\n\n    group_rows\nThe following objects are masked from 'package:stats':\n\n    filter, lag\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\nData used in this study (click on the link to download)\n \n  \n    datasets \n    description \n    type \n    res \n    geometry \n    crs \n    exclusion_area \n  \n \n\n  \n    dhm25m.tif \n    Terrain model (m) \n    Raster \n    25m \n     \n    CH1903/LV03 \n    No \n  \n  \n    eis25m.tif \n    Icing frequency (days/year) \n    Raster \n    25m \n     \n    CH1903/LV03 \n    No \n  \n  \n    wind25m.tif \n    Average wind speed (dm/s) \n    Raster \n    25m \n     \n    CH1903/LV03 \n    No \n  \n  \n    Bewohnte_Flaeche.gpkg \n    Settlements (incl. buffer 200m) \n    Geopackage \n     \n    Polygon \n    CH1903/LV03 \n    (Yes)/Distance \n  \n  \n    Nationale_Schutzgebiete.gpkg \n    National protection areas \n    Geopackage \n     \n    Polygon \n    CH1903/LV03 \n    (Yes)/Distance \n  \n  \n    Seeflaechen.gpkg \n    Lake areas \n    Geopackage \n     \n    Polygon \n    CH1903/LV03 \n    Yes \n  \n  \n    Strassen.gpkg \n    Streets \n    Geopackage \n     \n    Line \n    CH1903/LV03 \n    No/Distance \n  \n  \n    Untersuchungsgebiet_Schwyz.gpkg \n    Study area, canton of Schwyz \n    Geopackage \n     \n    Polygon \n    CH1903/LV03 \n    No \n  \n  \n    Waldgebiete.gpkg \n    Forest areas \n    Geopackage \n     \n    Polygon \n    CH1903/LV03 \n    (Yes)/Distance \n  \n\n\n\n\nThe raster data can be loaded by using the function terra::rast and the vector data with sf::read_sf. View the available data layers (see Table \\(\\ref{tab:datatable}\\)) and plot them in an appealing way. For visualization you can use the functions plot for raster data and ggplot for vector data.\n\nWe will use the following packages in this exercise:\n\nlibrary(sf)\n\nLinking to GEOS 3.8.0, GDAL 3.0.4, PROJ 6.3.1; sf_use_s2() is TRUE\n\nlibrary(terra)\n\nterra 1.6.7\n\nlibrary(dplyr)\nlibrary(tmap)"
  },
  {
    "objectID": "rauman/Rauman5_Uebung_mce.html#exercise-2-merge-exclusion-criteria",
    "href": "rauman/Rauman5_Uebung_mce.html#exercise-2-merge-exclusion-criteria",
    "title": "Rauman 5: Übung",
    "section": "Exercise 2: Merge exclusion criteria",
    "text": "Exercise 2: Merge exclusion criteria\nMerge the exclusion criteria settlement areas, national protected areas, lake areas and forest areas. These vector data sets are structured as data frames and therefore can be merged by simply combining them. Keep in mind that the data frames have different sizes. Additionally, we need to create a raster out of the newly created vector data set (exclusion area). For that you can use the function rasterize. The output should be a raster with 0 and 1, where the fields of the exclusion area have values of 0 and the remaining fields have values of 1 (see Figure (fig:figrasterizeexclusioncriteria)).\n\nTip: To achieve a raster with only 0 and 1 use the rasterize options field = 0 and background = 1.\nTip: In order to rasterize vector data, you need to create an empty raster beforehand. This raster should have the same boundaries (extent), resolution and coordinate system (crs) as the other raster sets. Use the following code to do so.\n\n\nr <- terra::rast(ext(kt_schwyz), \n          resolution = c(250, 250), \n          crs = \"EPSG:21781\")"
  },
  {
    "objectID": "rauman/Rauman5_Uebung_mce.html#exercise-3-calculate-slope",
    "href": "rauman/Rauman5_Uebung_mce.html#exercise-3-calculate-slope",
    "title": "Rauman 5: Übung",
    "section": "Exercise 3: Calculate slope",
    "text": "Exercise 3: Calculate slope\nNext, calculate the slope in degrees based on the terrain model (dhm25m). The terra package gives you a very helpful function called terrain.\nTip: When using the terrain function use the following options: v=“slope”, unit=“degrees”, neighbors=8."
  },
  {
    "objectID": "rauman/Rauman5_Uebung_mce.html#exercise-4-calculate-distances-to-criteria",
    "href": "rauman/Rauman5_Uebung_mce.html#exercise-4-calculate-distances-to-criteria",
    "title": "Rauman 5: Übung",
    "section": "Exercise 4: Calculate distances to criteria",
    "text": "Exercise 4: Calculate distances to criteria\nWithin the evaluation of suitable sites for wind turbines, the distance to roads, forest areas, national protected areas and inhabited areas are relevant. Depending on the criteria, a short or long distance has a positive influence on the evaluation of potential sites. For this purpose, perform a distance analysis with the selected criteria raster layers. Use the raster function distance for this calculation.\n\nTip: In order to perform the distance function, you need to rasterize the criteria as well. You can use the same command as in exercise 2 but use only option field = 1.\nTip: Use the crop function again to get only relevant data in the study area."
  },
  {
    "objectID": "rauman/Rauman5_Uebung_mce.html#exercise-5-standardize-and-grade-criteria-grading",
    "href": "rauman/Rauman5_Uebung_mce.html#exercise-5-standardize-and-grade-criteria-grading",
    "title": "Rauman 5: Übung",
    "section": "Exercise 5: Standardize and grade criteria (grading)",
    "text": "Exercise 5: Standardize and grade criteria (grading)\nThe data layers slope, wind speed, icing frequency and the in exercise 4 calculated distance layers have different units (dm/s, degrees, d/yr and m). These units can’t be directly calculated with each other. Therefore, the different layers need to be operationalized by performing a linear grading. The linear grading is done by using the function reclassify. Use the standards for the reclassification in Figure (fig:figreclassify).\n\nTip: Keep in mind the min and max values of each raster layer.\nTip: Here is an example code to reclassify the distances to settlements.\n\n\nsettlements_max <- minmax(settlements_ed)[2]\n\nreclass_settlements <- c(0,80,0,\n                        80,160,0.1,\n                        160,240,0.2,\n                        240,320,0.3,\n                        320,400,0.4,\n                        400,480,0.5,\n                        480,560,0.6,\n                        560,640,0.7,\n                        640,720,0.8,\n                        720,800,0.9,\n                        800,settlements_max,1.0) %>% matrix(ncol = 3, byrow = TRUE)\nreclass_settlements_ed <- terra::classify(settlements_ed, reclass_settlements)"
  },
  {
    "objectID": "rauman/Rauman5_Uebung_mce.html#exercise-6-weighting-criteria-with-ahp",
    "href": "rauman/Rauman5_Uebung_mce.html#exercise-6-weighting-criteria-with-ahp",
    "title": "Rauman 5: Übung",
    "section": "Exercise 6: Weighting criteria with AHP",
    "text": "Exercise 6: Weighting criteria with AHP\nPerform an AHP to weight the criteria underlying the MCE. First compare the criteria in pairs, and then calculate the weights - as you have learned in last week’s lesson. In the end you should have a list of 7 weights as shown below.\nTip: Check exercise 2 and 3 from last week’s session. Use the prepared R code to create your ahp matrix.\n      Wind       Streets          Ice    Settlements       Forest         Slope  Protected areas \n0.33862692    0.09816760    0.06166626    0.24969460    0.03515759    0.18043000      0.03625702 \n\nahp_matrix <- c(\n  1, 0, 0, 0, 0, 0, 0, #Wind\n  0, 1, 0, 0, 0, 0, 0, #Distance to streets\n  0, 0, 1, 0, 0, 0, 0, #Ice\n  0, 0, 0, 1, 0, 0, 0, #Distance to settlements\n  0, 0, 0, 0, 1, 0, 0, #Distance to forests\n  0, 0, 0, 0, 0, 1, 0, #Slope\n  0, 0, 0, 0, 0, 0, 1  #Distance to protected areas\n) %>% matrix(ncol = 7, byrow = TRUE)"
  },
  {
    "objectID": "rauman/Rauman5_Uebung_mce.html#exercise-7-weighted-overlay",
    "href": "rauman/Rauman5_Uebung_mce.html#exercise-7-weighted-overlay",
    "title": "Rauman 5: Übung",
    "section": "Exercise 7: Weighted Overlay",
    "text": "Exercise 7: Weighted Overlay\nThe linearly ranked criteria (Exercise 5) are now to be combined with each other, taking into account the weighting determined using the AHP (Exercise 6). This weighted overlay can be performed by using raster calculations and simply multiplying each criteria with its weight and adding them together (see figure @ref(fig:figweightedoverlay)).\n\nTip: As the raster sets have slightly different origins, increase the tolerance by using rasterOptions(tolerance = 0.5).\nTip: Also keep in mind the order of the weights in your list when doing the multiplication."
  },
  {
    "objectID": "rauman/Rauman5_Uebung_mce.html#exercise-8-intersecting-potential-areas-with-exclusion-criteria",
    "href": "rauman/Rauman5_Uebung_mce.html#exercise-8-intersecting-potential-areas-with-exclusion-criteria",
    "title": "Rauman 5: Übung",
    "section": "Exercise 8: Intersecting potential areas with exclusion criteria",
    "text": "Exercise 8: Intersecting potential areas with exclusion criteria\nBy simply multiplying the exclusion layer (result from exercise 2) with the weighted overlay layer (result from exercise 7) we are excluding all areas with value 0 (exclusion areas) and keeping all other areas with value 1 (e.g. 0x3=0, 1x3=3). As a conclusion of the study, create a final plot of the potential areas for wind power plants in the canton of Schwyz (like figure @ref(fig:figfinalresults)). Furthermore, discuss the results obtained and determine three possible locations within which concrete planning of wind power plants would be possible."
  },
  {
    "objectID": "rauman/Rauman5_Uebung_mce.html#musterlösung",
    "href": "rauman/Rauman5_Uebung_mce.html#musterlösung",
    "title": "Rauman 5: Übung",
    "section": "Musterlösung",
    "text": "Musterlösung\nR-Code"
  },
  {
    "objectID": "fallstudie_s/1_Einleitung.html#hintergrund",
    "href": "fallstudie_s/1_Einleitung.html#hintergrund",
    "title": "1. Einleitung",
    "section": "Hintergrund",
    "text": "Hintergrund\nDas rund 1100 ha grosse Naturschutzgebiet Wildnispark Zürich Sihlwald, welches im periurbanen Raum südlich von Zürich liegt, gilt seit dem 1. Januar 2010 als erster national anerkannter Naturerlebnispark. Er ist Teil des Wildnisparks Zürich. Seine Rolle als Naherholungsgebiet für die Stadt Zürich ist von grosser Bedeutung.\nIm Perimeter gelten verschiedene Regeln. So darf z. B. nur auf bestimmten Wegen mit den Velo gefahren und Hunde müssen an der Leine geführt werden. Damit soll im Schutzgebiet die Balance zwischen Schutz und Nutzen bewahrt werden, denn einerseits sollen die Besuchenden den Wald erleben dürfen, andererseits soll sich dieser, in der Kernzone, frei entwicklen dürfen.\n\nDamit diese Balance erreicht werden kann, muss das Management auf solide, empirisch erhobene Daten zur Natur und zu den Besuchenden zurückgreifen können. Das Besuchermonitoring deckt den zweiten Teil dieser notwendigen Daten ab.\nIm Wildnispark Zürich sind dazu mehrere automatische Zählstellen in Betrieb. Die Zählstellen erfassen stundenweise Besuchende. Einige Zählstellen erfassen richtungsgetrennt und / oder können zwischen verschiedenen Nutzergruppen wie Personen, die zu Fuss gehen, und Velofahrenden unterscheiden.\nIm Rahmen des Moduls Research Methods werden in der Fallstudie Profil S mehrere dieser automatischen Zählstellen genauer untersucht. Die Daten, welche im Besitz des WPZ sind, wurden bereits kalibriert. Das heisst, Zählungen während Wartungsarbeiten, bei Felhbetrieb o.ä. wurden bereits ausgeschlossen. Dies ist eine Zeitintensive Arbeit und wir dürfen hier mit einem wahren, sauber aufbereiteten “Datenschatz” arbeiten.\nPerimeter des Wildnispark Zürichs mit den ungefähren Standorten von drei ausgewählten automatischen Zählstellen.\n\n\n\n\nHinweis:\n\nDie Zähler 211 und 502 erfassen sowohl Fussgänger:innen als auch Fahrräder. Die Erfassung erfolgt Richtungsgetrennt.\nDer Zähler 204 kann nicht zwischen Nutzungsarten unterscheiden; er erfasst alle Passagen auf den Hochwachtturm als Fussgänger:innen. Der Sensror hat keine Richtungserkennung und die Besuchenden werden jeweils 2x gezählt, einmal beim Aufstieg und einmal beim Abstieg. Das ist im Kalibrierunsgfaktor berücksichtigt, die Kalibrierte Zahl gibt also die Anzahl der Turmbesuche an.\n\nDer Wildnispark wertet die Zahlen auf verschiedene Weise aus. So sind z. B. Jahresgänge (an welchen Monaten herrscht besonders viel Betrieb) und Nutzungszahlen bekannt. Vertiefte Auswertungen, die beispielsweise den Zusammenhang zwischen Besuchszahlen und dem Wetter untersuchen, werden nicht gemacht, da dies die Kapazitäten übersteigen würde.\nUnsere Analysen in diesem Modul helfen dem Management, ein besseres Verständnis zum Verhalten der Besuchenden zu erlangen und bilden Grundlagen für Managemententscheide in der Praxis."
  },
  {
    "objectID": "fallstudie_s/1_Einleitung.html#ziele",
    "href": "fallstudie_s/1_Einleitung.html#ziele",
    "title": "1. Einleitung",
    "section": "Ziele",
    "text": "Ziele\n\nIn dieser Fallstudie zeigen wir, welche Einfluss der Lockdown während der Covid19-Pandemie im Frühjahr 2020 sowie jener im Winter 2020/2021 auf die täglichen Besuchszahlen im Wildnispark Zürich hatte.\nErgänzend beschreiben wir den Zusammenhang der Besuchszahlen mit verschiedenen Wetterparametern. Die Hypothese lautet, je mehr Sonnenstunden und je höher die Temperatur, desto mehr Besuchende sind im Untersuchungsgebiet unterwegs; je mehr Niederschlag gemessen wird, desto weniger Besuchende werden gezählt.\nDa neben dem Wetter aber auch saisonale Muster, wie z.B. Schulferien, einen grossen Einfluss auf Besuchszahlen haben können, ziehen wir diese und weitere Parameter (Wochentage, Kalenderwoche, Jahr) ebenfalls in die Auswertung ein.\nDiese kombinierte, statistisch schliesssende, Betrachtung erlaubt uns Aussagen darüber, ob “nur” aufgrund des schönen Frühlings 2021 mehr Menschen in Wald unterwegs waren, oder ob der Lockdown tatsächlich einen so deutlich positiven Einfluss auf die Besuche hatte."
  },
  {
    "objectID": "fallstudie_s/1_Einleitung.html#grundlagen",
    "href": "fallstudie_s/1_Einleitung.html#grundlagen",
    "title": "1. Einleitung",
    "section": "Grundlagen",
    "text": "Grundlagen\nZur Verfügung stehen:\n\ndie stündlichen, richtungsgetrennten Zählungen von Fussgänger:innen und Velos an drei Zählstellen\nMeteodaten (Temperatur, Sonnenscheindauer, Niederschlagssumme)\nR-Skripte mit Hinweisen zur Auswertung"
  },
  {
    "objectID": "fallstudie_s/2_Felderhebung_Uebung.html#ziele",
    "href": "fallstudie_s/2_Felderhebung_Uebung.html#ziele",
    "title": "2: Felderhebung Übung",
    "section": "Ziele",
    "text": "Ziele\n\nDie Studierenden können das eingesetzte Gerät installieren und kennen die Vor- und Nachteile verschiedener Methoden.\nSie können die Daten auslesen und explorativ analysieren."
  },
  {
    "objectID": "fallstudie_s/2_Felderhebung_Uebung.html#grundlagen",
    "href": "fallstudie_s/2_Felderhebung_Uebung.html#grundlagen",
    "title": "2: Felderhebung Übung",
    "section": "Grundlagen",
    "text": "Grundlagen\nDie Geräte werden innerhalb der auf Abbildung 1 gekennzeichneten Standorte platziert. Damit soll überprüft werden, wie stark frequentiert die Waldränder der ökologisch aufgewerteten Seeparzelle sind.\n\nDatenschutz ist ein wichtiges Thema. Die Besuchenden werden über den Zweck der Kameras informiert, die Daten nach der Bearbeitung wieder gelöscht und nicht weitergegeben.\n\nNun geht es ins Feld uns die Geräte werden installiert."
  },
  {
    "objectID": "fallstudie_s/2_Felderhebung_Uebung.html#datenanalyse-in-r",
    "href": "fallstudie_s/2_Felderhebung_Uebung.html#datenanalyse-in-r",
    "title": "2: Felderhebung Übung",
    "section": "Datenanalyse in R",
    "text": "Datenanalyse in R\n\nVorbereitungen\nFuer diese Aufgabe benoetigen wir folgende Bibliotheken:\n\n# Benoetigte Bibliotheken ####\nlibrary(tidyverse) # Data wrangling und piping\nlibrary(lubridate) # Arbeiten mit Datumsformaten\nlibrary(data.table)# schnelles Dateneinlesen\n\nLese nun zuerst den bereitsgestellen, respektiven den selbst erstellten Datensatz (csv) mithilfe von fread() oder read.csv() ein und nennt ihn cam.\nPruefe die Daten. Wurden sie richtig eingelesen? Wie sieht die Struktur der Daten aus?\nTipp: Brauch zum pruefen den Befehl str() sowie head().\n\n\nAufgabe 1: Datentypen\nViele Befehle zum Einlesen erkennen die Datentypen automatisch. Bei Faktoren funktioniert das aber nicht (sie sind ja eigentlich einfach Text und R weiss nicht, was wir damit wollen).\nAuch das Datum muss vielfach manuell definiert werden (hier muessen wir R sagen, wie das Format dieses aussieht).\n\ncam <- cam %>% \n  mutate(Datum = as.Date(Datum, format = \"%d.%m.%Y\"))%>%\n  mutate(Kamerastandort = factor(Kamerastandort))%>%\n  ...\n\nDefiniert nun die restlichen (relevanten) Variablen als Faktor.\n\n\nAufgabe 2: Datensatz trennen\nUnser Datensatz enthaellt die Angeben zu ost und west. Wir wollen die Auswertungen aber pro Standort machen.\nTrennt den Datensatz aufgrund des Standorts. Nutzt dazu filter().\n\nost <- filter(DATENSATZ, SPALTENNAME == \"Attribut\")\nwest <- ...\n\n\n\nAufgabe 3: Verteilung pruefen\nBei explorativen Analysen macht es immer Sinn sich die Verteilung der Daten anzuschauen. Pruefe daher die Verteilung pro Datensatz mittels Histogram und Scatterplot.\nBeim Histogram sollen nur die Menschen angezeigt und die 0er ausgeschlossen werden. Das kann mit folgendem Code erreicht werden:\n\nhist(west$Anzahl[west$Art==\"Mensch\" &\n                  !west$Anzahl==0], # das \"!\" bedeutet \"nicht gleich\"\n     breaks = 10)                   # wie viele Balken brauchen wir im Histogram?\n\nBeim Scatterplot soll auf der x-Achse das Datum stehen, auf der y-Achse die Anzahl der Personen. Auch hier wollen wir keine Wildtiere im Plot.\n\n\nAufgabe 3: Daten ausschliessen\nFuer die weiteren Analysen schliessen wir die Wildtiere komplett aus.\n\nNutzt dazu wiederum den Befehl filter() und ueberschreibt die Datensaetze ost und west.\n\nDennoch wolle wir auch wissen, welche Tiere auf dem Areal (ost und west zusammen, also df cam) unterwegs sind.\n\nDafuer gibts einen separaten Datensatz namens Tiere. Nutzt dazu den Befehl filter().\n\n\n\nAufgabe 4: Explorative Analysen\nBerechnet zuerst die totale Anzahl Menschen / Standort mit sum(DATENSATZ$SPALTENNAME).\nGruppieren und summieren:\n\nBerechnet nun die Anzahl Menschen pro Aktivität und Standort (= Akt_ost und Akt_west).\nBerechnet auch die Anzahl Begleittier pro Kategorie und Standort (= Begleittier_ost und Begleittier_west).\n\nUntenstehender Code eigent sich dazu ganz gut:\n\nAkt_ost <- ost %>%\n  group_by(Aktivitaet)%>%      # Hier sagen wir nach was wir gruppieren \n  summarise (n = sum(Anzahl)) %>%      # und dann sagen wir, dass R zusammenfassen soll und zwar die Anzahl\n  mutate(freq = n / sum(n))%>% # und dann soll und R das prozentuale Verhaeltniss berechnen\n  arrange(desc(n))             # und dann das ganze absteigend sortieren\n\nNun soll noch berechnet werden, wie viele unterschiedliche Wildtiere auf dem ganzen Areal gezaehlt wurden.\nRecycelt dazu obenstenenden Code.\n\n\nAufgabe 5: Visualisieren\nVerteilung der Aktivitäten als Pie Chart\nZuerst eine Palette mit 5 Farben definieren:\n\npal <- hcl.colors(5, palette = \"heat\")\n\nDann als Kreisdiagramm plotten.\n\npie(Akt_west$n, labels = c(\"Anderes\", \"Biker\", \"Landwirtschaft\", \"Spaziergaenger\", \"unbestimmbar\"),\n    main = \"Prozentuales Verhaeltnis West\",\n    col = pal) \n\n\n\n\nHinweis: Die labels im base R plot müssen manuell definiert werden. ggplot als Alternative macht das selbst.\n\nFRAGE: eignen sich Pie Charts überhaupt für solche Darstellungen? Wie könnten die Aktivitäten auch noch dargestellt werden? Welches sind eure eigenen Ideen zur Visualisierung?\n\nBegleittier als Bar Chart\n\n# Begleittier als Bar Chart ####\nggplot(Begleit_ost,                      # hier den Datensatz spezifizieren\n       mapping=aes(x=Begleittier, y = n))+ # Absolute Anzahl darstellen\n  geom_col(width=0.9,position = \"dodge\")+# hier sage ich, dass ich ein Balkendiagramm will\n  labs(x=\"Begleittier\", y= \"Anzahl\")+    # Achsenbeschriftung setzen\n  theme_classic(base_size = 15)+         # Und zu guter letzt: Stil definieren\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) # sowie Achsenbeschr.ausrichten\n\n\n\n\nUnd schliesslich: Wildtier als Bar Chart Stellt hier auf der y-Achse die Anzahl nicht total sondern relativ (in Prozent) dar.\n\n\n\n\n\nBei Bedraf koennen die selben Plots fuer den zweiten Standort gemacht werden."
  },
  {
    "objectID": "fallstudie_s/2_Felderhebung_Loesung.html",
    "href": "fallstudie_s/2_Felderhebung_Loesung.html",
    "title": "2: Felderhebung Lösung",
    "section": "",
    "text": "#.###############################################################################################\n# Besuchermonitoring Grüntal - Auswertung der Besucherzahlen ####\n# Modul Research Methods, HS20. Adrian Hochreutener ####\n#.################################################################################################\n#.##############################################################################################\n# METADATA UND DEFINITIONEN ####\n#.################################################################################################\n# Ordnerstruktur ####\n# Im Ordner in dem das R-Projekt abgelegt ist muessen folgende Unterordner bestehen:\n# - Skripts\n# - Felderhebungen (Rohdaten hier ablegen)\n# - Results\n# Benoetigte Bibliotheken ####\nlibrary(tidyverse) # Data wrangling und piping\nlibrary(lubridate) # Arbeiten mit Datumsformaten\nlibrary(data.table)# schnelles Dateneinlesen\n\n#.###############################################################################################\n# 1. DATENIMPORT #####\n#.###############################################################################################\n\n# 1.1 Einlesen ####\n# lese die Daten mithilfe von data.table ein. Je nach Bedarf muss der Speicherort sowie der\n# Dateiname angepasst werden\ncam <- fread(here(\"data\",\"DummyData.csv\"))\n\n\n#.###############################################################################################\n# 2. VORBEREITUNG DER DATEN #####\n#.###############################################################################################\n\n# 2.1 erstes Sichten und anpassen der Datentypen ####\nstr(cam)\n\nClasses 'data.table' and 'data.frame':  100 obs. of  11 variables:\n $ Person_Auswertung: chr  \"Bsp\" \"Bsp\" \"Bsp\" \"Bsp\" ...\n $ Kamerastandort   : chr  \"ost\" \"ost\" \"ost\" \"ost\" ...\n $ ID_Foto          : chr  \"bsp_001\" \"bsp_002\" \"bsp_003\" \"bsp_004\" ...\n $ Datum            : chr  \"08.10.2020\" \"07.10.2020\" \"06.10.2020\" \"09.10.2020\" ...\n $ Stunde           : int  3 0 16 24 17 23 14 7 5 23 ...\n $ Art              : chr  \"Mensch\" \"Wildtier\" \"Wildtier\" \"Wildtier\" ...\n $ Anzahl           : int  3 4 1 6 7 10 2 6 9 6 ...\n $ Richtung         : chr  \"Bergauf\" \"Bergauf\" \"Bergauf\" \"Bergab\" ...\n $ Aktivitaet       : chr  \"Spaziergaenger\" \"0\" \"0\" \"0\" ...\n $ Begleittier      : chr  \"Hund_ohne_Leine\" \"0\" \"0\" \"0\" ...\n $ Wildtier         : chr  \"0\" \"Hase\" \"Hase\" \"Hase\" ...\n - attr(*, \".internal.selfref\")=<externalptr> \n\nhead(cam)\n\n   Person_Auswertung Kamerastandort ID_Foto      Datum Stunde      Art Anzahl\n1:               Bsp            ost bsp_001 08.10.2020      3   Mensch      3\n2:               Bsp            ost bsp_002 07.10.2020      0 Wildtier      4\n3:               Bsp            ost bsp_003 06.10.2020     16 Wildtier      1\n4:               Bsp            ost bsp_004 09.10.2020     24 Wildtier      6\n5:               Bsp            ost bsp_005 07.10.2020     17 Wildtier      7\n6:               Bsp            ost bsp_006 12.10.2020     23   Mensch     10\n   Richtung     Aktivitaet     Begleittier Wildtier\n1:  Bergauf Spaziergaenger Hund_ohne_Leine        0\n2:  Bergauf              0               0     Hase\n3:  Bergauf              0               0     Hase\n4:   Bergab              0               0     Hase\n5:  Bergauf              0               0    Fuchs\n6:  Bergauf   Unbestimmbar         Anderes        0\n\ncam <- cam %>% \n  mutate(Datum = as.Date(Datum, format = \"%d.%m.%Y\"))%>%\n  mutate(Kamerastandort = factor(Kamerastandort))%>%\n  mutate(Art = factor(Art))%>%\n  mutate(Richtung = factor(Richtung))%>%\n  mutate(Aktivität = factor(Aktivitaet))%>%\n  mutate(Begleittier = factor(Begleittier))%>%\n  mutate(Wildtier = factor(Wildtier))\n\n# Datensatz trennen ####\n# Kamera ost und West sind noch in einem Datensatz.\n# Wir betrachten jeden Standort einzeln --> trennen aufgrund Name Standort\nost <- filter(cam, Kamerastandort == \"ost\")\nwest <- filter(cam, Kamerastandort == \"west\")\n\n# 2.3 Verteilung pruefen ####\n# mittels Histogram \n# bei explorativen Analysen macht es immer Sinn sich die Verteilung der Daten anzuschauen\nhist(ost$Anzahl[ost$Art==\"Mensch\" &# wir sind vorerst nur an den Menschen interessiert\n                  !ost$Anzahl==0], # hier schliesse ich die Nuller aus der Visualisierung aus\n     breaks = 10) \n\n\n\n\n\n\n\nhist(west$Anzahl[west$Art==\"Mensch\" &\n                  !west$Anzahl==0], \n     breaks = 10) \n\n\n\n\n\n\n\n# 2.4 mittels Scatterplot ####\nplot(x=ost$Datum[ost$Art==\"Mensch\" &\n                   !ost$Anzahl==0], \n     y=ost$Anzahl[ost$Art==\"Mensch\" &\n                    !ost$Anzahl==0], \n     xlab = \"Datum\")\n\n\n\n\n\n\n\nplot(x=west$Datum[west$Art==\"Mensch\" &\n                   !west$Anzahl==0], \n     y=west$Anzahl[west$Art==\"Mensch\" &\n                    !west$Anzahl==0], \n     xlab = \"Datum\")\n\n\n\n\n\n\n\n# Filter ####\n# fuer die weiteren Analysen schliessen wir die Wildtiere komplett aus\nost <- filter(ost, Art == \"Mensch\")\nwest <- filter(west, Art == \"Mensch\")\n\n# Dennoch wolle wir auch wissen, welche Tiere auf dem Areal unterwegs sind\n# Dafuer gibts einen separaten Datensatz\nTiere <- filter(cam, cam$Art == \"Wildtier\")\n\n\n#.##############################################################################################\n# 3. ANALYSE #####\n#.###############################################################################################\n\n# Fuer die Analyse Eigenschaften Datensatz anschauen\nsummary(ost)\n\n Person_Auswertung  Kamerastandort   ID_Foto              Datum           \n Length:26          ost :26        Length:26          Min.   :2020-10-06  \n Class :character   west: 0        Class :character   1st Qu.:2020-10-07  \n Mode  :character                  Mode  :character   Median :2020-10-08  \n                                                      Mean   :2020-10-08  \n                                                      3rd Qu.:2020-10-10  \n                                                      Max.   :2020-10-12  \n     Stunde            Art         Anzahl               Richtung \n Min.   : 0.00   Mensch  :26   Min.   : 1.000   Bergab      : 7  \n 1st Qu.: 6.25   Wildtier: 0   1st Qu.: 3.000   Bergauf     :15  \n Median :14.00                 Median : 6.000   Unbestimmbar: 4  \n Mean   :13.04                 Mean   : 5.308                    \n 3rd Qu.:20.75                 3rd Qu.: 6.000                    \n Max.   :23.00                 Max.   :10.000                    \n  Aktivitaet                 Begleittier    Wildtier           Aktivität\n Length:26          0              :8    0      :26   0             :0  \n Class :character   Anderes        :6    Anderes: 0   Anderes       :8  \n Mode  :character   Hund_angeleint :9    Fuchs  : 0   Biker         :3  \n                    Hund_ohne_Leine:3    Hase   : 0   Landwirtschaft:3  \n                                         Reh    : 0   Spaziergaenger:7  \n                                                      Unbestimmbar  :5  \n\nsummary(west)\n\n Person_Auswertung  Kamerastandort   ID_Foto              Datum           \n Length:28          ost : 0        Length:28          Min.   :2020-10-06  \n Class :character   west:28        Class :character   1st Qu.:2020-10-07  \n Mode  :character                  Mode  :character   Median :2020-10-09  \n                                                      Mean   :2020-10-09  \n                                                      3rd Qu.:2020-10-11  \n                                                      Max.   :2020-10-12  \n     Stunde            Art         Anzahl              Richtung \n Min.   : 1.00   Mensch  :28   Min.   :0.000   Bergab      :11  \n 1st Qu.: 6.75   Wildtier: 0   1st Qu.:1.750   Bergauf     : 8  \n Median :11.00                 Median :3.500   Unbestimmbar: 9  \n Mean   :12.07                 Mean   :4.143                    \n 3rd Qu.:16.75                 3rd Qu.:7.250                    \n Max.   :24.00                 Max.   :9.000                    \n  Aktivitaet                 Begleittier    Wildtier           Aktivität\n Length:28          0              :5    0      :28   0             :0  \n Class :character   Anderes        :8    Anderes: 0   Anderes       :5  \n Mode  :character   Hund_angeleint :8    Fuchs  : 0   Biker         :8  \n                    Hund_ohne_Leine:7    Hase   : 0   Landwirtschaft:2  \n                                         Reh    : 0   Spaziergaenger:5  \n                                                      Unbestimmbar  :8  \n\n# Anzahl Total / standort ####\nAnzahl_Ost <- sum(ost$Anzahl)\nAnzahl_West <- sum(west$Anzahl)\n\n# Meiste Aktivitaet ####\nAkt_ost <- ost %>%\n  group_by(Aktivität)%>%       # Hier sagen wir nach was wir gruppieren \n  summarise (n = sum(Anzahl)) %>%      # und dann sagen wir, dass R zusammenfassen soll und zwar die Anzahl\n  mutate(freq = n / sum(n))%>% # und dann soll und R das prozentuale Verhaeltniss berechnen\n  arrange(desc(n))             # und dann das ganze absteigend sortieren\n\n# das ganze wiederholen wir fuer den zweiten Standort\n# Jetzt kommt die grosse Staerke von R. \n# Wir haben den Code zur Berechnung der Anzahl pro Gruppe bereits geschrieben.\n# fuer die folgenden Auswertungen koennen wir ihn einfach \"recyceln\"\n Akt_west<- west %>%\n  group_by(Aktivität)%>%\n  summarise (n = sum(Anzahl)) %>%\n  mutate(freq = n / sum(n))%>%\n  arrange(desc(n))\n\nAktivitaet_West <- west %>%\n  group_by(Aktivität)%>%\n  summarise (n = sum(Anzahl)) %>%\n  mutate(freq = n / sum(n))%>%\n  arrange(desc(n))\n\n# Begleittier\nBegleit_ost <- ost %>%\n  group_by(Begleittier)%>%\n  summarise (n = sum(Anzahl)) %>%\n  mutate(freq = n / sum(n))%>%\n  arrange(desc(n))\n# uns interessiert es nicht, wie viele Leute kein Begleittier dabei hatten\nBegleit_ost <- filter(Begleit_ost, !Begleittier == \"0\")\n\nBegleit_west <- west %>%\n  group_by(Begleittier)%>%\n  summarise (n = sum(Anzahl)) %>%\n  mutate(freq = n / sum(n))%>%\n  arrange(desc(n))\n\nBegleit_west <- filter(Begleit_west, !Begleittier == \"0\")\n\n# Wildtier\nWildtier <- Tiere %>%\n  group_by(Wildtier)%>%\n  summarise (n = sum(Anzahl)) %>%\n  mutate(freq = n / sum(n))%>%\n  arrange(desc(n))\n\n\n#.###############################################################################################\n# 4. VISUALISIERUNG #####\n#.###############################################################################################\n\n# Verteilung der Aktivitaeten als Pie Chart ####\n# Zuerst eine Palette mit 5 Farben definieren\npal <- hcl.colors(5, palette = \"heat\")\n# Dann als Kreisdiagramm plotten\npie(Akt_west$n, labels = c(\"Anderes\", \"Biker\", \"Landwirtschaft\", \"Spaziergaenger\", \"unbestimmbar\"),\n    main = \"Prozentuales Verhaeltnis West\",\n    col = pal) \n\n\n\n\n\n\n\n# Begleittier als Bar Chart ####\nggplot(Begleit_ost,                      # hier den Datensatz spezifizieren\n       mapping=aes(x=Begleittier, y = n))+ # Absolute Anzahl darstellen\n  geom_col(width=0.9,position = \"dodge\")+# hier sage ich, dass ich ein Balkendiagramm will\n  labs(x=\"Begleittier\", y= \"Anzahl\")+    # Achsenbeschriftung setzen\n  theme_classic(base_size = 15)+         # Und zu guter letzt: Stil definieren\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) # sowie Achsenbeschr.ausrichten\n\n\n\n\n\n\n\n# Wildtier als Bar Chart ####\nggplot(Wildtier, mapping=aes(x=Wildtier, y=freq*100))+ # kann auch prozentual dargestellt werden\n  geom_col(width=0.9,position = \"dodge\")+\n  labs(x=\"Begleittier\", y= \"Prozent [%]\")+\n  theme_classic(base_size = 15)+\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))\n\n\n\n\n\n\n\n# Bei Bedraf koennen die selben Plots fuer den zweiten Standort gemacht werden."
  },
  {
    "objectID": "fallstudie_s/3_Aufgabenstellung.html",
    "href": "fallstudie_s/3_Aufgabenstellung.html",
    "title": "3. Aufgabenstellung",
    "section": "",
    "text": "Abschlussbericht über die multivariate Analyse"
  },
  {
    "objectID": "fallstudie_s/3_Aufgabenstellung.html#ziele",
    "href": "fallstudie_s/3_Aufgabenstellung.html#ziele",
    "title": "3. Aufgabenstellung",
    "section": "Ziele",
    "text": "Ziele\nBisher habt ihr euch mit dem Untersuchungsgebiet beschäftigt und habt selbst ein (kleines) Besuchermonitoring durchgeführt. Das Besuchermonitoring Grüental ist nun abgeschlossen und wir beschäftigen uns voll und ganz mit dem Wildnispark Zürich.\nIm Rahmen dessen programmieren wir multivariate Modelle, welche den Zusammenhang zwischen der Anzahl Besuchenden und verschiedenen Einflussfaktoren (Lockdown, Wetter, Ferien, Wochentag, Kalenderwoche) beschreiben. Dank ihnen können wir sagen, wie die Besucher:innen auf die untersuchten Faktoren reagieren (siehe dazu [Einleitung], Ziele).\nKonkret sollen folgende Fragestellungen beantwortet werden:\n\n\nWelchen Einfluss haben neben den Phasen der Covid-Pandemie auch die Wetterparameter (Sonnenscheindauer, Tageshöchsttemperatur, Niederschlagssumme) sowie der Wochentag, die Ferien, die Kalenderwoche und das Jahr auf die Besuchszahlen?\nDabei interessiert uns besonders, wie stark die jeweiligen Einflüsse sind, welche Effektrichtungen beobachtbar sind und welche der untersuchten Parameter signifikant sind.\nKönnen deutliche Unterschiede zwischen den “normalen”, vor-Covid19-Jahren und danach bei Tages-, Wochen-, und / oder Saisongang sowie den wichtigsten, deskriptiven Kennzahlen gefunden werden?\n\n\n\nJede Gruppe wertet ausschliesslich Daten eines Zählers aus. Sprecht miteinander ab, wer welchen Zähler behandelt (204, 211 oder 502; Spezifikationen siehe [Einleitung], Hinweis). Jeder Zähler soll nur von einer Gruppe ausgewertet werden!\nFür euren Zähler stehen eventuell Zahlen zu Fussgänger:innen und Velos zur Verfügung (siehe [Einleitung], Hinweis). Entscheidet euch in diesem Fall selbst, ob ihr Fussgänger:innen ODER Velos auswerten wollt. Die anderen Daten dürft ihr vernachlässigen.\n\nIm Bericht sollen die Informationen und Erfahrungen aus dem gesamten Verlauf der Fallstudie in geeigneter Weise einfliessen. Bezüglich der Felderhebung Grüntal erwarten wir keine Angaben."
  },
  {
    "objectID": "fallstudie_s/3_Aufgabenstellung.html#erwartungen",
    "href": "fallstudie_s/3_Aufgabenstellung.html#erwartungen",
    "title": "3. Aufgabenstellung",
    "section": "Erwartungen",
    "text": "Erwartungen\n\nStruktur / Aufbau\n\n\nFragestellung (siehe oben; die Fragestellung ist vorgegeben, darf aber natürlich für den Bericht geschärft und optimal formuliert und konkretisiert werden.)\nMethoden (kurzes Kapitel mit den statistischen Analysen)\nResultate (deskriptive Statistik, multivariates Modell; kurzer Fliesstext sowie die notwendigen Tabellen und eine Auswahl möglichst informativer Grafiken)\nDiskussion (Diskussion der deskriptiven Analysen und der Modellergebnisse; dieser Abschnitt sollte die eigenen Resultate auch im Zusammenhang mit aktueller Fachliteratur reflektieren.)\nLiteraturverzeichnis (Tipp: Das Literaturverzeichnis sollte vollständig sein, sowie formal korrekt und einheitlich daherkommen. Wir erwarten speziell in der Diskussion eine Abstützung auf aktuelle Fachliteratur. Auf Moodle haben wir Euch eine Auswahl relevanter Papers bereitgestellt.)\nAnhang (für alle Auswertungen relevanter R-Code in geeigneter Form)\n\n\nGesamtumfang max. 7500 Zeichen (inkl. Leerzeichen; exkl. Einleitung, Tabellen, Literaturverzeichnis und Anhang)\nAbgabe am 9.1.2022 auf Moodle oder per Mail an hoce@zhaw.ch"
  },
  {
    "objectID": "fallstudie_s/3_Aufgabenstellung.html#bewertungskriterien",
    "href": "fallstudie_s/3_Aufgabenstellung.html#bewertungskriterien",
    "title": "3. Aufgabenstellung",
    "section": "Bewertungskriterien",
    "text": "Bewertungskriterien\n\nIst die Methode klar und verständlich formuliert?\nSind die deskriptiven Analysen klar beschrieben und geeignet visualisiert?\nIst die Variablenselektion klar beschrieben, plausibel und nachvollziehbar?\nSind die Modellresultate in Text- und Tabellenform korrekt beschrieben und geeignet visualisiert?\nIst die Diskussion klar formuliert und inhaltlich schlüssig?\nWie gut ist die Diskussion auf relevante und aktuelle Fachliteratur abgestützt?\nZusätzliche bewerten wir die inhaltliche Dichte der Arbeit und die formale Qualität (Sprache, Struktur, Aufbau, Darstellung, Literaturverzeichnis, Umgang mit Literatur im Text)\n\nZusammensetzung der Fallstudiennote:\n\nFallstudie-Leistungsnachweis 1 - Forschungsplan: 30%\nFallstudie-Leistungsnachweis 2 - Multivariate Analyse: 70%"
  },
  {
    "objectID": "fallstudie_s/4_Projektierung.html",
    "href": "fallstudie_s/4_Projektierung.html",
    "title": "4. Anleitung",
    "section": "",
    "text": "Arbeiten mit Projekten\nVor den eigentlichen Auswertungen muessen einige Vorbereitungen unternommen werden. Die Zeit, die man hier investiert, wird in der späteren Projektphase um ein Mehrfaches eingespart.\nIch empfehle generell mit Projekten zu arbeiten, da diese sehr einfach ausgetauscht (auf verschiedene Rechner) und somit auch reproduziert werden können. Wichtig ist, dass es keine absoluten Arbeitspfade sondern nur relative gibt. Der Datenimport (und -export) kann mithilfe dieser relativen Pfade stark vereinfacht werden. –> Kurz gesagt: Projekte helfen alles am richtigen Ort zu behalten (mehr zur Arbeit mit Projekten: https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects).\n–> File / New Project"
  },
  {
    "objectID": "fallstudie_s/4_Projektierung.html#aufgabe-1-projektaufbau",
    "href": "fallstudie_s/4_Projektierung.html#aufgabe-1-projektaufbau",
    "title": "4. Anleitung",
    "section": "Aufgabe 1: Projektaufbau",
    "text": "Aufgabe 1: Projektaufbau\nNutzt für allen Text, welcher nicht im Code integriert ist, das Symbol #. Wenn ihr den Text als Titel definieren wollt, so dass er in der Übersicht erscheint, müssen vor dem Wort # und nach dem Wort #### eingefügt werden.\n\n# Texte, vor denen ein # und nach denen #### stehen, sind Titel\n# Texte, vor denen ein # steht, erklaeren den Ablauf\n# Zeilen ohne vorangehendes # sind Operationen\n\n# Wenn man rechts neben \"Source\" und links neben \"Environment\" klickt \n# (oder CTRL + SHIFT + O --> Show document Outline), \n# oeffnet sich die UEbersicht zu den UEberschriften\n\nTipp:\n\nAlt + - = <-\nCtrl + Shift + M = %>%\nCtrl + Shift + C = # vor der ausgewaehlten Zeile\n\nZuerst immer den Titel des Projekts sowie den Autor/ die Autorin des Skripts nennen. Hier soll auch die Herkunft der Daten ersichtlich sein und falls externe Daten verwendet werden, sollte geklärt werden, wer Dateneigentümer ist (Wildnispark und Meteo Schweiz).\nIm Skript soll immer die Ordnerstruktur des Projekts genannt werden. So kann der Arbeitsvorgang auf verschiedenen Rechnern einfach reproduziert werden (ich verwende hier ein Projektornder mit den Unterordnern __skripts, data, results).\nBeschreibt zudem folgendes die verwendete Meteodaten (siehe dazu Metadata Meteodaten, –> order_XXX_legend.txt)\nEin Skript soll in R eigentlich immer (mehr oder weniger) nach dem selbem Schema aufgebaut sein. Dieses Schema enthällt (nach den bereits erwähnten Definitionen) 4 Kapitel:\n\nMetadaten und Definitionen\nDatenimport,\nVorbereitung,\nDeskriptive Analyse und Visualisierung und\nMultifaktorielle Analyse und Visualisierung.\n\nBereitet euer Sktipt mit diesen Kapitel vor.\n\n#.###########################################################################################\n# Einfluss von COVID19 auf das Naherholungsverhalten in WPZ ####\n# Fallstudie Modul Research Methods, HS21. Autor/in ####\n#.##########################################################################################\n\n#.##########################################################################################\n# METADATA UND DEFINITIONEN ####\n#.##########################################################################################\n\n# Datenherkunft ####\n# ...\n\n#.##########################################################################################\n# 1. DATENIMPORT #####\n#.##########################################################################################"
  },
  {
    "objectID": "fallstudie_s/4_Projektierung.html#aufgabe-2-laden-der-bibliotheken",
    "href": "fallstudie_s/4_Projektierung.html#aufgabe-2-laden-der-bibliotheken",
    "title": "4. Anleitung",
    "section": "Aufgabe 2: Laden der Bibliotheken",
    "text": "Aufgabe 2: Laden der Bibliotheken\nGeplottet wird mit ggplot, daher wird tidyverse geladen. Diese Bibliothek ergaenzt BASE R in vielerlei Hinsicht uns ist eigentlich fast immer nötig. Da wir es bei Besucherdaten immer mit einem zeitlichen Bezug zu tun haben, benoetigen wir eine passende Bibliothek. Ich arbeite mit lubridate, POSIXct waere natuerlich auch moeglich. Base R bietet verschiedene Funktionen um Daten einzulesen. data.table ergaenzt diese Basisfunktionen sehr gut. ggpubr brauchen wir für das Darstellen von mehreren verschiedenen Plots in nur einem. PerformanceAnalytics, MuMIn, AICcmodavg, fitdistrplus, lme4 und sjPlot werden fuer die spaeteren multivariaten Analysen benoetigt. Die Modellguete werden wir mittels lattice, blmeco und lattice pruefen.\n\nLadet nun also die benoetigten Bibliotheken.\nAllenfalls muessen diese zuerst mit install.packages(“NAME”) installiert werden.\n\n\n# Benoetigte Bibliotheken ####\nlibrary(tidyverse)            # Data wrangling und piping\nlibrary(lubridate)            # Arbeiten mit Datumsformaten\nlibrary(data.table)           # schnelles Dateneinlesen\nlibrary(ggpubr)               # to arrange multiple plots in one graph\nlibrary(PerformanceAnalytics) # Plotte Korrelationsmatrix\nlibrary(MuMIn)                # Multi-Model Inference\nlibrary(AICcmodavg)           # Modellaverageing\nlibrary(fitdistrplus)         # Prueft die Verteilung in Daten\nlibrary(lme4)                 # Multivariate Modelle\nlibrary(blmeco)               # Bayesian data analysis using linear models\nlibrary(sjPlot)               # Plotten von Modellergebnissen (tab_model)\nlibrary(lattice)              # einfaches plotten von Zusammenhängen zwischen Variablen"
  },
  {
    "objectID": "fallstudie_s/4_Projektierung.html#aufgabe-3-zeitliche-definitionen",
    "href": "fallstudie_s/4_Projektierung.html#aufgabe-3-zeitliche-definitionen",
    "title": "4. Anleitung",
    "section": "Aufgabe 3: Zeitliche Definitionen",
    "text": "Aufgabe 3: Zeitliche Definitionen\n\nDefiniert den zeitlichen Horizont, also Start sowie Ende der Untersuchungen. Bezieht in eure Auswertungen den gesamten verfügbaren Zeitraum ein.\n\nDafür müsst ihr in die Rohdaten eures Zählers schauen. Am einfachsten direkt in der .csv Datei.\n\ndepo_start <- as.Date(\"YYYY-MM-DD\")\ndepo_end <- ...\n\nWichtiger Teil unserer Auswertungen ist der Einfluss des Lockdown auf das Besuchsverhalten. -Wir müssen also Start und Ende der beiden Lockdowns in der Schweiz definieren:\n\nlock_1_start_2020 <- as.Date(\"2020-03-16\")\nlock_1_end_2020 <- as.Date(\"2020-05-11\")\n\nlock_2_start_2021 <- as.Date(\"2020-12-22\")\nlock_2_end_2021 <- as.Date(\"2021-03-01\")\n\nEbenfalls müssen die erste und letzte Kalenderwoche der Untersuchungsfrist definiert werden. Diese werden bei wochenweisen Analysen ausgeklammert da sie i.d.R. unvollstaendig sind (das ist ein späterer Arbeitsschritt). Geht wie oben vor. Tipp: der Befehl week() liefert euch die Kalenderwoche.\nFerienzeiten können einen grossen Einfluss auf das Besucheraufkommen haben. Die relevanten Ferienzeiträume (in meinem Beispiel ab 2019, je nach dem müsst ihr das anpassen) muüsen daher bekannt sein. Zur Definition der Ferien kann z.B. folgend vorgegangen werden:\n\n# (https://www.schulferien.org/schweiz/ferien/2020/)\nFruehlingsferien_2019_start <- as.Date(\"2019-04-13\")\nFruehlingsferien_2019_ende <- as.Date(\"2019-04-28\")\nSommerferien_2019_start <- as.Date(\"2019-07-6\")\nSommerferien_2019_ende <- as.Date(\"2019-08-18\")\nHerbstferien_2019_start <- as.Date(\"2019-10-05\")\nHerbstferien_2019_ende <- as.Date(\"2019-10-20\")\nWinterferien_2019_start <- as.Date(\"2019-12-21\")\nWinterferien_2019_ende <- as.Date(\"2020-01-02\")\n\nFruehlingsferien_2020_start <- as.Date(\"2020-04-11\")\nFruehlingsferien_2020_ende <- as.Date(\"2020-04-26\")\nSommerferien_2020_start <- as.Date(\"2020-07-11\")\nSommerferien_2020_ende <- as.Date(\"2020-08-16\")\nHerbstferien_2020_start <- as.Date(\"2020-10-03\")\nHerbstferien_2020_ende <- as.Date(\"2020-10-18\")\nWinterferien_2020_start <- as.Date(\"2020-12-19\")\nWinterferien_2020_ende <- as.Date(\"2021-01-03\")\n\nFruehlingsferien_2021_start <- as.Date(\"2021-04-24\")\nFruehlingsferien_2021_ende <- as.Date(\"2021-05-09\")\nSommerferien_2021_start <- as.Date(\"2021-07-17\")\n\nNun sind alle Vorbereitungen gemacht, die Projektstruktur aufgebaut und die eigentliche Arbeit kann beginnen."
  },
  {
    "objectID": "fallstudie_s/5_Import_Vorverarbeitung_Uebung.html#aufgabe-1-zähldaten",
    "href": "fallstudie_s/5_Import_Vorverarbeitung_Uebung.html#aufgabe-1-zähldaten",
    "title": "5. Übung",
    "section": "Aufgabe 1: Zähldaten",
    "text": "Aufgabe 1: Zähldaten\nDie Projektstruktur steht. Nun können die Daten eingelesen und die nötigen Datentypen definiert werden. Das tidyverse-Universum (u.a. pipes, ggplot usw.) ist in unseren Auswertungen zentral.\n\nDie Zähldaten des Wildnispark Zürich wurden vorgängig bereinigt (z.B. wurden Stundenwerte entfernt, an denen am Zähler Wartungsarbeiten stattgefunden haben). Das macht es für uns einfach, denn wir können die Daten ohne vorgängige Bereinigung einlesen. Behaltet aber im Hinterkopf, dass die Datenaufbereitung, die Datenbereinigung mit viel Aufwand verbunden ist.\n\nLest die Zählaten ein, speichert ihn unter der Variable depo und sichtet den Datensatz (z.B. str(), head(), view() usw.).\n\n\ndepo <- read_csv(\"./HIER RELATIVEN DATEIPFAD EINGEBEN\") \n## Speicherort sowie Dateiname anpassen\n\nHinweis: Im Stundenformat zeigen die Werte bei 11:00 die Zähldaten zwischen 11:00 bis 12:00 Uhr.\n\n1a)\n\nIm Datensatz des Wildnisparks sind Datum und Uhrzeit in einer Spalte. Diese müssen getrennt werden (Ich schlage hier den Ansatz des piping ( %>% ) vor. Damit können in einem “Rutsch” mehrere Operationen ausgeführt werden).\nEbenfalls muss das Datum als solches definiert werden. Welches Format hat es (im Code: format = “HIER DATUMSFORMAT”)?\nSchliesslich schneiden wir den Datensatz auf die Untersuchungsdauer zu.\n\n\nstr(depo)\n\ndepo <- depo %>%\n  mutate(Datum_Uhrzeit = as.character(DatumUhrzeit)) %>%\n  separate(Datum_Uhrzeit, into = c(\"Datum\", \"Zeit\"), sep = \" \")%>% # mit seperate() trennt man\n                                                                   # 1 Spalte in 2.\n  mutate(Datum = as.Date(Datum, format = \"HIER DATUMSFORMAT\")) %>% # hier wird Text zum Datum\n  # Schneide das df auf den gewünschten Zeitraum zu\n  filter(Datum >= depo_start, Datum <=  depo_end) # das Komma hat die gleiche Funktion wie ein &\n\n\n\n1b)\nIhr könnt selbst wählen, ob ihr Fussgänger:innen oder Velos untersuchen wollt (je nachdem ob sie in eurem Datensatz vorhanden sind).\n\nEntfernt die überflüssigen Spalten aus dem Datensatz.\n\n\n\n1c)\n\nBerechnen des Totals (IN + OUT), da dieses in den Daten nicht vorhanden ist (wiederum mit piping).\n\nTipp: Wenn man R sagt: “addiere mir Spalte x mit Spalte y”, dann macht R das für alle Zeilen in diesen zwei Spalten. Wenn man nun noch sagt: “speichere mir das Ergebnis dieser Addition in einer neuen Spalte namens Total”, dann hat man die Aufgabe bereits gelöst. Arbeitet mit mutate()).\n\nEntfernt nun alle NA-Werte mit na.omit()."
  },
  {
    "objectID": "fallstudie_s/5_Import_Vorverarbeitung_Uebung.html#aufgabe-2-meteodaten",
    "href": "fallstudie_s/5_Import_Vorverarbeitung_Uebung.html#aufgabe-2-meteodaten",
    "title": "5. Übung",
    "section": "Aufgabe 2: Meteodaten",
    "text": "Aufgabe 2: Meteodaten\n\n2a)\n\nLest die Meteodaten ein und speichert sie unter meteo.\n\n\n\n2b)\n\nAuch hier müssen die Datentypen manuell gesetzt werden.\n\nTipp: Das Datum wird als Integer erkannt. Zuerst muss es in Text umgewandelt werden aus dem dann das eigentliche Datum herausgelesen werden kann. Das ist mühsam - darum hier der Code.\n\nmeteo <- transform(meteo, time = as.Date(as.character(time), \"%Y%m%d\"))\n\nHinweis Was ist eigentlich Niederschlag:\nhttps://www.meteoschweiz.admin.ch/home/wetter/wetterbegriffe/niederschlag.html\n\nWerden den anderen Spalten die richtigen Typen zugewiesen? Falls nicht, ändert die Datentypen.\nNun schneiden wir den Datensatz auf die Untersuchungsdauer zu.\n\n\n\n2c)\n\nJetzt müssen auch hier alle nicht verfügbare Werte (NA’s) herausgefiltert werden.\n\nTipp: Entweder geht das mit na.omit() für alle Spalten oder, etwas konservativer, können mit filter() die zu filternden Spalten definiert werden. Mit folgendem Codeblock können z.B. alle Werte gefiltert werden, die in der Spalte stn nicht gleich NA sind (es werden also die Werte behalten, die vorhanden sind). Der Code muss für die anderen relevanten Spalten noch ergänzt werden.\n\nmeteo <- meteo %>%\n  filter(!is.na(stn))%>%\n  ...%>%\n  ...\n\nHinweis: … steht im Code für folgende oder vorhergehende Zeilen im Code (in einer Pipe)\n\nPrüft nun, wie die Struktur des data.frame (df) aussieht und ob alle NA Werte entfernt wurden (sum(is.na(df$Variable))). Stimmen alle Datentypen?"
  },
  {
    "objectID": "fallstudie_s/5_Import_Vorverarbeitung_Uebung.html#aufgabe-3-datenvorverarbeitung-mutationen",
    "href": "fallstudie_s/5_Import_Vorverarbeitung_Uebung.html#aufgabe-3-datenvorverarbeitung-mutationen",
    "title": "5. Übung",
    "section": "Aufgabe 3: Datenvorverarbeitung (Mutationen)",
    "text": "Aufgabe 3: Datenvorverarbeitung (Mutationen)\n\n3a)\nJetzt fügen wir viele Convinience Variabeln hinzu. Wir brauchen:\n\nWochentag; der Befehl dazu ist weekdays()\n\nTipp: R sortiert die Levels alphabetisch. Da das in unserem Fall aber sehr unpraktisch ist, müssen die Levels manuell bestimmt werden\n\n  ...\n  mutate(Wochentag = base::factor(Wochentag, \n                            levels = c(\"Montag\", \"Dienstag\", \"Mittwoch\", \n                                       \"Donnerstag\", \"Freitag\", \"Samstag\", \"Sonntag\")))\n  ...\n\nFrage: Was bedeutet base:: vor den eigentlichen Befehl?\n\nIst es ein Werktag oder Wochenende?\n\n\n  ...\n  mutate(Wochenende = if_else(Wochentag == \"Montag\" | Wochentag == \"Dienstag\" | \n                           Wochentag == \"Mittwoch\" | Wochentag == \"Donnerstag\" | \n                           Wochentag == \"Freitag\", \"Werktag\", \"Wochenende\"))\n  ...\n\nFrage: Was bedeuten die | (zu erstellen mit AltGr + 7)? Welches ist das if Argument, welches das else?\n\nKalenderwoche: week()\nMonat: month()\nJahr: year()\nPhase Covid (Code untenstehend)\n\nHinweis I: ich mache den letzten Punkt nachgelagert, da zu viele Operationen in einem Schritt auch schon mal etwas durcheinander erzeugen können. Hinweis II: Wir packen alle Phasen (normal, die beiden Lockdowns und Covid aber ohne Lockdown) in eine Spalte –> long-format ist schöner (und praktischer für das plotten) als wide-format.\n\ndepo <- depo %>%\nmutate(Phase = if_else(Datum >= lock_1_start_2020 & Datum <= lock_1_end_2020,\n                          \"Lockdown_1\",\n                       if_else(Datum >= lock_2_start_2021 & Datum <= lock_2_end_2021,\n                               \"Lockdown_2\",\n                               if_else(Datum < lock_1_start_2020,\n                                  \"Normal\", \"Covid\"))))\n\n## hat das gepklappt?!\nunique(depo$Phase)\n\nFrage: Welches ist das if Argument, welches das else?\n\nÄndert die Datentypen der Spalten Wochenende, KW, Phase zu factor und sortiert die Levels, so dass diese Sinn machen (z.B. in Phase = Normal, Lockdown 1, Lockdown 2, Covid).\n\n\n\n3b)\n\nNun soll noch die volle Stunde als Integer im Datensatz stehen. Diese Angabe muss etwas mühsam aus den Daten gezogen werden (darum hier der fertige Code dazu):\n\n\ndepo$Stunde <- as.numeric(format(as.POSIXct(depo$Zeit,format=\"%H:%M\"),\"%H\"))\n\n\n\n3c)\nDie Daten wurden durch den WPZ kalibriert (Kommastellen).\n\nRundet sie auf 0 Nachkommastellen (Ganzzahl; unser Modell kann nicht mit Kommazahlen in der ahbängigen Variable umgehen).\nDefiniert sie sicherheitshalber als Integer\nMacht das für IN, OUT und Total.\n\n\ndepo$... <- round(..., digits = 0)\ndepo$... <- as.integer(...)"
  },
  {
    "objectID": "fallstudie_s/5_Import_Vorverarbeitung_Uebung.html#aufgabe-4-aggregierung-der-stundendaten",
    "href": "fallstudie_s/5_Import_Vorverarbeitung_Uebung.html#aufgabe-4-aggregierung-der-stundendaten",
    "title": "5. Übung",
    "section": "Aufgabe 4: Aggregierung der Stundendaten",
    "text": "Aufgabe 4: Aggregierung der Stundendaten\n\n4a)\nUnsere Daten liegen im Stundenformat vor. Für einige Auswertungen müssen wir aber auf ganze Tage zurückgreifen können.\n\nDie Stundendaten müssen zu ganzen Tagen aggregiert werden. Macht das wiederum einer Pipe Bezieht folgende Gruppierungen (group_by()) mit ein: Datum, Wochentag, Wochenende, KW, Monat, Jahr, Phase. Summiert die Zählmengen separat (Total, IN, OUT) auf und speichert das Resultat unter depo_d.\n\nTipp: Wenn man die Convinience Variablen als grouping variable einspeisst, dann werden sie in das neue df übernommen und müssen nicht nochmals hinzugefügt werden\n\ndepo_d <- depo %>% \n  group_by(VARIABLE1, VARIABLE2, ...) %>%   # Gruppieren nach den Variablen\n  summarise(Total = sum(Fuss_IN + Fuss_OUT),# Berechnen der gewünschten Werte\n            Fuss_IN = sum(Fuss_IN),\n            ...\n\n\n\n4b)\n\nAggregiere die Stundenwerte nach dem Monat (Gruppierungen Monat, Jahr) und speichert das neue df unter depo_m.\n\nTipp: Braucht wiederum group_by() und summarise(). Nun brauchen wir nur noch das Total, keine Richtungstrennung mehr.\n\nFügt den neu erstellten df eine Spalte mit Jahr + Monat hinzu. Das ist etwas mühsam, darum hier der fertige Code dazu:\n\n\n## vergewissere, dass sicher df\ndepo_m <- as.data.frame(depo_m)\n## sortiere das df anhand zwei Spalten aufsteigend (damit die Reihenfolge sicher stimmt)\ndepo_m[with(depo_m, order(Jahr, Monat)),]\n\ndepo_m <- depo_m %>% \n  mutate(Jahr = as.factor(Jahr)) %>% # mache dann aus Jahr und Monat Faktoren\n  mutate(Monat = as.factor(Monat)) %>% \n  mutate(Ym = paste(Jahr, Monat)) %>% # und mache eine neue Spalte, in der Jahr und Monat in zusammen sind\n  mutate(Ym= factor(Ym, levels=unique(Ym))) # auch dass soll ein Faktor sein, \n        # die Levels sind die einzelnen Einträge in der Spalte (welche ja bereits geordnet sind)\n\n\n\n4c)\nMacht euch mit den Daten vertraut. Plottet sie, seht euch die df’s an, versteht, was sie repräsentieren.\nZ.B. sind folgende Befehle und Plots wichtig:\n\nstr()\nsummarize()\nhead()\nScatterplot, x = Datum, y = Anzahl pro Zeiteinheit\nHistrogram\nusw.\n\nHinweis: Geht noch nicht zu weit mit euren Plots. Die Idee ist, dass man sich einen Überblick über die Daten verschafft und noch keine “analysierenden” Plots erstellt.\n–> Erklärt dem Plenum am 26.10.2021 was ihr gemacht habt, was eure Daten zeigen und präsentiert diese einfachen Plots. \nNachdem nun alle Daten vorbereitet sind folgt im nächsten Schritt die Analyse."
  },
  {
    "objectID": "fallstudie_s/5_Import_Vorverarbeitung_Loesung.html",
    "href": "fallstudie_s/5_Import_Vorverarbeitung_Loesung.html",
    "title": "5. Lösung",
    "section": "",
    "text": "Aufgabe 1: Zähldaten\n\n#.################################################################################################\n# 1. DATENIMPORT #####\n#.################################################################################################\n\n# Beim Daten einlesen koennen sogleich die Datentypen und erste Bereinigungen vorgenommen werden\n\n# 1.1 Zaehldaten ####\n# Die Zaehldaten des Wildnispark wurden vorgaengig bereinigt. z.B. wurden Stundenwerte \n# entfernt, an denen am Zaehler Wartungsarbeiten stattgefunden haben.\n\n# lese die Daten mithilfe der Bibliothek data.table ein (alternative zu read_csv und dergleichen). \n# Je nach Bedarf muss der Speicherort sowie der Dateiname angepasst werden\ndepo <- fread(here(\"data\",\"211_sihlwaldstrasse_2017_2021.csv\"))\n\n# Hinweis zu den Daten:\n# In hourly analysis format, the data at 11:00 am corresponds to the counts saved between \n# 11:00 am and 12:00 am.\n\n# Anpassen der Datentypen und erstes Sichten\nstr(depo)\n\ndepo <- depo %>%\n  mutate(Datum_Uhrzeit = as.character(DatumUhrzeit)) %>%\n  separate(Datum_Uhrzeit, into = c(\"Datum\", \"Zeit\"), sep = \" \")%>%\n  mutate(Datum = as.Date(Datum, format = \"%d.%m.%Y\")) %>% \n  # Schneide das df auf den gewuenschten Zeitraum zu\n  filter(Datum >= depo_start, Datum <=  depo_end) # das Komma hat die gleiche Funktion wie ein &\n\n# In dieser Auswertung werden nur Velos betrachtet!\ndepo <- depo[,-c(1,4,5), drop=FALSE] # mit diesem Befehl lassen wir Spalten \"fallen\", \n                                     # aendern aber nichts an der Form des data.frames\n\n# Berechnen des Totals, da dieses in den Daten nicht vorhanden ist\ndepo <- depo%>%\n  mutate(Total = Fuss_IN + Fuss_OUT)\n\n# Entferne die NA's in dem df.\ndepo <- na.omit(depo)\n\n\n\nAufgabe 2: Meteodaten\n\n# 1.2 Meteodaten ####\n# Einlesen\nmeteo <- fread(here(\"data\",\"order_97149_data.txt\"))\n\n# Datentypen setzen\n# Das Datum wird als Integer erkannt. Zuerst muss es in Text umgewaldelt werden aus dem dann\n# das eigentliche Datum herausgelesen werden kann\nmeteo <- transform(meteo, time = as.Date(as.character(time), \"%Y%m%d\"))\n\n# Die eigentlichen Messwerte sind alle nummerisch\nmeteo <- meteo%>%\n  mutate(tre200jx = as.numeric(tre200jx))%>%\n  mutate(rre150j0 = as.numeric(rre150j0))%>%\n  mutate(sremaxdv = as.numeric(sremaxdv)) %>% \n  filter(time >= depo_start, time <=  depo_end) # schneide dann auf Untersuchungsdauer\n\n# Was ist eigentlich Niederschlag:\n# https://www.meteoschweiz.admin.ch/home/wetter/wetterbegriffe/niederschlag.html\n\n# Filtere Werte mit NA\nmeteo <- meteo %>%\n  filter(!is.na(stn)) %>%\n  filter(!is.na(time))%>%\n  filter(!is.na(tre200jx))%>%\n  filter(!is.na(rre150j0))%>%\n  filter(!is.na(sremaxdv))\n# Pruefe ob alles funktioniert hat\nstr(meteo)\nsum(is.na(meteo)) # zeigt die Anzahl NA's im data.frame an\n\n\n\nAufgabe 3: Datenvorverarbeitung (Mutationen)\n\n#.################################################################################################\n# 2. VORBEREITUNG DER DATEN #####\n#.################################################################################################\n\n# 2.1 Convinience Variablen ####\n# fuege dem Dataframe (df) die Wochentage hinzu\ndepo <- depo %>% \n  mutate(Wochentag = weekdays(Datum)) %>% \n  # R sortiert die Levels aplhabetisch. Da das in unserem Fall aber sehr unpraktisch ist,\n  # muessen die Levels manuell manuell bestimmt werden\n  mutate(Wochentag = base::factor(Wochentag, \n                            levels = c(\"Montag\", \"Dienstag\", \"Mittwoch\", \n                                       \"Donnerstag\", \"Freitag\", \"Samstag\", \"Sonntag\"))) %>% \n  # Werktag oder Wochenende hinzufuegen\n  mutate(Wochenende = if_else(Wochentag == \"Montag\" | Wochentag == \"Dienstag\" | \n                           Wochentag == \"Mittwoch\" | Wochentag == \"Donnerstag\" | \n                           Wochentag == \"Freitag\", \"Werktag\", \"Wochenende\"))%>%\n  #Kalenderwoche hinzufuegen\n  mutate(KW= week(Datum))%>%\n  # monat und Jahr\n  mutate(Monat = month(Datum)) %>% \n  mutate(Jahr = year(Datum))\n\n# Lockdown \n# Hinweis: ich mache das nachgelagert, da ich die Erfahrung hatte, dass zu viele \n# Operationen in einem Schritt auch schon mal durcheinander erzeugen koennen.\n# Hinweis II: Wir packen alle Phasen (normal, die beiden Lockdowns und Covid aber ohne Lockdown)\n# in eine Spalte --> long ist schoener als wide\ndepo <- depo %>%\nmutate(Phase = if_else(Datum >= lock_1_start_2020 & Datum <= lock_1_end_2020,\n                          \"Lockdown_1\",\n                       if_else(Datum >= lock_2_start_2021 & Datum <= lock_2_end_2021,\n                               \"Lockdown_2\",\n                               if_else(Datum < lock_1_start_2020,\n                                  \"Normal\", \"Covid\"))))\n\n# hat das gepklappt?!\nunique(depo$Phase)\n\n# aendere die Datentypen\ndepo <- depo %>% \n  mutate(Wochenende = as.factor(Wochenende)) %>% \n  mutate(KW = factor(KW)) %>% \n  # mit factor() koennen die levels direkt einfach selbst definiert werden.\n  # wichtig: speizfizieren, dass aus R base, ansonsten kommt es zu einem \n  # mix-up mit anderen packages\n  mutate(Phase = base::factor(Phase, levels = c(\"Normal\", \"Lockdown_1\", \"Lockdown_2\", \"Covid\")))\n\nstr(depo)\n  \n# Fuer einige Auswertungen muss auf die Stunden als nummerischer Wert zurueckgegriffen werden\ndepo$Stunde <- as.numeric(format(as.POSIXct(depo$Zeit,format=\"%H:%M\"),\"%H\"))\n\n# Die Daten wurden kalibriert. Wir runden sie fuer unserer Analysen auf Ganzzahlen\ndepo$Total <- round(depo$Total, digits = 0)\ndepo$Fuss_IN <- round(depo$Fuss_IN, digits = 0)\ndepo$Fuss_OUT <- round(depo$Fuss_OUT, digits = 0)\n\n\n\nAufgabe 4: Aggregierung der Stundendaten\n\n# 2.3 Aggregierung der Stundendaten zu ganzen Tagen ####\n# Zur Berechnung von Kennwerten ist es hilfreich, wenn neben den Stundendaten auch auf Ganztagesdaten\n# zurueckgegriffen werden kann\n# hier werden also pro Nutzergruppe und Richtung die Stundenwerte pro Tag aufsummiert\ndepo_d <- depo %>% \n  group_by(Datum, Wochentag, Wochenende, KW, Monat, Jahr, Phase) %>% \n  summarise(Total = sum(Fuss_IN + Fuss_OUT), \n            Fuss_IN = sum(Fuss_IN),\n            Fuss_OUT = sum(Fuss_OUT)) \n# Wenn man die Convinience Variablen als grouping variable einspeisst, dann werden sie in \n# das neue df uebernommen und muessen nicht nochmals hinzugefuegt werden\n\n# pruefe das df\nhead(depo_d)\n\n# Gruppiere die Werte nach Monat\ndepo_m <- depo %>% \n  group_by(Jahr, Monat) %>% \n  summarise(Total = sum(Total)) \n# sortiere das df aufsteigend (nur das es sicher stimmt)\ndepo_m <- as.data.frame(depo_m)\ndepo_m[\n  with(depo_m, order(Jahr, Monat)),]\n# mache dann aus Jahr und Monat faktoren\ndepo_m <- depo_m %>% \n  mutate(Jahr = as.factor(Jahr)) %>% \n  mutate(Monat = as.factor(Monat)) %>% \n  mutate(Ym = paste(Jahr, Monat)) %>% # und mache eine neue Spalte, in der Jahr und\n  mutate(Ym= factor(Ym, levels=unique(Ym))) # Monat in zusammen sind\n\n# Beispiele pruefen der Daten:\n\n# Verteilung mittels Histogram pruefen\nhist(depo$Total[!depo$Total==0] , breaks = 100) \n\n\n\n\n\n\n\n# hier schliesse ich die Nuller aus der Visualisierung aus\n\n# Verteilung mittels Scatterplot pruefen\nplot(x=depo$Datum, y=depo$Total)\n\n\n\n\n\n\n\n# Temperaturmaximum\nggplot(data=meteo, mapping=aes(x=time, y=tre200jx))+\n  geom_point()+\n  geom_smooth(col=\"red\")"
  },
  {
    "objectID": "fallstudie_s/6_Deskriptive_Analysen_Uebung.html#a",
    "href": "fallstudie_s/6_Deskriptive_Analysen_Uebung.html#a",
    "title": "6. Übung",
    "section": "2a)",
    "text": "2a)\n\nBerechnet zuerst die Totale Anzahl pro Wochentag pro Phase.\n\n\nmean_phase_wd <- depo_d %>% \n  group_by(...) %>% \n  ...\n\n\nSpeichert das als .csv\n\n\nwrite.csv(mean_phase_wd, \"results/mean_phase_wd.csv\")"
  },
  {
    "objectID": "fallstudie_s/6_Deskriptive_Analysen_Uebung.html#b",
    "href": "fallstudie_s/6_Deskriptive_Analysen_Uebung.html#b",
    "title": "6. Übung",
    "section": "2b)",
    "text": "2b)\n\nErstellt einen Boxplot nach untenstehender Vorgabe:\n\n\n\n\n\n\nHinweis: - Nutzt zum plotten ggplot() - folgende Codeschnipsel helfen euch:\n\nggplot(data = depo_d)+\n  geom_boxplot(mapping = aes(x= Wochentag, y = Total, fill = Phase))+\n  ...\n\n\nExportiert auch diesen Plot mit ggsave(). Welche Breite und Höhe passt hier?"
  },
  {
    "objectID": "fallstudie_s/6_Deskriptive_Analysen_Uebung.html#c",
    "href": "fallstudie_s/6_Deskriptive_Analysen_Uebung.html#c",
    "title": "6. Übung",
    "section": "2c)",
    "text": "2c)\nSind die Unterschiede zwischen Werktag und Wochenende wirklich signifikant? Falls ja, in allen Phasen oder nur während bestimmter?\n\nPrüft das pro Phase mit einem einfachen t.test."
  },
  {
    "objectID": "fallstudie_s/6_Deskriptive_Analysen_Uebung.html#a-1",
    "href": "fallstudie_s/6_Deskriptive_Analysen_Uebung.html#a-1",
    "title": "6. Übung",
    "section": "3a)",
    "text": "3a)\n\nBerechnet zuerst den Mittelwert der Totalen Besuchszahlen pro Wochentag pro Stunde pro Phase. (ganz ähnlich wie unter 2a) und speichert das df unter Mean_h.\n\nggplots haben Daten lieber im Format long als wide."
  },
  {
    "objectID": "fallstudie_s/6_Deskriptive_Analysen_Uebung.html#b-1",
    "href": "fallstudie_s/6_Deskriptive_Analysen_Uebung.html#b-1",
    "title": "6. Übung",
    "section": "3b)",
    "text": "3b)\n\nPlottet den Tagesgang, unterteilt nach den 7 Wochentagen nun für unsere 4 Phasen.\n\n\n\nWarning: Removed 24 row(s) containing missing values (geom_path).\n\n\n\n\n\nFür die Phase “Normal” benutze ich folgenden Codeschnipsel. Speichert den Plot ab (hier: tag_norm).\n\ntag_norm <- ggplot(subset(Mean_h, Phase %in% c(\"Normal\")), \n                     mapping=aes(x = Stunde, y = Total, colour = Wochentag, linetype = Wochentag))+\n  ...\n\nHinweis: Achtet darauf, dass die Skalierung der y-Achse bei allen 4 Plots dieselbe ist (z.B. immer vom 0 bis 25)."
  },
  {
    "objectID": "fallstudie_s/6_Deskriptive_Analysen_Uebung.html#c-1",
    "href": "fallstudie_s/6_Deskriptive_Analysen_Uebung.html#c-1",
    "title": "6. Übung",
    "section": "3c)",
    "text": "3c)\n\nArrangiert die vier erstellten Plots und speichert das Resultat. Das ist etwas tricky, darum hier der vollständige Code.\n\n\n# Arrange und Export Tagesgang\nggarrange(tag_lock_1+            # plot 1 aufrufen\n            rremove(\"x.text\")+   # plot 1 braucht es nicht alle Achsenbeschriftungen\n            rremove(\"x.title\"),            \n          tag_lock_2+            # plot 2 aufrufen\n            rremove(\"y.text\")+   # bei plot 2 brauchen wir keine Achsenbeschriftung\n            rremove(\"y.title\")+\n            rremove(\"x.text\")+\n            rremove(\"x.title\"),\n          tag_norm,\n          tag_covid+\n            rremove(\"y.text\")+   \n            rremove(\"y.title\"),\n          ncol = 2, nrow = 2,    # definieren, wie die plots angeordnet werden\n          heights = c(0.9, 1),  # beide plots sind wegen der fehlenden Beschriftung nicht gleich hoch\n          widths = c(1,0.9),    \n          labels = c(\"a) Lockdown 1\", \"b) Lockdown 2\", \"c) Normal\", \"d) Covid\"),\n          label.x = 0.1,        # wo stehen die Plottitel\n          label.y = 0.99,\n          common.legend = TRUE, legend = \"bottom\") # wir brauchen nur eine Legende, unten\n\nggsave(\"Tagesgang.png\", width=25, height=25, units=\"cm\", dpi=1000,\n       path = \"results/\")"
  },
  {
    "objectID": "fallstudie_s/6_Deskriptive_Analysen_Uebung.html#a-2",
    "href": "fallstudie_s/6_Deskriptive_Analysen_Uebung.html#a-2",
    "title": "6. Übung",
    "section": "4a)",
    "text": "4a)\n\nGruppiert nach Phase und berechnet dieses mal die Summe (nicht den Durchschnitt) Total, IN und OUT (ähnlich wie in 2a und 3a).\nSpeichert das Ergebnis als .csv"
  },
  {
    "objectID": "fallstudie_s/6_Deskriptive_Analysen_Uebung.html#b-2",
    "href": "fallstudie_s/6_Deskriptive_Analysen_Uebung.html#b-2",
    "title": "6. Übung",
    "section": "4b)",
    "text": "4b)\nDie Zeitreihen der 4 Phasen unterscheiden sich deutlich voneinander. Totale Summen sind da kaum miteinander vergleichbar, besser eignet sich der Durchschnitt oder der Median.\n\nGruppiert nach Phase und berechnet den Durchschnitt Total, IN und OUT und speichert das df unter mean_phase_d.\nErgänzt das mit der prozentualen Richtungsverteilung\n\n\nmean_phase_d <- mean_phase_d %>% \n  mutate(Proz_IN = round(100/Total*IN, 1)) %>% # berechnen und auf eine Nachkommastelle runden\n  ...\n\n\nSpeichert das Ergebnis als .csv\nSelektiert nun die absoluten Zahlen im df mean_phase_d sowie die relativen und speichert das jeweils in einem df mean_phase_d_abs und mean_phase_d_proz.\n\n\nmean_phase_d_abs <- mean_phase_d %>% dplyr::select(-c(Total, Proz_IN, Proz_OUT))\n\n\ntransformiert beide df mittels pivot_longer() von wide zu long:\n\n\nmean_phase_d_abs <- pivot_longer(mean_phase_d_abs, cols = c(\"IN\",\"OUT\"), \n             names_to = \"Gruppe\", values_to = \"Durchschnitt\")"
  },
  {
    "objectID": "fallstudie_s/6_Deskriptive_Analysen_Uebung.html#c-2",
    "href": "fallstudie_s/6_Deskriptive_Analysen_Uebung.html#c-2",
    "title": "6. Übung",
    "section": "4c)",
    "text": "4c)\nNun visualisieren wie die Verteilung der absoluten und der relativen Zahlen nach Phasen in einem Barplot.\n\nErstellt je einen Plot zu den absoluten und den relativen Zahlen nach den unterstehenden Vorgaben und speichert beide Plots im Environment:\n\n\n\nError in FUN(X[[i]], ...): object 'Durchschnitt' not found\n\n\nError in FUN(X[[i]], ...): object 'Durchschnitt' not found\n\n\n\nArrangiert beide Plots nebeneinander und exportiert das Ergebnis (Arrangieren siehe 3c)."
  },
  {
    "objectID": "fallstudie_s/6_Desktiptive_Analysen_Loesung.html",
    "href": "fallstudie_s/6_Desktiptive_Analysen_Loesung.html",
    "title": "6. Lösung",
    "section": "",
    "text": "# Statistik: Unterschied WE und WO waehrend Lockdown 1\nt.test(depo_d$Total [depo_d$Phase == \"Lockdown_1\" & depo_d$Wochenende==\"Werktag\"], \n       depo_d$Total [depo_d$Phase == \"Lockdown_1\" & depo_d$Wochenende==\"Wochenende\"])\n\nError in t.test.default(depo_d$Total[depo_d$Phase == \"Lockdown_1\" & depo_d$Wochenende == : not enough 'x' observations\n\n\n\n# 3.3 Tagesgang ####\n# Bei diesen Berechnungen wird jeweils der Mittelwert pro Stunde berechnet. \n# wiederum nutzen wir dafuer \"pipes\"\nMean_h <- depo %>% \n  group_by(Wochentag, Stunde, Phase) %>% \n  summarise(Total = mean(Total)) \n\n# transformiere fuer Plotting\nMean_h<- reshape2::melt(Mean_h,measure.vars = c(\"Total\"),\n                             value.name = \"Durchschnitt\",variable.name = \"Gruppe\")\n\nError in loadNamespace(x): there is no package called 'reshape2'\n\n# Plotte den Tagesgang, unterteilt nach Wochentagen\n\n# Normal\ntag_norm <- ggplot(subset(Mean_h, Phase %in% c(\"Normal\")), \n                     mapping=aes(x = Stunde, y = Durchschnitt, colour = Wochentag, linetype = Wochentag))+\n  geom_line(size = 2)+\n  scale_colour_viridis_d()+\n  scale_linetype_manual(values = c(rep(\"solid\", 5),  \"twodash\", \"twodash\"))+\n  scale_x_continuous(breaks = c(seq(0, 23, by = 2)), labels = c(seq(0, 23, by = 2)))+\n  labs(x=\"Uhrzeit [h]\", y= \"∅ Fussganger_Innen / h\", title = \"\")+\n  lims(y = c(0,25))+\n  theme_linedraw(base_size = 15)+\n  theme(legend.position = \"right\")\n\n# Lockdown 1\n\ntag_lock_1 <- ggplot(subset(Mean_h, Phase %in% c(\"Lockdown_1\")), \n                     mapping=aes(x = Stunde, y = Durchschnitt, colour = Wochentag, linetype = Wochentag))+\n  geom_line(size = 2)+\n  scale_colour_viridis_d()+\n  scale_linetype_manual(values = c(rep(\"solid\", 5), \"twodash\", \"twodash\"))+\n  scale_x_continuous(breaks = c(seq(0, 23, by = 2)), labels = c(seq(0, 23, by = 2)))+\n  labs(x=\"Uhrzeit [h]\", y= \"∅ Fussganger_Innen / h\", title = \"\")+\n  lims(y = c(0,25))+\n  theme_linedraw(base_size = 15)+\n  theme(legend.position = \"right\")\n\n# Lockdown 2\ntag_lock_2 <- ggplot(subset(Mean_h, Phase %in% c(\"Lockdown_2\")), \n                     mapping=aes(x = Stunde, y = Durchschnitt, colour = Wochentag, linetype = Wochentag))+\n  geom_line(size = 2)+\n  scale_colour_viridis_d()+\n  scale_linetype_manual(values = c(rep(\"solid\", 5), \"twodash\", \"twodash\"))+\n  scale_x_continuous(breaks = c(seq(0, 23, by = 2)), labels = c(seq(0, 23, by = 2)))+\n  labs(x=\"Uhrzeit [h]\", y= \"∅ Fussganger_Innen / h\", title = \"\")+\n  lims(y = c(0,25))+\n  theme_linedraw(base_size = 15)+\n  theme(legend.position = \"right\")\n\n# Covid\ntag_covid <- ggplot(subset(Mean_h, Phase %in% c(\"Covid\")), \n                     mapping=aes(x = Stunde, y = Durchschnitt, colour = Wochentag, linetype = Wochentag))+\n  geom_line(size = 2)+\n  scale_colour_viridis_d()+\n  scale_linetype_manual(values = c(rep(\"solid\", 5), \"twodash\", \"twodash\"))+\n  scale_x_continuous(breaks = c(seq(0, 23, by = 2)), labels = c(seq(0, 23, by = 2)))+\n  labs(x=\"Uhrzeit [h]\", y= \"∅ Fussganger_Innen / h\", title = \"\")+\n  lims(y = c(0,25))+\n  theme_linedraw(base_size = 15)+\n  theme(legend.position = \"right\")\n\n# Arrange und Export Tagesgang\nggarrange(tag_lock_1+            # plot 1 aufrufen\n            rremove(\"x.text\")+   # plot 1 braucht es nicht alle Achsenbeschriftungen\n            rremove(\"x.title\"),            \n          tag_lock_2+            # plot 2 aufrufen\n            rremove(\"y.text\")+   # bei plot 2 brauchen wir keine Achsenbeschriftung\n            rremove(\"y.title\")+\n            rremove(\"x.text\")+\n            rremove(\"x.title\"),\n          tag_norm,\n          tag_covid+\n            rremove(\"y.text\")+   \n            rremove(\"y.title\"),\n          ncol = 2, nrow = 2,    # definieren, wie die plots angeordnet werden\n          heights = c(0.9, 1),  # beide plots sind wegen der fehlenden Beschriftung nicht gleich hoch\n          widths = c(1,0.9),    \n          labels = c(\"a) Lockdown 1\", \"b) Lockdown 2\", \"c) Normal\", \"d) Covid\"),\n          label.x = 0.1,        # wo stehen die Plottitel\n          label.y = 0.99,\n          common.legend = TRUE, legend = \"bottom\") # wir brauchen nur eine Legende, unten\n\nError in FUN(X[[i]], ...): object 'Durchschnitt' not found\n\n# 3.4 Kennzahlen ####\ntotal_phase <- depo_d %>% \n  # gruppiere nach Phasen inkl. Normal. Diese Levels haben wir bereits definiert\n  group_by(Phase) %>% \n  summarise(Total = sum(Total),\n            IN = sum(Fuss_IN),\n            OUT = sum(Fuss_OUT))\n\n# mean besser Vergleichbar, da Zeitreihen unterschiedlich lange\nmean_phase_d <- depo_d %>% \n  group_by(Phase) %>% \n  summarise(Total = mean(Total),\n            IN = mean(Fuss_IN),\n            OUT = mean(Fuss_OUT))\n# berechne prozentuale Richtungsverteilung\nmean_phase_d <- mean_phase_d %>% \n  mutate(Proz_IN = round(100/Total*IN, 1)) %>% # berechnen und auf eine Nachkommastelle runden\n  mutate(Proz_OUT = round(100/Total*OUT,1))\n\n# selektiere absolute Zahlen\n# behalte rel. Spalten (nur die relativen Prozentangaben)\nmean_phase_d_abs <- mean_phase_d[,-c(2,5,6), drop=FALSE]\n# transformiere fuer Plotting\nmean_phase_d_abs <- reshape2::melt(mean_phase_d_abs, \n                                     measure.vars = c(\"IN\",\"OUT\"),\n                                     value.name = \"Durchschnitt\",variable.name = \"Gruppe\")\n\nError in loadNamespace(x): there is no package called 'reshape2'\n\n# selektiere relative Zahlen\n# behalte rel. Spalten (nur die relativen Prozentangaben)\nmean_phase_d_proz <- mean_phase_d[,-c(2:4), drop=FALSE]\n# transformiere fuer Plotting\nmean_phase_d_proz <- reshape2::melt(mean_phase_d_proz, \n                                 measure.vars = c(\"Proz_IN\",\"Proz_OUT\"),\n                                 value.name = \"Durchschnitt\",variable.name = \"Gruppe\")\n\nError in loadNamespace(x): there is no package called 'reshape2'\n\n# Visualisierung abs\nabs <- ggplot(data = mean_phase_d_abs, mapping = aes(x = Gruppe, y = Durchschnitt, fill = Phase))+\n  geom_col(position = \"dodge\", width = 0.8)+\n  scale_fill_manual(values = c(\"royalblue\", \"red4\", \"orangered\", \"gold2\"), name = \"Phase\")+\n  scale_x_discrete(labels = c(\"IN\", \"OUT\"))+\n  labs(y = \"Durchschnitt [mean]\", x= \"Bewegungsrichtung\")+\n  theme_classic(base_size = 15)+\n  theme(legend.position = \"bottom\")\n\n# Visualisierung %\nproz <- ggplot(data = mean_phase_d_proz, mapping = aes(x = Gruppe, y = Durchschnitt, fill = Phase))+\n  geom_col(position = \"dodge\", width = 0.8)+\n  scale_fill_manual(values = c(\"royalblue\", \"red4\", \"orangered\", \"gold2\"), name = \"Phase\")+\n  scale_x_discrete(labels = c(\"IN\", \"OUT\"))+\n  labs(y = \"Durchschnitt [%]\", x= \"Bewegungsrichtung\")+\n  theme_classic(base_size = 15)+\n  theme(legend.position = \"bottom\")\n\n# Arrange und Export Verteilung\nggarrange(abs,            # plot 1 aufrufen\n          proz,            # plot 2 aufrufen\n          ncol = 2, nrow = 1,    # definieren, wie die plots angeordnet werden\n          heights = c(1),        # beide sind bleich hoch\n          widths = c(1,0.95),    # plot 2 ist aufgrund der fehlenden y-achsenbesch. etwas schmaler\n          labels = c(\"a) Absolute Verteilung\", \"b) Relative Verteilung\"),\n          label.x = 0,        # wo stehen die labels\n          label.y = 1.0,\n          common.legend = TRUE, legend = \"bottom\") # wir brauchen nur eine Legende, unten\n\nError in FUN(X[[i]], ...): object 'Durchschnitt' not found\n\n\n\nAufgabe 1: Verlauf der Besuchszahlen / m\n\n#.################################################################################################\n# 3. DESKRIPTIVE ANALYSE UND VISUALISIERUNG #####\n#.################################################################################################\n\n# 3.1 Verlauf der Besuchszahlen / m ####\n# Monatliche Summen am Standort\n\n# wann beginnt die Datenreihe schon wieder?\nfirst(depo_m$Ym)\n# und wann ist die fertig?\nlast(depo_m$Ym)\n\n# Plotte\nggplot(depo_m, mapping = aes(Ym, Total, group = 1))+ # group = 1 braucht R, dass aus den Einzelpunkten ein Zusammenhang hergestellt wird\n  #zeichne Lockdown 1\n  geom_rect(mapping = aes(xmin=\"2020 3\", xmax=\"2020 5\",\n                          ymin =0, ymax=max(Total+(Total/100*10))),\n            fill = \"lightskyblue\", alpha = 0.4, colour = NA)+\n  #zeichne Lockdown 2\n  geom_rect(mapping = aes(xmin=\"2020 12\", xmax=\"2021 3\", \n                          ymin =0, ymax=max(Total+(Total/100*10))), \n            fill = \"lightskyblue\", alpha = 0.4, colour = NA)+\n  geom_line(alpha = 0.6, size = 1.5)+\n  scale_x_discrete(breaks = c(\"2019 1\", \"2019 7\",\"2019 1\",\"2020 1\",\"2020 7\",\"2021 1\",\"2021 7\"),\n                   labels = c(\"2019 1\", \"2019 7\",\"2019 1\",\"2020 1\",\"2020 7\",\"2021 1\",\"2021 7\"))+\n  labs(title= \"\", y=\"Fussgaenger:innen pro Monat\", x = \"Jahr\")+\n  theme_linedraw(base_size = 15)+\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))\n\n\n\n\n\n\n\n# Beachtet, dass ich in der Musterloesung keine Resultate exportiere, untenstehend aber der Code dazu:\n\n# ggsave(\"Entwicklung_Zaehlstelle.png\", width=20, height=10, units=\"cm\", dpi=1000, \n#        path = \"_fallstudien/_R_analysis/results/\") \n\n\n\nAufgabe 2: Wochengang\n\n# mean / d / phase\nmean_phase_wd <- depo_d %>% \n  group_by(Wochentag, Phase) %>% \n  summarise(Total = mean(Total))\n\n# write.csv(mean_phase_wd, \"_fallstudien/_R_analysis/results/mean_phase_wd.csv\")\n\n#plot\nggplot(data = depo_d)+\n  geom_boxplot(mapping = aes(x= Wochentag, y = Total, fill = Phase))+\n  labs(title=\"\", y= \"Fussgaenger:innen pro Tag\")+\n  scale_fill_manual(values = c(\"royalblue\", \"red4\", \"orangered\", \"gold2\"))+\n  theme_classic(base_size = 15)+\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1),\n        legend.title = element_blank())\n\n\n\n\n\n\n\n# ggsave(\"Wochengang_Lockdown.png\", width=15, height=15, units=\"cm\", dpi=1000, \n#        path = \"_fallstudien/_R_analysis/results/\")\n\n\n# Statistik: Unterschied WE und WO waehrend Lockdown 1\nt.test(depo_d$Total [depo_d$Phase == \"Lockdown_1\" & depo_d$Wochenende==\"Werktag\"], \n       depo_d$Total [depo_d$Phase == \"Lockdown_1\" & depo_d$Wochenende==\"Wochenende\"])\n\nError in t.test.default(depo_d$Total[depo_d$Phase == \"Lockdown_1\" & depo_d$Wochenende == : not enough 'x' observations\n\n\n\n\nAufgabe 3: Tagesgang\n\n# 3.3 Tagesgang ####\n# Bei diesen Berechnungen wird jeweils der Mittelwert pro Stunde berechnet. \n# wiederum nutzen wir dafuer \"pipes\"\nMean_h <- depo %>% \n  group_by(Wochentag, Stunde, Phase) %>% \n  summarise(Total = mean(Total)) \n\n# Plotte den Tagesgang, unterteilt nach Wochentagen\n\n# Normal\ntag_norm <- ggplot(subset(Mean_h, Phase %in% c(\"Normal\")), \n                   mapping=aes(x = Stunde, y = Total, colour = Wochentag, linetype = Wochentag))+\n  geom_line(size = 2)+\n  scale_colour_viridis_d()+\n  scale_linetype_manual(values = c(rep(\"solid\", 5),  \"twodash\", \"twodash\"))+\n  scale_x_continuous(breaks = c(seq(0, 23, by = 2)), labels = c(seq(0, 23, by = 2)))+\n  labs(x=\"Uhrzeit [h]\", y= \"∅ Fussganger_Innen / h\", title = \"\")+\n  lims(y = c(0,25))+\n  theme_linedraw(base_size = 15)+\n  theme(legend.position = \"right\")\n\n# Lockdown 1\ntag_lock_1 <- ggplot(subset(Mean_h, Phase %in% c(\"Lockdown_1\")), \n                     mapping=aes(x = Stunde, y = Total, colour = Wochentag, linetype = Wochentag))+\n  geom_line(size = 2)+\n  scale_colour_viridis_d()+\n  scale_linetype_manual(values = c(rep(\"solid\", 5), \"twodash\", \"twodash\"))+\n  scale_x_continuous(breaks = c(seq(0, 23, by = 2)), labels = c(seq(0, 23, by = 2)))+\n  labs(x=\"Uhrzeit [h]\", y= \"∅ Fussganger_Innen / h\", title = \"\")+\n  lims(y = c(0,25))+\n  theme_linedraw(base_size = 15)+\n  theme(legend.position = \"right\")\n\n# Lockdown 2\ntag_lock_2 <- ggplot(subset(Mean_h, Phase %in% c(\"Lockdown_2\")), \n                     mapping=aes(x = Stunde, y = Total, colour = Wochentag, linetype = Wochentag))+\n  geom_line(size = 2)+\n  scale_colour_viridis_d()+\n  scale_linetype_manual(values = c(rep(\"solid\", 5), \"twodash\", \"twodash\"))+\n  scale_x_continuous(breaks = c(seq(0, 23, by = 2)), labels = c(seq(0, 23, by = 2)))+\n  labs(x=\"Uhrzeit [h]\", y= \"∅ Fussganger_Innen / h\", title = \"\")+\n  lims(y = c(0,25))+\n  theme_linedraw(base_size = 15)+\n  theme(legend.position = \"right\")\n\n# Covid\ntag_covid <- ggplot(subset(Mean_h, Phase %in% c(\"Covid\")), \n                    mapping=aes(x = Stunde, y = Total, colour = Wochentag, linetype = Wochentag))+\n  geom_line(size = 2)+\n  scale_colour_viridis_d()+\n  scale_linetype_manual(values = c(rep(\"solid\", 5), \"twodash\", \"twodash\"))+\n  scale_x_continuous(breaks = c(seq(0, 23, by = 2)), labels = c(seq(0, 23, by = 2)))+\n  labs(x=\"Uhrzeit [h]\", y= \"∅ Fussganger_Innen / h\", title = \"\")+\n  lims(y = c(0,25))+\n  theme_linedraw(base_size = 15)+\n  theme(legend.position = \"right\")\n\n# Arrange und Export Tagesgang\nggarrange(tag_lock_1+            # plot 1 aufrufen\n            rremove(\"x.text\")+   # plot 1 braucht es nicht alle Achsenbeschriftungen\n            rremove(\"x.title\"),            \n          tag_lock_2+            # plot 2 aufrufen\n            rremove(\"y.text\")+   # bei plot 2 brauchen wir keine Achsenbeschriftung\n            rremove(\"y.title\")+\n            rremove(\"x.text\")+\n            rremove(\"x.title\"),\n          tag_norm,\n          tag_covid+\n            rremove(\"y.text\")+   \n            rremove(\"y.title\"),\n          ncol = 2, nrow = 2,    # definieren, wie die plots angeordnet werden\n          heights = c(0.9, 1),  # beide plots sind wegen der fehlenden Beschriftung nicht gleich hoch\n          widths = c(1,0.9),    \n          labels = c(\"a) Lockdown 1\", \"b) Lockdown 2\", \"c) Normal\", \"d) Covid\"),\n          label.x = 0.1,        # wo stehen die Plottitel\n          label.y = 0.99,\n          common.legend = TRUE, legend = \"bottom\") # wir brauchen nur eine Legende, unten\n\n\n\n\n\n\n\n# ggsave(\"Tagesgang.png\", width=25, height=25, units=\"cm\", dpi=1000,\n#        path = \"_fallstudien/_R_analysis/results/\")\n\n\n\nAufgabe 4: Kennzahlen\n\n# 3.4 Kennzahlen ####\ntotal_phase <- depo_d %>% \n  # gruppiere nach Phasen inkl. Normal. Diese Levels haben wir bereits definiert\n  group_by(Phase) %>% \n  summarise(Total = sum(Total),\n            IN = sum(Fuss_IN),\n            OUT = sum(Fuss_OUT))\n\n# write.csv(total_phase, \"_fallstudien/_R_analysis/results/total_phase.csv\")\n\n# mean besser Vergleichbar, da Zeitreihen unterschiedlich lange\nmean_phase_d <- depo_d %>% \n  group_by(Phase) %>% \n  summarise(Total = mean(Total),\n            IN = mean(Fuss_IN),\n            OUT = mean(Fuss_OUT))\n# berechne prozentuale Richtungsverteilung\nmean_phase_d <- mean_phase_d %>% \n  mutate(Proz_IN = round(100/Total*IN, 1)) %>% # berechnen und auf eine Nachkommastelle runden\n  mutate(Proz_OUT = round(100/Total*OUT,1))\n\n# write.csv(mean_phase_d, \"_fallstudien/_R_analysis/results/mean_phase_d.csv\")\n\n# selektiere absolute Zahlen\n# behalte rel. Spalten (nur die relativen Prozentangaben)\nmean_phase_d_abs <- mean_phase_d %>% dplyr::select(-c(Total, Proz_IN, Proz_OUT))\n\n# transformiere fuer Plotting\nmean_phase_d_abs <- pivot_longer(mean_phase_d_abs, cols = c(\"IN\",\"OUT\"), \n             names_to = \"Gruppe\", values_to = \"Durchschnitt\")\n\n# selektiere relative Zahlen\n# behalte rel. Spalten (nur die relativen Prozentangaben)\nmean_phase_d_proz <- mean_phase_d %>% dplyr::select(-c(Total:OUT))\n\n# transformiere fuer Plotting\nmean_phase_d_proz <- pivot_longer(mean_phase_d_proz, cols = c(\"Proz_IN\",\"Proz_OUT\"), \n                                  names_to = \"Gruppe\", values_to = \"Durchschnitt\")\n\n# Visualisierung abs\nabs <- ggplot(data = mean_phase_d_abs, mapping = aes(x = Gruppe, y = Durchschnitt, fill = Phase))+\n  geom_col(position = \"dodge\", width = 0.8)+\n  scale_fill_manual(values = c(\"royalblue\", \"red4\", \"orangered\", \"gold2\"), name = \"Phase\")+\n  scale_x_discrete(labels = c(\"IN\", \"OUT\"))+\n  labs(y = \"Durchschnitt [mean]\", x= \"Bewegungsrichtung\")+\n  theme_classic(base_size = 15)+\n  theme(legend.position = \"bottom\")\n\n# Visualisierung %\nproz <- ggplot(data = mean_phase_d_proz, mapping = aes(x = Gruppe, y = Durchschnitt, fill = Phase))+\n  geom_col(position = \"dodge\", width = 0.8)+\n  scale_fill_manual(values = c(\"royalblue\", \"red4\", \"orangered\", \"gold2\"), name = \"Phase\")+\n  scale_x_discrete(labels = c(\"IN\", \"OUT\"))+\n  labs(y = \"Durchschnitt [%]\", x= \"Bewegungsrichtung\")+\n  theme_classic(base_size = 15)+\n  theme(legend.position = \"bottom\")\n\n# Arrange und Export Verteilung\nggarrange(abs,            # plot 1 aufrufen\n          proz,            # plot 2 aufrufen\n          ncol = 2, nrow = 1,    # definieren, wie die plots angeordnet werden\n          heights = c(1),        # beide sind bleich hoch\n          widths = c(1,0.95),    # plot 2 ist aufgrund der fehlenden y-achsenbesch. etwas schmaler\n          labels = c(\"a) Absolute Verteilung\", \"b) Relative Verteilung\"),\n          label.x = 0,        # wo stehen die labels\n          label.y = 1.0,\n          common.legend = TRUE, legend = \"bottom\") # wir brauchen nur eine Legende, unten\n\n\n\n\n\n\n\n# ggsave(\"Verteilung.png\", width=20, height=15, units=\"cm\", dpi=1000,\n#        path = \"_fallstudien/_R_analysis/results/\")"
  },
  {
    "objectID": "fallstudie_s/7_Multivariate_Statistik_Loesung.html",
    "href": "fallstudie_s/7_Multivariate_Statistik_Loesung.html",
    "title": "7. Lösung",
    "section": "",
    "text": "Aufgabe 1: Verbinden von Daten (Join)\n\n# 4.1 Einflussfaktoren Besucherzahl ####\n# Erstelle ein df indem die taeglichen Zaehldaten und Meteodaten vereint sind\numwelt <- inner_join(depo_d, meteo, by = c(\"Datum\" = \"time\"))\n# Das zusammenfuehren folgt evtl. in NA-Werten bei gewissen Tagen\nsum(is.na(umwelt))\n\n\n\nAufgabe 2: Convinience Variablen, Faktoren, Skalieren\n\n# nochmals einige Convinience Variablen\numwelt <- umwelt%>%\n  mutate(Ferien = if_else(  \n    Datum >= Fruehlingsferien_2019_start & Datum <= Fruehlingsferien_2019_ende |\n      Datum >= Sommerferien_2019_start & Datum <= Sommerferien_2019_ende |\n      Datum >= Herbstferien_2019_start & Datum <= Herbstferien_2019_ende |\n      Datum >= Winterferien_2019_start & Datum <= Winterferien_2019_ende |\n      Datum >= Fruehlingsferien_2020_start & Datum <= Fruehlingsferien_2020_ende |\n      Datum >= Sommerferien_2020_start & Datum <= Sommerferien_2020_ende |\n      Datum >= Herbstferien_2020_start & Datum <= Herbstferien_2020_ende |\n      Datum >= Winterferien_2020_start & Datum <= Winterferien_2020_ende |\n      Datum >= Fruehlingsferien_2021_start & Datum <= Fruehlingsferien_2021_ende |\n      Datum >= Sommerferien_2021_start & Datum <= max(depo$Datum),\n        \"1\", \"0\"))%>%\n  mutate(Ferien = factor(Ferien))\n\n# Faktor und integer\n# Im GLMM wird die Kalenderwoche und das Jahr als random factor definiert. Dazu muss sie als\n# Faktor vorliegen.\numwelt <- umwelt %>% \n  mutate(Jahr = as.factor(Jahr)) %>% \n  mutate(KW = as.factor(KW))\n\n# Unser Modell kann nur mit ganzen Zahlen umgehen. Zum Glueck habe wir die Zaehldaten\n# bereits gerundet.\n\n# pruefe str des df\nsummary(umwelt)\nstr(umwelt)\nsum(is.na(umwelt))\n\n# unser Datensatz muss ein df sein, damit scale funktioniert\numwelt <- as.data.frame(umwelt)\n\n#  Variablen skalieren\n# Skalieren der Variablen, damit ihr Einfluss vergleichbar wird \n# (Problem verschiedene Skalen der Variablen (bspw. Temperatur in Grad Celsius, \n# Niederschlag in Millimeter und Sonnenscheindauer in Minuten)\n\numwelt <- umwelt %>% \n  mutate(tre200jx_scaled = scale(tre200jx), \n         rre150j0_scaled = scale(rre150j0), \n         sremaxdv_scaled = scale(sremaxdv))\n\n\n\nAufgabe 3: Korrelationen und Variablenselektion\n\n# 4.2 Variablenselektion ####\n# Korrelierende Variablen koennen das Modelergebnis verfaelschen. Daher muss vor der\n# Modelldefinition auf Korrelation getestet werden.\n\n# Erklaerende Variablen definieren\n# Hier wird die Korrelation zwischen den (nummerischen) erklaerenden Variablen berechnet\ncor <-  cor(umwelt[,16:(ncol(umwelt))]) # in den [] waehle ich die skalierten Spalten.\n# Mit dem folgenden Code kann eine simple Korrelationsmatrix aufgebaut werden\n# hier kann auch die Schwelle für die Korrelation gesetzt werden, \n# 0.7 ist liberal / 0.5 konservativ\n# https://researchbasics.education.uconn.edu/r_critical_value_table/\ncor[abs(cor)<0.7] <-  0 #Setzt alle Werte kleiner 0.7 auf 0 (diese sind dann ok, alles groesser ist problematisch!)\ncor\n\n# Korrelationsmatrix erstellen\n# Zur Visualisierung kann ein einfacher Plot erstellt werden:\nchart.Correlation(umwelt[,16:(ncol(umwelt))], histogram=TRUE, pch=19)\n\n\n\n\n\n\n\n\n\n\nAufgabe 4 (OPTIONAL): Automatische Variablenselektion\n\n# # Automatisierte Variablenselektion \n# # fuehre die dredge-Funktion und ein Modelaveraging durch\n# # Hier wird die Formel für die dredge-Funktion vorbereitet\n# f <- Total ~ Wochentag + Ferien + Phase +\n#   tre200jx_scaled + rre150j0_scaled + sremaxdv_scaled\n# # Jetzt kommt der Random-Factor hinzu und es wird eine Formel daraus gemacht\n# f_dredge <- paste(c(f, \"+ (1|KW)\", \"+ (1|Jahr)\"), collapse = \" \") %>% \n#   as.formula()\n# # Das Modell mit dieser Formel ausführen\n# m <- glmer(f_dredge, data = umwelt, family = poisson, na.action = \"na.fail\")\n# # Das Modell in die dredge-Funktion einfügen (siehe auch ?dredge)\n# all_m <- dredge(m)\n# # suche das beste Modell\n# print(all_m)\n# # Importance values der Variablen \n# # hier wird die wichtigkeit der Variablen in den verschiedenen Modellen abgelesen\n# MuMIn::importance(all_m) \n# \n# # Schliesslich wird ein Modelaverage durchgeführt \n# # Schwellenwert für das delta-AIC = 2\n# avgmodel <- model.avg(all_m, rank = \"AICc\", subset = delta < 500) \n# summary(avgmodel)\n\n\n\nAufgabe 5: Verteilung der abhängigen Variabel pruefen\n\n# 4.3 Pruefe Verteilung ####\n# pruefe zuerst nochmals, ob wir NA im df haben:\nsum(is.na(umwelt$Total))\n\nf1<-fitdist(umwelt$Total,\"norm\")  # Normalverteilung\n# f1_1<-fitdist((umwelt$Total + 1),\"lnorm\")  # log-Normalvert (beachte, dass ich +1 rechne. \n# log muss positiv sein; allerdings kann man die\n# Verteilungen dann nicht mehr miteinander vergleichen). \nf2<-fitdist(umwelt$Total,\"pois\")  # Poisson\nf3<-fitdist(umwelt$Total,\"nbinom\")  # negativ binomial\nf4<-fitdist(umwelt$Total,\"exp\")  # exponentiell\n# f5<-fitdist(umwelt$Total,\"gamma\")  # gamma (berechnung mit meinen Daten nicht möglich)\nf6<-fitdist(umwelt$Total,\"logis\")  # logistisch\nf7<-fitdist(umwelt$Total,\"geom\")  # geometrisch\n# f8<-fitdist(umwelt$Total,\"weibull\")  # Weibull (berechnung mit meinen Daten nicht möglich)\n\ngofstat(list(f1,f2,f3,f4,f6,f7), \n        fitnames = c(\"Normalverteilung\", \"Poisson\",\n                     \"negativ binomial\",\"exponentiell\", \"logistisch\",\n                     \"geometrisch\"))\n\n# die 2 besten (gemaess Akaike's Information Criterion) als Plot + normalverteilt, \nplot.legend <- c(\"Normalverteilung\", \"exponentiell\", \"negativ binomial\")\n# vergleicht mehrere theoretische Verteilungen mit den empirischen Daten\ncdfcomp(list(f1, f4, f3), legendtext = plot.legend)\n\n\n\n\n\n\n\n# --> Verteilung ist gemäss AICc negativ binomial. --> ich entscheide \n# mich für letztere.\n\n\n\nAufgabe 6: Multivariates Modell berechnen\n\n# 4.4 Berechne verschiedene Modelle ####\n\n# Hinweise zu GLMM: https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html\n\n# Ich verwende hier die Funktion glmer aus der Bibliothek lme4. glmer sei neuer, \n# schneller und zuverlaessiger als vergleichbare Funktionen.\n# Die Totale Besucheranzahl soll durch verschiedene Parameter erklaert werden. \n# Die saisonalitaet (KW, Jahr) soll hierbei nicht beachtet werden, \n# sie wird als random Faktor bestimmt --> Saisonbereinigung.\n\n# Einfacher Start\n# Auch wenn wir gerade herausgefunden haben, dass die Verteilung negativ binomial ist,\n# berechne ich für den Vergleich zuerst ein einfaches Modell der Familie poisson.\nTages_Model <- glmer(Total ~ Wochentag + Ferien + Phase +\n                       tre200jx_scaled + rre150j0_scaled + sremaxdv_scaled +\n                       (1|KW) + (1|Jahr), family = poisson, data = umwelt)\n\nError: Invalid grouping factor specification, KW\n\nsummary(Tages_Model)\n\nError in h(simpleError(msg, call)): error in evaluating the argument 'object' in selecting a method for function 'summary': object 'Tages_Model' not found\n\n# Inspektionsplots\nplot(Tages_Model, type = c(\"p\", \"smooth\"))\n\nError in plot(Tages_Model, type = c(\"p\", \"smooth\")): object 'Tages_Model' not found\n\nqqmath(Tages_Model)\n\nError in qqmath(Tages_Model): object 'Tages_Model' not found\n\n# pruefe auf Overdispersion\ndispersion_glmer(Tages_Model) #it shouldn't be over 1.4\n\nError in resid(modelglmer): object 'Tages_Model' not found\n\n# wir gut erklaert das Modell?\nr.squaredGLMM(Tages_Model) \n\nError in r.squaredGLMM(Tages_Model): object 'Tages_Model' not found\n\n# Berechne ein negativ binomiales Modell\n# gemäss AICc die beste Verteilung\nTages_Model_nb <- glmer.nb(Total ~ Wochentag + Ferien + Phase +\n                             tre200jx_scaled + rre150j0_scaled + sremaxdv_scaled +\n                             (1|KW) + (1|Jahr), data = umwelt)\n\nError: Invalid grouping factor specification, KW\n\nsummary(Tages_Model_nb)\n\nError in h(simpleError(msg, call)): error in evaluating the argument 'object' in selecting a method for function 'summary': object 'Tages_Model_nb' not found\n\nplot(Tages_Model_nb, type = c(\"p\", \"smooth\"))\n\nError in plot(Tages_Model_nb, type = c(\"p\", \"smooth\")): object 'Tages_Model_nb' not found\n\nqqmath(Tages_Model_nb)\n\nError in qqmath(Tages_Model_nb): object 'Tages_Model_nb' not found\n\ndispersion_glmer(Tages_Model_nb)\n\nError in resid(modelglmer): object 'Tages_Model_nb' not found\n\nr.squaredGLMM(Tages_Model_nb) \n\nError in r.squaredGLMM(Tages_Model_nb): object 'Tages_Model_nb' not found\n\n# auf quadratischen Term testen (\"es gehen weniger Leute in den Wald, wenn es zu heiss ist\")\nTages_Model_nb_quad <- glmer.nb(Total ~ Wochentag + Ferien + Phase +\n                                  tre200jx_scaled + I(tre200jx_scaled^2) + \n                                  rre150j0_scaled + sremaxdv_scaled +\n                                  (1|KW) + (1|Jahr), data = umwelt)\n\nError: Invalid grouping factor specification, KW\n\nsummary(Tages_Model_nb_quad)\n\nError in h(simpleError(msg, call)): error in evaluating the argument 'object' in selecting a method for function 'summary': object 'Tages_Model_nb_quad' not found\n\nplot(Tages_Model_nb_quad, type = c(\"p\", \"smooth\"))\n\nError in plot(Tages_Model_nb_quad, type = c(\"p\", \"smooth\")): object 'Tages_Model_nb_quad' not found\n\nqqmath(Tages_Model_nb_quad)\n\nError in qqmath(Tages_Model_nb_quad): object 'Tages_Model_nb_quad' not found\n\ndispersion_glmer(Tages_Model_nb_quad)\n\nError in resid(modelglmer): object 'Tages_Model_nb_quad' not found\n\nr.squaredGLMM(Tages_Model_nb_quad) \n\nError in r.squaredGLMM(Tages_Model_nb_quad): object 'Tages_Model_nb_quad' not found\n\n# Interaktion testen, da Ferien und / oder Wochentage einen Einfluss auf\n# die Besuchszahlen waehrend des Lockown haben koennen!\n# (Achtung: Rechenintensiv!)\n# Tages_Model_nb_int <- glmer.nb(Anzahl_Total ~  Wochentag  * Ferien + Phase +\n#                                  tre200jx_scaled + I(tre200jx_scaled^2) * \n#                                  rre150j0_scaled + sremaxdv_scaled +\n#                                  (1|KW) + (1|Jahr), data = umwelt)\n# \n# summary(Tages_Model_nb_int)\n# plot(Tages_Model_nb_int, type = c(\"p\", \"smooth\"))\n# qqmath(Tages_Model_nb_int)\n# dispersion_glmer(Tages_Model_nb_int)\n# r.squaredGLMM(Tages_Model_nb_int) \n\n# Vergleich der Modellguete mittels AICc\ncand.models<-list()\ncand.models[[1]] <- Tages_Model\n\nError in eval(expr, envir, enclos): object 'Tages_Model' not found\n\ncand.models[[2]] <- Tages_Model_nb\n\nError in eval(expr, envir, enclos): object 'Tages_Model_nb' not found\n\ncand.models[[3]] <- Tages_Model_nb_quad\n\nError in eval(expr, envir, enclos): object 'Tages_Model_nb_quad' not found\n\nModnames<-c(\"Tages_Model\",\"Tages_Model_nb\", \n            \"Tages_Model_nb_quad\")\naictab(cand.set=cand.models,modnames=Modnames)\n\nError in `$<-.data.frame`(`*tmp*`, \"Delta_AICc\", value = numeric(0)): replacement has 0 rows, data has 3\n\n#K = Anzahl geschaetzter Parameter (2 Funktionsparameter und die Varianz)\n#Delta_AICc <2 = Statistisch gleichwertig\n#AICcWt =  Akaike weight in %\n\n# --> Ich entscheide mich bei diesen drei Modellen für das Tages_Model_nb_quad\n# Warum: statistisch gleichwertig und ich denke die Quadratur macht Sinn!\n\n# Berechne ein Modell mit exponentieller Verteilung:\n# gemäss AICc der Verteilung die zweitbeste\n# https://stats.stackexchange.com/questions/240455/fitting-exponential-regression-model-by-mle\nTages_Model_exp <- glmer((Total+1) ~ Wochentag + Ferien + Phase +\n                           tre200jx_scaled + rre150j0_scaled + sremaxdv_scaled +\n                           (1|KW) + (1|Jahr), family = Gamma(link=\"log\"), data = umwelt)\n\nError: Invalid grouping factor specification, KW\n\nsummary(Tages_Model_exp, dispersion=1)\n\nError in h(simpleError(msg, call)): error in evaluating the argument 'object' in selecting a method for function 'summary': object 'Tages_Model_exp' not found\n\n# Inspektionsplots\nplot(Tages_Model_exp, type = c(\"p\", \"smooth\"))\n\nError in plot(Tages_Model_exp, type = c(\"p\", \"smooth\")): object 'Tages_Model_exp' not found\n\nqqmath(Tages_Model_exp)\n\nError in qqmath(Tages_Model_exp): object 'Tages_Model_exp' not found\n\n# pruefe auf Overdispersion\ndispersion_glmer(Tages_Model_exp) #it shouldn't be over 1.4\n\nError in resid(modelglmer): object 'Tages_Model_exp' not found\n\n# wir gut erklaert das Modell?\nr.squaredGLMM(Tages_Model_exp) \n\nError in r.squaredGLMM(Tages_Model_exp): object 'Tages_Model_exp' not found\n\n# --> Die zweitbeste Verteilung (exp) führt auch nicht dazu, dass die Modellvoraussetzungen besser\n# erfüllt werden\n\n# 4.5 Transformationen ####\n# Die Modellvoraussetzungen waren überall mehr oder weniger verletzt.\n# Das ist ein Problem, allerdings auch nicht ein so grosses.\n# (man sollte es aber trotzdem ernst nehmen)\n# Schielzeth et al. Robustness of linear mixed‐effects models to violations of distributional assumptions\n# https://besjournals.onlinelibrary.wiley.com/doi/10.1111/2041-210X.13434\n# Lo and Andrews, To transform or not to transform: using generalized linear mixed models to analyse reaction time data\n# https://www.frontiersin.org/articles/10.3389/fpsyg.2015.01171/full\n\n# die Lösung ist nun, die Daten zu transformieren:\n# mehr unter: https://www.datanovia.com/en/lessons/transform-data-to-normal-distribution-in-r/\n\n# berechne skewness coefficient \nlibrary(moments)\nskewness(umwelt$Total)\n# A positive value means the distribution is positively skewed (rechtsschief).\n# The most frequent values are low; tail is toward the high values (on the right-hand side)\n\n# log 10, da stark rechtsschief\nTages_Model_quad_Jahr_log10 <- lmer(log10(Total+1) ~ Wochentag + Ferien + Phase +\n                                             tre200jx_scaled + I(tre200jx_scaled^2) + \n                                             rre150j0_scaled + sremaxdv_scaled +\n                                             (1|KW) + (1|Jahr), data = umwelt)\n\nError in lme4::lFormula(formula = log10(Total + 1) ~ Wochentag + Ferien + : 0 (non-NA) cases\n\nsummary(Tages_Model_quad_Jahr_log10)\n\nError in h(simpleError(msg, call)): error in evaluating the argument 'object' in selecting a method for function 'summary': object 'Tages_Model_quad_Jahr_log10' not found\n\nplot(Tages_Model_quad_Jahr_log10, type = c(\"p\", \"smooth\"))\n\nError in plot(Tages_Model_quad_Jahr_log10, type = c(\"p\", \"smooth\")): object 'Tages_Model_quad_Jahr_log10' not found\n\nqqmath(Tages_Model_quad_Jahr_log10)\n\nError in qqmath(Tages_Model_quad_Jahr_log10): object 'Tages_Model_quad_Jahr_log10' not found\n\ndispersion_glmer(Tages_Model_quad_Jahr_log10)\n\nError in resid(modelglmer): object 'Tages_Model_quad_Jahr_log10' not found\n\nr.squaredGLMM(Tages_Model_quad_Jahr_log10) \n\nError in r.squaredGLMM(Tages_Model_quad_Jahr_log10): object 'Tages_Model_quad_Jahr_log10' not found\n\n# lmer zeigt keine p-Werte, da diese schwer zu berechnen sind. Alternative Packages berechnen diese\n# anhand der Teststatistik. Achtung: die Werte sind wahrscheinlich nicht präzise!\n# https://stat.ethz.ch/pipermail/r-sig-mixed-models/2008q2/000904.html\ntab_model(Tages_Model_quad_Jahr_log10, transform = NULL, show.se = TRUE)\n\nError in tab_model(Tages_Model_quad_Jahr_log10, transform = NULL, show.se = TRUE): object 'Tages_Model_quad_Jahr_log10' not found\n\n# natural log, da stark rechtsschief\nTages_Model_quad_Jahr_ln <- lmer(log(Total+1) ~ Wochentag + Ferien + Phase +\n                                          tre200jx_scaled + I(tre200jx_scaled^2) + \n                                          rre150j0_scaled + sremaxdv_scaled +\n                                          (1|KW) + (1|Jahr), data = umwelt)\n\nError in lme4::lFormula(formula = log(Total + 1) ~ Wochentag + Ferien + : 0 (non-NA) cases\n\nsummary(Tages_Model_quad_Jahr_ln)\n\nError in h(simpleError(msg, call)): error in evaluating the argument 'object' in selecting a method for function 'summary': object 'Tages_Model_quad_Jahr_ln' not found\n\nplot(Tages_Model_quad_Jahr_ln, type = c(\"p\", \"smooth\"))\n\nError in plot(Tages_Model_quad_Jahr_ln, type = c(\"p\", \"smooth\")): object 'Tages_Model_quad_Jahr_ln' not found\n\nqqmath(Tages_Model_quad_Jahr_ln)\n\nError in qqmath(Tages_Model_quad_Jahr_ln): object 'Tages_Model_quad_Jahr_ln' not found\n\ndispersion_glmer(Tages_Model_quad_Jahr_ln)\n\nError in resid(modelglmer): object 'Tages_Model_quad_Jahr_ln' not found\n\nr.squaredGLMM(Tages_Model_quad_Jahr_ln) \n\nError in r.squaredGLMM(Tages_Model_quad_Jahr_ln): object 'Tages_Model_quad_Jahr_ln' not found\n\n# --> Die Modellvoraussetzungen sind deutlich besser erfüllt jetzt wo wir Transformationen \n# benutzt haben. log10 und ln performen beide gleich gut. Da log10 in meinem Bsp\n# aber deutlich mehr der Varianz erklärt, entscheide ich mich schliesslich für dieses Modell.\n\n# Zusatz: ACHTUNG - Ruecktransformierte Regressionskoeffizienten zu erlangen (fuer die Interpretation, das Plotten), \n# ist zudem nicht moeglich (Regressionskoeffizienten sind nur im transformierten Raum linear). \n# Ein ruecktransformierter Regressionskoeffiziente haette eine nicht-lineare Beziehung mit der \n# abhaengigen Variable.\n\n# 4.6 Exportiere die Modellresultate ####\n# (des besten Modells)\ntab_model(Tages_Model_quad_Jahr_log10, transform = NULL, show.se = TRUE)\n\nError in tab_model(Tages_Model_quad_Jahr_log10, transform = NULL, show.se = TRUE): object 'Tages_Model_quad_Jahr_log10' not found\n\n# The marginal R squared values are those associated with your fixed effects, \n# the conditional ones are those of your fixed effects plus the random effects. \n# Usually we will be interested in the marginal effects.\n\n\n\nAufgabe 7: Modellvisualisierung\n\n# 4.7 Visualisiere Modellresultate ####\n\n# ZUSATZ: Wir haben die Wetterparameter skaliert. \n# Fuer die Plots muss das beruecksichtigt werden: wir stellen nicht die wirklichen Werte\n# dar sondern die skalierten. Mit folgendem Befehl kann man die Skalierung nachvollziehen:\n# attributes(umwelt$tre200jx_scaled)\n# Die Skalierung kann rueckgaengig gemacht werden, indem man die Skalierten werte mit\n# dem scaling factor multipliziert und dann den Durchschnitt addiert:\n# Bsp.: d$s.x * attr(d$s.x, 'scaled:scale') + attr(d$s.x, 'scaled:center')\n# mehr dazu: https://stackoverflow.com/questions/10287545/backtransform-scale-for-plotting\n# --> wir bleiben aber bei den skalierten Werten, leben damit und sind uns dessen bewusst.\n\n# Auch beim Plotten der Modellresultate gilt: \n# visualisiere nur die Parameter welche nach der Modellselektion uebig bleiben\n# und signifikant sind!\n# plot_model / type = \"pred\" sagt die Werte \"voraus\"\n# achte auf gleiche Skalierung der y-Achse (Vergleichbarkeit)\n\n# Temperatur\nt <- plot_model(Tages_Model_quad_Jahr_log10, type = \"pred\", terms = \n                  \"tre200jx_scaled [all]\", # [all] = Model contains polynomial or cubic / \n                #quadratic terms. Consider using `terms=\"tre200jx_scaled [all]\"` \n                # to get smooth plots. See also package-vignette \n                # 'Marginal Effects at Specific Values'.\n                title = \"\", axis.title = c(\"Tagesmaximaltemperatur [°C]\", \n                                           \"Fussgaenger:innen pro Tag [log]\"))\n\nError in insight::model_info(model): object 'Tages_Model_quad_Jahr_log10' not found\n\n# fuege die Achsenbeschriftung hinzu. Hier wird auf die unskalierten Werte zugegriffen.\nlabels <- round(seq(floor(min(umwelt$tre200jx)), ceiling(max(umwelt$tre200jx)),\n                    # length.out = ___ --> Anpassen gemaess breaks auf dem Plot\n                    length.out = 5), 0) \n(Tempplot <- t + \n    scale_x_continuous(breaks = c(-2,-1,0,1,2), \n                       labels = c(labels))+\n    # fuege die y- Achsenbeschriftung hinzu. Hier transformieren wir die Werte zurueck\n    scale_y_continuous(breaks = c(0,0.5,1,1.5,2),\n                       labels = round(c(10^0, 10^0.5, 10^1, 10^1.5, 10^2),0),\n                       limits = c(0, 2))+ \n    theme_classic(base_size = 20))\n\n# ggsave(\"temp.png\", width=15, height=15, units=\"cm\", dpi=1000, \n#        path = \"fallstudien/_R_analysis/results/\") \n\n# Regen\nt <- plot_model(Tages_Model_quad_Jahr_log10, type = \"pred\", terms = \"rre150j0_scaled\", \n                title = \"\", axis.title = c(\"Halbtagessumme Niederschlag [mm]\", \n                                           \"Fussgaenger:innen pro Tag [log]\"))\n\nError in insight::model_info(model): object 'Tages_Model_quad_Jahr_log10' not found\n\nlabels <- round(seq(floor(min(umwelt$rre150j0)), ceiling(max(umwelt$rre150j0)),\n                    length.out = 4), 0)\n(t + scale_x_continuous(breaks = c(0,4,8,12), labels = c(labels))+\n    scale_y_continuous(breaks = c(0,0.5,1,1.5,2),\n                       labels = round(c(10^0, 10^0.5, 10^1, 10^1.5, 10^2),0),\n                       limits = c(0, 2))+ \n    theme_classic(base_size = 20))\n\n# ggsave(\"rain.png\", width=15, height=15, units=\"cm\", dpi=1000, \n#        path = \"fallstudien/_R_analysis/results/\") \n\n# Sonne\nt <- plot_model(Tages_Model_quad_Jahr_log10, type = \"pred\", terms = \"sremaxdv_scaled\", \n                title = \"\", axis.title = c(\"Sonnenscheindauer [min]\", \n                                           \"Fussgaenger:innen pro Tag [log]\"))\n\nError in insight::model_info(model): object 'Tages_Model_quad_Jahr_log10' not found\n\nlabels <- round(seq(floor(min(umwelt$sremaxdv)), ceiling(max(umwelt$sremaxdv)),\n                    length.out = 3), 0)\n(t + scale_x_continuous(breaks = c(-1,0,1), labels = c(labels))+\n    scale_y_continuous(breaks = c(0,0.5,1,1.5,2),\n                       labels = round(c(10^0, 10^0.5, 10^1, 10^1.5, 10^2),0),\n                       limits = c(0, 2))+ \n    theme_classic(base_size = 20))\n\n# ggsave(\"sun.png\", width=15, height=15, units=\"cm\", dpi=1000, \n#        path = \"fallstudien/_R_analysis/results/\") \n\n# Phase\nt <- plot_model(Tages_Model_quad_Jahr_log10, type = \"pred\", terms = \"Phase\", \n                title = \"\", axis.title = c(\"Phase\", \n                                           \"Fussgaenger:innen pro Tag [log]\"))\n\nError in insight::model_info(model): object 'Tages_Model_quad_Jahr_log10' not found\n\n(lockplot <- t + \n    scale_y_continuous(breaks = c(0,0.5,1,1.5,2),\n                       labels = round(c(10^0, 10^0.5, 10^1, 10^1.5, 10^2),0),\n                       limits = c(0, 2))+ \n    theme_classic(base_size = 20))\n\n# ggsave(\"phase.png\", width=15, height=15, units=\"cm\", dpi=1000, \n#        path = \"fallstudien/_R_analysis/results/\") \n\n# Ferien\nt <- plot_model(Tages_Model_quad_Jahr_log10, type = \"pred\", terms = \"Ferien\", \n                title = \"\", axis.title = c(\"Ferien\", \n                                           \"Fussgaenger:innen pro Tag [log]\"))\n\nError in insight::model_info(model): object 'Tages_Model_quad_Jahr_log10' not found\n\n(ferienplot <- t + scale_x_continuous(breaks = c(0,1), labels = c(\"Nein\", \"Ja\"))+\n    scale_y_continuous(breaks = c(0,0.5,1,1.5,2),\n                       labels = round(c(10^0, 10^0.5, 10^1, 10^1.5, 10^2),0),\n                       limits = c(0, 2))+ \n    theme_classic(base_size = 20))\n\n# ggsave(\"ferien.png\", width=15, height=15, units=\"cm\", dpi=1000, \n#        path = \"fallstudien/_R_analysis/results/\") \n\n# Wochentag\nt <- plot_model(Tages_Model_quad_Jahr_log10, type = \"pred\", terms = \"Wochentag\", \n                title = \"\", axis.title = c(\"Wochentag\", \"Fussgaenger:innen pro Tag [log]\"))\n\nError in insight::model_info(model): object 'Tages_Model_quad_Jahr_log10' not found\n\n(wdplot <- t + scale_y_continuous(breaks = c(0,0.5,1,1.5,2),\n                                  labels = round(c(10^0, 10^0.5, 10^1, 10^1.5, 10^2),0),\n                                  limits = c(0, 2))+ \n    theme_classic(base_size = 20))+\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))\n\n# ggsave(\"wd.png\", width=15, height=15, units=\"cm\", dpi=1000, \n#        path = \"fallstudien/_R_analysis/results/\")"
  },
  {
    "objectID": "fallstudie_s/7_Multivariate_Statistik_Uebung.html",
    "href": "fallstudie_s/7_Multivariate_Statistik_Uebung.html",
    "title": "7. Übung",
    "section": "",
    "text": "Nachdem die deskriptiven Resultate vorliegen, kann jetzt die Berechnung eines multivariaten Modells angegangen werden. Das Ziel ist es, den Zusammenhang zwischen der gesamten Anzahl Besucher:innen (Total) und verschiedenen erklärenden Variablen (Wetter, Ferien, Phase Covid, Wochentag, KW, Jahr) aufzuzeigen."
  },
  {
    "objectID": "fallstudie_s/7_Multivariate_Statistik_Uebung.html#aufgabe-4-optional-automatische-variablenselektion",
    "href": "fallstudie_s/7_Multivariate_Statistik_Uebung.html#aufgabe-4-optional-automatische-variablenselektion",
    "title": "7. Übung",
    "section": "Aufgabe 4 (OPTIONAL): Automatische Variablenselektion",
    "text": "Aufgabe 4 (OPTIONAL): Automatische Variablenselektion\nFühre die dredge-Funktion und ein Modelaveraging durch. Der Code dazu ist unten. Was passiert in der Funktion? Macht es Sinn, die Funktion auszuführen?\nHinweis: untenstehender Code ist rechenentensiv.\n\nf <- Total ~ Wochentag + Ferien + Phase +\n  tre200jx_scaled + rre150j0_scaled + sremaxdv_scaled\n# Jetzt kommt der Random-Factor hinzu und es wird eine Formel daraus gemacht\nf_dredge <- paste(c(f, \"+ (1|KW)\", \"+ (1|Jahr)\"), collapse = \" \") %>% \n  as.formula()\n# Das Modell mit dieser Formel ausführen\nm <- glmer(f_dredge, data = umwelt, family = poisson, na.action = \"na.fail\")\n# Das Modell in die dredge-Funktion einfügen (siehe auch ?dredge)\nall_m <- dredge(m)\n# suche das beste Modell\nprint(all_m)\n# Importance values der Variablen \n# hier wird die wichtigkeit der Variablen in den verschiedenen Modellen abgelesen\nMuMIn::importance(all_m) \n\n# Schliesslich wird ein Modelaverage durchgeführt \n# Schwellenwert für das delta-AIC = 2\navgmodel <- model.avg(all_m, rank = \"AICc\", subset = delta < 2) \nsummary(avgmodel)"
  },
  {
    "objectID": "fallstudie_s/7_Multivariate_Statistik_Uebung.html#aufgabe-5-verteilung-der-abhängigen-variabel-pruefen",
    "href": "fallstudie_s/7_Multivariate_Statistik_Uebung.html#aufgabe-5-verteilung-der-abhängigen-variabel-pruefen",
    "title": "7. Übung",
    "section": "Aufgabe 5: Verteilung der abhängigen Variabel pruefen",
    "text": "Aufgabe 5: Verteilung der abhängigen Variabel pruefen\nDie Verteilung der abhängigen Variabel bestimmt generell, was für ein Modell geschrieben werden kann. Die Modelle gehen von einer gegebenen Verteilung aus. Wenn diese Annahme verletzt wird, kann es sein, dass das Modellergebnis nicht valide ist.\n\nFolgender Codeblock zeigt, wie die Daten auf verschiedene Verteilungen passen.\n\nHinweis: es kann sein, dass nicht jede Verteilung geplottet werden kann, es erscheint eine Fehlermeldung. Das ist nicht weiter schlimm, die betreffende Verteilung kann gelöscht werden. Analog muss das auch im Befehl gofstat() passieren.\n\nDie besten drei Verteilungen (gemäss AIC) sollen zur Visualisierung geplottet werden. Dabei gilt, je besser die schwarze Punktlinie (eure Daten) auf die farbigen Linien (theoretische Verteilungen) passen, desto besser ist diese Verteilung geeignet.\n\nHinweis: CDF = Cumulative distribution function; Wikipedia = “Anschaulich entspricht dabei der Wert der Verteilungsfunktion an der Stelle x der Wahrscheinlichkeit, dass die zugehörige Zufallsvariable X einen Wert kleiner oder gleich x annimmt.”\n\nf1<-fitdist(umwelt$Anzahl_Total,\"norm\")  # Normalverteilung\nf1_1<-fitdist(umwelt$Anzahl_Total,\"lnorm\")  # log-Normalvert. \nf2<-fitdist(umwelt$Anzahl_Total,\"pois\")  # Poisson\nf3<-fitdist(umwelt$Anzahl_Total,\"nbinom\")  # negativ binomial\nf4<-fitdist(umwelt$Anzahl_Total,\"exp\")  # exponentiell\nf5<-fitdist(umwelt$Anzahl_Total,\"gamma\")  # gamma\nf6<-fitdist(umwelt$Anzahl_Total,\"logis\")  # logistisch\nf7<-fitdist(umwelt$Anzahl_Total,\"geom\")  # geometrisch\nf8<-fitdist(umwelt$Anzahl_Total,\"weibull\")  # Weibull\n\ngofstat(list(f1,f1_1,f2,f3,f4,f5,f6,f7,f8), \n        fitnames = c(\"Normalverteilung\", \"log-Normalverteilung\", \"Poisson\",\n                     \"negativ binomial\",\"exponentiell\",\"gamma\", \"logistisch\",\n                     \"geometrisch\",\"weibull\"))\n\n## die 4 besten (gemaess Akaike's Information Criterion) als Plot, \nplot.legend <- c(\"log norm\", \"weibull\", \"gamma \", \"negativ binomial\")\n## vergleicht mehrere theoretische Verteilungen mit den empirischen Daten\ncdfcomp(list(f1_1, f8, f5, f3), legendtext = plot.legend)\n\nWie sind unsere Daten verteilt? Welche Modelle können wir anwenden?"
  },
  {
    "objectID": "fallstudie_s/7_Multivariate_Statistik_Uebung.html#aufgabe-6-multivariates-modell-berechnen",
    "href": "fallstudie_s/7_Multivariate_Statistik_Uebung.html#aufgabe-6-multivariates-modell-berechnen",
    "title": "7. Übung",
    "section": "Aufgabe 6: Multivariates Modell berechnen",
    "text": "Aufgabe 6: Multivariates Modell berechnen\nIch verwende die Funktion glmer() aus der Bibliothek lme4. glmer ist neuer, schneller und zuverlässiger als vergleichbare Funktionen (diese Bibliothek wird auch in vielen wissenschaftlichen Papern im Feld Biologie / Wildtiermamagement zitiert).\nHinweise zu GLMM: https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html\n\n6a)\nHinweis: Auch wenn wir gerade herausgefunden haben, dass die Verteilung negativ binomial (in meinem Fall) ist, berechne ich für den Vergleich zuerst ein “einfaches Modell” der Familie poisson. Alternative Modelle rechnen wir in 6c.\n\nDie Totale Besucheranzahl soll durch die Wetterparameter, den Wochentag, die Ferien sowie die Covid-Phasen erklärt werden (Datensatz “umwelt”). Die Saisonalität (KW und Jahr) soll hierbei nicht beachtet werden, sie werden als “random factor” bestimmt.\n\nFrage: Warum bestimmen wir KW und Jahr als random factor?\nFalls ihr der Meinung seid, KW und / oder Jahr sind keine “guten” random factor, dann nehmt sie nicht an random factor ins Modell sondern als erklärende Variable. Begründet das unbedingt in eurer Methodik.\nDie Modellformel lautet:\n\nTages_Model <- glmer(ABHAENGIGE VARIABLE ~ ERKLAERENDE VARIABLE 1 + ERKLAERENDE VARIABLE 2 +\n                      ERKLAERENDE VARIABLE 3 + ERKLAERENDE VARIABLE 4 + \n                      ERKLAERENDE VARIABLE 5 + ERKLAERENDE VARIABLE 6 +\n                     (1|RANDOM FACTOR A)+ (1|RANDOM FACTOR B),\n                     family = poisson, data = DATENSATZ))\n\nsummary(Tages_Model) #Zeigt das Resultat des Modells\n\nFrage: Was bedeutet “family = poisson”?\nLöst zuerst Aufgabe 6b bevor ihr alternative Modelle rechnet; das kommt in Aufgabe 6c!\n\n\n6b) Modelldiagnostik\n\nPrüft optisch ob euer Modell valide ist.\n\nHinweis: glmer bringt einige eigene Funktionen mit, mit denen sich testen lässt, ob das Modell valide ist. Unten sind sie aufgeführt (–> analog zu den Funktionen aus der Vorlesung, aber halt für glmer).\n\n## Verteilung der Residuen (Varainzhomogenitaet)\nplot(Tages_Model, type = c(\"p\", \"smooth\"))\n## Pruefen auf Normalverteilung der Residuen\nqqmath(Tages_Model)\n\n## Overdispersion describes the observation that variation is higher than would be expected.\ndispersion_glmer(Tages_Model) #it shouldn't be over 1.4\n## zeige die erklaerte Varianz (je hoeher r2m ist, desto besser!)\nr.squaredGLMM(Tages_Model) \n\nSind die Voraussetzungen des Modells erfuellt?\n\n\n6c) Alternative Modelle\nWir sind auf der Suche nach dem minimalen adäquaten Modell. Das ist ein iterativer Prozess. Wir schreiben ein Modell, prüfen ob die Voraussetzungen erfüllt sind und ob die abhängige Variable besser erklärt wird als im vorhergehenden. Und machen das nochmals und nochmals…\n\nÜber family = kann in der Funktion _glmer()__ einiges (aber leider nicht alles so einfach [z.B. negativ binomiale Modelle]) angepasst werden: https://stat.ethz.ch/R-manual/R-devel/library/stats/html/family.html\nAuch über link = kann man anpassen: https://stat.ethz.ch/R-manual/R-devel/library/stats/html/make.link.html\nUnsere (meine) Daten sind negativ binomial verteilt. Daher sollte wir unbedingt ein solches Modell programmieren. –> Funktion glmer.nb()\nFalls die Daten exponentiell Verteilt sind, hier der Link zu einem Blogeintrag dazu: https://stats.stackexchange.com/questions/240455/fitting-exponential-regression-model-by-mle\nHypothese: “Es gehen weniger Leute in den Wald, wenn es zu heiss ist” –> auf quadratischen Term Temperatur testen (Codeblock unten).\n\n\n...\ntre200jx_scaled + I(tre200jx_scaled^2) + \n  ...\n\n\nKönnte es zwischen einzelnen Variablen zu Interaktionen kommen, die plausible sind? (z. B.: Im Winter hat Niederschlag einen negativeren Effekt als im Sommer, wenn es heiss ist) –> Falls ja: testen!\n\nHinweis: Interaktionen berechnen ist sehr rechenintensiv. Auch die Interpretation der Resultate wird nicht unbedingt einfacher. Wenn ihr auf Interaktionen testet, dann geht “langsam” vor, probiert nicht zu viel auf einmal.\n\nWenn ihr verschiedene Modelle gerechnet habt, können diese über den AICc verglichen werden. Folgender Code kann dazu genutzt werden:\n\n\n## Vergleich der Modellguete mittels AICc\ncand.models<-list()\ncand.models[[1]] <- Tages_Model\ncand.models[[2]] <- Tages_Model_nb\ncand.models[[3]] <- Tages_Model_nb_quad\n\nModnames<-c(\"Tages_Model\",\"Tages_Model_nb\", \n            \"Tages_Model_nb_quad\")\naictab(cand.set=cand.models,modnames=Modnames)\n##K = Anzahl geschaetzter Parameter (2 Funktionsparameter und die Varianz)\n##Delta_AICc <2 = Statistisch gleichwertig\n##AICcWt =  Akaike weight in %\n\n\n\n6d) (OPTIONAL) Transformationen\nBei meinen Daten waren die Modellvoraussetzungen überall mehr oder weniger verletzt. Das ist ein Problem, allerdings auch nicht ein so grosses (man sollte es aber trotzdem ernst nehmen). Mehr dazu unter:\nSchielzeth et al. Robustness of linear mixed‐effects models to violations of distributional assumptions https://besjournals.onlinelibrary.wiley.com/doi/10.1111/2041-210X.13434 Lo and Andrews, To transform or not to transform: using generalized linear mixed models to analyse reaction time data https://www.frontiersin.org/articles/10.3389/fpsyg.2015.01171/full\nFalls die Voraussetzungen stark verletzt werden, wäre eine Transformation angezeigt.\nMehr dazu unter:\nhttps://www.datanovia.com/en/lessons/transform-data-to-normal-distribution-in-r/\n\nBerechne den skewness coefficient\n\n\nlibrary(moments)\nskewness(umwelt$Anzahl_Total)\n## A positive value means the distribution is positively skewed (rechtsschief).\n## The most frequent values are low; tail is toward the high values (on the right-hand side)\n\n\nWelche Transformation kann angewandt werden?\nWas spricht gegen eine Transformation (auch im Hinblick zur Visualisierung und Interpretation)? Was spricht dafür?\n\n\n\n6c) Exportiere die Modellresultate (des besten Modells)\nModellresultate können mit summary() angezeigt werden. Ich verwende aber lieber die Funktion tab_model()! Die Resultate werden gerundet und praktisch im separaten Fenster angezeigt. Von dort kann man sie via copy + paste ins (z.B.) Word bringen.\n\ntab_model(MODELLNAME, transform = NULL, show.se = TRUE)\n## The marginal R squared values are those associated with your fixed effects, \n## the conditional ones are those of your fixed effects plus the random effects. \n## Usually we will be interested in the marginal effects."
  },
  {
    "objectID": "fallstudie_s/7_Multivariate_Statistik_Uebung.html#aufgabe-7-modellvisualisierung",
    "href": "fallstudie_s/7_Multivariate_Statistik_Uebung.html#aufgabe-7-modellvisualisierung",
    "title": "7. Übung",
    "section": "Aufgabe 7: Modellvisualisierung",
    "text": "Aufgabe 7: Modellvisualisierung\n\nVisualisiert die (signifikanten) Ergebnisse eures Modells.\n\nDas Resultat soll sich für kontinuierliche Variablen an untenstehendem Plot orientieren:\n\n\n\nFür diskrete Variablen haltet ihr euch bitte an diesen Plot:\n\n\n\nEinige Codeblocks, die euch dabei helfen können:\n\nt <- plot_model(NAME_DES_BESTEN_MODELLS, # hier sagen wir, aus welchem Modell geplottet werden soll\n                \n                # Wir moechten nicht nur die tatsaechlichen Werte geplottet, sondern \n                # \"Vorhersagen\" / predictions (fuer jeden Wert auf der x-Achse soll es auch einen\n                # auf der y-Achse geben)\n                type = \"pred\", \n                \n                # jetzt nennen wir den Term aus dem Modell:\n                # [all] = Unser Modell enthaellt polynomial oder cubic / quadratic Terme. \n                # mit [all] tragen wir dem Rechnung und zeichnen \"smooth\" plots\n                terms = \"tre200jx_scaled [all]\", \n                \n                # und schliesslich setzen wir die Achsentitel\n                title = \"\", axis.title = c(\"Tagesmaximaltemperatur [°C]\", \n                                           \"Fussgaenger:innen pro Tag [log]\"))\n\n## Vorbereitungen zum Hinzufuegen der Achsenbeschriftung (Aktuell sehen wir noch die skalierten Werte). \n## Nun sollen aber die unskalierten Werte gezeigt werden.\nlabels <- round(seq(floor(min(umwelt$tre200jx)), ceiling(max(umwelt$tre200jx)),\n                    \n                    # length.out = ___ --> Anpassen gemaess der Anzahl zu sehender breaks auf dem Plot\n                    length.out = 5), 0) \n## Schliesslich fuegen wir die Achsenbeschriftung hinzu.\n(Tempplot <- t + \n    \n    # fuege die x- Achsenbeschriftung hinzu.\n    # breaks = c() --> Anpassen gemaess der zu sehender breaks auf dem Plot\n    scale_x_continuous(breaks = c(-2,-1,0,1,2), \n                       labels = c(labels))+\n    \n    # fuege die y- Achsenbeschriftung hinzu. Hier transformieren wir die Werte zurueck\n    # Hinweis: falls ihr keine Transformation gemacht habt, muessen die y-Werte auch nicht \n    # zuruecktransformiert werden.\n    scale_y_continuous(breaks = c(0,0.5,1,1.5,2),\n                       labels = round(c(10^0, 10^0.5, 10^1, 10^1.5, 10^2),0),\n                       limits = c(0, 2))+ \n    theme_classic(base_size = 15))\n\n## Exportiere das Resultat\nggsave(\"temp.png\", width=15, height=15, units=\"cm\", dpi=1000, \n       path = \"fallstudien/_R_analysis/results/\") \n\nHinweis: damit unsere Plots verglichen werden können, sollen sie alle dieselbe Skalierung (limits) auf der y-Achse haben. Das wird erreicht, indem man bei jedem Plot die limits in scale_y_continuous() gleichsetzt.\n#w# Abschluss\nNun habt ihr verschiedenste Ergebnisse vorliegen. In einem wissenschaftlichen Bericht sollen aber niemals alle Ergebnisse abgebildet werden. Eine Faustregel besagt, dass nur signifikante Ergebnisse visualisiert werden. Entscheidet euch daher, was ihr in eurem Bericht abbilden wollt und was lediglich besprochen werden soll."
  },
  {
    "objectID": "fallstudie_n/1_Vorbemerkung.html",
    "href": "fallstudie_n/1_Vorbemerkung.html",
    "title": "1. Vorbemerkung",
    "section": "",
    "text": "Aktuell dient diese Plattform für die BiEc Fallstudie - Profil N einzig der Bereitstellung von Aufgaben die von euch im Rahmen dieses Fallstudienprojekts erarbeitet werden sollen. Die Aufgaben werden in den meisten Fällen mit Code-Beispielen erläutert oder benötigten Code-snippets resp. Funktionen werden mitgeliefert. Im Laufe des Semesters werden hier ausserdem häppchenweise (mögliche) Lösungen zu den Aufgaben aufgeschaltet. Alles grundlegende Material und alle Unterlagen zu den theoretischen Inputs sind weiterhin und ausschliesslich im Moodlekurs Research Methods - Fallstudie BiEc zu finden. Die für die Aufgaben benötigten Datengrundlagen sind ebenfalls im entsprechenden Abschnitt auf Moodle zu finden. Frohes Schaffen!"
  },
  {
    "objectID": "fallstudie_n/2_Datenverarbeitung_Loesung.html#projektaufbau-rstudio-projekte",
    "href": "fallstudie_n/2_Datenverarbeitung_Loesung.html#projektaufbau-rstudio-projekte",
    "title": "2. Lösung",
    "section": "Projektaufbau RStudio-Projekte",
    "text": "Projektaufbau RStudio-Projekte\nVor den eigentlichen Auswertungen muessen einige vorbereitende Arbeiten unternommen werden. Die Zeit, die man hier investiert, wird in der spaeteren Projektphase um ein vielfaches eingespart. Im Skript soll die Ordnerstruktur des Projekts genannt werden, damit der Arbeitsvorgang auf verschiedenen Rechnern reproduzierbar ist.\nArbeitet mit Projekten, da diese sehr einfach ausgetauscht und somit auch reproduziert werden koennen; es gibt keine absoluten Arbeitspfade sondern nur relative. Der Datenimport (und auch der Export) kann mithilfe dieser relativen Pfaden stark vereinfacht werden. Projekte helfen alles am richtigen Ort zu behalten. (mehr zur Arbeit mit Projekten: https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects)"
  },
  {
    "objectID": "fallstudie_n/2_Datenverarbeitung_Loesung.html#aufbau-von-r-skripten",
    "href": "fallstudie_n/2_Datenverarbeitung_Loesung.html#aufbau-von-r-skripten",
    "title": "2. Lösung",
    "section": "Aufbau von R-Skripten",
    "text": "Aufbau von R-Skripten\n\nIm Kopf des Skripts zuerst immer den Titel des Projekts sowie die Autor:innen des Skripts nennen. Hier soll auch die Herkunft der Daten ersichtlich sein und falls externe Daten verwendet werden, sollte geklaert werden, wer die Datenherrschaft hat (Rehdaten: Forschungsgruppe WILMA).\n\n#.##################################################################################\n# Daten(vor)verarbeitung Fallstudie WPZ  ####\n# Modul Research Methods, HS21. Autor/in ####\n#.##################################################################################\n\n# Beschreibt zudem folgendes:\n# • Ordnerstruktur (ich verwende hier den Projektordner mit den Unterordnern Skripts, \n# Feldaufnahmen, Data, Results, Plots)\n# • Verwendete Daten\n\n# Ein Skript soll in R eigentlich immer nach dem selbem Schema aufgebaut sein. \n# Dieses Schema beinhaltet (nach dem bereits erwaehnten Kopf des Skripts) 4 Kapitel: \n\n\n\n1. Datenimport\n\n\n2. Datenvorverarbeitung\n\n\n3. Analyse\n\n\n4. Visualisierung\n\n\nBereitet euer Skript also nach dieser Struktur vor. Nutzt fuer den Text, welcher nicht Code ist, vor dem Text das Symbol #. Wenn ihr den Text als Titel definieren wollt, der die grobe Struktur des Skripts absteckt, baut in wie in folgendem Beispiel auf:\n\n#.###################################################################################\n# METADATA ####\n#.###################################################################################\n# Datenherkunft ####\n# ...\n\n#.###################################################################################\n# 1. DATENIMPORT ####\n#.###################################################################################\n\n\n\nlibraries laden: hier tidyverse\n\nlibrary(tidyverse)\n\n\n\nHerunterladen der Daten der Feldaufnahme von Moodle, Einlesen, Sichtung der Datensaetze und der Datentypen\n\n# Die Datensätze aller Teams müssen erst noch in CSVs umgewandelt werden, bevor sie \n# eingelesen werden können \n\ndf_team1 <- read_delim(here(\"data\",\"Felderhebung Waldstruktur_TEAM_1_türkis.csv\"), \n                       delim = \";\")\n\ndf_team2 <- read_delim(here(\"data\",\"Felderhebung_Team_2.csv\"), delim = \";\")\n# Achtung! Beim Datensatz des Teams 2 ist eine zusaetzliche Zeile eingefuegt, die\n# das Einlesen erschwert. --> loeschen\n# Ausserdem gibt es bei den Zeilen DG Rubus, DG Strauchschicht und DG Baumschicht ein  \n# Problem mit dem Datentyp resp. den Zahlen. \n# --> manuell in Excel , suchen und mit . ersetzen\n\ndf_team3 <- read_delim(here(\"data\",\"ReMe_Felderhebung_Gruppe3.csv\"), delim = \",\")\n# Achtung! Hier ist beim Einlesen etwas falsch gelaufen. --> \",\" statt \";\"\n\ndf_team4 <- read_delim(here(\"data\",\"Felderhebung_Waldstruktur_Team_4.csv\"), \n                       delim = \";\")\n\ndf_team5 <- read_delim(here(\"data\",\"Felderhebung_Waldstruktur_Team5.csv\"), \n                       delim = \";\")\n# Achtung! Beim Umwandeln in das CSV muss hier die Titelzeile entfernt werden damit\n# das Einlesen reibungslos funktioniert\n\ndf_team6 <- read_delim(here(\"data\",\"Aufnahmen_Landforst_HS21_Gruppe_6.csv\"), \n                       delim = \";\")\n\n# hier koennen die Probekreise mit den Angaben zur Anzahl Rehlokalisationen und der \n# LIDAR-basierten Ableitung der Waldstruktur eingelesen werden\n\ndf_reh <- read_delim(here(\"data\",\"Aufgabe3_Reh_Waldstruktur_211014.csv\"), delim = \";\")\nstr(df_reh)\n\n# Die eingelesenen Datensaetze anschauen und versuchen zu einem Gesamtdatensatz  \n# verbinden. Ist der Output zufriedenstellend?\n\ndf_gesamt <- bind_rows(df_team1, df_team2, df_team3, df_team4, df_team5, df_team6)\nstr(df_gesamt)\n\n\n\nAufgabe 1:\n\n1.1 Einfuegen zusaetzliche Spalte pro Datensatz mit der Gruppenzugehoerigkeit (Team1-6)\n\n\n1.2 Spaltenumbenennung damit die Bezeichungen in allen Datensaetzen gleich sind und der Gesamtdatensatz zusammengefuegt werden kann\n\n\n–> Befehle mutate und rename, mit pipes (%>%) in einem Schritt moeglich\n\n#.#################################################################################\n# 2. DATENVORVERARBEITUNG #####\n#.#################################################################################\n\ndf_team1 <- df_team1 %>%\n  mutate(team = \"team1\") %>%\n  rename(KreisID = \"Kreis (r=12.5)\",\n         X = \"x\",\n         Y = \"y\",\n         DG_Rubus = \"Deckungsgrad Rubus sp. [%]\",\n         DG_Strauchschicht = \"DG Strauchschicht [%] (0.5-3m)\",\n         DG_Baumschicht = \"DG Baumschicht [%] (ab 3m)\")\n\ndf_team2 <- df_team2 %>%\n  mutate(team = \"team2\") %>%\n  rename(KreisID = \"Kreis (r 12.5m)\",\n         DG_Rubus = \"DG Rubus sp. [%]\",\n         DG_Strauchschicht = \"DG Strauchschicht [%] (0.5-3m)\",\n         DG_Baumschicht = \"DG Baumschicht [%] (ab 3m)\")\n\ndf_team3 <- df_team3 %>%\n  mutate(team = \"team3\") %>%\n  rename(KreisID = \"Kreis (r 12.5m)\",\n         DG_Rubus = \"Deckungsgrad Rubus sp. [%]\",\n         DG_Strauchschicht = \"DG Strauchschicht [%] (0.5-3m)\",\n         DG_Baumschicht = \"DG Baumschicht [%] (ab 3m)\")\n\ndf_team4 <- df_team4 %>%\n  mutate(team = \"team4\") %>%\n  rename(KreisID = \"Kreis\",\n         DG_Rubus = \"Deckungsgrad Rubus sp.\",\n         DG_Strauchschicht = \"DG Strauchschicht\",\n         DG_Baumschicht = \"DG Baumschicht\")\n\ndf_team5 <- df_team5 %>%\n  mutate(team = \"team5\") %>%\n  rename(KreisID = \"Kreis (r12.5)\",\n         DG_Rubus = \"Deckungsgrad Rubus sp.[%]\",\n         DG_Strauchschicht = \"DG Strauchschicht [%] (0.5-3m)\",\n         DG_Baumschicht = \"DG Baumschicht [%] (ab 3m)\")\n\ndf_team6 <- df_team6 %>%\n  mutate(team = \"team6\") %>%\n  rename(KreisID = \"Kreis (r 12.5m)\",\n         DG_Rubus = \"Deckungsgrad Rubus sp. [%]\",\n         DG_Strauchschicht = \"DG Strauchschicht [%] (0.5-3m)\",\n         DG_Baumschicht = \"DG Baumschicht [%] (ab 3m)\")\n\n\n\n\nAufgabe 2:\n\nZusammenfuehren der Teildatensaetze zu einem Datensatz\n\ndf_gesamt <- bind_rows(df_team1, df_team2, df_team3, df_team4, df_team5, df_team6)\n\n\n\n\nAufgabe 3:\n\nVerbinden (join) des Datensatzes der Felderhebungen mit dem Datensatz der Rehe.\n\n\nZiel: ein Datensatz mit allen Kreisen der Felderhebung, angereichert mit den Umweltvariablen Understory und Overstory aus den LIDAR-Daten (DG_us, DG_os) aus dem Rehdatensatz.\n\n\n–> Welche Art von join? Welche Spalten zum Verbinden (by = ?) der Datensaetze\n\ndf_with_LIDAR <- left_join(df_gesamt,df_reh, by = c(\"X\" = \"x\", \"Y\" = \"y\"))\n\n\n\n\nAufgabe 4:\n\nScatterplot der korrespondondierenden Umweltvariablen aus den Felderhebungen gegen die Umweltvariablen aus den LIDAR-Daten erstellen (zusaetzlich Einfaerben der Gruppen und Regressionslinie darueberlegen).\n\n#.#####################################################################################\n# 4. VISUALISERUNG #####\n#.#####################################################################################\n\nggplot(df_with_LIDAR, aes(DG_us, DG_Strauchschicht, color = team)) + geom_point() + \n  stat_smooth(method = \"lm\")\n\n`geom_smooth()` using formula 'y ~ x'\n\n\nWarning: Removed 1 rows containing non-finite values (stat_smooth).\n\n\nWarning: Removed 1 rows containing missing values (geom_point).\n\n\n\n\nwrite_delim(df_with_LIDAR, here(\"data\",\"df_with_lidar.csv\"), delim = \";\")"
  },
  {
    "objectID": "fallstudie_n/2_Datenverarbeitung_Uebung.html#projektaufbau-rstudio-projekte",
    "href": "fallstudie_n/2_Datenverarbeitung_Uebung.html#projektaufbau-rstudio-projekte",
    "title": "2. Übung",
    "section": "Projektaufbau RStudio-Projekte",
    "text": "Projektaufbau RStudio-Projekte\nVor den eigentlichen Auswertungen muessen einige vorbereitende Arbeiten unternommen werden. Die Zeit, die man hier investiert, wird in der spaeteren Projektphase um ein vielfaches eingespart. Im Skript soll die Ordnerstruktur des Projekts genannt werden, damit der Arbeitsvorgang auf verschiedenen Rechnern reproduzierbar ist.\nArbeitet mit Projekten, da diese sehr einfach ausgetauscht und somit auch reproduziert werden koennen; es gibt keine absoluten Arbeitspfade sondern nur relative. Der Datenimport (und auch der Export) kann mithilfe dieser relativen Pfaden stark vereinfacht werden. Projekte helfen alles am richtigen Ort zu behalten. (mehr zur Arbeit mit Projekten: https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects)"
  },
  {
    "objectID": "fallstudie_n/2_Datenverarbeitung_Uebung.html#aufbau-von-r-skripten",
    "href": "fallstudie_n/2_Datenverarbeitung_Uebung.html#aufbau-von-r-skripten",
    "title": "2. Übung",
    "section": "Aufbau von R-Skripten",
    "text": "Aufbau von R-Skripten\nIm Kopf des Skripts zuerst immer den Titel des Projekts sowie die Autor:innen des Skripts nennen. Hier soll auch die Herkunft der Daten ersichtlich sein und falls externe Daten verwendet werden, sollte geklaert werden, wer die Datenherrschaft hat (Rehdaten: Forschungsgruppe WILMA).\n\n#.##################################################################################\n# Daten(vor)verarbeitung Fallstudie WPZ  ####\n# Modul Research Methods, HS21. Autor/in ####\n#.##################################################################################\n\n# Beschreibt zudem folgendes:\n# • Ordnerstruktur (ich verwende hier den Projektordner mit den Unterordnern Skripts, \n# Feldaufnahmen, Data, Results, Plots)\n# • Verwendete Daten\n\n# Ein Skript soll in R eigentlich immer nach dem selbem Schema aufgebaut sein. \n# Dieses Schema beinhaltet (nach dem bereits erwaehnten Kopf des Skripts) 4 Kapitel: \n\n\n1. Datenimport\n\n\n2. Datenvorverarbeitung\n\n\n3. Analyse\n\n\n4. Visualisierung\nBereitet euer Skript also nach dieser Struktur vor. Nutzt fuer den Text, welcher nicht Code ist, vor dem Text das Symbol #. Wenn ihr den Text als Titel definieren wollt, der die grobe Struktur des Skripts absteckt, baut in wie in folgendem Beispiel auf:\n\n#.###################################################################################\n# METADATA ####\n#.###################################################################################\n# Datenherkunft ####\n# ...\n\n#.###################################################################################\n# 1. DATENIMPORT ####\n#.###################################################################################\n\n\n\nlibraries laden: hier tidyverse\n\nlibrary(tidyverse)\n\nHerunterladen der Daten der Feldaufnahmen von Moodle (Aufgabe3_Feldaufnahmen_alle_Gruppen.zip), Einlesen, Sichtung der Datensaetze und der Datentypen\n\n# Die Datensätze aller Teams müssen erst noch in CSVs umgewandelt werden, bevor sie \n# eingelesen werden können \n\ndf_team1 <- read_delim(here(\"data\",\"Felderhebung Waldstruktur_TEAM_1_türkis.csv\"), \n                       delim = \";\")\n\ndf_team2 <- read_delim(here(\"data\",\"Felderhebung_Team_2.csv\"), delim = \";\")\n# Achtung! Beim Datensatz des Teams 2 ist eine zusaetzliche Zeile eingefuegt, die\n# das Einlesen erschwert.\n# Ausserdem gibt es bei den Zeilen DG Rubus, DG Strauchschicht und DG Baumschicht ein  \n# Problem mit dem Datentyp resp. den Zahlen.\n\ndf_team3 <- read_delim(here(\"data\",\"ReMe_Felderhebung_Gruppe3.csv\"), delim = \";\")\n# Achtung! Hier ist beim Einlesen etwas falsch gelaufen. \n\ndf_team4 <- read_delim(here(\"data\",\"Felderhebung_Waldstruktur_Team_4.csv\"), \n                       delim = \";\")\n\ndf_team5 <- read_delim(here(\"data\",\"Felderhebung_Waldstruktur_Team5.csv\"), \n                       delim = \";\")\n# Achtung! Beim Umwandeln in das CSV muss hier die Titelzeile entfernt werden damit\n# das Einlesen reibungslos funktioniert\n\ndf_team6 <- read_delim(here(\"data\",\"Aufnahmen_Landforst_HS21_Gruppe_6.csv\"), \n                       delim = \";\")\n\n# hier koennen die Probekreise mit den Angaben zur Anzahl Rehlokalisationen und der \n# LIDAR-basierten Ableitung der Waldstruktur eingelesen werden\n\ndf_reh <- read_delim(here(\"data\",\"Aufgabe3_Reh_Waldstruktur_211014.csv\"), delim = \";\")\nstr(df_reh)\n\n# Die eingelesenen Datensaetze anschauen und versuchen zu einem Gesamtdatensatz  \n# verbinden. Ist der Output zufriedenstellend?\n\ndf_gesamt <- bind_rows(df_team1, df_team2, df_team3, df_team4, df_team5, df_team6)\nstr(df_gesamt)\n\n\n\nAufgabe 1:\n\n1.1 Einfuegen zusaetzliche Spalte pro Datensatz mit der Gruppenzugehoerigkeit (Team1-6)\n1.2 Spaltenumbenennung damit die Bezeichungen in allen Datensaetzen gleich sind und der Gesamtdatensatz zusammengefuegt werden kann –> Befehle mutate und rename, mit pipes (%>%) in einem Schritt moeglich\n\n\n\nAufgabe 2:\nZusammenfuehren der Teildatensaetze zu einem Datensatz\n\n\nAufgabe 3:\nVerbinden (join) des Datensatzes der Felderhebungen mit dem Datensatz der Rehe.\nZiel: ein Datensatz mit allen Kreisen der Felderhebung, angereichert mit den Umweltvariablen Understory und Overstory aus den LIDAR-Daten (DG_us, DG_os) aus dem Rehdatensatz. –> Welche Art von join? Welche Spalten zum Verbinden (by = ?) der Datensaetze\n\n\nAufgabe 4:\nScatterplot der korrespondondierenden Umweltvariablen aus den Felderhebungen gegen die Umweltvariablen aus den LIDAR-Daten erstellen (zusaetzlich Einfaerben der Gruppen und Regressionslinie darueberlegen)."
  },
  {
    "objectID": "fallstudie_n/3_Berechnung_Homeranges_Loesung.html#berechung-der-home-ranges-der-rehe",
    "href": "fallstudie_n/3_Berechnung_Homeranges_Loesung.html#berechung-der-home-ranges-der-rehe",
    "title": "3. Lösung",
    "section": "Berechung der Home-Ranges der Rehe",
    "text": "Berechung der Home-Ranges der Rehe\n\nBenötigte Libraries laden\n\nipak <- function(pkg){\nnew.pkg <- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\nif (length(new.pkg))\ninstall.packages(new.pkg, repos = \"http://cran.us.r-project.org\", dependencies = TRUE)\nsapply(pkg, require, character.only = TRUE)\n}\npackages <- c(\"sf\", \"raster\", \"tidyverse\", \"adehabitatHR\", \"maptools\", \"sp\", \n              \"ggspatial\", \"rgeos\", \"rgdal\", \"pastecs\")\nipak(packages)\n\n\n\nEinlesen des Gesamtdatensatzes von Moodle, Sichtung des Datensatzes und der Datentypen\n\nRehe <- read_delim(here(\"data\",\"Aufgabe3_Homeranges_Rehe_landforst_20211014.csv\"), delim = \";\")\n\nstr(Rehe)\n\n\n\nAufgabe 1: In Datensatz Rehe eine neue Spalte mit Datum und Zeit in einer Spalte kreieren. Beim Format hat sich ein Fehler eingeschlichen. Findet ihr ihn?\n\nRehe <- Rehe %>%\n  mutate(UTC_DateTime = as.POSIXct(paste(UTC_Date, UTC_Time), \n                                   format = \"%Y-%m-%d %H:%M:%S\"))\n\n\n\nHier einige Zeilen Code, um eine HomeRange zu berechnen.\n\n\nAufgabe 2: Herumschrauben an den Einstellungen von:\n\n- href (in der Funktion kernelUD)\n\n\n- an der Ausdehung, resp. prozentualer Anteil Punkte in der HR (Funktion getverticeshr)\n\n\n–> Ziel: eine Karte erstellen mit der Visualiserung mindestens einer HR\n\nx <- Rehe$X[Rehe$TierID== \"RE13\"]    \ny <- Rehe$Y[Rehe$TierID== \"RE13\"]\nxy <- data.frame(cbind (x, y, rep(1, length(x))))       \ncoordinates(xy)<-c(\"x\",\"y\")                             \nproj4string(xy)<-CRS(\"+init=epsg:21781\")  \n\nplot(xy, col = \"blue\", pch = 19, cex = 1.5)\n\n# Berechnung von href nach: Pebsworth et al. (2012) Evaluating home range techniques: \n# use of Global Positioning System (GPS) collar data from chacma baboons\n\nsigma <- 0.5*(sd(x)+sd(y))                              \nn <- length(x)\nhref <- sigma * n^(-1/6)*0.9  \n\n# scaled reference: href * 0.9\n\nkud <- kernelUD(xy, h=href, grid=25)             \n\n# Berechnung der Home Range (95% Isopleth)\n\nhomerange <- getverticeshr(kud, percent=50)             \n\n# Schreibt HR in den oben beschriebenen Ordner (als Shapefile)\n\nhr <- st_as_sf(homerange)\n\nst_write(hr, dsn= \"Results\", layer=\"HR_RE13\", driver=\"ESRI Shapefile\",  \n         delete_layer = T )\n\n\n# mit diesem Befehl kann die HR geplottet werden\n\nggplot(hr, aes(color = \"red\", fill=\"red\")) + \n  geom_sf(size = 1, alpha = 0.3) +\ncoord_sf(datum = sf::st_crs(21781))+\ntheme(\naxis.title = element_blank(),\naxis.text = element_blank(),\naxis.ticks = element_blank(),\nlegend.position=\"none\"\n)\n\n# und die Punkte der GPS-Lokalisationen darüber gelegt werden \n\nxy_p <- st_as_sf(xy)\n\nggplot(hr, aes(color = \"red\", fill=\"red\")) + \n  geom_sf(size = 1, alpha = 0.3) +\ngeom_sf(data = xy_p, aes(fill = \"red\")) +\ncoord_sf(datum = sf::st_crs(21781))+\ntheme(\naxis.title = element_blank(),\naxis.text = element_blank(),\naxis.ticks = element_blank(),\nlegend.position=\"none\"\n)\n\n\n\nCode um die Homerange auf der Landeskarte 1:25000 zu plotten. Transparenz kann mit alpha angepasst werden\n\npk25_wpz <- brick(here(\"data\",\"pk25_wpz.tif\"))\n\nxy_p <- st_as_sf(xy)\n\nggplot(hr, aes(color = \"red\", fill=\"red\")) +\nannotation_spatial(pk25_wpz) +\ngeom_sf(size = 1, alpha = 0.3) +\ngeom_sf(data = xy_p, aes(fill = \"red\")) +\ncoord_sf(datum = sf::st_crs(21781))+\ntheme(\naxis.title = element_blank(),\naxis.text = element_blank(),\naxis.ticks = element_blank(),\nlegend.position=\"none\"\n)\n\n\n\n\n\n\n\nNachbauen des Sampling Grids mit den Kreisen (Wird als Grundlage für Extraktion der Umweltvariablen innerhalb der Homeranges benötigt)\n\nXmin bzw. Ymin des Grids: c(684000, 234000)\n\n\ncellsize des Grids: c(25, 25)\n\n\nAnzahl Kreise in X und Y Richtung: c(100, 160)\n\nx25       <- GridTopology(c(684000, 234000), c(25, 25), c(100, 160)) \ndata25    = data.frame(1:(100*160))           \n# Erstellt aus der GridTopology und den Daten ein SpatialGridDataFrame\ngrid25    <- SpatialGridDataFrame(x25, data25,  proj4string <- CRS(\"+init=epsg:21781\"))\npixel25   <- as(grid25, \"SpatialPixelsDataFrame\")\n\n\n# zweites Sampling Grid für einen Ausschnitt aufbauen, plotten\n# -> dient nur der Visualisierung des Sampling Grids um einen Eindruck zu erhalten\n\nx       <- GridTopology(c(684200, 236900), c(25, 25), c(35, 35)) \ndata    = data.frame(1:(35*35))           \n# Erstellt aus der GridTopology und den Daten ein SpatialGridDataFrame\ngrid    <- SpatialGridDataFrame(x, data,  proj4string <- CRS(\"+init=epsg:21781\"))\npixel  <- as(grid, \"SpatialPixelsDataFrame\")\n\npoints <- as(pixel, \"SpatialPointsDataFrame\")\n\ngrid_plot <- st_buffer(st_as_sf(points), 12.5)\n\nplot(st_geometry(grid_plot))\n\nggplot(grid_plot, color = \"black\", fill=NA) + \n  geom_sf() +\ngeom_sf(data = xy_p, color = \"blue\",  ) +\n  geom_sf(data = hr, color = \"red\", fill = NA, size = 2) +\ncoord_sf(datum = sf::st_crs(21781))+\ntheme(\naxis.title = element_blank(),\naxis.text = element_blank(),\naxis.ticks = element_blank(),\nlegend.position=\"none\"\n)\n\n\n\n\nAufgabe 3: Testen der Variablen der Vegetationsschichten von letzter Woche auf einen linearen Zusammenhang (Korrelation; Funktion cor.test). DG_Baumschicht vs. DG_os / DG_Strauchschicht vs. DG_us aus dem Datensatz df_with_lidar den wir letzte Woche erstellt haben\n\nDie Theorie zu Korrelation folgt erst ab 1.11\n\ndf_with_lidar <- read_delim(here(\"data\",\"df_with_lidar.csv\"), delim =\";\")\n\nlibrary(pastecs)\n\nround(stat.desc(cbind(df_with_lidar$DG_us,df_with_lidar$DG_os,\n                      df_with_lidar$DG_Strauchschicht,df_with_lidar$DG_Baumschicht), \n                basic= F, norm= T), 3)\n\n                 V1     V2      V3      V4\nmedian        0.272  0.824  35.000  50.000\nmean          0.295  0.788  38.949  50.273\nSE.mean       0.013  0.013   1.926   1.782\nCI.mean.0.95  0.027  0.026   3.806   3.522\nvar           0.027  0.025 556.585 476.549\nstd.dev       0.164  0.159  23.592  21.830\ncoef.var      0.557  0.201   0.606   0.434\nskewness      0.661 -1.034   0.438   0.042\nskew.2SE      1.662 -2.601   1.106   0.105\nkurtosis     -0.068  0.670  -0.834  -0.930\nkurt.2SE     -0.087  0.848  -1.059  -1.182\nnormtest.W    0.959  0.910   0.953   0.974\nnormtest.p    0.000  0.000   0.000   0.007\n\n# Histogram der Verteilung und die aus den Daten berechnete Normalverteilung als Linie \n# dargestellt\n\nggplot(df_with_lidar, aes(DG_os)) + geom_histogram(aes(y=..density..), \n        color = \"black\", fill = \"white\") + \n     stat_function(fun = dnorm, args = list(mean = \n        mean(df_with_lidar$DG_os, na.rm = T), \n        sd = sd(df_with_lidar$DG_os, na.rm = T)), color = \"black\",size = 1)\n\nWarning: Removed 1 rows containing non-finite values (stat_bin).\n\n\n\n\n# testen auf Korrelation \n\ncor.test(~ DG_Baumschicht+DG_os, data = df_with_lidar, method=\"pearson\")\n\n\n    Pearson's product-moment correlation\n\ndata:  DG_Baumschicht and DG_os\nt = 5.4918, df = 147, p-value = 1.705e-07\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.2696941 0.5377248\nsample estimates:\n      cor \n0.4126009"
  },
  {
    "objectID": "fallstudie_n/3_Berechnung_Homeranges_Uebung.html#berechung-der-home-ranges-der-rehe",
    "href": "fallstudie_n/3_Berechnung_Homeranges_Uebung.html#berechung-der-home-ranges-der-rehe",
    "title": "3. Aufgabe",
    "section": "Berechung der Home-Ranges der Rehe",
    "text": "Berechung der Home-Ranges der Rehe\n\nBenötigte Libraries laden\n\nipak <- function(pkg){\nnew.pkg <- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\nif (length(new.pkg))\ninstall.packages(new.pkg, dependencies = TRUE)\nsapply(pkg, require, character.only = TRUE)\n}\npackages <- c(\"sf\", \"raster\", \"tidyverse\", \"adehabitatHR\", \"maptools\", \"sp\", \n              \"ggspatial\", \"rgeos\", \"rgdal\")\nipak(packages)\n\n\n\nEinlesen des Gesamtdatensatzes von Moodle, Sichtung des Datensatzes und der Datentypen\n\nRehe <- read_delim(here(\"data\",\"Aufgabe3_Homeranges_Rehe_landforst_20211014.csv\"), delim = \";\")\n\nstr(Rehe)\n\n\n\nAufgabe 1: In Datensatz Rehe eine neue Spalte mit Datum und Zeit in einer Spalte kreieren. Beim Format hat sich ein Fehler eingeschlichen. Findet ihr ihn?\n\nRehe <- Rehe %>%\n  mutate(UTC_DateTime = as.POSIXct(paste(UTC_Date, UTC_Time), \n                                   format = \"%Y-%m-%d %H:%M:%S\"))\n\n\n\nHier einige Zeilen Code, um eine HomeRange zu berechnen.\n\n\nAufgabe 2: Herumschrauben an den Einstellungen von:\n\n- href (in der Funktion kernelUD)\n\n\n- an der Ausdehung, resp. prozentualer Anteil Punkte in der HR (Funktion getverticeshr)\n\n\n–> Ziel: eine Karte erstellen mit der Visualiserung mindestens einer HR\n\nx <- Rehe$X[Rehe$TierID== \"RE13\"]    \ny <- Rehe$Y[Rehe$TierID== \"RE13\"]\nxy <- data.frame(cbind (x, y, rep(1, length(x))))       \ncoordinates(xy)<-c(\"x\",\"y\")                             \nproj4string(xy)<-CRS(\"+init=epsg:21781\")  \n\nplot(xy, col = \"blue\", pch = 19, cex = 1.5)\n\n# Berechnung von href nach: Pebsworth et al. (2012) Evaluating home range techniques: \n# use of Global Positioning System (GPS) collar data from chacma baboons\n\nsigma <- 0.5*(sd(x)+sd(y))                              \nn <- length(x)\nhref <- sigma * n^(-1/6)*0.9  \n\n# scaled reference: href * 0.9\n\nkud <- kernelUD(xy, h=href, grid=25)             \n\n# Berechnung der Home Range (95% Isopleth)\n\nhomerange <- getverticeshr(kud, percent=95)             \n\n# Schreibt HR in den oben beschriebenen Ordner (als Shapefile)\n\nhr <- st_as_sf(homerange)\n\nst_write(hr, dsn= \"Results\", layer=\"HR_RE13\", driver=\"ESRI Shapefile\",  \n         delete_layer = T )\n\n\n# mit diesem Befehl kann die HR geplottet werden\n\nggplot(hr, aes(color = \"red\", fill=\"red\")) + \n  geom_sf(size = 1, alpha = 0.3) +\ncoord_sf(datum = sf::st_crs(21781))+\ntheme(\naxis.title = element_blank(),\naxis.text = element_blank(),\naxis.ticks = element_blank(),\nlegend.position=\"none\"\n)\n\n# und die Punkte der GPS-Lokalisationen darüber gelegt werden \n\nxy_p <- st_as_sf(xy)\n\nggplot(hr, aes(color = \"red\", fill=\"red\")) + \n  geom_sf(size = 1, alpha = 0.3) +\ngeom_sf(data = xy_p, aes(fill = \"red\")) +\ncoord_sf(datum = sf::st_crs(21781))+\ntheme(\naxis.title = element_blank(),\naxis.text = element_blank(),\naxis.ticks = element_blank(),\nlegend.position=\"none\"\n)\n\n\n\nCode um die Homerange auf der Landeskarte 1:25000 zu plotten. Transparenz kann mit alpha angepasst werden\n\npk25_wpz <- brick(here(\"data\",\"pk25_wpz.tif\"))\n\nxy_p <- st_as_sf(xy)\n\nggplot(hr, aes(color = \"red\", fill=\"red\")) +\nannotation_spatial(pk25_wpz) +\ngeom_sf(size = 1, alpha = 0.3) +\ngeom_sf(data = xy_p, aes(fill = \"red\")) +\ncoord_sf(datum = sf::st_crs(21781))+\ntheme(\naxis.title = element_blank(),\naxis.text = element_blank(),\naxis.ticks = element_blank(),\nlegend.position=\"none\"\n)\n\n\n\n\nNachbauen des Sampling Grids mit den Kreisen (Wird als Grundlage für Extraktion der Umweltvariablen innerhalb der Homeranges benötigt)\n\nXmin bzw. Ymin des Grids: c(684000, 234000)\n\n\ncellsize des Grids: c(25, 25)\n\n\nAnzahl Kreise in X und Y Richtung: c(100, 160)\n\nx25       <- GridTopology(c(684000, 234000), c(25, 25), c(100, 160)) \ndata25    = data.frame(1:(100*160))           \n# Erstellt aus der GridTopology und den Daten ein SpatialGridDataFrame\ngrid25    <- SpatialGridDataFrame(x25, data25,  proj4string <- CRS(\"+init=epsg:21781\"))\npixel25   <- as(grid25, \"SpatialPixelsDataFrame\")\n\n\n# zweites Sampling Grid für einen Ausschnitt aufbauen, plotten\n# -> dient nur der Visualisierung des Sampling Grids um einen Eindruck zu erhalten\n\nx       <- GridTopology(c(684200, 236900), c(25, 25), c(35, 35)) \ndata    = data.frame(1:(35*35))           \n# Erstellt aus der GridTopology und den Daten ein SpatialGridDataFrame\ngrid    <- SpatialGridDataFrame(x, data,  proj4string <- CRS(\"+init=epsg:21781\"))\npixel  <- as(grid, \"SpatialPixelsDataFrame\")\n\npoints <- as(pixel, \"SpatialPointsDataFrame\")\n\ngrid_plot <- st_buffer(st_as_sf(points), 12.5)\n\nplot(st_geometry(grid_plot))\n\nggplot(grid_plot, color = \"black\", fill=NA) + \n  geom_sf() +\ngeom_sf(data = xy_p, color = \"blue\",  ) +\n  geom_sf(data = hr, color = \"red\", fill = NA, size = 2) +\ncoord_sf(datum = sf::st_crs(21781))+\ntheme(\naxis.title = element_blank(),\naxis.text = element_blank(),\naxis.ticks = element_blank(),\nlegend.position=\"none\"\n)\n\n\n\n\nAufgabe 3: Testen der Variablen der Vegetationsschichten von letzter Woche auf einen linearen Zusammenhang (Korrelation; Funktion cor.test). DG_Baumschicht vs. DG_os / DG_Strauchschicht vs. DG_us aus dem Datensatz df_with_lidar den wir letzte Woche erstellt haben\n\nDie Theorie zu Korrelation folgt erst ab 1.11"
  },
  {
    "objectID": "fallstudie_n/4_Multivariate_Modelle_Loesung.html#einstieg-habitatselektionsmodell-multivariate-modelle",
    "href": "fallstudie_n/4_Multivariate_Modelle_Loesung.html#einstieg-habitatselektionsmodell-multivariate-modelle",
    "title": "4. Lösung",
    "section": "Einstieg Habitatselektionsmodell / Multivariate Modelle",
    "text": "Einstieg Habitatselektionsmodell / Multivariate Modelle\n\nlibraries laden\n\n### Funktion um Packages direkt zu installieren und / oder zu laden\n\nipak <- function(pkg){\n  new.pkg <- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n  if (length(new.pkg)) \n    install.packages(new.pkg, repos = \"http://cran.us.r-project.org\", dependencies = TRUE)\n  sapply(pkg, require, character.only = TRUE)\n}\n\npackages <- c(\"tidyverse\", \"PerformanceAnalytics\", \"pastecs\", \"psych\", \"car\")\n\nipak(packages)\n\n\n\nAufgabe 1: Einlesen des Gesamtdatensatzes von Moodle\n\nSichtung des Datensatzes und der Datentypen\nKontrolle wieviele Rehe in diesem Datensatz enthalten sind\n\n\nDF_mod <- read_delim(here(\"data\",\"Aufgabe4_Datensatz_Habitatnutzung_Modelle_20211101_moodle.csv\"), \n                     delim = \";\")\n\nstr(DF_mod)\n\nspec_tbl_df [8,370 × 16] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ pres_abs          : num [1:8370] 0 1 0 1 1 1 1 1 1 0 ...\n $ nmb               : num [1:8370] 0 1 0 1 3 1 5 8 2 0 ...\n $ x                 : num [1:8370] 682250 682250 682225 682225 682225 ...\n $ y                 : num [1:8370] 237725 237750 237700 237725 237750 ...\n $ forest            : num [1:8370] 0 0 0 0 0 0 0 0 0 0 ...\n $ slope             : num [1:8370] 10.15 9.97 8.85 10.33 12.66 ...\n $ dist_road_all     : num [1:8370] 60.3 56.3 53.1 68.5 50.2 ...\n $ dist_road_only    : num [1:8370] 60.3 56.3 53.1 68.5 50.2 ...\n $ dist_build        : num [1:8370] 36.16 12.24 57.51 32.52 7.86 ...\n $ forest_prop       : num [1:8370] 0.022246 0.000931 0.018563 0.000824 0 ...\n $ us                : num [1:8370] 0 0.0218 0 0 0 ...\n $ os                : num [1:8370] 0 0.06744 0 0 0.00406 ...\n $ GPStot            : num [1:8370] 420 420 420 420 420 420 420 420 420 420 ...\n $ id                : chr [1:8370] \"RE03\" \"RE03\" \"RE03\" \"RE03\" ...\n $ time_of_day       : chr [1:8370] \"night\" \"night\" \"night\" \"night\" ...\n $ stoerungskategorie: chr [1:8370] \"gering\" \"gering\" \"gering\" \"gering\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   pres_abs = col_double(),\n  ..   nmb = col_double(),\n  ..   x = col_double(),\n  ..   y = col_double(),\n  ..   forest = col_double(),\n  ..   slope = col_double(),\n  ..   dist_road_all = col_double(),\n  ..   dist_road_only = col_double(),\n  ..   dist_build = col_double(),\n  ..   forest_prop = col_double(),\n  ..   us = col_double(),\n  ..   os = col_double(),\n  ..   GPStot = col_double(),\n  ..   id = col_character(),\n  ..   time_of_day = col_character(),\n  ..   stoerungskategorie = col_character()\n  .. )\n - attr(*, \"problems\")=<externalptr> \n\nclass(DF_mod$time_of_day)\n\n[1] \"character\"\n\ntable(DF_mod$id)\n\n\nRE02 RE03 RE04 RE05 RE06 RE07 RE08 RE09 RE10 RE11 RE12 RE13 \n1204  260  512  388  306  484  980  836  492 1208  652 1048 \n\nDF_mod %>% group_by(id) %>% summarize(anzahl = n())\n\n# A tibble: 12 × 2\n   id    anzahl\n   <chr>  <int>\n 1 RE02    1204\n 2 RE03     260\n 3 RE04     512\n 4 RE05     388\n 5 RE06     306\n 6 RE07     484\n 7 RE08     980\n 8 RE09     836\n 9 RE10     492\n10 RE11    1208\n11 RE12     652\n12 RE13    1048\n\nlength(unique(DF_mod$id))\n\n[1] 12\n\n\n\n\nAufgabe 2: Unterteilung des Datensatzes in Teildatensätze entsprechend der Tageszeit\n\nDF_mod_night <- DF_mod %>%\n  filter(time_of_day == \"night\")\n\nDF_mod_day <- DF_mod %>%\n  filter(time_of_day == \"day\")\n\n# Kontrolle\ntable(DF_mod_night$time_of_day)\n\n\nnight \n 4185 \n\ntable(DF_mod_day$time_of_day)\n\n\n day \n4185 \n\n\n\n\nAufgabe 3: Erstellen von Density Plots der Praesenz / Absenz in Abhaengigkeit der unabhaengigen Variablen (für Tag und Nacht)\n\n# Ein Satz Density Plots für den Tagesdatensatz und einer für den Nachtdatensatz \n\npar(mfrow=c(3,3), mar=c(4, 4, 3, 3))\nfor (i in 6:12) {                           # innerhalb des for()-loops die Nummern der \n                                            # gewuenschten Spalten einstellen\n  d  <-  DF_mod_day %>% pull(i)\n  d  <-  density(d)\n  dp  <-  DF_mod_day %>% filter(pres_abs == 1) %>% pull(i)\n  dp <- density(dp)\n  da  <-  DF_mod_day %>% filter(pres_abs == 0) %>% pull(i)\n  da <- density(da)\n  plot(0,0, type=\"l\", xlim=range(c(dp$x,da$x)), ylim=range(dp$y,da$y), \n       xlab=names(DF_mod_day[i]), ylab=\"Density\")\n  lines(dp$x, dp$y, col=\"blue\")             # Praesenz\n  lines(da$x, da$y, col=\"red\")              # Absenz\n}\n\n\n\n\n\n\nAufgabe 4: Testen erklärenden Variablen auf Normalverteilung (nur kontinuierlichen)\n\n# klassischer Weg mit shapiro-wilk (mehrere Spalten, verschiedenene statistische\n# Kenngrössen werden angezeigt. Normalverteilung: Wert ganz unten. p>0.05 = ja)\n\nround(stat.desc(DF_mod_day[6:12], basic= F, norm= T), 3)\n\n               slope dist_road_all dist_road_only dist_build forest_prop     us\nmedian        13.442        28.248         33.693    129.838       0.632  0.059\nmean          15.055        41.289         46.374    154.502       0.590  0.120\nSE.mean        0.158         0.642          0.665      1.625       0.005  0.002\nCI.mean.0.95   0.311         1.259          1.303      3.186       0.010  0.005\nvar          104.994      1725.368       1848.647  11050.895       0.107  0.023\nstd.dev       10.247        41.538         42.996    105.123       0.328  0.151\ncoef.var       0.681         1.006          0.927      0.680       0.555  1.259\nskewness       0.753         1.914          1.666      0.631      -0.366  1.674\nskew.2SE       9.945        25.285         22.003      8.341      -4.832 22.115\nkurtosis      -0.042         4.250          3.147     -0.537      -1.094  3.061\nkurt.2SE      -0.279        28.079         20.790     -3.545      -7.226 20.226\nnormtest.W     0.942         0.800          0.837      0.943       0.919  0.792\nnormtest.p     0.000         0.000          0.000      0.000       0.000  0.000\n                  os\nmedian         0.754\nmean           0.586\nSE.mean        0.006\nCI.mean.0.95   0.013\nvar            0.173\nstd.dev        0.416\ncoef.var       0.710\nskewness      -0.388\nskew.2SE      -5.124\nkurtosis      -1.586\nkurt.2SE     -10.481\nnormtest.W     0.791\nnormtest.p     0.000\n\n# empfohlener Weg\n\nggplot(DF_mod_day, aes(slope)) + geom_histogram(aes(y=..density..), color = \"black\", \n                                                fill = \"white\") + \n  stat_function(fun = dnorm, args = list(mean = mean(DF_mod_day$slope, na.rm = T), \n                                         sd = sd(DF_mod_day$slope, na.rm = T)), \n                color = \"black\",size = 1)\n\n\n\n# Aufgabe 4: die Korrelation bei einem Teildatensatz testen reicht, \n# denn die verwendeten Kreise sind die selben am Tag und in der Nacht, \n# nur die Nutzung durch das Reh nicht\n\n\n\nAufgabe 5: Explorative Analysen der Variablen mit Scatterplots, Scatterplotmatrizen\n\nZu Scatterplots und Scatterplotmatrizen gibt es viele verschiedene Funktionen / Packages, schaut im Internet und sucht euch eines welches euch passt.\nTesten der Korrelation zwischen den Variablen (Parametrisch oder nicht-parametrische Methode? Ausserdem: gewisse Scatterplotmatrizen zeigen euch die Koeffizenten direkt an)\n\n\nchart.Correlation(DF_mod_day[6:12], histogram=TRUE, pch=19, method = \"kendall\")\n\n\n\n#?chart.Correlation\n\npairs.panels(DF_mod_day[6:12], \n             method = \"kendall\", # correlation method\n             hist.col = \"#00AFBB\",\n             density = TRUE,  # show density plots\n             ellipses = TRUE # show correlation ellipses\n             )\n\n\n\n# Aufgabe 5: die Korrelation bei einem Teildatensatz testen reicht, \n# denn die verwendeten Kreise sind die selben am Tag und in der Nacht, \n# nur die Nutzung durch das Reh nicht."
  },
  {
    "objectID": "fallstudie_n/4_Multivariate_Modelle_Uebung.html#einstieg-multivariate-modelle-habitatselektionsmodell",
    "href": "fallstudie_n/4_Multivariate_Modelle_Uebung.html#einstieg-multivariate-modelle-habitatselektionsmodell",
    "title": "4. Übung",
    "section": "Einstieg Multivariate Modelle / Habitatselektionsmodell",
    "text": "Einstieg Multivariate Modelle / Habitatselektionsmodell\n\nlibraries laden\n\n### Funktion um Packages direkt zu installieren und / oder zu laden\nipak <- function(pkg){\n  new.pkg <- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n  if (length(new.pkg)) \n    install.packages(new.pkg, dependencies = TRUE)\n  sapply(pkg, require, character.only = TRUE)\n}\n\npackages <- c(\"sp\", \"raster\", \"tidyverse\", \"PerformanceAnalytics\", \"pastecs\", \"car\")\n\nipak(packages)\n\n\n\nAufgabe 1: Einlesen des Gesamtdatensatzes für die Multivariate Analyse von Moodle\n\n1) Sichtung des Datensatzes, der Variablen und der Datentypen\n\n\n2) Kontrolle wieviele Rehe in diesem Datensatz enthalten sind\n\n\n\nAufgabe 2: Unterteilung des Datensatzes in Teildatensätze entsprechend der Tageszeit\n\n\nAufgabe 3: Erstellen von Density Plots der Praesenz / Absenz in Abhaengigkeit der unabhaengigen Variablen\n\n# Ein Satz Density Plots für den Tagesdatensatz und einer für den Nachtdatensatz \n\npar(mfrow=c(3,3), mar=c(4, 4, 3, 3))\nfor (i in 6:12) {          # innerhalb des for()-loops die Nummern der gewuenschten \n                           # Spalten einstellen\n  d  <-  DF_mod_day %>% pull(i)\n  d  <-  density(d)\n  dp  <-  DF_mod_day %>% filter(pres_abs == 1) %>% pull(i)\n  dp <- density(dp)\n  da  <-  DF_mod_day %>% filter(pres_abs == 0) %>% pull(i)\n  da <- density(da)\n  plot(0,0, type=\"l\", xlim=range(c(dp$x,da$x)), ylim=range(dp$y,da$y), \n       xlab=names(DF_mod_day[i]), ylab=\"Density\")\n  lines(dp$x, dp$y, col=\"blue\")             # Praesenz\n  lines(da$x, da$y, col=\"red\")              # Absenz\n}\n\n\n\nAufgabe 4: Testen eurer erklärenden Variablen auf Normalverteilung (nur kontinuierliche)\n\n\nAufgabe 5: Explorative Analysen der Variablen mit Scatterplots / Scatterplotmatrizen\n\n1) Zu Scatterplots und Scatterplotmatrizen gibt es viele verschiedene Funktionen / Packages, schaut im Internet und sucht euch eines welches euch passt.\n\n\n2) Testen der Korrelation zwischen den Variablen (Parametrisch oder nicht-parametrische Methode? Ausserdem: gewisse Scatterplotmatrizen zeigen euch die Koeffizenten direkt an)"
  },
  {
    "objectID": "fallstudie_n/5_Variablenselektion_Loesung.html#variablenselektion-multivariate-modelle-habitatselektionsmodell",
    "href": "fallstudie_n/5_Variablenselektion_Loesung.html#variablenselektion-multivariate-modelle-habitatselektionsmodell",
    "title": "5. Lösung",
    "section": "Variablenselektion Multivariate Modelle / Habitatselektionsmodell",
    "text": "Variablenselektion Multivariate Modelle / Habitatselektionsmodell\n\nlibraries laden\n\n### Funktion um Packages direkt zu installieren und / oder zu laden\n\nipak <- function(pkg){\n  new.pkg <- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n  if (length(new.pkg)) \n    install.packages(new.pkg, repos = \"http://cran.us.r-project.org\", dependencies = TRUE)\n  sapply(pkg, require, character.only = TRUE)\n}\n\npackages <- c(\"sp\", \"raster\", \"tidyverse\", \"PerformanceAnalytics\", \"pastecs\", \"lme4\", \n              \"bbmle\", \"MuMIn\", \"MASS\", \"magrittr\")\n\nipak(packages)\n\n\n\nVariablenselektion\n\n\n-> Vorgehen analog Coppes et al. \n\n\nAufgabe 1: Mit dem folgenden Code kann eine simple Korrelationsmatrix aufgebaut werden.\n\nDF_mod <- read_delim(here(\"data\",\"Aufgabe4_Datensatz_Habitatnutzung_Modelle_20211101_moodle.csv\"), \n                     delim = \";\")\n\nDF_mod_day <- DF_mod %>%\n  filter(time_of_day == \"day\")\n\n\nround(cor(DF_mod_day[,6:12], method = \"kendall\"),2)\n\n# hier kann die Schwelle fuer die Korrelation gesetzt werden, 0.7 ist liberal / \n# 0.5 konservativ\n\ncor <- round(cor(DF_mod_day[,6:12], method = \"kendall\"),2) \ncor[abs(cor)<0.7] <-0\ncor\n\n\n\nSelektion der Variablen in einem univariaten Model\n\n\nAufgabe 2: Skalieren der Variablen, damit ihr Einfluss vergleichbar wird (Problem verschiedene Skalen der Variablen (bspw. Neigung in Grad, Distanz in Metern))\n\nDF_mod_day %<>%\n  mutate(slope_scaled = scale(slope),\n         us_scaled = scale(us),\n         os_scaled = scale(os),\n         forest_prop_scaled = scale(forest_prop),\n         dist_road_all_scaled = scale(dist_road_all),\n         dist_road_only_scaled = scale(dist_road_only),\n         dist_build_scaled = scale(dist_build),\n         id = as.factor(id))\n\n\n\nAufgabe 3: Ein erstes GLMM (Generalized Linear Mixed Effects Modell) aufbauen: Funktion und Modelformel\n\n\nwichtige Seite auf der man viele Hilfestellungen zu GLMM’s finden kann:\n\n\nhttps://bbolker.github.io/mixedmodels-misc/glmmFAQ.html\n\n# wir werden das package lme4 mit der Funktion glmer verwenden \n# ausserdem brauchen wir noch das package bbmle\n# --> installieren & laden\n\n# die Hilfe von glmer aufrufen: ?glmer\n\n# glmer(formula, data = , family = binomial)\n\n# 1) formula: \n# Abhaengige Variable ~ Erklaerende Variable + Random Factor \n# In unseren Modellen kontrollieren wir fuer individuelle Unterschiede bei den Rehen \n# indem wir einen Random Factor definieren => (1 | id) \n\n# 2) data: \n# euer Datensatz\n\n# 3) family: \n# hier binomial\n\n# warum binomial? Verteilung Daten der Abhaengigen Variable Präsenz/Absenz \n\nggplot(DF_mod_day, aes(pres_abs)) + geom_histogram()\n\n# --> Binaere Verteilung => Binomiale Verteilung mit n = 1 \n\n# und wie schaut es bei der Verteilung der Daten der Abhaengigen Variable \n# Nutzungsintensitaet (nmb) aus?\n\nggplot(DF_mod_day, aes(nmb)) + geom_histogram()\n\n# --> Negativbinomiale Verteilung \n\n\n\nAufgabe 4: Mit der GLMM Formel bauen wir in einem ersten Schritt eine univariate Variablenselektion auf.\n\nAls abhaengige Variable verwenden wir in der ersten Phase die Praesenz/Absenz der Rehe in den Kreisen\n\n# Die erklaerende Variable in m1 ist die erste Variable der korrelierenden Beziehung\n# Die erklaerende Variable in m2 ist die zweite Variable der korrelierenden Beziehung\n\nm1 <- glmer(Abhaengige_Variable ~ Erklaerende_Variable + (1 | id), data = DF_mod_day, \n            family = binomial)\nm2 <- glmer(Abhaengige_Variable ~ Erklaerende_Variable + (1 | id), data = DF_mod_day, \n            family = binomial)\n\n# mit dieser Funktion koennen die Modellergebnisse inspiziert werden\nsummary(m1)\n\n# Mit dieser Funktion kann der Informationgehalt der beiden Modelle gegeneinander \n# abgeschaetzt werden\nbbmle::AICtab(m1, m2)\n\n# tieferer AIC -> besser (AIC = Akaike information criterion) -> als deltaAIC ausgewiesen\n\n# Hier ein Beispiel: Distanz zu Strassen und Wegen versus Distanz zu Strassen\n\nm1 <- glmer(pres_abs ~ dist_road_all_scaled + (1 | id), data = DF_mod_day, \n            family = binomial)\nm2 <- glmer(pres_abs ~ dist_road_only_scaled + (1 | id), data = DF_mod_day, \n            family = binomial)\n\n# summary(m1)  \n\nbbmle::AICtab(m1, m2)\n\n# --> tiefer AIC -> besser == Distanz zu Strassen\n\n# ==> dieses Vorgehen muss nun für alle korrelierten Variablen für jeden Teildatensatz \n# (geringe Störung/starke Störung) durchgeführt werden, um nur noch nicht (R < 0.7) \n# korrelierte Variablen in das Modell einfliessen zu lassen \n\n\n\n\nSelektion der Variablen in einem multivariaten Model\n\nAufgabe 5: Mit folgendem Code kann eine automatisierte Variablenselektion (dredge-Funktion) und ein Modelaveraging aufgebaut werden (siehe auch Stats-Skript von J.Dengler & Team)\n\n# hier wird die Formel für die dredge-Funktion vorbereitet (die Variablen V1-V6 \n# sind jene welche nach der univariaten Variablenselektion noch übrig bleiben)  \n\nf <- pres_abs ~ \n  slope_scaled +\n  us_scaled +\n  os_scaled +\n  forest_prop_scaled +\n  dist_road_only_scaled +\n  dist_build_scaled \n\n# inn diesem Befehl kommt der Random-Factor (das Reh) hinzu und es wird eine Formel \n# daraus gemacht\n\nf_dredge <- paste(c(f, \"+ (1 | id)\"), collapse = \" \") %>% as.formula()\n\n# Das Modell mit dieser Formel ausführen\n\nm <- glmer(f_dredge, data = DF_mod_day, family = binomial, na.action = \"na.fail\")\n\n# Das Modell in die dredge-Funktion einfügen (siehe auch unbedingt ?dredge)\n\nall_m <- dredge(m)\n\n# Importance values der einzelnen Variablen (Gibt an, wie bedeutsam eine bestimmte \n# Variable ist, wenn man viele verschiedene Modelle vergleicht (multimodel inference))\n\nimportance(all_m)\n\nWarning: 'importance' is deprecated.\nUse 'sw' instead.\nSee help(\"Deprecated\")\n\n\nfunction (x) \nUseMethod(\"sw\")\n<bytecode: 0x556968847eb8>\n<environment: namespace:MuMIn>\n\n# Schlussendlich wird ein Modelaverage durchgeführt (Schwellenwert für das delta-AIC = 2)\n\navgmodel <- model.avg(all_m, rank=\"AICc\", subset = delta < 2)\nsummary(avgmodel)\n\n\nCall:\nmodel.avg(object = get.models(object = all_m, subset = delta < \n    2), rank = \"AICc\")\n\nComponent model call: \nglmer(formula = pres_abs ~ <3 unique rhs>, data = DF_mod_day, family = \n     binomial, na.action = na.fail)\n\nComponent models: \n       df   logLik    AICc delta weight\n12356   7 -2337.01 4688.05  0.00   0.48\n123456  8 -2336.48 4689.00  0.95   0.30\n2356    6 -2338.78 4689.58  1.53   0.22\n\nTerm codes: \n    dist_build_scaled dist_road_only_scaled    forest_prop_scaled \n                    1                     2                     3 \n            os_scaled          slope_scaled             us_scaled \n                    4                     5                     6 \n\nModel-averaged coefficients:  \n(full average) \n                      Estimate Std. Error Adjusted SE z value Pr(>|z|)    \n(Intercept)           -0.49074    0.14774     0.14779   3.321 0.000898 ***\ndist_build_scaled     -0.07877    0.06433     0.06434   1.224 0.220854    \ndist_road_only_scaled  0.44281    0.04792     0.04793   9.239  < 2e-16 ***\nforest_prop_scaled     0.83786    0.06487     0.06489  12.912  < 2e-16 ***\nslope_scaled          -0.13548    0.04973     0.04975   2.723 0.006463 ** \nus_scaled              0.40130    0.04101     0.04102   9.784  < 2e-16 ***\nos_scaled              0.01926    0.04529     0.04530   0.425 0.670605    \n \n(conditional average) \n                      Estimate Std. Error Adjusted SE z value Pr(>|z|)    \n(Intercept)           -0.49074    0.14774     0.14779   3.321 0.000898 ***\ndist_build_scaled     -0.10139    0.05508     0.05510   1.840 0.065753 .  \ndist_road_only_scaled  0.44281    0.04792     0.04793   9.239  < 2e-16 ***\nforest_prop_scaled     0.83786    0.06487     0.06489  12.912  < 2e-16 ***\nslope_scaled          -0.13548    0.04973     0.04975   2.723 0.006463 ** \nus_scaled              0.40130    0.04101     0.04102   9.784  < 2e-16 ***\nos_scaled              0.06466    0.06284     0.06286   1.029 0.303635    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# ==> für alle weiteren Datensätze muss der gleiche Prozess der Variablenselektion \n# durchgespielt werden."
  },
  {
    "objectID": "fallstudie_n/5_Variablenselektion_Uebung.html#variablenselektion-multivariate-modelle-habitatselektionsmodell",
    "href": "fallstudie_n/5_Variablenselektion_Uebung.html#variablenselektion-multivariate-modelle-habitatselektionsmodell",
    "title": "5. Aufgabe",
    "section": "Variablenselektion Multivariate Modelle / Habitatselektionsmodell",
    "text": "Variablenselektion Multivariate Modelle / Habitatselektionsmodell\n\nlibraries laden\n\n### Funktion um Packages direkt zu installieren und / oder zu laden\n\nipak <- function(pkg){\n  new.pkg <- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n  if (length(new.pkg)) \n    install.packages(new.pkg, dependencies = TRUE)\n  sapply(pkg, require, character.only = TRUE)\n}\n\npackages <- c(\"sp\", \"raster\", \"tidyverse\", \"PerformanceAnalytics\", \"pastecs\", \"lme4\", \n              \"bbmle\", \"MuMIn\", \"MASS\", \"magrittr\")\n\nipak(packages)\n\n\n\nVariablenselektion\n\n\n-> Vorgehen analog Coppes et al. \n\n\nAufgabe 1: Mit dem folgenden Code kann eine simple Korrelationsmatrix aufgebaut werden.\n\nDF_mod <- read_delim(here(\"data\",\"Aufgabe4_Datensatz_Habitatnutzung_Modelle_20211101_moodle.csv\"), \n                     delim = \";\")\n\nDF_mod_day <- DF_mod %>%\n  filter(time_of_day == \"day\")\n\n\nround(cor(DF_mod_day[,6:12], method = \"kendall\"),2)\n\n# hier kann die Schwelle fuer die Korrelation gesetzt werden, 0.7 ist liberal / \n# 0.5 konservativ\n\ncor <- round(cor(DF_mod_day[,6:12], method = \"kendall\"),2) \ncor[abs(cor)<0.7] <-0\ncor\n\n\n\nSelektion der Variablen in einem univariaten Model\n\n\nAufgabe 2: Skalieren der Variablen, damit ihr Einfluss vergleichbar wird (Problem verschiedene Skalen der Variablen (bspw. Neigung in Grad, Distanz in Metern))\n\nDF_mod_day %<>%\n  mutate(slope_scaled = scale(slope),\n         us_scaled = scale(us),\n         os_scaled = scale(os),\n         forest_prop_scaled = scale(forest_prop),\n         dist_road_all_scaled = scale(dist_road_all),\n         dist_road_only_scaled = scale(dist_road_only),\n         dist_build_scaled = scale(dist_build),\n         id = as.factor(id))\n\n\n\nAufgabe 3: Ein erstes GLMM (Generalized Linear Mixed Effects Modell) aufbauen: Funktion und Modelformel\n\n\nwichtige Seite auf der man viele Hilfestellungen zu GLMM’s finden kann:\n\n\nhttps://bbolker.github.io/mixedmodels-misc/glmmFAQ.html\n\n# wir werden das package lme4 mit der Funktion glmer verwenden \n# ausserdem brauchen wir noch das package bbmle\n# --> installieren & laden\n\n# die Hilfe von glmer aufrufen: ?glmer\n\n# glmer(formula, data = , family = binomial)\n\n# 1) formula: \n# Abhaengige Variable ~ Erklaerende Variable + Random Factor \n# In unseren Modellen kontrollieren wir fuer individuelle Unterschiede bei den Rehen \n# indem wir einen Random Factor definieren => (1 | id) \n\n# 2) data: \n# euer Datensatz\n\n# 3) family: \n# hier binomial\n\n# warum binomial? Verteilung Daten der Abhaengigen Variable Präsenz/Absenz \n\nggplot(DF_mod_day, aes(pres_abs)) + geom_histogram()\n\n# --> Binaere Verteilung => Binomiale Verteilung mit n = 1 \n\n# und wie schaut es bei der Verteilung der Daten der Abhaengigen Variable \n# Nutzungsintensitaet (nmb) aus?\n\nggplot(DF_mod_day, aes(nmb)) + geom_histogram()\n\n# --> Negativbinomiale Verteilung \n\n\n\nAufgabe 4: Mit der GLMM Formel bauen wir in einem ersten Schritt eine univariate Variablenselektion auf.\n\nAls abhaengige Variable verwenden wir in der ersten Phase die Praesenz/Absenz der Rehe in den Kreisen\n\n# Die erklaerende Variable in m1 ist die erste Variable der korrelierenden Beziehung\n# Die erklaerende Variable in m2 ist die zweite Variable der korrelierenden Beziehung\n\nm1 <- glmer(Abhaengige_Variable ~ Erklaerende_Variable + (1 | id), data = DF_mod_day, \n            family = binomial)\nm2 <- glmer(Abhaengige_Variable ~ Erklaerende_Variable + (1 | id), data = DF_mod_day, \n            family = binomial)\n\n# mit dieser Funktion koennen die Modellergebnisse inspiziert werden\nsummary(m1)\n\n# Mit dieser Funktion kann der Informationgehalt der beiden Modelle gegeneinander \n# abgeschaetzt werden\nbbmle::AICtab(m1, m2)\n\n# tieferer AIC -> besser (AIC = Akaike information criterion)\n\n# ==> dieses Vorgehen muss nun für alle korrelierten Variablen für jeden Teildatensatz \n# (Tag/Nacht) durchgeführt werden, um nur noch nicht (R < 0.7) korrelierte Variablen \n# in das Modell einfliessen zu lassen \n\n\n\n\nSelektion der Variablen in einem multivariaten Model\n\nAufgabe 5: Mit folgendem Code kann eine automatisierte Variablenselektion (dredge-Funktion) und ein Modelaveraging aufgebaut werden (siehe auch Stats-Skript von J.Dengler & Team)\n\n# hier wird die Formel für die dredge-Funktion vorbereitet (die Variablen V1-V6 \n# sind jene welche nach der univariaten Variablenselektion noch übrig bleiben)  \n\nf <- pres_abs ~ \n  V1 +\n  V2 +\n  V3 +\n  V4 +\n  V5 +\n  V6 \n\n# inn diesem Befehl kommt der Random-Factor (das Reh) hinzu und es wird eine Formel \n# daraus gemacht\n\nf_dredge <- paste(c(f, \"+ (1 | id)\"), collapse = \" \") %>% as.formula()\n\n# Das Modell mit dieser Formel ausführen\n\nm <- glmer(f_dredge, data = DF_mod_day, family = binomial, na.action = \"na.fail\")\n\n# Das Modell in die dredge-Funktion einfügen (siehe auch unbedingt ?dredge)\n\nall_m <- dredge(m)\n\n# Importance values der einzelnen Variablen (Gibt an, wie bedeutsam eine bestimmte \n# Variable ist, wenn man viele verschiedene Modelle vergleicht (multimodel inference))\n\nimportance(all_m)\n\n# Schlussendlich wird ein Modelaverage durchgeführt (Schwellenwert für das delta-AIC = 2)\n\navgmodel <- model.avg(all_m, rank=\"AICc\", subset = delta < 2)\nsummary(avgmodel)\n\n# ==> für den Nachtdatensatz muss der gleiche Prozess der Variablenselektion \n# durchgespielt werden."
  },
  {
    "objectID": "fallstudie_n/6_Guete_und_Diagnostics_Loesung.html#modellguete-und--diagnostics-mm-habitatselektionsmodell",
    "href": "fallstudie_n/6_Guete_und_Diagnostics_Loesung.html#modellguete-und--diagnostics-mm-habitatselektionsmodell",
    "title": "6. Lösung",
    "section": "Modellguete und -diagnostics MM / Habitatselektionsmodell",
    "text": "Modellguete und -diagnostics MM / Habitatselektionsmodell\n\nNeue packages die wir fuer die Modelle und die Diagnostics brauchen\n\n# neue Packages: DHARMa, car, MASS, ROCR, sjPlot, sjstats, ggeffects, cowplot, gstat\n\nipak <- function(pkg){\n  new.pkg <- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n  if (length(new.pkg)) \n    install.packages(new.pkg, repos = \"http://cran.us.r-project.org\", dependencies = TRUE)\n  sapply(pkg, require, character.only = TRUE)\n}\n\npackages <- c(\"lme4\", \"bbmle\", \"MuMIn\", \"tidyverse\", \"DHARMa\", \"car\", \"MASS\", \"ROCR\", \n              \"sjPlot\",  \"ggeffects\", \"sjstats\", \"cowplot\", \"magrittr\", \"gstat\")\n\nipak(packages)\n\n\n\nDer Modellfit von letzter Woche als Ausgangspunkt für die heutige Übung\n\nDF_mod_day <- read_delim(here(\"data\",\"Aufgabe4_Datensatz_Habitatnutzung_Modelle_20211101_moodle.csv\"), \n                         delim = \";\") %>%\n  filter(time_of_day == \"day\") %>%\n  mutate(slope_scaled = scale(slope),\n         us_scaled = scale(us),\n         os_scaled = scale(os),\n         forest_prop_scaled = scale(forest_prop),\n         dist_road_all_scaled = scale(dist_road_all),\n         dist_road_only_scaled = scale(dist_road_only),\n         dist_build_scaled = scale(dist_build),\n         id = as.factor(id))\n\nf <- pres_abs ~\n  slope_scaled +\n  us_scaled +\n  os_scaled +\n  forest_prop_scaled +\n  dist_road_only_scaled +\n  dist_build_scaled\n\nf <- paste(c(f, \"+ (1 | id)\"), collapse = \" \") %>% as.formula()\n\nm_day <- glmer(f, data= DF_mod_day, family = binomial, na.action = \"na.fail\")\n\nall_m <- dredge(m_day)\n\navgmodel <- model.avg(all_m, rank=\"AICc\", subset = delta < 2)\nsummary(avgmodel)\n\n\n\nDie Modellresultate aus dem avgmodel sind grundaetzlich die finalen Resultate die bereits interpretiert werden koennten. Allerdings funktionieren die Diagnosetests und die Darstellung der Resultate mit diesem gemittelten Modell nicht sehr gut, weshalb wir einen re-fit mit glmer machen muessen (an den Resultaten aendert sich dadurch nichts)\n\nf_pres_abs <- pres_abs ~\n  dist_build_scaled +\n  dist_road_only_scaled +\n  forest_prop_scaled +\n  slope_scaled +\n  us_scaled +\n  os_scaled +\n  (1|id)\n\nm_day <- glmer(f_pres_abs, data= DF_mod_day, family = binomial, na.action = \"na.fail\")\n\n# hier noch zum Vergleich, dass die Resulate sich nur marginal veraendern \n\nsummary(avgmodel)\nsummary(m_day)\n\n# https://stats.stackexchange.com/questions/153611/interpreting-random-effect-variance-in-glmer\n\n# 95% range of the roe deer effects is approximately: -0.97 - 0.97\n\n\n\nAufgabe 1: Berechung der AUC (area under the receiver operating characteristic curve)\n\n= Mass der Modellguete\n\n\n\nFuer die Berechnung des AUC findet ihr weiterfuehrende Informationen unter: https://www.wsl.ch/staff/niklaus.zimmermann/programs/progs/simtest.pdf)\n\nprob <- predict(m_day,type=c(\"response\"))   \npred <- prediction(prob, DF_mod_day$pres_abs)    \n\n# AUC\n\nauc <- performance(pred, measure = \"auc\")@y.values[[1]]\nauc\n\n[1] 0.7769194\n\n\n\n\nAufgabe 2: Interpretieren der Modell-Residuen mittels Tests auf verschiedene Aspekte\n\n\nModel testing for over/underdispersion, zeroinflation and spatial autocorrelation following the DHARMa package.\n\n\nunbedingt die Vignette des DHARMa-Package konsultieren: https://cran.r-project.org/web/packages/DHARMa/vignettes/DHARMa.html\n\n# Residuals werden ueber eine Simulation auf eine Standard-Skala transformiert und \n# koennen anschliessend getestet werden. Dabei kann die Anzahl Simulationen eingestellt \n# werden (dauert je nach dem sehr lange)\n\nsimulationOutput <- simulateResiduals(fittedModel = m_day, n = 10000)\n\n# plotting and testing scaled residuals\n\nplot(simulationOutput)\n\n\n\ntestResiduals(simulationOutput)\n\n\n\n\n$uniformity\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  simulationOutput$scaledResiduals\nD = 0.02194, p-value = 0.03559\nalternative hypothesis: two-sided\n\n\n$dispersion\n\n    DHARMa nonparametric dispersion test via sd of residuals fitted vs.\n    simulated\n\ndata:  simulationOutput\ndispersion = 0.9797, p-value = 0.4862\nalternative hypothesis: two.sided\n\n\n$outliers\n\n    DHARMa outlier test based on exact binomial test with approximate\n    expectations\n\ndata:  simulationOutput\noutliers at both margin(s) = 1, observations = 4185, p-value = 0.567\nalternative hypothesis: true probability of success is not equal to 0.00019998\n95 percent confidence interval:\n 6.049637e-06 1.330610e-03\nsample estimates:\nfrequency of outliers (expected: 0.0001999800019998 ) \n                                         0.0002389486 \n\n\n$uniformity\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  simulationOutput$scaledResiduals\nD = 0.02194, p-value = 0.03559\nalternative hypothesis: two-sided\n\n\n$dispersion\n\n    DHARMa nonparametric dispersion test via sd of residuals fitted vs.\n    simulated\n\ndata:  simulationOutput\ndispersion = 0.9797, p-value = 0.4862\nalternative hypothesis: two.sided\n\n\n$outliers\n\n    DHARMa outlier test based on exact binomial test with approximate\n    expectations\n\ndata:  simulationOutput\noutliers at both margin(s) = 1, observations = 4185, p-value = 0.567\nalternative hypothesis: true probability of success is not equal to 0.00019998\n95 percent confidence interval:\n 6.049637e-06 1.330610e-03\nsample estimates:\nfrequency of outliers (expected: 0.0001999800019998 ) \n                                         0.0002389486 \n\n# The most common concern for GLMMs is overdispersion, underdispersion and \n# zero-inflation.\n\n# separate test for dispersion\n\ntestDispersion(simulationOutput)\n\n\n\n\n\n    DHARMa nonparametric dispersion test via sd of residuals fitted vs.\n    simulated\n\ndata:  simulationOutput\ndispersion = 0.9797, p-value = 0.4862\nalternative hypothesis: two.sided\n\n# test for Zeroinflation\n\ntestZeroInflation(simulationOutput)\n\n\n\n\n\n    DHARMa zero-inflation test via comparison to expected zeros with\n    simulation under H0 = fitted model\n\ndata:  simulationOutput\nratioObsSim = 1.0388, p-value = 0.4544\nalternative hypothesis: two.sided\n\n# test for spatial Autocorrelation\n\n# calculating x, y positions per group\ngroupLocations = aggregate(DF_mod_day[, 3:4], list(DF_mod_day$x, DF_mod_day$y), mean)\ngroupLocations$group <- paste(groupLocations$Group.1,groupLocations$Group.2)\n\n# calculating residuals per group\nres2 = recalculateResiduals(simulationOutput, group = groupLocations$group)\n\n# running the spatial test on grouped residuals\ntestSpatialAutocorrelation(res2, groupLocations$x, groupLocations$y, plot = F)\n\n\n    DHARMa Moran's I test for distance-based autocorrelation\n\ndata:  res2\nobserved = 0.0149472, expected = -0.0002661, sd = 0.0010081, p-value <\n2.2e-16\nalternative hypothesis: Distance-based autocorrelation\n\n# Testen auf Multicollinearitaet (dh zu starke Korrelationen im finalen Modell, zB falls \n# auf Grund der oekologischen Plausibilitaet stark korrelierte Variablen im Modell)\n# use VIF values: if values less then 5 is ok (sometimes > 10), if mean of VIF values \n# not substantially greater than 1 (say 5), no need to worry.\n\ncar::vif(m_day)\n\n    dist_build_scaled dist_road_only_scaled    forest_prop_scaled \n             1.508601              1.129678              2.542469 \n         slope_scaled             us_scaled             os_scaled \n             1.500132              1.219238              2.455757 \n\nmean(car::vif(m_day))\n\n[1] 1.725979\n\n\n\n\nAufgabe 4: Ermittlung des individuellen Beitrags der einzelen Variablen im Gesamtmodell\n\nBestimmen delta AIC nach Coppes et al. 2017 -> Vergleich des Gesamtmodells gegenüber einem Modell ohne die entsprechende Variable.\n\nm_os <- glmer(pres_abs ~\n  dist_build_scaled +\n  dist_road_only_scaled +\n  forest_prop_scaled +\n  slope_scaled +\n  us_scaled +\n  (1|id), data= DF_mod_day, family = binomial, na.action = \"na.fail\")\n\nm_us <- glmer(pres_abs ~\n  dist_build_scaled +\n  dist_road_only_scaled +\n  forest_prop_scaled +\n  slope_scaled +\n  os_scaled +\n  (1|id), data= DF_mod_day, family = binomial, na.action = \"na.fail\")\n\nm_road <- glmer(pres_abs ~\n  dist_build_scaled +\n  forest_prop_scaled +\n  slope_scaled +\n  us_scaled +\n  os_scaled +\n  (1|id), data= DF_mod_day, family = binomial, na.action = \"na.fail\")\n\nm_forest <- glmer(pres_abs ~\n  dist_build_scaled +\n  dist_road_only_scaled +\n  slope_scaled +\n  us_scaled +\n  os_scaled +\n  (1|id), data= DF_mod_day, family = binomial, na.action = \"na.fail\")\n\nm_build <- glmer(pres_abs ~\n  dist_build_scaled +\n  dist_road_only_scaled +\n  forest_prop_scaled +\n  slope_scaled +\n  us_scaled +\n  os_scaled +\n  (1|id), data= DF_mod_day, family = binomial, na.action = \"na.fail\")\n\nm_slope <- glmer(pres_abs ~\n  dist_build_scaled +\n  dist_road_only_scaled +\n  forest_prop_scaled +\n  us_scaled +\n  os_scaled +\n  (1|id), data= DF_mod_day, family = binomial, na.action = \"na.fail\")\n\nbbmle::AICtab(m_day, m_os, m_us, m_road, m_forest, m_build, m_slope)\n\n         dAIC  df\nm_os       0.0 7 \nm_day      0.9 8 \nm_build    0.9 8 \nm_slope    7.3 7 \nm_us      92.2 7 \nm_road    94.3 7 \nm_forest 141.9 7"
  },
  {
    "objectID": "fallstudie_n/6_Guete_und_Diagnostics_Uebung.html#modellguete-und--diagnostics-mm-habitatselektionsmodell",
    "href": "fallstudie_n/6_Guete_und_Diagnostics_Uebung.html#modellguete-und--diagnostics-mm-habitatselektionsmodell",
    "title": "6. Übung",
    "section": "Modellguete und -diagnostics MM / Habitatselektionsmodell",
    "text": "Modellguete und -diagnostics MM / Habitatselektionsmodell\n\nNeue packages die wir fuer die Modelle und die Diagnostics brauchen\n\n# neue Packages: DHARMa, car, MASS, ROCR, sjPlot, sjstats, ggeffects, cowplot, gstat\n\nipak <- function(pkg){\n  new.pkg <- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n  if (length(new.pkg)) \n    install.packages(new.pkg, repos = \"http://cran.us.r-project.org\", dependencies = TRUE)\n  sapply(pkg, require, character.only = TRUE)\n}\n\npackages <- c(\"lme4\", \"bbmle\", \"MuMIn\", \"tidyverse\", \"DHARMa\", \"car\", \"MASS\", \"ROCR\", \n              \"sjPlot\",  \"ggeffects\", \"sjstats\", \"cowplot\", \"magrittr\", \"gstat\")\n\nipak(packages)\n\n\n\nDer Modellfit von letzter Woche als Ausgangspunkt für die heutige Übung\n\nDF_mod_day <- read_delim(here(\"data\",\"Aufgabe4_Datensatz_Habitatnutzung_Modelle_20211101_moodle.csv\"), \n                         delim = \";\") %>%\n  filter(time_of_day == \"day\") %>%\n  mutate(slope_scaled = scale(slope),\n         us_scaled = scale(us),\n         os_scaled = scale(os),\n         forest_prop_scaled = scale(forest_prop),\n         dist_road_all_scaled = scale(dist_road_all),\n         dist_road_only_scaled = scale(dist_road_only),\n         dist_build_scaled = scale(dist_build),\n         id = as.factor(id))\n\nf <- pres_abs ~\n  slope_scaled +\n  us_scaled +\n  os_scaled +\n  forest_prop_scaled +\n  dist_road_only_scaled +\n  dist_build_scaled\n\nf <- paste(c(f, \"+ (1 | id)\"), collapse = \" \") %>% as.formula()\n\nm_day <- glmer(f, data= DF_mod_day, family = binomial, na.action = \"na.fail\")\n\nall_m <- dredge(m_day)\n\navgmodel <- model.avg(all_m, rank=\"AICc\", subset = delta < 2)\nsummary(avgmodel)\n\n\n\nDie Modellresultate aus dem avgmodel sind grundaetzlich die finalen Resultate die bereits interpretiert werden koennten. Allerdings funktionieren die Diagnosetests und die Darstellung der Resultate mit diesem gemittelten Modell nicht sehr gut, weshalb wir einen re-fit mit glmer machen muessen (an den Resultaten aendert sich dadurch nichts)\n\nf_pres_abs <- pres_abs ~\n  dist_build_scaled +\n  dist_road_only_scaled +\n  forest_prop_scaled +\n  slope_scaled +\n  us_scaled +\n  os_scaled +\n  (1|id)\n\nm_day <- glmer(f_pres_abs, data= DF_mod_day, family = binomial, na.action = \"na.fail\")\n\n# hier noch zum Vergleich, dass die Resulate sich nur marginal veraendern \n\nsummary(avgmodel)\nsummary(m_day)\n\n\n\nAufgabe 1: Berechung der AUC (area under the receiver operating characteristic curve)\n\n= Mass der Modellguete\n\n\n\nFuer die Berechnung des AUC findet ihr weiterfuehrende Informationen unter: https://www.wsl.ch/staff/niklaus.zimmermann/programs/progs/simtest.pdf)\n\nprob <- predict(m_day,type=c(\"response\"))   \npred <- prediction(prob, DF_mod_day$pres_abs)    \n\n?prediction\n\n# AUC\n\nauc <- performance(pred, measure = \"auc\")@y.values[[1]]\nauc\n\n\n\nAufgabe 2: Interpretieren der Modell-Residuen mittels Tests auf verschiedene Aspekte\n\n\nModel testing for over/underdispersion, zeroinflation and spatial autocorrelation following the DHARMa package.\n\n\nunbedingt die Vignette des DHARMa-Package konsultieren: https://cran.r-project.org/web/packages/DHARMa/vignettes/DHARMa.html\n\n# Residuals werden ueber eine Simulation auf eine Standard-Skala transformiert und \n# koennen anschliessend getestet werden. Dabei kann die Anzahl Simulationen eingestellt \n# werden (dauert je nach dem sehr lange)\n\nsimulationOutput <- simulateResiduals(fittedModel = m_day, n = 10000)\n\n# plotting and testing scaled residuals\n\nplot(simulationOutput)\n\ntestResiduals(simulationOutput)\n\n# The most common concern for GLMMs is overdispersion, underdispersion and \n# zero-inflation.\n\n# separate test for dispersion\n\ntestDispersion(simulationOutput)\n\n# test for Zeroinflation\n\ntestZeroInflation(simulationOutput)\n\n# test for spatial Autocorrelation\n\n# calculating x, y positions per group\ngroupLocations = aggregate(DF_mod_day[, 3:4], list(DF_mod_day$x, DF_mod_day$y), mean)\ngroupLocations$group <- paste(groupLocations$Group.1,groupLocations$Group.2)\n\n# calculating residuals per group\nres2 = recalculateResiduals(simulationOutput, group = groupLocations$group)\n\n# running the spatial test on grouped residuals\ntestSpatialAutocorrelation(res2, groupLocations$x, groupLocations$y, plot = F)\n\n# Testen auf Multicollinearitaet (dh zu starke Korrelationen im finalen Modell, zB falls \n# auf Grund der oekologischen Plausibilitaet stark korrelierte Variablen im Modell)\n# use VIF values: if values less then 5 is ok (sometimes > 10), if mean of VIF values \n# not substantially greater than 1 (say 5), no need to worry.\n\ncar::vif(m_day)\nmean(car::vif(m_day))\n\n\n\nAufgabe 3: Graphische Darstellung der Modellresultate\n\n# graphische Darstellung der gesamten Modellresultate\n\nplot_model(m_day,transform = NULL, show.values = TRUE, value.offset = .3)\n\n# Plotten der vorhergesagten Wahrscheinlichkeit, dass ein Kreis besetzt ist, in \n# Abhaengigkeit der erklaerenden Variable basierend auf den Modellresultaten.\n\nplot_model(m_day,type = \"pred\", terms = \"us_scaled [all]\")\n\n# Problem: skalierte Variablen lassen sich nicht so ohne weiteres plotten, hier ein quick-\n# and-dirty hack um das Problem zu umgehen. Die Einstellungen muessen fuer jede Variable \n# geaendert werden\n\np <- plot_model(m_day,type = \"pred\", terms = \"us_scaled [all]\") \n\nlabels <- round(seq(floor(min(DF_mod_day$us)), ceiling(max(DF_mod_day$us)), \n                    length.out = 8),2)\n\np <- p + scale_x_continuous(breaks=c(-1,0,1,2,3,4,5,6), labels=c(labels))\n\np\n\n# Funktion um viele Plots auf einem zusammenbringen: cowplot-package (hat auch sonst \n# gute Funktionen fuer schoene layouts von Plots)\n\ncowplot::plot_grid()\n\n\n\nAufgabe 4: Ermittlung des individuellen Beitrags der einzelen Variablen im Gesamtmodell\n\nBestimmen delta AIC nach Coppes et al. 2017 -> Vergleich des Gesamtmodells gegenüber einem Modell ohne die entsprechende Variable."
  },
  {
    "objectID": "fallstudie_n/7_Modelle_mit_Nutzungsintensitaet.html#neue-packages-die-wir-fuer-die-modelle-und-die-diagnostics-brauchen",
    "href": "fallstudie_n/7_Modelle_mit_Nutzungsintensitaet.html#neue-packages-die-wir-fuer-die-modelle-und-die-diagnostics-brauchen",
    "title": "7. Nutzungsintensität",
    "section": "Neue packages die wir fuer die Modelle und die Diagnostics brauchen",
    "text": "Neue packages die wir fuer die Modelle und die Diagnostics brauchen\n\n# neue Packages: DHARMa, car, MASS, ROCR, sjPlot, sjstats, rms, ggeffects, cowplot\n\nipak <- function(pkg){\n  new.pkg <- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\n  if (length(new.pkg)) \n    install.packages(new.pkg, repos = \"http://cran.us.r-project.org\", dependencies = TRUE)\n  sapply(pkg, require, character.only = TRUE)\n}\n\npackages <- c(\"lme4\", \"bbmle\", \"MuMIn\", \"tidyverse\", \"DHARMa\", \"car\", \"MASS\", \"ROCR\", \"sjPlot\", \"rms\", \"ggeffects\", \"sjstats\", \"cowplot\",\"glmmTMB\", \"performance\", \"kableExtra\")\n\nipak(packages)\n\n\nDF_mod_day <- read_delim(here(\"data\",\"Datensatz_Habitatnutzung_Modelle_20191101.csv\"), delim = \";\") %>%\n  filter(time_of_day == \"day\") %>%\n  mutate(slope_scaled = scale(slope),\n         us_scaled = scale(us),\n         os_scaled = scale(os),\n         forest_prop_scaled = scale(forest_prop),\n         dist_road_scaled = scale(dist_road_all),\n         dist_road_only_scaled = scale(dist_road_only),\n         dist_build_scaled = scale(dist_build),\n         id = as.factor(id))"
  },
  {
    "objectID": "fallstudie_n/7_Modelle_mit_Nutzungsintensitaet.html#ursprüngliche-funktion-und-modelformel",
    "href": "fallstudie_n/7_Modelle_mit_Nutzungsintensitaet.html#ursprüngliche-funktion-und-modelformel",
    "title": "7. Nutzungsintensität",
    "section": "ursprüngliche Funktion und Modelformel",
    "text": "ursprüngliche Funktion und Modelformel\n\n# glmer(formula, data = , family = binomial)\n\n# 1) formula: \n# Abhaengige Variable ~ Erklaerende Variable + Random Factor \n# In unseren Modellen kontrollieren wir fuer individuelle Unterschiede bei den Rehen \n# indem wir einen Random Factor definieren => (1 | id) \n\n# 2) data: \n# euer Datensatz\n\n# 3) family: \n# binomial\n\n# Verteilung der abhängigen Variable bei der Nutzungsintensität aus?\n\nggplot(DF_mod_day, aes(nmb)) + geom_histogram()"
  },
  {
    "objectID": "fallstudie_n/7_Modelle_mit_Nutzungsintensitaet.html#hinsichtlich-der-nutzungsintensität-müssen-wir-die-formel-erweitern",
    "href": "fallstudie_n/7_Modelle_mit_Nutzungsintensitaet.html#hinsichtlich-der-nutzungsintensität-müssen-wir-die-formel-erweitern",
    "title": "7. Nutzungsintensität",
    "section": "Hinsichtlich der Nutzungsintensität müssen wir die Formel erweitern:",
    "text": "Hinsichtlich der Nutzungsintensität müssen wir die Formel erweitern:\n\n# Erweiterung um einen sog. Offset-Term, der hier gebraucht wird, um für die Anzahl der GPS Lokalisationen (in der Spalte GPStot aufgeführt) zu korrigeren (eigentlich eine Skalierung der abhängigen Variable um die relative Nutzungsintensität zu modellieren) \n\nf_count <- nmb ~\n  slope_scaled +\n  dist_road_scaled +\n  forest_prop_scaled +\n  os_scaled +\n  us_scaled +\n  dist_build_scaled +\n  offset(log(GPStot)) +\n  (1|id)\n\n### Für die Nutzungsintensität brauchen wir ein neues package (glmmTMB) um das GLMM fitten zu können. glmer kann leider mit der negativ binomial - Verteilung nicht in jedem Fall umgehen. \n\nm <- glmmTMB(f_count, data=DF_mod_day, family = glmmTMB::nbinom2())\n\n# Das Modell in die dredge-Funktion einfügen (siehe auch unbedingt ?dredge)\n\nall_m <- dredge(m)\n\navgmodel <- model.avg(all_m, rank=\"AICc\", subset = delta < 2)\nsummary(avgmodel)"
  },
  {
    "objectID": "fallstudie_n/7_Modelle_mit_Nutzungsintensitaet.html#model-testing-for-overunderdispersion-zeroinflation-and-spatial-autocorrelation-following-the-dharma-package.",
    "href": "fallstudie_n/7_Modelle_mit_Nutzungsintensitaet.html#model-testing-for-overunderdispersion-zeroinflation-and-spatial-autocorrelation-following-the-dharma-package.",
    "title": "7. Nutzungsintensität",
    "section": "Model testing for over/underdispersion, zeroinflation and spatial autocorrelation following the DHARMa package.",
    "text": "Model testing for over/underdispersion, zeroinflation and spatial autocorrelation following the DHARMa package."
  },
  {
    "objectID": "fallstudie_n/7_Modelle_mit_Nutzungsintensitaet.html#unbedingt-die-vignette-des-dharma-package-konsultieren-httpscran.r-project.orgwebpackagesdharmavignettesdharma.html",
    "href": "fallstudie_n/7_Modelle_mit_Nutzungsintensitaet.html#unbedingt-die-vignette-des-dharma-package-konsultieren-httpscran.r-project.orgwebpackagesdharmavignettesdharma.html",
    "title": "7. Nutzungsintensität",
    "section": "unbedingt die Vignette des DHARMa-Package konsultieren: https://cran.r-project.org/web/packages/DHARMa/vignettes/DHARMa.html",
    "text": "unbedingt die Vignette des DHARMa-Package konsultieren: https://cran.r-project.org/web/packages/DHARMa/vignettes/DHARMa.html\n\nf_count <- nmb ~\n  slope_scaled +\n  dist_road_scaled +\n  forest_prop_scaled +\n  os_scaled +\n  us_scaled +\n  offset(log(GPStot)) +\n  (1|id)\n\n### Für die Nutzungsintensität brauchen wir ein neues package (glmmTMB) um das GLMM fitten zu können. glmer kann leider mit der negativ binomial - Verteilung nicht in jedem Fall umgehen. \n\nm_day_count <- glmmTMB(f_count, data=DF_mod_day, family = glmmTMB::nbinom2())\n\nsummary(m_day_count)\n\ntab_model(m_day_count, transform = NULL, show.se = T)\n\n# Residuals werden ueber eine Simulation auf eine Standard-Skala transformiert und kaennen anschliessend getestet werden. Dabei kann die Anzahl Simulationen eingestellt werden (dauert je nach dem sehr lange)\n\nsimulationOutput <- simulateResiduals(fittedModel = m_day_count, n = 10000)\n\n# plotting and testing scaled residuals\n\nplot(simulationOutput)\n\ntestResiduals(simulationOutput)\n\n# The most common concern for GLMMs is overdispersion, underdispersion and zero-inflation.\n\n# separate test for dispersion\n\ntestDispersion(simulationOutput)\n\n# test for Zeroinflation\n\ntestZeroInflation(simulationOutput)\n\n# test for spatial Autocorrelation\n\ndM = as.matrix(dist(cbind(DF_mod_day$x, DF_mod_day$y)))\n\ntestSpatialAutocorrelation(simulationOutput, distMat = dM, plot = F)\n\n# Testen auf Multicollinearitaet (dh zu starke Korrelationen im finalen Modell, zB falls auf Grund der oekologsichen Plausibilitaet stark korrelierte Variablen im Modell)\n#--> funktioniert bei glmmTMB Modellen mit dieser Funktion aus dem performace package:\n\ncheck_collinearity(m_day_count)"
  },
  {
    "objectID": "fallstudie_n/7_Modelle_mit_Nutzungsintensitaet.html#auc-funktioniert-nicht-bei-nicht-binären-abhängigen-variablen-daher-müssen-wir-eine-andere-möglichkeit-finden-um-den-goodness-of-fit-der-modelle-abzuschätzen",
    "href": "fallstudie_n/7_Modelle_mit_Nutzungsintensitaet.html#auc-funktioniert-nicht-bei-nicht-binären-abhängigen-variablen-daher-müssen-wir-eine-andere-möglichkeit-finden-um-den-goodness-of-fit-der-modelle-abzuschätzen",
    "title": "7. Nutzungsintensität",
    "section": "AUC funktioniert nicht bei nicht-binären abhängigen Variablen, daher müssen wir eine andere Möglichkeit finden um den Goodness-of-fit der Modelle abzuschätzen:",
    "text": "AUC funktioniert nicht bei nicht-binären abhängigen Variablen, daher müssen wir eine andere Möglichkeit finden um den Goodness-of-fit der Modelle abzuschätzen:\n\n# Zitat B.Bokler 2013: \"GLMMs are still part of the statistical frontier, and not all of the answers about how to use them are known (even by experts)\"\n\nr2(m_day_count)"
  },
  {
    "objectID": "fallstudie_n/7_Modelle_mit_Nutzungsintensitaet.html#plots-der-vorhergesagten-relativen-nutzungsintensität-funktionieren-nach-dem-selben-prinzip-das-wir-bereits-kennen",
    "href": "fallstudie_n/7_Modelle_mit_Nutzungsintensitaet.html#plots-der-vorhergesagten-relativen-nutzungsintensität-funktionieren-nach-dem-selben-prinzip-das-wir-bereits-kennen",
    "title": "7. Nutzungsintensität",
    "section": "Plots der vorhergesagten relativen Nutzungsintensität funktionieren nach dem selben Prinzip das wir bereits kennen:",
    "text": "Plots der vorhergesagten relativen Nutzungsintensität funktionieren nach dem selben Prinzip das wir bereits kennen:\n\n# graphische Darstellung der gesamten Modellresultate\n\nplot_model(m_day_count,transform = NULL, show.values = TRUE, value.offset = .3)\n\n# Plotten der vorhergesagten Wahrscheinlichkeit, dass ein Kreis besetzt ist, in Abhaengigkeit der erklaerenden Variable basierend auf den Modellresultaten.\n\nplot_model(m_day_count,type = \"pred\", terms = \"dist_road_scaled\")\n\n# Problem: skalierte Variablen lassen sich nicht so ohne Weiteres plotten, hier ein quick-and-dirty hack um das Problem zu umgehen. Die Einstellungen muessen fuer jede Variable geaendert werden\n\np <- plot_model(m_day_count,type = \"pred\", terms = \"dist_road_scaled\") \n\nlabels <- round(seq(floor(min(DF_mod_day$dist_road_all)), ceiling(max(DF_mod_day$dist_road_all)), length.out = 6),2)\n\np <- p + scale_x_continuous(breaks=c(-1,0,1,2,3,4), labels=c(labels))\n\np"
  }
]