---
date: 2022-11-07
lesson: Stat3
thema: Lineare Modelle II
index: 1
format:
  html:
    code-tools:
      source: true
---

# Stat3: Demo

-   Download dieses Demoscript via "\</\>Code" (oben rechts)
-   Datensatz [ipomopsis.csv](https://moodle.zhaw.ch/mod/resource/view.php?id=604493)
-   Datensatz [loyn.csv](https://moodle.zhaw.ch/mod/resource/view.php?id=604496)

## ANCOVA

Experiment zur Fruchtproduktion ("Fruit") von Ipomopsis sp. ("Fruit") in Abhängigkeit von der Beweidung ("Grazing" mit 2 Levels: "Grazed", "Ungrazed") und korrigiert für die Pflanzengrösse vor der Beweidung (hier ausgedrückt als Durchmesser an der Spitze des Wurzelstock: "Root")

```{r}
# Daten einlesen und anschauen

compensation <- read.delim("datasets/statistik/ipomopsis.csv", sep = ",", stringsAsFactors = T)
 head(compensation)
summary(compensation)

```

```{r}
# Pflanzengrösse ("Root") vs. Fruchtproduktion ("Fruit") 
plot(Fruit~Root, data = compensation)
```

-\> Je grösser die Pflanze, desto grösser ihre Fruchtproduktion.

```{r}
# Beweidung ("Grazing") vs. Fruchtroduktion ("Fruit)
boxplot(Fruit~Grazing, data = compensation)
```

> > In der beweideten Gruppe scheint die Fruchtproduktion grösser. Liegt dies an der Beweidung oder an den Pflanzengrössen in der Gruppe?

```{r}
#Lineare Modelle definieren und anschauen

aoc.1 <- lm(Fruit~Root * Grazing, data = compensation)
summary.aov(aoc.1)

aoc.2 <- lm(Fruit~Grazing * Root, data = compensation)
summary.aov(aoc.2)

aoc.3 <- lm(Fruit~Grazing + Root, data = compensation)
summary.aov(aoc.2)
summary.lm(aoc.3)
```

```{r}
# Plotten der Daten
library(tidyverse)
ggplot(compensation, aes(Root, Fruit, color = Grazing)) +
  geom_point() + 
  theme_classic()
```

```{r}
# Ploten mit base R
plot(Fruit~Root, pch = 16, col = Grazing, data = compensation)
legend("topleft", c("grazed", "ungrazed"), col = c("black","red"), pch = 16) 
```

## Polynomische Regression

```{r}
# Daten generieren und Modelle rechnen
e <- c(20, 19, 25, 10, 8, 15, 13, 18, 11, 14, 25, 39, 38, 28, 24)
f <- c(12, 15, 10, 7, 2, 10, 12, 11, 13, 10, 9, 2, 4, 7, 13)

lm.1 <- lm(f~e)
lm.quad <- lm(f~e + I(e^2))

```

```{r}
summary(lm.1)
```

-\> kein signifikanter Zusammenhang und entsprechend kleines Bestimmtheitsmass (adj. R^2^ = 0.07)

```{r}
summary(lm.quad)
```

-\> signifikanter Zusammenhang und viel besseres Bestimmtheitsmass (adj. R^2^ = 0.60)

```{r}
# Modelle plotten

par(mfrow = c(1, 2))

# 1. lineares Modell
plot(f~e, xlim = c(0, 40), ylim = c(0, 20), main = "Lineares Modell")
abline(lm.1, col = "blue")

# 2. quadratisches Modell
plot(f~e, xlim = c(0, 40), ylim = c(0, 20), main = "Quadratisches  Modell")
xv <- seq(0, 40, 0.1) # Input für Modellvoraussage via predict ()
yv2 <- predict(lm.quad, list(e = xv))
lines(xv, yv2, col = "red")

```

```{r}
# Residualplots
par(mfrow = c(2, 2))
plot(lm.1, main = "Lineares Modell")
plot(lm.quad, main = "Quadratisches  Modell")
```

### Simulation Overfitting

```{r}
# Beispieldaten mit 6 Datenpunkten
test <- data.frame("x" = c(1, 2, 3, 4, 5, 6), "y" = c(34, 21, 70, 47, 23, 45))

par(mfrow=c(1,1))
plot(y~x, data = test)
```

```{r}
# Zunehmend komplizierte Modelle definieren und anschauen
lm.0 <- lm(y~1, data = test)
lm.1 <- lm(y~x, data = test)
lm.2 <- lm(y~x+ I(x^2), data = test)
lm.3 <- lm(y~x+ I(x^2) + I(x^3), data = test)
lm.4 <- lm(y~x+ I(x^2) + I(x^3) + I(x^4), data = test)
lm.5 <- lm(y~x+ I(x^2) + I(x^3) + I(x^4) + I(x^5), data = test)
lm.6 <- lm(y~x+ I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6), data = test)
summary(lm.0)
summary(lm.1)
summary(lm.2)
summary(lm.3)
summary(lm.4)
summary(lm.5)


```

```{r}
# Modelle plotten
xv <- seq(from = 0, to = 10, by = 0.1)

plot(y~x, cex = 2, col = "black", lwd = 3, data = test)
yv <- predict(lm.1, list(x = xv))
lines(xv, yv, col = "red", lwd = 3)
yv <- predict(lm.2, list(x = xv))
lines(xv, yv, col = "blue", lwd = 3)
yv<-predict(lm.3, list(x = xv))
lines(xv, yv, col = "green", lwd =3)
yv <- predict(lm.4, list(x = xv))
lines(xv, yv, col = "orange", lwd = 3)
yv <- predict(lm.5, list(x = xv))
lines(xv, yv, col = "black", lwd = 3)
```

## Multiple lineare Regression (basierend auf Logan, Beispiel 9A)

```{r}
# Daten laden und anschauen
loyn <- read.delim("datasets/statistik/loyn.csv", sep = ",")
summary(loyn)
```

### Korrelation zwischen den Prädiktoren

```{r}
cor <- cor(loyn[, 3:8]) # Korrelationen rechnen details siehe: "?cor"

# Korrelationen Visualisieren (google: "correlation plot r"...)
if(!require(corrplot)){install.packages("corrplot")}
library(corrplot)

corrplot.mixed(cor, lower = 'ellipse', upper = "number", order = 'AOE')

```

```{r}
# Volles Modell definieren
# Eigentlich bestünde das volle Modell aus 6 Prädiktoren
# Aus pragmatischen Gründen wird hier nur mit 3 Prädiktoren weitergerechnet

lm.1 <- lm (ABUND ~ YR.ISOL + ALT + GRAZE, data = loyn)
if(!require(car)){install.packages("car")} 
library(car)

vif(lm.1) 

influence.measures(lm.1)

```

### Modellvereinfachung

```{r}

lm.1 <- lm(ABUND ~ YR.ISOL + ALT + GRAZE, data = loyn)
summary(lm.1)

lm.2 <- update(lm.1,~.-YR.ISOL)
anova(lm.1, lm.2)
summary(lm.2)

lm.3 <- update(lm.2,~.-ALT)
anova(lm.2, lm.3)
summary(lm.3)

par(mfrow = c(2, 2))
plot(lm.1)
```

### Hierarchical partitioning

```{r}

if(!require(hier.part)){install.packages("hier.part")}
library(hier.part)

loyn.preds <-with(loyn, data.frame(YR.ISOL, ALT, GRAZE))
hier.part(loyn$ABUND, loyn.preds, gof = "Rsqu")
```

### Partial regressions

```{r}

avPlots(lm.1, ask = F)
```

## Multimodel inference

```{r}
if(!require(MuMIn)){install.packages("MuMIn")}
library(MuMIn)

global.model <- lm(ABUND ~ YR.ISOL + ALT + GRAZE, data = loyn)
options(na.action = "na.fail")

allmodels <- dredge(global.model)
allmodels
sw(allmodels)

avgmodel <- model.avg(allmodels, subset = TRUE)
summary(avgmodel)
```
