[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Research Methods HS22",
    "section": "",
    "text": "Das Modul „Research Methods” vermittelt vertiefte Methodenkompetenzen für praxisorientiertes und angewandtes wissenschaftliches Arbeiten im Fachbereich „Umwelt und Natürliche Ressourcen” auf MSc-Niveau. Die Studierenden erarbeiten sich vertiefte Methodenkompetenzen für die analytische Betrachtung der Zusammenhänge im Gesamtsystem „Umwelt und Natürliche Ressourcen”. Die Studierenden erlernen die methodischen Kompetenzen, auf denen die nachfolgenden Module im MSc Programm UNR aufbauen. Das Modul vermittelt einerseits allgemeine, fächerübergreifende methodische Kompetenzen (z.B. Wissenschaftstheorie, computer-gestützte Datenverarbeitung und Statistik).\nHier werden die Unterlagen für die R-Übungsteile bereitgestellt. Es werden sukzessive sowohl Demo-Files, Aufgabenstellungen und Lösungen veröffentlicht.\nDiese Website wurde am 2022-09-12 11:26:34 zum letzten Mal aktualisiert."
  },
  {
    "objectID": "PrePro.html",
    "href": "PrePro.html",
    "title": "Pre-Processing",
    "section": "",
    "text": "Die Lesson vermittelt zentralste Fertigkeiten zur Vorverarbeitung von strukturierten Daten in der umweltwissenschaftlichen Forschung: Datensätze verbinden (Joins) und umformen („reshape”, „split-apply-combine”). Im Anwendungskontext haben Daten selten von Anfang an diejenige Struktur, welche für die statistische Auswertung oder für die Informationsvisualisierung erforderlich wäre. In dieser lesson lernen die Studierenden die für diese oft zeitraubenden Preprocessing-Schritte notwendigen Konzepte und R-Werkzeuge kennen und kompetent anzuwenden.\n\n\n\n\n\n\n   \n     \n     \n       Sortieren nach\n       Voreinstellung\n         \n          Datum - Datum (aufsteigend)\n        \n         \n          Datum - Neueste\n        \n         \n          Titel\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitel\n\n\nDatum\n\n\nLesson\n\n\nThema\n\n\n\n\n\n\nPrepro 1: Demo\n\n\n2022-10-11\n\n\nPrePro1\n\n\nDatentypen\n\n\n\n\nPrePro 1: Übung\n\n\n2022-10-11\n\n\nPrePro1\n\n\nDatentypen\n\n\n\n\nPrepro 2: Demo\n\n\n2022-10-17\n\n\nPrePro2\n\n\nPiping / Joins\n\n\n\n\nPrepro 2: Übung A\n\n\n2022-10-17\n\n\nPrePro2\n\n\nPiping / Joins\n\n\n\n\nPrepro 2: Übung B\n\n\n2022-10-17\n\n\nPrePro2\n\n\nPiping / Joins\n\n\n\n\nPrepro 3: Demo\n\n\n2022-10-18\n\n\nPrePro3\n\n\nSplit-Apply-Combine\n\n\n\n\nPrepro 3: Übung\n\n\n2022-10-18\n\n\nPrePro3\n\n\nSplit-Apply-Combine\n\n\n\n\n\n\nKeine Treffer"
  },
  {
    "objectID": "prepro/Prepro1_Demo.html",
    "href": "prepro/Prepro1_Demo.html",
    "title": "Prepro 1: Demo",
    "section": "",
    "text": "Datentypen\n\nNumerics\nUnter die Kategorie numeric fallen in R zwei Datentypen:\n\ndouble: Gleitkommazahl (z.B. 10.3, 7.3)\ninteger: Ganzzahl (z.B. 10, 7)\n\n\nDoubles\nFolgendermassen wird eine Gleitkommazahl einer Variabel zuweisen:\n\nx <- 10.3\n\nx\n## [1] 10.3\n\ntypeof(x)\n## [1] \"double\"\n\nStatt <- kann auch = verwendet werden. Dies funktioniert aber nicht in allen Situationen, und ist zudem leicht mit == zu verwechseln.\n\ny = 7.3\n\ny\n## [1] 7.3\n\nOhne explizite Zuweisung nimmt R immer den Datentyp doublean:\n\nz <- 42\ntypeof(z)\n## [1] \"double\"\nis.integer(z)\n## [1] FALSE\nis.numeric(z)\n## [1] TRUE\nis.double(z)\n## [1] TRUE\n\n\n\n\nGanzzahl / Integer\nErst wenn man eine Zahl explizit als integer definiert (mit as.integer() oder L), wird sie auch als solches abgespeichert.\n\na <- as.integer(z)\nis.numeric(a)\n## [1] TRUE\nis.integer(a)\n## [1] TRUE\n\nc <- 8L\nis.numeric(c)\n## [1] TRUE\nis.integer(c)\n## [1] TRUE\n\n\ntypeof(a)\n## [1] \"integer\"\n\nis.numeric(a)\n## [1] TRUE\nis.integer(a)\n## [1] TRUE\n\nMit c() können eine Reihe von Werten in einer Variabel zugewiesen werden (als vector). Es gibt zudem auch character vectors.\n\nvector <- c(10,20,33,42,54,66,77)\nvector\n## [1] 10 20 33 42 54 66 77\nvector[5]\n## [1] 54\nvector[2:4]\n## [1] 20 33 42\n\nvector2 <- vector[2:4]\n\nEine Ganzzahl kann explizit mit as.integer() definiert werden.\n\na <- as.integer(7)\nb <- as.integer(3.14)\na\n## [1] 7\nb\n## [1] 3\ntypeof(a)\n## [1] \"integer\"\ntypeof(b)\n## [1] \"integer\"\nis.integer(a)\n## [1] TRUE\nis.integer(b)\n## [1] TRUE\n\nEine Zeichenkette kann als Zahl eingelesen werden.\n\nc <- as.integer(\"3.14\")\nc\n## [1] 3\ntypeof(c)\n## [1] \"integer\"\n\n\n\nLogische Abfragen\nWird auch auch als boolesch (Eng. boolean) bezeichnet.\n\ne <- 3\nf <- 6\ng <- e > f\ne\n## [1] 3\nf\n## [1] 6\ng\n## [1] FALSE\ntypeof(g)\n## [1] \"logical\"\n\n\n\nLogische Operationen\n\nsonnig <- TRUE\ntrocken <- FALSE\n\nsonnig & !trocken\n## [1] TRUE\n\nOft braucht man auch das Gegenteil / die Negation eines Wertes. Dies wird mittels ! erreicht\n\nu <- TRUE\nv <- !u \nv\n## [1] FALSE\n\n\n\nZeichenketten\nZeichenketten (Eng. character) stellen Text dar\n\ns <- as.character(3.14)\ns\n## [1] \"3.14\"\ntypeof(s)\n## [1] \"character\"\n\nZeichenketten verbinden / zusammenfügen (Eng. concatenate)\n\nfname <- \"Hans\"\nlname <- \"Muster\"\npaste(fname,lname)\n## [1] \"Hans Muster\"\n\nfname2 <- \"hans\"\nfname == fname2\n## [1] FALSE\n\n\n\nFactors\nMit Factors wird in R eine Sammlung von Zeichenketten bezeichnet, die sich wiederholen, z.B. Wochentage (es gibt nur 7 unterschiedliche Werte für “Wochentage”).\n\nwochentage <- c(\"Donnerstag\",\"Freitag\",\"Samstag\",\"Sonntag\",\"Montag\",\"Dienstag\",\"Mittwoch\",\n                \"Donnerstag\",\"Freitag\",\"Samstag\",\"Sonntag\", \"Montag\",\"Dienstag\",\"Mittwoch\")\n\ntypeof(wochentage)\n## [1] \"character\"\n\nwochentage_fac <- as.factor(wochentage)\n\nwochentage\n##  [1] \"Donnerstag\" \"Freitag\"    \"Samstag\"    \"Sonntag\"    \"Montag\"    \n##  [6] \"Dienstag\"   \"Mittwoch\"   \"Donnerstag\" \"Freitag\"    \"Samstag\"   \n## [11] \"Sonntag\"    \"Montag\"     \"Dienstag\"   \"Mittwoch\"\nwochentage_fac\n##  [1] Donnerstag Freitag    Samstag    Sonntag    Montag     Dienstag  \n##  [7] Mittwoch   Donnerstag Freitag    Samstag    Sonntag    Montag    \n## [13] Dienstag   Mittwoch  \n## Levels: Dienstag Donnerstag Freitag Mittwoch Montag Samstag Sonntag\n\nWie man oben sieht, unterscheiden sich character vectors und factors v.a. dadurch, dass letztere über sogenannte levels verfügt. Diese levels entsprechen den eindeutigen Werten.\nZudem ist fällt auf, dass die Reihenfolge der Wohentag alphabetisch sortiert ist. Eine Ordnung kann man mit dem Befehl ordered = T festlegen, dabei muss die Reihenfolge der Werte im Argument levels = explizit festgehalten werden1.\n\nfactor(wochentage, levels = c(\"Montag\", \"Dienstag\", \"Mittwoch\", \"Donnerstag\", \"Freitag\", \"Samstag\", \"Sonntag\"), ordered = TRUE)\n##  [1] Donnerstag Freitag    Samstag    Sonntag    Montag     Dienstag  \n##  [7] Mittwoch   Donnerstag Freitag    Samstag    Sonntag    Montag    \n## [13] Dienstag   Mittwoch  \n## 7 Levels: Montag < Dienstag < Mittwoch < Donnerstag < Freitag < ... < Sonntag\n\nBeachtet das <-Zeichen zwischen den Levels!\n\n\nZeit/Datum\nUm in R mit Datum/Zeit Datentypen umzugehen, müssen sie als POSIXct eingelesen werden. Anders als Beispielsweise bei Excel, sollten in R Datum und Uhrzeit immer zusammen gespeichert werden (in einem Objekt oder einem Vektor).\n\ndatum <- \"2017-10-01 13:45:10\"\n\n# konvertiert character in POSIXct:\nas.POSIXct(datum) \n## [1] \"2017-10-01 13:45:10 CEST\"\n\nWenn das die Zeichenkette in dem obigen Format (Jahr-Monat-Tag Stunde:Minute:Sekunde) daher kommt, braucht as.POSIXctkeine weiteren Informationen.\nSollte das Format von dem aber Abweichen, muss man der Funktion das genaue Schema jedoch mitteilen. Der Syntax dafür kann via ?strptime nachgeschlagen werden.\n\ndatum <- \"01.10.2017 13:45\"\n\n# konvertiert character in POSIXct:\nas.POSIXct(datum,format = \"%d.%m.%Y %H:%M\")\n## [1] \"2017-10-01 13:45:00 CEST\"\n\ndatum <- as.POSIXct(datum,format = \"%d.%m.%Y %H:%M\")\n\nBeachtet, dass in den den obigen Beispiel R automatisch eine Zeitzone angenommen hat (CEST). R geht davon aus, dass die Zeitzone der System Timezone (Sys.timezone()) entspricht.\nUm nun aus dem Datum wieder Spezifische bestandteile zu extrahieren, kann man theoretisch die gleichen Codes nochmal verwenden.\n\nstrftime(datum, format = \"%m\") # extrahiert den Monat als Zahl\n## [1] \"10\"\nstrftime(datum, format = \"%b\") # extrahiert den Monat mit Namen (abgekürzt)\n## [1] \"Okt\"\nstrftime(datum, format = \"%B\") # extrahiert den Monat mit Namen (ausgeschrieben)\n## [1] \"Oktober\"\n\nEinfacher sind an dieser Stelle aber die Functions aus lubridate:\n\nlibrary(lubridate)\n\nmonth(datum)                             # extrahiert den Monat als Zahl\n## [1] 10\nmonth(datum, label = TRUE, abbr = TRUE)  # extrahiert den Monat mit Namen (abgekürzt)\n## [1] Okt\n## 12 Levels: Jan < Feb < Mär < Apr < Mai < Jun < Jul < Aug < Sep < ... < Dez\nmonth(datum, label = TRUE, abbr = FALSE) # extrahiert den Monat mit Namen (ausgeschrieben)\n## [1] Oktober\n## 12 Levels: Januar < Februar < März < April < Mai < Juni < Juli < ... < Dezember\n\n\n\n\nData Frames und Conveniance Variabeln\nEine data.frame ist die gängigste Art, Tabellarische Daten zu speichern.\n\ndf <- data.frame(\n  Stadt = c(\"Zürich\",\"Genf\",\"Basel\",\"Bern\",\"Lausanne\"),\n  Einwohner = c(396027,194565,175131,140634,135629),\n  Ankunft = c(\"1.1.2017 10:00\",\"1.1.2017 14:00\",\n              \"1.1.2017 13:00\",\"1.1.2017 18:00\",\"1.1.2017 21:00\")\n)\n\nstr(df)\n## 'data.frame':    5 obs. of  3 variables:\n##  $ Stadt    : chr  \"Zürich\" \"Genf\" \"Basel\" \"Bern\" ...\n##  $ Einwohner: num  396027 194565 175131 140634 135629\n##  $ Ankunft  : chr  \"1.1.2017 10:00\" \"1.1.2017 14:00\" \"1.1.2017 13:00\" \"1.1.2017 18:00\" ...\n\nIn der obigen data.frame wurde die Spalte Einwohner als Fliesskommazahl abgespeichert. Dies ist zwar nicht tragisch, aber da wir wissen das es sich hier sicher um Ganzzahlen handelt, können wir das korrigieren. Wichtiger ist aber, dass wir die Ankunftszeit (SpalteAnkunft) von einem Factor in ein Zeitformat (POSIXct) umwandeln.\n\ndf$Einwohner <- as.integer(df$Einwohner)\n\ndf$Einwohner\n## [1] 396027 194565 175131 140634 135629\n\ndf$Ankunft <- as.POSIXct(df$Ankunft, format = \"%d.%m.%Y %H:%M\")\n\ndf$Ankunft\n## [1] \"2017-01-01 10:00:00 CET\" \"2017-01-01 14:00:00 CET\"\n## [3] \"2017-01-01 13:00:00 CET\" \"2017-01-01 18:00:00 CET\"\n## [5] \"2017-01-01 21:00:00 CET\"\n\nDiese Rohdaten können nun helfen, um Hilfsvariablen (convenience variables) zu erstellen. Z.B. können wir die Städte einteilen in gross, mittel und klein.\n\ndf$Groesse[df$Einwohner > 300000] <- \"gross\"\ndf$Groesse[df$Einwohner <= 300000 & df$Einwohner > 150000] <- \"mittel\"\ndf$Groesse[df$Einwohner <= 150000] <- \"klein\"\n\ndf$Groesse\n## [1] \"gross\"  \"mittel\" \"mittel\" \"klein\"  \"klein\"\n\nOder aber, die Ankunftszeit kann von der Spalte Ankunftabgeleitet werden.\n\ndf$Ankunft_stunde <- hour(df$Ankunft)\n\ndf$Ankunft_stunde\n## [1] 10 14 13 18 21\n\n\n\n\n\n\n\n\n\n\nordered = T kann nur bei der Funktion factor() spezifiziert werden, nicht bei as.factor(). Ansonsten sind factor() und as.factor() sehr ähnlich.↩︎"
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html",
    "href": "prepro/Prepro1_Uebung.html",
    "title": "PrePro 1: Übung",
    "section": "",
    "text": "Wir empfehlen die Verwendung von “Projects” innerhalb von RStudio. RStudio legt für jedes Projekt dann einen Ordner an, in welches die Projekt-Datei abgelegt wird (Dateiendung .Rproj). Sollen innerhalb des Projekts dann R-Skripts geladen oder erzeugt werden, werden diese dann auch im angelegten Ordner abgelegt. Mehr zu RStudio Projects findet ihr hier.\nDas Verwenden von Projects bringt verschiedene Vorteile, wie zum Beispiel:\n\nFestlegen der Working Directory ohne die Verwendung des expliziten Pfades (setwd()). Das ist sinnvoll, da sich dieser Pfad ändern kann (Zusammenarbeit mit anderen Usern, Ausführung des Scripts zu einem späteren Zeitpunkt)\nAutomatisches Zwischenspeichern geöffneter Scripts und Wiederherstellung der geöffneten Scripts bei der nächsten Session\nFestlegen verschiedener projektspezifischer Optionen\nVerwendung von Versionsverwaltungssystemen (z.B. git)"
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#arbeiten-mit-libraries-packages",
    "href": "prepro/Prepro1_Uebung.html#arbeiten-mit-libraries-packages",
    "title": "PrePro 1: Übung",
    "section": "Arbeiten mit Libraries / Packages",
    "text": "Arbeiten mit Libraries / Packages\nR ist ohne Zusatzpackete nicht mehr denkbar. Die allermeisten Packages werden auf CRAN gehostet und können leicht mittels install.packages() installiert werden. Eine sehr wichtige Sammlung von Packages wird von RStudio entwickelt. Unter dem Namen Tidyverse werden eine Reihe von Packages angeboten, den R-Alltag enorm erleichtert. Wir werden später näher auf das “Tidy”-Universum eingehen, an dieser Stelle können wir einfach mal die wichtigsten Packages aus tidyverse installieren (heute werden wir davon nur einen kleinen Teil benutzen).\n\ninstall.packages(\"dplyr\")\ninstall.packages(\"tidyr\")      # ← MIT Anführungs-/Schlusszeichen\ninstall.packages(\"readr\")\ninstall.packages(\"lubridate\")\ninstall.packages(\"ggplot2\")\n\nUm ein package in R verwenden zu können, gibt es zwei Möglichkeiten:\n\nentweder man lädt es zu Beginn der R-session mittles library(dplyr) (ohne Anführungs- und Schlusszeichen).\noder man ruft eine function mit vorangestelltem Packetname sowie zwei Doppelpunkten auf. dplyr::filter() ruft die Funktion filter() des Packets dplyr auf.\n\nLetztere Notation ist vor allem dann sinnvoll, wenn sich zwei unterschiedliche Funktionen mit dem gleichen namen in verschiedenen pacakges existieren. filter() existiert als Funktion einersits im package dplyr sowie in stats. Dieses Phänomen nennt man “masking”.\nZu Beginn laden wir die nötigen Pakete :\n\nlibrary(readr)                 # ← OHNE Anführungs-/Schlusszeichen\nlibrary(lubridate)\n\n\n\n\n\n\n\nHinweis\n\n\n\nWir nutzen readr um csvs zu importieren und verwenden die Funktion read_csv (mit underscore) als alternative zu read.csv (mit Punkt). Das ist eine persönliche Präferenz1, es ist euch überlassen welche Funktion ihr verwendet. Beachtet, dass die beiden Funktionen leicht andere Parameter erwarten."
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#aufgabe-1",
    "href": "prepro/Prepro1_Uebung.html#aufgabe-1",
    "title": "PrePro 1: Übung",
    "section": "Aufgabe 1",
    "text": "Aufgabe 1\nErstelle eine data.frame mit nachstehenden Daten.\n\n\nMusterlösung\ndf <- data.frame(\n  Tierart = c(\"Fuchs\",\"Bär\",\"Hase\",\"Elch\"),\n  Anzahl = c(2,5,1,3),\n  Gewicht = c(4.4, 40.3,1.1,120),\n  Geschlecht = c(\"m\",\"f\",\"m\",\"m\"),\n  Beschreibung = c(\"Rötlich\",\"Braun, gross\", \"klein, mit langen Ohren\",\"Lange Beine, Schaufelgeweih\")\n  )\n\n\n\n\n\n\n\nTierart\nAnzahl\nGewicht\nGeschlecht\nBeschreibung\n\n\n\n\nFuchs\n2\n4.4\nm\nRötlich\n\n\nBär\n5\n40.3\nf\nBraun, gross\n\n\nHase\n1\n1.1\nm\nklein, mit langen Ohren\n\n\nElch\n3\n120.0\nm\nLange Beine, Schaufelgeweih"
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#aufgabe-2",
    "href": "prepro/Prepro1_Uebung.html#aufgabe-2",
    "title": "PrePro 1: Übung",
    "section": "Aufgabe 2",
    "text": "Aufgabe 2\nWas für Datentypen wurden in der letzten Aufgabe automatisch angenommen? Ermittle diese mit str() und prüfe, ob diese sinnvoll sind und wandle um wo nötig.\n\n\nMusterlösung\nstr(df)\n## 'data.frame':    4 obs. of  5 variables:\n##  $ Tierart     : chr  \"Fuchs\" \"Bär\" \"Hase\" \"Elch\"\n##  $ Anzahl      : num  2 5 1 3\n##  $ Gewicht     : num  4.4 40.3 1.1 120\n##  $ Geschlecht  : chr  \"m\" \"f\" \"m\" \"m\"\n##  $ Beschreibung: chr  \"Rötlich\" \"Braun, gross\" \"klein, mit langen Ohren\" \"Lange Beine, Schaufelgeweih\"\ntypeof(df$Anzahl)\n## [1] \"double\"\n# Anzahl wurde als `double` interpretiert, ist aber eigentlich ein `integer`. \n\ndf$Anzahl <- as.integer(df$Anzahl)"
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#aufgabe-3",
    "href": "prepro/Prepro1_Uebung.html#aufgabe-3",
    "title": "PrePro 1: Übung",
    "section": "Aufgabe 3",
    "text": "Aufgabe 3\nNutze die Spalte Gewicht um die Tiere in 3 Gewichtskategorien einzuteilen:\n\nleicht: < 5kg\nmittel: 5 - 100 kg\nschwer: > 100kg\n\n\n\nMusterlösung\ndf$Gewichtsklasse[df$Gewicht > 100] <- \"schwer\"\ndf$Gewichtsklasse[df$Gewicht <= 100 & df$Gewicht > 5] <- \"mittel\"\ndf$Gewichtsklasse[df$Gewicht <= 5] <- \"leicht\"\n\n\nDas Resultat:\n\n\n\n\n\n\n\n\n\n\n\n\n\nTierart\nAnzahl\nGewicht\nGeschlecht\nBeschreibung\nGewichtsklasse\n\n\n\n\nFuchs\n2\n4.4\nm\nRötlich\nleicht\n\n\nBär\n5\n40.3\nf\nBraun, gross\nmittel\n\n\nHase\n1\n1.1\nm\nklein, mit langen Ohren\nleicht\n\n\nElch\n3\n120.0\nm\nLange Beine, Schaufelgeweih\nschwer"
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#aufgabe-4",
    "href": "prepro/Prepro1_Uebung.html#aufgabe-4",
    "title": "PrePro 1: Übung",
    "section": "Aufgabe 4",
    "text": "Aufgabe 4\nImportiere den Datensatz weather.csv (Quelle MeteoSchweiz). Es handelt sich dabei um die stündlich gemittelten Temperaturdaten an verschiedenen Standorten in der Schweiz.\n\n\n\n\n\n\nWarnung\n\n\n\nWenn du read_csv verwenden möchtest: Diese Funktion erwartet leicht andere inputs als read.csv, konsultiere dazu die Hilfe zu read_csv (mit ?read_csv).\n\n\n\n\nMusterlösung\nwetter <- read_csv(\"datasets/prepro/weather.csv\")\n\n\n\n\n\n\n\nstn\ntime\ntre200h0\n\n\n\n\nABO\n2000010100\n-2.6\n\n\nABO\n2000010101\n-2.5\n\n\nABO\n2000010102\n-3.1\n\n\nABO\n2000010103\n-2.4\n\n\nABO\n2000010104\n-2.5\n\n\nABO\n2000010105\n-3.0\n\n\nABO\n2000010106\n-3.7\n\n\nABO\n2000010107\n-4.4\n\n\nABO\n2000010108\n-4.1\n\n\nABO\n2000010109\n-4.1"
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#aufgabe-5",
    "href": "prepro/Prepro1_Uebung.html#aufgabe-5",
    "title": "PrePro 1: Übung",
    "section": "Aufgabe 5",
    "text": "Aufgabe 5\nSchau dir die Rückmeldung von read_csv()an. Sind die Daten korrekt interpretiert worden?\n\n\nMusterlösung\n# Die Spalte 'time' wurde als 'integer' interpretiert. Dabei handelt es\n# sich offensichtlich um Zeitangaben."
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#aufgabe-6",
    "href": "prepro/Prepro1_Uebung.html#aufgabe-6",
    "title": "PrePro 1: Übung",
    "section": "Aufgabe 6",
    "text": "Aufgabe 6\nDie Spalte time ist eine Datum/Zeitangabe im Format JJJJMMTTHH (siehe meta.txt). Damit R dies als Datum-/Zeitangabe erkennt, müssen wir die Spalte in einem R-Format (POSIXct) einlesen und dabei R mitteilen, wie sie aktuell formatiert ist. Lies die Spalte mit as.POSIXct() ein und spezifiziere sowohl format wie auch tz.\n\n\n\n\n\n\nTipp\n\n\n\n\nWenn keine Zeitzone festgelegt wird, trifft as.POSIXct() eine Annahme (basierend auf Sys.timezone()). In unserem Fall handelt es sich aber um Werte in UTC (siehe metadata.csv)\nas.POSIXcterwartet character: Wenn du eine Fehlermeldung hast die 'origin' must be supplied (o.ä) heisst, hast du der Funktion vermutlich einen Numeric übergeben.\n\n\n\n\n\nMusterlösung\nwetter$time <- as.POSIXct(as.character(wetter$time), format = \"%Y%m%d%H\",tz = \"UTC\")\n\n\n\n\n\nDie neue Tabelle sollte so aussehen\n\n\nstn\ntime\ntre200h0\n\n\n\n\nABO\n2000-01-01 00:00:00\n-2.6\n\n\nABO\n2000-01-01 01:00:00\n-2.5\n\n\nABO\n2000-01-01 02:00:00\n-3.1\n\n\nABO\n2000-01-01 03:00:00\n-2.4\n\n\nABO\n2000-01-01 04:00:00\n-2.5\n\n\nABO\n2000-01-01 05:00:00\n-3.0\n\n\nABO\n2000-01-01 06:00:00\n-3.7\n\n\nABO\n2000-01-01 07:00:00\n-4.4\n\n\nABO\n2000-01-01 08:00:00\n-4.1\n\n\nABO\n2000-01-01 09:00:00\n-4.1"
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#aufgabe-7",
    "href": "prepro/Prepro1_Uebung.html#aufgabe-7",
    "title": "PrePro 1: Übung",
    "section": "Aufgabe 7",
    "text": "Aufgabe 7\nErstelle zwei neue Spalten mit Wochentag (Montag, Dienstag, etc) und Kalenderwoche. Verwende dazu die neu erstellte POSIXct-Spalte sowie eine geeignete Funktion aus lubridate.\n\n\nMusterlösung\nwetter$wochentag <- wday(wetter$time,label = T)\nwetter$kw <- week(wetter$time)\n\n\n\n\n\n\n\nstn\ntime\ntre200h0\nwochentag\nkw\n\n\n\n\nABO\n2000-01-01 00:00:00\n-2.6\nSa.\n1\n\n\nABO\n2000-01-01 01:00:00\n-2.5\nSa.\n1\n\n\nABO\n2000-01-01 02:00:00\n-3.1\nSa.\n1\n\n\nABO\n2000-01-01 03:00:00\n-2.4\nSa.\n1\n\n\nABO\n2000-01-01 04:00:00\n-2.5\nSa.\n1\n\n\nABO\n2000-01-01 05:00:00\n-3.0\nSa.\n1\n\n\nABO\n2000-01-01 06:00:00\n-3.7\nSa.\n1\n\n\nABO\n2000-01-01 07:00:00\n-4.4\nSa.\n1\n\n\nABO\n2000-01-01 08:00:00\n-4.1\nSa.\n1\n\n\nABO\n2000-01-01 09:00:00\n-4.1\nSa.\n1"
  },
  {
    "objectID": "prepro/Prepro1_Uebung.html#aufgabe-8",
    "href": "prepro/Prepro1_Uebung.html#aufgabe-8",
    "title": "PrePro 1: Übung",
    "section": "Aufgabe 8",
    "text": "Aufgabe 8\nErstelle eine neue Spalte basierend auf die Temperaturwerte mit der Einteilung “kalt” (unter Null Grad) und “warm” (über Null Grad)\n\n\nMusterlösung\nwetter$temp_kat[wetter$tre200h0>0] <- \"warm\"\nwetter$temp_kat[wetter$tre200h0<=0] <- \"kalt\"\n\n\n\n\n\n\n\nstn\ntime\ntre200h0\nwochentag\nkw\ntemp_kat\n\n\n\n\nABO\n2000-01-01 00:00:00\n-2.6\nSa.\n1\nkalt\n\n\nABO\n2000-01-01 01:00:00\n-2.5\nSa.\n1\nkalt\n\n\nABO\n2000-01-01 02:00:00\n-3.1\nSa.\n1\nkalt\n\n\nABO\n2000-01-01 03:00:00\n-2.4\nSa.\n1\nkalt\n\n\nABO\n2000-01-01 04:00:00\n-2.5\nSa.\n1\nkalt\n\n\nABO\n2000-01-01 05:00:00\n-3.0\nSa.\n1\nkalt\n\n\nABO\n2000-01-01 06:00:00\n-3.7\nSa.\n1\nkalt\n\n\nABO\n2000-01-01 07:00:00\n-4.4\nSa.\n1\nkalt\n\n\nABO\n2000-01-01 08:00:00\n-4.1\nSa.\n1\nkalt\n\n\nABO\n2000-01-01 09:00:00\n-4.1\nSa.\n1\nkalt"
  },
  {
    "objectID": "prepro/Prepro2_Demo.html",
    "href": "prepro/Prepro2_Demo.html",
    "title": "Prepro 2: Demo",
    "section": "",
    "text": "Diese Demo kann auch als R Script heruntergeladen werden (Rechtsklick → Save Target As..)"
  },
  {
    "objectID": "prepro/Prepro2_Demo.html#piping",
    "href": "prepro/Prepro2_Demo.html#piping",
    "title": "Prepro 2: Demo",
    "section": "Piping",
    "text": "Piping\nGegeben ist ein character string (diary). Wir wollen aus diesem Text die Temperaturangabe aus dem String extrahieren und danach den Wert von Kelvin in Celsius nach der folgenden Formel umwandeln und zum Schluss den Mittelwert über all diese Werte berechnen.\n\\[°C = K - 273.15\\]\n\ndiary <- c(\n  \"The temperature is 310° Kelvin\",\n  \"The temperature is 322° Kelvin\",\n  \"The temperature is 410° Kelvin\"\n)\n\ndiary\n## [1] \"The temperature is 310° Kelvin\" \"The temperature is 322° Kelvin\"\n## [3] \"The temperature is 410° Kelvin\"\n\nDazu brauchen wir die Funktion substr(), welche aus einem character einen Teil “raus schnipseln” kann.\n\n# Wenn die Buchstaben einzelne _Elemente_ eines Vektors wären, würden wir diese\n# folgendermassen subsetten:\n\ncharvec1 <- c(\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\")\ncharvec1[4:6]\n## [1] \"d\" \"e\" \"f\"\n\n# Aber wenn diese in einem einzigen character gespeichert sind, brauchen wir substr:\ncharvec2 <- \"abcdefgh\"\nsubstr(charvec2, 4, 6)\n## [1] \"def\"\n\nZudem nutzen haben wir eine Hilfsfunktion subtrahieren, welche zwei Werte annimmt, den minuend und den subtrahend:\n\nsubtrahieren <- function(minuend, subtrahend){\n  minuend - subtrahend\n}\n\nsubtrahieren(10, 4)\n## [1] 6\n\nÜbersetzt in R-Code entsteht folgende Operation:\n\noutput <- mean(subtrahieren(as.numeric(substr(diary, 20, 22)),273.15))\n#                                             \\_1_/\n#                                      \\________2__________/\n#                           \\___________________3___________/\n#              \\________________________________4__________________/\n#         \\_____________________________________5____________________/\n\n# 1. Nimm diary\n# 2. Extrahiere auf jeder Zeile die Werte 20 bis 22\n# 3. Konvertiere \"character\" zu \"numeric\"\n# 4. Subtrahiere 273.15\n# 5. Berechne den Mittlwert\n\nDie ganze Operation liest sich etwas leichter, wenn diese sequentiell notiert wird:\n\ntemp <- substr(diary, 20, 22)       # 2\ntemp <- as.numeric(temp)            # 3\ntemp <- subtrahieren(temp, 273.15)  # 4\noutput <- mean(temp)                # 5\n\nUmständlich ist dabei einfach, dass die Zwischenresultate immer abgespeichert und in der darauf folgenden Operation wieder abgerufen werden müssen. Hier kommt “piping” ins Spiel: Mit “piping” wird der Output der einen Funktion der erste Parameter der darauf folgenden Funktion.\n\nlibrary(magrittr)\n\ndiary |>                            # 1\n  substr(20, 22) |>                 # 2\n  as.numeric() |>                   # 3 \n  subtrahieren(273.15) |>           # 4\n  mean()                            # 5\n## [1] 74.18333\n\n\n\n\n\n\n\nWichtig\n\n\n\n\nder |> Pipe Operator wurde erst in R 4.1 eingeführt\nNeben dem base R Pipe Operator existiert im Package magrittr ein sehr ähnlicher1 Pipe Operator: %>%\nDie Tastenkombination Ctrl+Shift+M in RStudio fügt einen Pipe Operator ein.\nWelcher Pipe Operator |> oder %>% mit der obigen Tastenkombination eingeführt wird, kann über die RStudio Settings Tools → Global Options → Code → Häckchen setzen bei Use nativ pipe operator"
  },
  {
    "objectID": "prepro/Prepro2_Demo.html#joins",
    "href": "prepro/Prepro2_Demo.html#joins",
    "title": "Prepro 2: Demo",
    "section": "Joins",
    "text": "Joins\n\nstudierende <- data.frame(\n  Matrikel_Nr = c(100002, 100003, 200003),\n  Studi = c(\"Patrick\", \"Manuela\", \"Eva\"),\n  PLZ = c(8006, 8001, 8820)\n)\n\nstudierende\n##   Matrikel_Nr   Studi  PLZ\n## 1      100002 Patrick 8006\n## 2      100003 Manuela 8001\n## 3      200003     Eva 8820\n\nortschaften <- data.frame(\n  PLZ = c(8003, 8006, 8810, 8820),\n  Ortsname = c(\"Zürich\", \"Zürich\", \"Horgen\", \"Wädenswil\")\n)\n\nortschaften\n##    PLZ  Ortsname\n## 1 8003    Zürich\n## 2 8006    Zürich\n## 3 8810    Horgen\n## 4 8820 Wädenswil\n\n\n#Load library\nlibrary(dplyr)\n\ninner_join(studierende, ortschaften, by = \"PLZ\")\n##   Matrikel_Nr   Studi  PLZ  Ortsname\n## 1      100002 Patrick 8006    Zürich\n## 2      200003     Eva 8820 Wädenswil\n\nleft_join(studierende, ortschaften, by = \"PLZ\")\n##   Matrikel_Nr   Studi  PLZ  Ortsname\n## 1      100002 Patrick 8006    Zürich\n## 2      100003 Manuela 8001      <NA>\n## 3      200003     Eva 8820 Wädenswil\n\nright_join(studierende, ortschaften, by = \"PLZ\")\n##   Matrikel_Nr   Studi  PLZ  Ortsname\n## 1      100002 Patrick 8006    Zürich\n## 2      200003     Eva 8820 Wädenswil\n## 3          NA    <NA> 8003    Zürich\n## 4          NA    <NA> 8810    Horgen\n\nfull_join(studierende, ortschaften, by = \"PLZ\")\n##   Matrikel_Nr   Studi  PLZ  Ortsname\n## 1      100002 Patrick 8006    Zürich\n## 2      100003 Manuela 8001      <NA>\n## 3      200003     Eva 8820 Wädenswil\n## 4          NA    <NA> 8003    Zürich\n## 5          NA    <NA> 8810    Horgen\n\n\nstudierende <- data.frame(\n  Matrikel_Nr = c(100002, 100003, 200003),\n  Studi = c(\"Patrick\", \"Manuela\", \"Pascal\"),\n  Wohnort = c(8006, 8001, 8006)\n)\n\nleft_join(studierende, ortschaften, by = c(\"Wohnort\" = \"PLZ\"))\n##   Matrikel_Nr   Studi Wohnort Ortsname\n## 1      100002 Patrick    8006   Zürich\n## 2      100003 Manuela    8001     <NA>\n## 3      200003  Pascal    8006   Zürich"
  },
  {
    "objectID": "prepro/Prepro2_Uebung_A.html",
    "href": "prepro/Prepro2_Uebung_A.html",
    "title": "Prepro 2: Übung A",
    "section": "",
    "text": "library(dplyr)\nlibrary(readr)\nlibrary(lubridate)\n\n# Alternativ kannst du alle tidyverse packages mit library(tidyverse) laden"
  },
  {
    "objectID": "prepro/Prepro2_Uebung_A.html#aufgabe-1",
    "href": "prepro/Prepro2_Uebung_A.html#aufgabe-1",
    "title": "Prepro 2: Übung A",
    "section": "Aufgabe 1",
    "text": "Aufgabe 1\nLade die Wetterdaten von letzer Woche runter (weather.csv, Quelle MeteoSchweiz) und importiere sie in R. Sorge dafür, dass die Spalten korrekt formatiert sind (stn als factor, time als POSIXct, tre200h0 als numeric.)\n\n\nMusterlösung\n# Variante 1\nwetter <- read_csv(\"datasets/prepro/weather.csv\")\nwetter$stn <- as.factor(wetter$stn)\nwetter$time <- as.POSIXct(as.character(wetter$time), format = \"%Y%m%d%H\", tz = \"UTC\")\n\n\n\n\nMusterlösung\n# Variate 2 (für Profis)\nwetter <- read_csv(\"data/weather.csv\",\n                  col_types = cols(\n                    col_factor(levels = NULL),    \n                    col_datetime(format = \"%Y%m%d%H\"),\n                    col_double()\n                    )\n                  )\n## Error: 'data/weather.csv' does not exist in current working directory ('C:/Users/luoe/Documents/Coding/Research Methods/HS22')."
  },
  {
    "objectID": "prepro/Prepro2_Uebung_A.html#aufgabe-2",
    "href": "prepro/Prepro2_Uebung_A.html#aufgabe-2",
    "title": "Prepro 2: Übung A",
    "section": "Aufgabe 2",
    "text": "Aufgabe 2\nLade metadata herunter und lese es ebenfalls als csv ein.\n\n\n\n\n\n\nTipp\n\n\n\nWenn Umlaute und Sonderzeichen nicht korrekt dargestellt werden (z.B. das è in Genève), hat das vermutlich mit der Zeichencodierung zu tun. Das File ist aktuell in UTF-8 codiert. Wenn Umlaute nicht korrekt dargestellt werden, hat R diese Codierung nicht erkannt und sie muss in der Import-Funktion spezifitiert werden. Dies wird je nach verwendete import Funktion unterschiedlich gemacht:\n\nFunktionen aus dem Package readr: locale = locale(encoding = \"UTF-8\")\nBase-R Funktionen: fileEncoding = \"UTF-8\"\n\nWenn ihr die codierung eines Files nicht kennt, könnt wie folgt vorgehen: Anleitung für Windows, für Mac und für Linux.\n\n\n\n\nMusterlösung\nmetadata <- read_delim(\"data/metadata.csv\", delim = \";\", locale = locale(encoding = \"UTF-8\"))\n## Error: 'data/metadata.csv' does not exist in current working directory ('C:/Users/luoe/Documents/Coding/Research Methods/HS22')."
  },
  {
    "objectID": "prepro/Prepro2_Uebung_A.html#aufgabe-3",
    "href": "prepro/Prepro2_Uebung_A.html#aufgabe-3",
    "title": "Prepro 2: Übung A",
    "section": "Aufgabe 3",
    "text": "Aufgabe 3\nNun wollen wir den Datensatz wettermit den Informationen aus metadata anreichern. Uns interessiert aber nur das Stationskürzel, der Name, die x/y Koordinaten sowie die Meereshöhe, selektiere diese Spalten.\n\n\nMusterlösung\nmetadata <- metadata[,c(\"stn\", \"Name\", \"x\",\"y\",\"Meereshoehe\")]\n## Error in eval(expr, envir, enclos): Objekt 'metadata' nicht gefunden"
  },
  {
    "objectID": "prepro/Prepro2_Uebung_A.html#aufgabe-4",
    "href": "prepro/Prepro2_Uebung_A.html#aufgabe-4",
    "title": "Prepro 2: Übung A",
    "section": "Aufgabe 4",
    "text": "Aufgabe 4\nJetzt kann metadata mit dem Datensatz wetter verbunden werden. Überlege dir, welcher Join dafür sinnvoll ist und mit welchem Attribut wir “joinen” können.\nNutze die Join-Möglichkeiten von dplyr (Hilfe via ?dplyr::join) um die Datensätze wetter und metadata zu verbinden.\n\n\nMusterlösung\nwetter <- left_join(wetter,metadata,by = \"stn\")\n## Error in is.data.frame(y): Objekt 'metadata' nicht gefunden\n\n# Jointyp: Left-Join auf 'wetter', da uns nur die Stationen im Datensatz 'wetter' interessieren.\n# Attribut: \"stn\""
  },
  {
    "objectID": "prepro/Prepro2_Uebung_A.html#aufgabe-5",
    "href": "prepro/Prepro2_Uebung_A.html#aufgabe-5",
    "title": "Prepro 2: Übung A",
    "section": "Aufgabe 5",
    "text": "Aufgabe 5\nErstelle eine neue Spalte month welche den jeweiligen Monat (aus time) beinhaltet. Nutze dafür die Funktion lubridate::month().\n\n\nMusterlösung\nwetter$month <- month(wetter$time)"
  },
  {
    "objectID": "prepro/Prepro2_Uebung_A.html#aufgabe-6",
    "href": "prepro/Prepro2_Uebung_A.html#aufgabe-6",
    "title": "Prepro 2: Übung A",
    "section": "Aufgabe 6",
    "text": "Aufgabe 6\nBerechne mit der Spalte month die Durchschnittstemperatur pro Monat.\n\n\nMusterlösung\nmean(wetter$tre200h0[wetter$month == 1])\n## [1] -1.963239\nmean(wetter$tre200h0[wetter$month == 2])\n## [1] 0.3552632\nmean(wetter$tre200h0[wetter$month == 3])\n## [1] 2.965054\n\n# usw. für alle 12 Monate"
  },
  {
    "objectID": "prepro/Prepro2_Uebung_B.html",
    "href": "prepro/Prepro2_Uebung_B.html",
    "title": "Prepro 2: Übung B",
    "section": "",
    "text": "# Benötigte Packages\nlibrary(dplyr)\nlibrary(readr)"
  },
  {
    "objectID": "prepro/Prepro2_Uebung_B.html#aufgabe-1",
    "href": "prepro/Prepro2_Uebung_B.html#aufgabe-1",
    "title": "Prepro 2: Übung B",
    "section": "Aufgabe 1",
    "text": "Aufgabe 1\nGegeben sind die Daten von drei Sensoren (sensor1.csv, sensor2.csv, sensor3.csv). Lade die Datensätze runter und lese sie ein.\n\n\nMusterlösung\nsensor1 <- read_delim(\"data/sensor1.csv\",\";\")\n## Error: 'data/sensor1.csv' does not exist in current working directory ('C:/Users/luoe/Documents/Coding/Research Methods/HS22').\nsensor2 <- read_delim(\"data/sensor2.csv\",\";\")\n## Error: 'data/sensor2.csv' does not exist in current working directory ('C:/Users/luoe/Documents/Coding/Research Methods/HS22').\nsensor3 <- read_delim(\"data/sensor3.csv\",\";\")\n## Error: 'data/sensor3.csv' does not exist in current working directory ('C:/Users/luoe/Documents/Coding/Research Methods/HS22')."
  },
  {
    "objectID": "prepro/Prepro2_Uebung_B.html#aufgabe-2",
    "href": "prepro/Prepro2_Uebung_B.html#aufgabe-2",
    "title": "Prepro 2: Übung B",
    "section": "Aufgabe 2",
    "text": "Aufgabe 2\nErstelle aus den 3 Dataframes eine einzige Dataframe, die aussieht wie unten dargestellt. Nutze dafür zwei joins aus dplyr um 3 data.frames miteinander zu verbinden. Bereinige im Anschluss die Spaltennamen (wie geht das?).\n\n\nMusterlösung\nsensor1_2 <- full_join(sensor1, sensor2, \"Datetime\")\n## Error in full_join(sensor1, sensor2, \"Datetime\"): Objekt 'sensor1' nicht gefunden\n\nsensor1_2 <- rename(sensor1_2, sensor1 = Temp.x, sensor2 = Temp.y)\n## Error in rename(sensor1_2, sensor1 = Temp.x, sensor2 = Temp.y): Objekt 'sensor1_2' nicht gefunden\n\nsensor_all <- full_join(sensor1_2, sensor3, by = \"Datetime\")\n## Error in full_join(sensor1_2, sensor3, by = \"Datetime\"): Objekt 'sensor1_2' nicht gefunden\n\nsensor_all <- rename(sensor_all, sensor3 = Temp)\n## Error in rename(sensor_all, sensor3 = Temp): Objekt 'sensor_all' nicht gefunden\n\n\n\n## Error in head(sensor_all): Objekt 'sensor_all' nicht gefunden"
  },
  {
    "objectID": "prepro/Prepro2_Uebung_B.html#aufgabe-3",
    "href": "prepro/Prepro2_Uebung_B.html#aufgabe-3",
    "title": "Prepro 2: Übung B",
    "section": "Aufgabe 3",
    "text": "Aufgabe 3\nImportiere die Datei sensor_fail.csv in R.\n\n\nMusterlösung\nsensor_fail <- read_delim(\"datasets/prepro/sensor_fail.csv\", delim = \";\")\n\n\nsensor_fail.csv hat eine Variabel SensorStatus: 1 bedeutet der Sensor misst, 0 bedeutet der Sensor misst nicht. Fälschlicherweise wurde auch dann der Messwert Temp = 0 erfasst, wenn Sensorstatus = 0. Richtig wäre hier NA (not available). Korrigiere den Datensatz entsprechend.\n\n\n\n\n\nSensor\nTemp\nHum_%\nDatetime\nSensorStatus\n\n\n\n\nSen102\n0.6\n98\n16102017_1800\n1\n\n\nSen102\n0.3\n96\n17102017_1800\n1\n\n\nSen102\n0.0\n87\n18102017_1800\n1\n\n\nSen102\n0.0\n86\n19102017_1800\n0\n\n\nSen102\n0.0\n98\n23102017_1800\n0\n\n\nSen102\n0.0\n98\n24102017_1800\n0\n\n\nSen102\n0.0\n96\n25102017_1800\n1\n\n\nSen103\n-0.3\n87\n26102017_1800\n1\n\n\nSen103\n-0.7\n98\n27102017_1800\n1\n\n\nSen103\n-1.2\n98\n28102017_1800\n1\n\n\n\n\n\n\n\nMusterlösung\n# mit base-R: \nsensor_fail$Temp_correct[sensor_fail$SensorStatus == 0] <- NA\n\n# das gleiche mit dplyr:\nsensor_fail <- sensor_fail |>\n  mutate(Temp_correct = ifelse(SensorStatus == 0, NA, Temp))"
  },
  {
    "objectID": "prepro/Prepro2_Uebung_B.html#aufgabe-4",
    "href": "prepro/Prepro2_Uebung_B.html#aufgabe-4",
    "title": "Prepro 2: Übung B",
    "section": "Aufgabe 4",
    "text": "Aufgabe 4\nWarum spielt das es eine Rolle, ob 0 oder NA erfasst wird? Berechne die Mittlere der Temperatur / Feuchtigkeit nach der Korrektur.\n\n\nMusterlösung\n\n# Mittelwerte der falschen Sensordaten: 0 fliesst in die Berechnung \n# ein und verfälscht den Mittelwert\nmean(sensor_fail$Temp)\n## [1] -0.13\n\n# Mittelwerte der korrigierten Sensordaten: mit na.rm = TRUE werden \n# NA-Werte aus der Berechnung entfernt. \nmean(sensor_fail$Temp_correct, na.rm = TRUE)\n## [1] -0.1857143"
  },
  {
    "objectID": "prepro/Prepro3_Demo.html",
    "href": "prepro/Prepro3_Demo.html",
    "title": "Prepro 3: Demo",
    "section": "",
    "text": "In dieser Demo möchten wir weitere Werkzeuge aus dem Tidyverse vorstellen und mit Beispielen illustrieren. Die tidyverse-Tools erleichtern den Umgang mit Daten ungeheuer und haben sich mittlerweile zu einem “must have” im Umgang mit Daten in R entwickelt.\nWir können Euch nicht sämtliche Möglichkeiten von tidyverse zeigen. Wir fokussieren uns deshalb auf weitere wichtige Komponenten und zeigen zusätzliche Funktionalitäten, die wir oft verwenden und Euch ggf. noch nicht bekannt sind. Wer sich vertieft mit dem Thema auseinandersetzen möchte, der sollte sich unbedingt das Buch Wickham und Grolemund (2017) beschaffen. Eine umfangreiche, aber nicht ganz vollständige Version gibt es online1 , das vollständige eBook kann über die Bibliothek bezogen werden2.\nWir benötigen dazu folgende Packages:"
  },
  {
    "objectID": "prepro/Prepro3_Demo.html#split-apply-combine",
    "href": "prepro/Prepro3_Demo.html#split-apply-combine",
    "title": "Prepro 3: Demo",
    "section": "Split-Apply-Combine",
    "text": "Split-Apply-Combine\n\nDaten Laden\nWir laden die Wetterdaten (Quelle MeteoSchweiz) von der letzten Übung.\n\nwetter <- read_csv(\"datasets/prepro/weather.csv\")\n\nwetter <- wetter  |>\n  mutate(\n    stn = as.factor(stn),\n    time = as.POSIXct(as.character(time), format = \"%Y%m%d%H\")\n  )\n\n\n\nKennwerte berechnen\nWir möchten den Mittelwert aller gemessenen Temperaturwerte berechnen. Dazu könnten wir folgenden Befehl verwenden:\n\nmean(wetter$tre200h0, na.rm = TRUE) \n## [1] 6.324744\n\nDie Option na.rm = T bedeutet, dass NA Werte von der Berechnung ausgeschlossen werden sollen.\nMit der selben Herangehensweise können diverse Werte berechnet werden (z.B. das Maximum (max()), Minimum (min()), Median (median()) u.v.m.).\nDiese Herangehensweise funktioniert nur dann gut, wenn wir die Kennwerte über alle Beobachtungen für eine Variable (Spalte) berechnen wollen. Sobald wir die Beobachtungen gruppieren wollen, wird es schwierig. Zum Beispiel, wenn wir die durchschnittliche Temperatur pro Monat berechnen wollen.\n\n\nConvenience Variablen\nUm diese Aufgabe zu lösen, muss zuerst der Monat extrahiert werden (der Monat ist die convenience variable). Hierfür brauchen wir die Funktion lubridate::month().\nNun kann kann die convenience Variable “Month” erstellt werden. Ohne dpylr wird eine neue Spalte folgendermassen hinzugefügt.\n\nwetter$month <- month(wetter$time)\n\nMit dplyr (siehe 3) sieht der gleiche Befehl folgendermassen aus:\n\nwetter <- mutate(wetter,month = month(time))\n\nDer grosse Vorteil von dplyr ist an dieser Stelle noch nicht ersichtlich. Dieser wird aber später klar.\n\n\nKennwerte nach Gruppen berechnen\nUm mit base R den Mittelwert pro Monat zu berechnen, kann man zuerst ein Subset mit [] erstellen und davon den Mittelwert berechnen, z.B. folgendermassen:\n\nmean(wetter$tre200h0[wetter$month == 1], na.rm = TRUE)\n## [1] -1.963239\n\nDies müssen wir pro Monat wiederholen, was natürlich sehr umständlich ist. Deshalb nutzen wir das package dplyr. Damit geht die Aufgabe (Temperaturmittel pro Monat berechnen) folgendermassen:\n\nsummarise(group_by(wetter,month),temp_mittel = mean(tre200h0, na.rm = TRUE))\n## # A tibble: 13 × 2\n##    month temp_mittel\n##    <dbl>       <dbl>\n##  1     1      -1.96 \n##  2     2       0.355\n##  3     3       2.97 \n##  4     4       4.20 \n##  5     5      11.0  \n##  6     6      12.4  \n##  7     7      13.0  \n##  8     8      15.0  \n##  9     9       9.49 \n## 10    10       8.79 \n## 11    11       1.21 \n## 12    12      -0.898\n## 13    NA       2.95\n\n\n\nVerketten vs. verschachteln\nAuf Deutsch übersetzt heisst die obige Operation folgendermassen:\n\nnimm den Datensatz wetter\nBilde Gruppen pro Jahr (group_by(wetter,year))\nBerechne das Temperaturmittel (mean(tre200h0))\n\nDiese Übersetzung R-> Deutsch unterscheidet sich vor allem darin, dass die Operation auf Deutsch verkettet ausgesprochen wird (Operation 1->2->3) während der Computer verschachtelt liest 3(2(1)). Um R näher an die gesprochene Sprache zu bringen, kann man den |>-Operator verwenden (siehe 4).\n\n# 1 nimm den Datensatz \"wetter\"\n# 2 Bilde Gruppen pro Monat\n# 3 berechne das Temperaturmittel \n\nsummarise(group_by(wetter,month),temp_mittel = mean(tre200h0))\n#                  \\_1_/\n#         \\__________2_________/\n#\\___________________3_______________________________________/\n\n# wird zu:\n\nwetter |>                                 # 1\n  group_by(month) |>                      # 2\n  summarise(temp_mittel = mean(tre200h0)) # 3\n\nDieses Verketten mittels |> (genannt “pipe”) macht den Code einiges schreib- und leserfreundlicher, und wir werden ihn in den nachfolgenden Übungen verwenden. Die “pipe” wird mit dem package magrittr bereitgestellt und mit dplyr mitinstalliert.\nZu dplyr gibt es etliche Tutorials online (siehe5), deshalb werden wir diese Tools nicht in allen Details erläutern. Nur noch folgenden wichtigen Unterschied zu zwei wichtigen Funktionen in dpylr: mutate() und summarise().\n\nsummarise() fasst einen Datensatz zusammen. Dabei reduziert sich die Anzahl Beobachtungen (Zeilen) auf die Anzahl Gruppen (z.B. eine zusammengefasste Beobachtung (Zeile) pro Jahr). Zudem reduziert sich die Anzahl Variablen (Spalten) auf diejenigen, die in der “summarise” Funktion spezifiziert wurde (z.B. temp_mittel).\nmit mutate wird ein data.frame vom Umfang her belassen, es werden lediglich zusätzliche Variablen (Spalten) hinzugefügt (siehe Beispiel unten).\n\n\n# Maximal und minimal Temperatur pro Kalenderwoche\nweather_summary <- wetter |>               #1) nimm den Datensatz \"wetter\"\n  filter(month == 1) |>                    #2) filter auf den Monat Januar\n  mutate(day = day(time)) |>               #3) erstelle eine neue Spalte \"day\"\n  group_by(day) |>                         #4) Nutze die neue Spalte um Gruppen zu bilden\n  summarise(\n    temp_max = max(tre200h0, na.rm = TRUE), #5) Berechne das Maximum \n    temp_min = min(tre200h0, na.rm = TRUE)  #6) Berechne das Minimum\n    )   \n\nweather_summary\n## # A tibble: 31 × 3\n##      day temp_max temp_min\n##    <int>    <dbl>    <dbl>\n##  1     1      5.8     -4.4\n##  2     2      2.8     -4.3\n##  3     3      4.2     -3.1\n##  4     4      4.7     -2.8\n##  5     5     11.4     -0.6\n##  6     6      6.7     -1.6\n##  7     7      2.9     -2.8\n##  8     8      0.2     -3.6\n##  9     9      2.1     -8.8\n## 10    10      1.6     -2.4\n## # … with 21 more rows"
  },
  {
    "objectID": "prepro/Prepro3_Demo.html#reshaping-data",
    "href": "prepro/Prepro3_Demo.html#reshaping-data",
    "title": "Prepro 3: Demo",
    "section": "Reshaping data",
    "text": "Reshaping data\n\nBreit → lang\nDie Umformung von Tabellen breit→lang erfolgt mittels tidyr(siehe 6). Auch dieses Package funktioniert wunderbar mit piping (|>).\n\nweather_summary |>\n  pivot_longer(c(temp_max,temp_min))\n## # A tibble: 62 × 3\n##      day name     value\n##    <int> <chr>    <dbl>\n##  1     1 temp_max   5.8\n##  2     1 temp_min  -4.4\n##  3     2 temp_max   2.8\n##  4     2 temp_min  -4.3\n##  5     3 temp_max   4.2\n##  6     3 temp_min  -3.1\n##  7     4 temp_max   4.7\n##  8     4 temp_min  -2.8\n##  9     5 temp_max  11.4\n## 10     5 temp_min  -0.6\n## # … with 52 more rows\n\nIm Befehl pivot_longer() müssen wir festlegen, welche Spalten zusammengefasst werden sollen (hier: temp_max,temp_min,temp_mean). Alternativ können wir angeben, welche Spalten wir nicht zusammenfassen wollen:\n\nweather_summary |>\n  pivot_longer(-day)\n## # A tibble: 62 × 3\n##      day name     value\n##    <int> <chr>    <dbl>\n##  1     1 temp_max   5.8\n##  2     1 temp_min  -4.4\n##  3     2 temp_max   2.8\n##  4     2 temp_min  -4.3\n##  5     3 temp_max   4.2\n##  6     3 temp_min  -3.1\n##  7     4 temp_max   4.7\n##  8     4 temp_min  -2.8\n##  9     5 temp_max  11.4\n## 10     5 temp_min  -0.6\n## # … with 52 more rows\n\nWenn wir die Namen neuen Spalten festlegen wollen (anstelle von name und value) erreichen wir dies mit names_to bzw. values_to:\n\nweather_summary_long <- weather_summary |>\n  pivot_longer(-day, names_to = \"Messtyp\", values_to = \"Messwert\")\n\nDie ersten 6 Zeilen von weather_summary_long:\n\n\n\n\n\nday\nMesstyp\nMesswert\n\n\n\n\n1\ntemp_max\n5.8\n\n\n1\ntemp_min\n-4.4\n\n\n2\ntemp_max\n2.8\n\n\n2\ntemp_min\n-4.3\n\n\n3\ntemp_max\n4.2\n\n\n3\ntemp_min\n-3.1\n\n\n\n\n\nDie ersten 6 Zeilen von wetter_sry:\n\n\n\n\n\nday\ntemp_max\ntemp_min\n\n\n\n\n1\n5.8\n-4.4\n\n\n2\n2.8\n-4.3\n\n\n3\n4.2\n-3.1\n\n\n4\n4.7\n-2.8\n\n\n5\n11.4\n-0.6\n\n\n6\n6.7\n-1.6\n\n\n\n\n\nBeachte: weather_summary_long umfasst 62 Beobachtungen (Zeilen), das sind doppelt soviel wie weather_summary, da wir ja zwei Spalten zusammengefasst haben.\n\nnrow(weather_summary)\n## [1] 31\nnrow(weather_summary_long)\n## [1] 62\n\nLange Tabellen sind in verschiedenen Situationen praktischer. Beispielsweise ist das Visualisieren mittels ggplot2 (dieses Package werdet ihr im Block “InfoVis” kennenlernen) mit long tables wesentlich einfacher.\n\nggplot(weather_summary_long, aes(day,Messwert, colour = Messtyp)) +\n  geom_line()\n\n\n\n\nAbbildung 6.1: Generierter Plot\n\n\n\n\n\n\nLang → breit\nDas Gegenstück zu pivot_longer ist pivot_wider. Mit dieser Funktion können wir eine lange Tabelle in eine breite überführen. Dazu müssen wir in names_from angeben, aus welcher Spalte die neuen Spaltennamen erstellt werden sollen (names_from) und aus welcher Spalte die Werte entstammen sollen (values_from):\n\nweather_summary_long |>\n  pivot_wider(names_from = Messtyp, values_from = Messwert)\n## # A tibble: 31 × 3\n##      day temp_max temp_min\n##    <int>    <dbl>    <dbl>\n##  1     1      5.8     -4.4\n##  2     2      2.8     -4.3\n##  3     3      4.2     -3.1\n##  4     4      4.7     -2.8\n##  5     5     11.4     -0.6\n##  6     6      6.7     -1.6\n##  7     7      2.9     -2.8\n##  8     8      0.2     -3.6\n##  9     9      2.1     -8.8\n## 10    10      1.6     -2.4\n## # … with 21 more rows\n\nZum Vergleich: mit einer wide table müssen wir in ggplot2 jede Spalte einzeln plotten. Dies ist bei wenigen Variabeln wie hier noch nicht problematisch, aber bei einer hohen Anzahl wird dies schnell mühsam.\n\nggplot(weather_summary) +\n  geom_line(aes(day, temp_max)) +\n  geom_line(aes(day, temp_min))\n\n\n\n\nAbbildung 6.2: Generierter Plot\n\n\n\n\n\n\n\n\nWickham, Hadley, und Garrett Grolemund. 2017. R for Data Science. O’Reilly. https://ebookcentral.proquest.com/lib/zhaw/detail.action?docID=4770093."
  },
  {
    "objectID": "prepro/Prepro3_Uebung.html",
    "href": "prepro/Prepro3_Uebung.html",
    "title": "Prepro 3: Übung",
    "section": "",
    "text": "Gegeben sei ein Datensatz “sensors_combined.csv”, mit den Temperaturwerten von drei verschiedenen Sensoren. Lade diesen Datensatz herunter, importiere ihn als csv in R (als sensors_combined).\nFormatiere die Datetime Spalte in POSIXct um. Verwende dazu die Funktion as.POSIXct (lies mit ?strftime() nochmal nach wie du das spezfische Format (die “Schablone”) festlegen kannst."
  },
  {
    "objectID": "prepro/Prepro3_Uebung.html#aufgabe-2",
    "href": "prepro/Prepro3_Uebung.html#aufgabe-2",
    "title": "Prepro 3: Übung",
    "section": "Aufgabe 2",
    "text": "Aufgabe 2\nÜberführe die Tabelle in ein langes Format (verwende dazu die Funktion pivot_longer aus tidyr) und speichere den output als sensors_long.\nTipp:\n\nim Argument cols kannst du entweder die Spalten auflisten, die “pivotiert” werden sollen.\nAlternativ kannst du (mit vorangestelltem Minuszeichen, -) die Spalte, bezeichnen, die nicht pivotiert werden soll.\nIn beiden Fällen musst du die Spalten weder mit Anführungs- und Schlusszeichen noch mit dem $-Zeichen versehen."
  },
  {
    "objectID": "prepro/Prepro3_Uebung.html#aufgabe-3",
    "href": "prepro/Prepro3_Uebung.html#aufgabe-3",
    "title": "Prepro 3: Übung",
    "section": "Aufgabe 3",
    "text": "Aufgabe 3\nGruppiere sensors_long nach der neuen Spalte wo die Sensor-Information enthalten ist (default: name) mit group_by und berechne die mittlere Temperatur pro Sensor (summarise). Hinweis: Beide Funktionen sind Teil des Packages dplyr.\nDer Output sieht folgendermassen aus:\n\n\n# A tibble: 3 × 2\n  name    temp_mean\n  <chr>       <dbl>\n1 sensor1      14.7\n2 sensor2      12.0\n3 sensor3      14.4"
  },
  {
    "objectID": "prepro/Prepro3_Uebung.html#aufgabe-4",
    "href": "prepro/Prepro3_Uebung.html#aufgabe-4",
    "title": "Prepro 3: Übung",
    "section": "Aufgabe 4",
    "text": "Aufgabe 4\nErstelle für sensors_long eine neue convenience Variabel month welche den Monat beinhaltet (Tipp: verwende dazu die Funktion month aus lubridate). Gruppiere nun nach month und Sensor und berechne die mittlere Temperatur."
  },
  {
    "objectID": "prepro/Prepro3_Uebung.html#aufgabe-5",
    "href": "prepro/Prepro3_Uebung.html#aufgabe-5",
    "title": "Prepro 3: Übung",
    "section": "Aufgabe 5",
    "text": "Aufgabe 5\nLade jetzt nochmal den Datensatz weather.csv (Quelle MeteoSchweiz) herunter und importiere ihn als CSV mit den korrekten Spaltentypen (stn als factor, time als POSIXct, tre200h0 als double)."
  },
  {
    "objectID": "prepro/Prepro3_Uebung.html#aufgabe-6",
    "href": "prepro/Prepro3_Uebung.html#aufgabe-6",
    "title": "Prepro 3: Übung",
    "section": "Aufgabe 6",
    "text": "Aufgabe 6\nErstelle nun eine convenience Variable für die Kalenderwoche pro Messung (lubridate::isoweek). Berechne im Anschluss den mittleren Temperaturwert pro Kalenderwoche.\n\n\n\nVisualisiere im Anschluss das Resultat:\n\n\n\n\nAbbildung 7.1: Generierter Plot"
  },
  {
    "objectID": "prepro/Prepro3_Uebung.html#aufgabe-7",
    "href": "prepro/Prepro3_Uebung.html#aufgabe-7",
    "title": "Prepro 3: Übung",
    "section": "Aufgabe 7",
    "text": "Aufgabe 7\nIn der vorherigen Aufgabe haben wir die mittlere Temperatur pro Kalenderwoche über alle Jahre (2000 und 2001) berechnet. Wenn wir die Jahre aber miteinander vergleichen wollen, müssen wir das Jahr als zusätzliche convenience Variable erstellen und danach gruppieren. Versuche dies mit den Wetterdaten und visualisiere den Output anschliessend."
  },
  {
    "objectID": "prepro/Prepro3_Uebung.html#aufgabe-8",
    "href": "prepro/Prepro3_Uebung.html#aufgabe-8",
    "title": "Prepro 3: Übung",
    "section": "Aufgabe 8",
    "text": "Aufgabe 8\nÜberführe den Output aus der letzten Übung in eine wide table. Nun lassen sich die beiden Jahre viel besser miteinander vergleichen."
  },
  {
    "objectID": "InfoVis.html",
    "href": "InfoVis.html",
    "title": "InfoVis",
    "section": "",
    "text": "Die konventionelle schliessende Statistik arbeitet in der Regel konfirmatorisch, sprich aus der bestehenden Theorie heraus werden Hypothesen formuliert, welche sodann durch Experimente geprüft und akzeptiert oder verworfen werden. Die Explorative Datenanalyse (EDA) nimmt dazu eine antagonistische Analyseperspektive ein und will in den Daten zunächst Zusammenhänge aufdecken, welche dann wiederum zur Formulierung von prüfbaren Hypothesen führen kann. Die Einheit stellt dazu den klassischen 5-stufigen EDA-Prozess nach Tukey (1980!) vor. Abschliessend wird dann noch die Brücke geschlagen zur modernen Umsetzung der EDA in Form von Visual Analytics."
  },
  {
    "objectID": "InfoVis.html#infovis-2",
    "href": "InfoVis.html#infovis-2",
    "title": "InfoVis",
    "section": "Infovis 2",
    "text": "Infovis 2\nDie Informationsvisualisierung ist eine vielseitige, effektive und effiziente Methode für die explorative Datenanalyse. Während Scatterplots und Histogramme weitherum bekannt sind, bieten weniger bekannte Informationsvisualisierungs-Typen wie etwa Parallelkoordinatenplots, TreeMaps oder Chorddiagramme originelle alternative Darstellungsformen zur visuellen Analyse von Datensätze, welche stets grösser und komplexer werden. Die Studierenden lernen in dieser lesson eine Reihe von Informationsvisualisierungstypen kennen, lernen diese zielführend zu gestalten und selber zu erstellen."
  },
  {
    "objectID": "infovis/Infovis1_Demo.html",
    "href": "infovis/Infovis1_Demo.html",
    "title": "Infovis 1: Demo A",
    "section": "",
    "text": "library(readr)\nlibrary(lubridate)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\nAls erstes laden wir den Temperaturdatensatz temperature_SHA_ZER.csv ein. Es handelt sich dabei um eine leicht modifizierte Variante der Daten aus PrePro1 und PrePro2."
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#base-plot-vs.-ggplot",
    "href": "infovis/Infovis1_Demo.html#base-plot-vs.-ggplot",
    "title": "Infovis 1: Demo A",
    "section": "Base-plot vs. ggplot",
    "text": "Base-plot vs. ggplot\nUm in “base-R” einen Scatterplot zu erstellen wo Datum der Temperatur gegenübersteht, gehen wir wie folgt vor:\n\nplot(temperature$time, temperature$SHA, type = \"l\", col = \"red\")\nlines(temperature$time, temperature$ZER, col = \"blue\")\n\n\n\n\nAbbildung 8.1: Generierter Plot\n\n\n\n\nIn ggplot sieht das etwas anders und auf den ersten Blick etwas komplizierter aus: Ein plot wird durch den Befehl ggplot() initiiert. Hier wird einerseits der Datensatz festgelegt, auf dem der Plot beruht (data =), sowie die Variablen innerhalb des Datensatzes, die Einfluss auf den Plot ausüben (mapping = aes()).\n\n# Datensatz: \"temperature\" | Beeinflussende Variabeln: \"time\" und \"temp\"\nggplot(data = temperature, mapping = aes(time,SHA))             \n\n\n\n\nAbbildung 8.2: Generierter Plot\n\n\n\n\nWeiter braucht es mindestens ein “Layer” der beschreibt, wie die Daten dargestellt werden sollen (z.B. geom_point()). Anders als bei “Piping” (|>) wird ein Layer mit + hinzugefügt.\n\nggplot(data = temperature, mapping = aes(time,SHA)) +         \n  # Layer: \"geom_point\" entspricht Punkten in einem Scatterplot \n  geom_point()                                    \n\n\n\n\nAbbildung 8.3: Generierter Plot\n\n\n\n\nDa ggplot die Eingaben in der Reihenfolge data = und dann mapping =erwartet, können wir diese Spezifizierungen auch weglassen.\n\n#| eval: false\nggplot(temperature, aes(time,SHA)) +\n  geom_point()\n\n\n\n\nAbbildung 8.4: Generierter Plot"
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#long-vs.-wide",
    "href": "infovis/Infovis1_Demo.html#long-vs.-wide",
    "title": "Infovis 1: Demo A",
    "section": "Long vs. wide",
    "text": "Long vs. wide\nWie wir in PrePro 2 bereits erwähnt haben, ist ggplot2 auf long tables ausgelegt. Wir überführen deshalb an dieser Stelle die breite in eine lange Tabelle:\n\ntemperature_long <- pivot_longer(temperature, -time, names_to = \"station\", values_to = \"temp\")\n\nNun wollen wir die unterschiedlichen Stationen unterschiedlich einfärben. Da wir Variablen definieren wollen, welche Einfluss auf die Grafik haben sollen, gehört diese Information in aes().\n\nggplot(temperature_long, aes(time,temp, colour = station)) +\n  geom_point()\n\n\n\n\nAbbildung 8.5: Generierter Plot\n\n\n\n\nWir können noch einen Layer mit Linien hinzufügen:\n\nggplot(temperature_long, aes(time,temp, colour = station)) +\n  geom_point()+\n  geom_line()\n\n\n\n\nAbbildung 8.6: Generierter Plot"
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#beschriftungen-labels",
    "href": "infovis/Infovis1_Demo.html#beschriftungen-labels",
    "title": "Infovis 1: Demo A",
    "section": "Beschriftungen (labels)",
    "text": "Beschriftungen (labels)\nWeiter können wir die Achsen beschriften und einen Titel hinzufügen. Zudem lasse ich die Punkte (geom_point()) nun weg, da mir diese nicht gefallen.\n\nggplot(temperature_long, aes(time,temp, colour = station)) +\n  geom_line() +\n  labs(\n    x = \"Zeit\",\n    y = \"Temperatur in Grad C°\", \n    title = \"Temperaturdaten Schweiz\",\n    subtitle = \"2001 bis 2002\",\n    color = \"Station\"\n    )\n\n\n\n\nAbbildung 8.7: Generierter Plot"
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#split-apply-combine",
    "href": "infovis/Infovis1_Demo.html#split-apply-combine",
    "title": "Infovis 1: Demo A",
    "section": "Split Apply Combine",
    "text": "Split Apply Combine\nIm obigen Plot fällt auf, dass stündliche Werte eine zu hohe Auflösung ist, wenn wir daten über 2 Jahre visualisieren. Mit Split Apply Combine (PrePro 3) können wir die Auflösung unserer Daten verändern:\n\ntemperature_day <- temperature_long |>\n  mutate(time = as.Date(time)) \n\ntemperature_day\n\n# A tibble: 35,088 × 3\n   time       station  temp\n   <date>     <chr>   <dbl>\n 1 2000-01-01 SHA       0.2\n 2 2000-01-01 ZER      -8.8\n 3 2000-01-01 SHA       0.3\n 4 2000-01-01 ZER      -8.7\n 5 2000-01-01 SHA       0.3\n 6 2000-01-01 ZER      -9  \n 7 2000-01-01 SHA       0.3\n 8 2000-01-01 ZER      -8.7\n 9 2000-01-01 SHA       0.4\n10 2000-01-01 ZER      -8.5\n# … with 35,078 more rows\n\ntemperature_day <- temperature_day |>\n  group_by(station, time) |>\n  summarise(temp = mean(temp))\n\ntemperature_day  \n\n# A tibble: 1,462 × 3\n# Groups:   station [2]\n   station time        temp\n   <chr>   <date>     <dbl>\n 1 SHA     2000-01-01  1.25\n 2 SHA     2000-01-02  1.73\n 3 SHA     2000-01-03  1.59\n 4 SHA     2000-01-04  1.78\n 5 SHA     2000-01-05  4.66\n 6 SHA     2000-01-06  3.49\n 7 SHA     2000-01-07  3.87\n 8 SHA     2000-01-08  3.28\n 9 SHA     2000-01-09  3.24\n10 SHA     2000-01-10  3.24\n# … with 1,452 more rows"
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#xy-achse-anpassen",
    "href": "infovis/Infovis1_Demo.html#xy-achse-anpassen",
    "title": "Infovis 1: Demo A",
    "section": "X/Y-Achse anpassen",
    "text": "X/Y-Achse anpassen\nMan kann auch Einfluss auf die x-/y-Achsen nehmen. Dabei muss man zuerst festlegen, was für ein Achsentyp der Plot hat (vorher hat ggplot eine Annahme auf der Basis der Daten getroffen).\nBei unserer y-Achse handelt es sich um numerische Daten, ggplot nennt diese: scale_y_continuous(). Unter ggplot2.tidyverse.org findet man noch andere x/y-Achsentypen (scale_x_irgenwas bzw. scale_y_irgendwas).\n\nggplot(temperature_day, aes(time,temp, colour = station)) +\n  geom_line() +\n  labs(\n    x = \"Zeit\",\n    y = \"Temperatur in Grad C°\", \n    title = \"Temperaturdaten Schweiz\",\n    subtitle = \"2001 bis 2002\",\n    color = \"Station\"\n    ) +    \n  scale_y_continuous(limits = c(-30,30))    # y-Achsenabschnitt bestimmen\n\n\n\n\nAbbildung 8.8: Generierter Plot\n\n\n\n\nDas gleiche Spiel kann man für die y-Achse betreiben. Bei unserer y-Achse handelt es sich ja um Datumsangaben. ggplot nennt diese: scale_x_date().\n\nggplot(temperature_day, aes(time,temp, colour = station)) +\n  geom_line() +\n  labs(\n    x = \"Zeit\",\n    y = \"Temperatur in Grad C°\", \n    title = \"Temperaturdaten Schweiz\",\n    subtitle = \"2001 bis 2002\",\n    color = \"Station\"\n    ) +    \n  scale_y_continuous(limits = c(-30,30)) +\n  scale_x_date(date_breaks = \"3 months\", \n                   date_labels = \"%b\")\n\n\n\n\nAbbildung 8.9: Generierter Plot"
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#themes",
    "href": "infovis/Infovis1_Demo.html#themes",
    "title": "Infovis 1: Demo A",
    "section": "Themes",
    "text": "Themes\nMit theme verändert man das allgmeine Layout der Plots. Beispielsweise kann man mit theme_classic() ggplot-Grafiken etwas weniger “Poppig” erscheinen lassen: so sind sie besser für Bachelor- / Masterarbeiten sowie Publikationen geeignet. theme_classic() kann man indiviudell pro Plot anwenden, oder für die aktuelle Session global setzen (s.u.)\nIndividuell pro Plot:\n\nggplot(temperature_day, aes(time,temp, colour = station)) +\n  geom_line() +\n  theme_classic()\n\nGlobal (für alle nachfolgenden Plots der aktuellen Session):\n\ntheme_set(theme_classic())"
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#facets-small-multiples",
    "href": "infovis/Infovis1_Demo.html#facets-small-multiples",
    "title": "Infovis 1: Demo A",
    "section": "Facets / Small Multiples",
    "text": "Facets / Small Multiples\nSehr praktisch sind auch die Funktionen für “Small multiples”. Dies erreicht man mit facet_wrap() (oder facet_grid(), mehr dazu später). Man muss mit einem Tilde-Symbol “~” nur festlegen, welche Variable für das Aufteilen des Plots in kleinere Subplots verantwortlich sein soll.\n\nggplot(temperature_day, aes(time,temp, colour = station)) +\n  geom_line() +\n  labs(\n    x = \"Zeit\",\n    y = \"Temperatur in Grad C°\", \n    title = \"Temperaturdaten Schweiz\",\n    subtitle = \"2001 bis 2002\",\n    color = \"Station\"\n    ) +    \n  scale_y_continuous(limits = c(-30,30)) +\n  scale_x_date(date_breaks = \"3 months\", \n                   date_labels = \"%b\") +\n  facet_wrap(station~.)\n\n\n\n\nAbbildung 8.10: Generierter Plot\n\n\n\n\nAuch facet_wrap kann man auf seine Bedürfnisse anpassen: Beispielweise kann man mit ncol = die Anzahl facets pro Zeile bestimmen.\nZudem brauchen wir die Legende nicht mehr, da der Stationsnamen über jedem Facet steht. Ich setze deshalb theme(legend.position=\"none\")\n\nggplot(temperature_day, aes(time,temp, colour = station)) +\n  geom_line() +\n  labs(\n    x = \"Zeit\",\n    y = \"Temperatur in Grad C°\", \n    title = \"Temperaturdaten Schweiz\",\n    subtitle = \"2001 bis 2002\"\n    ) +  \n  scale_y_continuous(limits = c(-30,30)) +\n  scale_x_date(date_breaks = \"3 months\", \n                   date_labels = \"%b\") +\n  facet_wrap(~station,ncol = 1) +\n  theme(legend.position=\"none\")\n\n\n\n\nAbbildung 8.11: Generierter Plot"
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#in-variabel-abspeichern-und-exportieren",
    "href": "infovis/Infovis1_Demo.html#in-variabel-abspeichern-und-exportieren",
    "title": "Infovis 1: Demo A",
    "section": "In Variabel abspeichern und Exportieren",
    "text": "In Variabel abspeichern und Exportieren\nGenau wie data.frames und andere Objekte, kann man einen ganzen Plot auch in einer Variabel speichern. Dies kann nützlich sein um einen Plot zu exportieren (als png, jpg usw.) oder sukzessive erweitern wie in diesem Beispiel.\n\np <- ggplot(temperature_day, aes(time,temp, colour = station)) +\n  geom_line() +\n  labs(\n    x = \"Zeit\",\n    y = \"Temperatur in Grad C°\", \n    title = \"Temperaturdaten Schweiz\",\n    subtitle = \"2001 bis 2002\"\n    ) +\n  scale_y_continuous(limits = c(-30,30)) +\n  scale_x_date(date_breaks = \"3 months\", \n                   date_labels = \"%b\") +\n  facet_wrap(~station,ncol = 1)\n  # ich habe an dieser Stelle theme(legend.position=\"none\") entfernt\n\nFolgendermassen kann ich den Plot als png-File abspeichern (ohne Angabe von “plot =” wird einfach der letzte Plot gespeichert)\n\nggsave(filename = \"plot.png\",plot = p)\n\n.. und so kann ich einen bestehenden Plot (in einer Variabel) mit einem Layer / einer Option erweitern\n\np +\n  theme(legend.position=\"none\")\n\nWie üblich wurde diese Änderung nicht gespeichert, sondern nur das Resultat davon ausgeben. Wenn die Änderung in meinem Plot (in der Variabel) abspeichern will, muss ich die Variabel überschreiben:\n\np <- p +\n  theme(legend.position=\"none\")"
  },
  {
    "objectID": "infovis/Infovis1_Demo.html#smoothing",
    "href": "infovis/Infovis1_Demo.html#smoothing",
    "title": "Infovis 1: Demo A",
    "section": "Smoothing",
    "text": "Smoothing\nMit geom_smooth() kann ggplot eine Trendlinie auf der Baiss von Punktdaten berechnen. Die zugrunde liegende statistische Methode kann selbst gewählt werden (ohne Angabe verwendet ggplot bei < 1’000 Messungen stats::loess, ansonsten mgcv::gam)\n\np <- p +\n  geom_smooth(colour = \"black\")\n\np\n\n\n\n\nAbbildung 8.12: Generierter Plot"
  },
  {
    "objectID": "infovis/Infovis1_Script_eda.html",
    "href": "infovis/Infovis1_Script_eda.html",
    "title": "Infovis 1: Demo B",
    "section": "",
    "text": "ggplot(people, aes(x=age, y=height)) + \n  geom_point() +\n  scale_y_continuous(limits = c(0.75, 2))\n\n\n\n\n# Go to help page: http://docs.ggplot2.org/current/ -> Search for icon of fit-line\n# http://docs.ggplot2.org/current/geom_smooth.html\n\n\n# build a scatterplot for a first inspection, with regression line\nggplot(people, aes(x=age, y=height)) + \n  geom_point() + \n  scale_y_continuous(limits=c(0, 2.0)) +\n  geom_smooth()\n\n\n\n\n\n# stem and leaf plot\nstem(people$height)\n## \n##   The decimal point is 1 digit(s) to the left of the |\n## \n##    8 | 25593\n##   10 | 037\n##   12 | 523\n##   14 | 19556\n##   16 | 255789916\n##   18 | 04774\nstem(people$height, scale=2)\n## \n##   The decimal point is 1 digit(s) to the left of the |\n## \n##    8 | 2559\n##    9 | 3\n##   10 | \n##   11 | 037\n##   12 | 5\n##   13 | 23\n##   14 | 19\n##   15 | 556\n##   16 | 2557899\n##   17 | 16\n##   18 | 0477\n##   19 | 4\n\n\n# explore the two variables with box-whiskerplots\nsummary(people$age)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##    5.00    8.70   30.20   59.14   65.15  512.30\nboxplot(people$age)\n\n\n\n\n\nsummary(people$height)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   0.820   1.190   1.555   1.455   1.690   1.940\nboxplot(people$height)\n\n\n\n\n\n# explore data with a histgram\nggplot(people, aes(x=age)) + \n  geom_histogram(binwidth=20)  \n\n\n\n\n\ndensity(x = people$height)\n## \n## Call:\n##  density.default(x = people$height)\n## \n## Data: people$height (30 obs.);   Bandwidth 'bw' = 0.1576\n## \n##        x                y           \n##  Min.   :0.3472   Min.   :0.001593  \n##  1st Qu.:0.8636   1st Qu.:0.102953  \n##  Median :1.3800   Median :0.510601  \n##  Mean   :1.3800   Mean   :0.483553  \n##  3rd Qu.:1.8964   3rd Qu.:0.722660  \n##  Max.   :2.4128   Max.   :1.216350\n\n# re-expression: use log or sqrt axes\n#\n# Find here guideline about scaling axes \n# http://www.cookbook-r.com/Graphs/Axes_(ggplot2)/\n# http://docs.ggplot2.org/0.9.3.1/scale_continuous.html\n\n\n# logarithmic axis: respond to skewness in the data, e.g. log10 \nggplot(people, aes(x=age, y=height)) + \n  geom_point() + \n  scale_y_continuous(limits=c(0, 2.0)) +\n  geom_smooth() +\n  scale_x_log10()\n\n\n\n\n\n# outliers: Remove very small and very old people\n\npeopleClean <- people |>\n  filter(ID != 27) |>    # Diese Person war zu klein.\n  filter(age < 100)       # Fehler in der Erhebung des Alters\n\n\nggplot(peopleClean, aes(x=age)) + \n  geom_histogram(binwidth=10)\n\n\n\n\n\nggplot(peopleClean, aes(x=age, y=height)) + \n  geom_point() + \n  scale_y_continuous(limits=c(0, 2.0)) +\n  geom_smooth()\n\n\n\n\n\n# with custom binwidth\nggplot(peopleClean, aes(x=age)) + \n  geom_histogram(binwidth=10) + \n  theme_bw() # specifying the theme\n\n\n\n\n\n# quadratic axis\nggplot(peopleClean, aes(x=age, y=height)) + \n  geom_point() + scale_y_continuous(limits=c(0, 2.0)) +\n  geom_smooth(method=\"lm\", fill='lightblue', size=0.5, alpha=0.5) + \n  scale_x_sqrt()\n\n\n\n\n\n# filter \"teenies\": No trend\nfilter(peopleClean, age < 15) |>\n  ggplot(aes(x=age, y=height)) + \n  geom_point() + \n  scale_y_continuous(limits=c(0, 2.0)) +\n  geom_smooth(method=\"lm\", fill='lightblue', size=0.5, alpha=0.5)\n\n\n\n\n\n# filter \"teenies\": No trend\npeopleClean |>\n  filter(age > 55) |>\n  ggplot(aes(x=age, y=height)) + \n  geom_point() + \n  scale_y_continuous(limits=c(0, 2.0)) +\n  geom_smooth(method=\"lm\", fill='lightblue', size=0.5, alpha=0.5)\n\n\n\n\n\n# Onwards towards multidimensional data\n\n# Finally, make a scatterplot matrix\npairs(peopleClean[,2:4], panel=panel.smooth)\n\n\n\n\n\npairs(peopleClean[,2:4], panel=panel.smooth)"
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html",
    "href": "infovis/Infovis1_Uebung.html",
    "title": "Infovis 1: Übung",
    "section": "",
    "text": "In dieser Übung geht es darum, die Grafiken aus dem Blog-post Kovic (2014) zu rekonstruieren. Der urspüngliche Blogpost ist nicht mehr verfügbar, wir haben deshalb eine Kopie auf folgender Website gehostet:\nhttps://researchmethods-zhaw.github.io/blog.tagesanzeiger.ch/\nSchau dir die Grafiken in dem Blogpost durch. Freundlicherweise wurden im Blogbeitrag die ggplot2 Standardeinstellungen benutzt, was die Rekonstruktion relativ einfach macht. Die Links im Text verweisen auf die Originalgrafik, die eingebetteten Plots sind meine eigenen Rekonstruktionen.\nImportiere als erstes den Datensatz tagi_data_kanton.csv (dieser ist auch auf der Blog-Seite verfügbar)."
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#aufgabe-1",
    "href": "infovis/Infovis1_Uebung.html#aufgabe-1",
    "title": "Infovis 1: Übung",
    "section": "Aufgabe 1",
    "text": "Aufgabe 1\nRekonstrukturiere folgenden Plot aus Kovic (2014) mithilfe von ggplot und dem tagi_data_kanton.csv Datensatz:\nTipp:\n\nNutze ggplot(kanton, aes(auslanderanteil, ja_anteil)) um den ggplot zu initiieren. Füge danach ein einen Punkte Layer hinzu (geom_point())\nNutze coord_fixed() um die beiden Achsen in ein fixes Verhältnis zu setzen (1:1).\nOptional:\n\nSetze die Achsen Start- und Endwerte mittels scale_y_continuous bzw. scale_x_continuous.\nSetze analog Kovic (2014) die breaks (0.0, 0.1…0.7) manuell (innerhalb scale_*_continuous)\nNutze labs() für die Beschriftung der Achsen\n\n\n\n\n\n\n\nAbbildung 10.1: Generierter Plot"
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#aufgabe-2",
    "href": "infovis/Infovis1_Uebung.html#aufgabe-2",
    "title": "Infovis 1: Übung",
    "section": "Aufgabe 2",
    "text": "Aufgabe 2\nRekonstrukturiere folgenden Plot aus Kovic (2014) mithilfe von ggplot:\nTipp:\n\nNutze geom_smooth\n\n\n\n\n\n\nAbbildung 10.2: Generierter Plot"
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#aufgabe-3",
    "href": "infovis/Infovis1_Uebung.html#aufgabe-3",
    "title": "Infovis 1: Übung",
    "section": "Aufgabe 3",
    "text": "Aufgabe 3\nImportiere die Gemeindedaten tagi_data_gemeinden.csv.\nRekonstrukturiere folgenden Plot aus Kovic (2014) mithilfe von ggplot und dem tagi_data_gemeinden.csv Datensatz:\nTipp:\n\nNutze geom_point()\nNutze labs()\nNutze coord_fixed()\n\n\n\n\n\n\nAbbildung 10.3: Generierter Plot"
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#aufgabe-4",
    "href": "infovis/Infovis1_Uebung.html#aufgabe-4",
    "title": "Infovis 1: Übung",
    "section": "Aufgabe 4",
    "text": "Aufgabe 4\nRekonstrukturiere folgenden Plot aus Kovic (2014) mithilfe von ggplot und dem tagi_data_gemeinden.csv Datensatz:\nTipp:\n\nNutze geom_smooth\n\n\n\n\n\n\nAbbildung 10.4: Generierter Plot"
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#aufgabe-5",
    "href": "infovis/Infovis1_Uebung.html#aufgabe-5",
    "title": "Infovis 1: Übung",
    "section": "Aufgabe 5",
    "text": "Aufgabe 5\nRekonstrukturiere folgenden Plot aus Kovic (2014) mithilfe von ggplot und dem tagi_data_gemeinden.csv Datensatz:\nTipp:\n\nNutze facet_wrap um einen Plot pro Kanton darzustellen.\n\n\n\n\n\n\nAbbildung 10.5: Generierter Plot"
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#aufgabe-6",
    "href": "infovis/Infovis1_Uebung.html#aufgabe-6",
    "title": "Infovis 1: Übung",
    "section": "Aufgabe 6",
    "text": "Aufgabe 6\nRekonstrukturiere folgenden Plot aus Kovic (2014) mithilfe von ggplot und dem tagi_data_gemeinden.csv Datensatz:\nTipp:\n\nNutze geom_smooth\n\n\n\n\n\n\nAbbildung 10.6: Generierter Plot"
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#aufgabe-7",
    "href": "infovis/Infovis1_Uebung.html#aufgabe-7",
    "title": "Infovis 1: Übung",
    "section": "Aufgabe 7",
    "text": "Aufgabe 7\nRekonstrukturieren folgenden Plot aus Kovic (2014) mithilfe von ggplot und dem tagi_data_gemeinden.csv Datensatz:\nTipp:\n\nNutze facet_wrap\n\n\n\n\n\n\nAbbildung 10.7: Generierter Plot"
  },
  {
    "objectID": "infovis/Infovis1_Uebung.html#aufgabe-8",
    "href": "infovis/Infovis1_Uebung.html#aufgabe-8",
    "title": "Infovis 1: Übung",
    "section": "Aufgabe 8",
    "text": "Aufgabe 8\nRekonstrukturiere folgenden Plot aus Kovic (2014) mithilfe von ggplot und dem tagi_data_gemeinden.csv Datensatz:\nTipp:\n\nNutze geom_smooth\n\n\n\n\n\n\nAbbildung 10.8: Generierter Plot\n\n\n\n\n\n\n\n\nKovic, Marko. 2014. „Je weniger Ausländer, desto mehr Ja-Stimmen? Wirklich?“ Tagesanzeiger Datenblog. https://blog.tagesanzeiger.ch/datenblog/index.php/668/je-weniger-auslaender-desto-mehr-ja-stimmen-wirklich."
  },
  {
    "objectID": "infovis/Infovis2_Uebung_A.html",
    "href": "infovis/Infovis2_Uebung_A.html",
    "title": "Infovis 2: Übung A",
    "section": "",
    "text": "Für die heutige Übung brauchst du den Datensatz temperature_2005.csv. Dabei handelt es sich wieder um Teperaturwerte verschiedener Stationen, diesmal aus dem Jahr 2005. Das Datum ist so formatiert, dass R (isbesondere read_csv) es korrekt als datetime erkennen und als POSIXct einlesen sollte."
  },
  {
    "objectID": "infovis/Infovis2_Uebung_A.html#aufgabe-1",
    "href": "infovis/Infovis2_Uebung_A.html#aufgabe-1",
    "title": "Infovis 2: Übung A",
    "section": "Aufgabe 1",
    "text": "Aufgabe 1\nMache aus der wide table eine long table die wie folgt aussieht.\n\n\n\n\n\ntime\nstation\ntemperature\n\n\n\n\n2005-01-01\nALT\n1.3\n\n\n2005-01-01\nBUS\n1.5\n\n\n2005-01-01\nGVE\n1.1\n\n\n2005-01-01\nINT\n0.2\n\n\n2005-01-01\nOTL\n2.2\n\n\n2005-01-01\nLUG\n1.7\n\n\n\n\n\nLade anschliessend temperature_2005_metadata.csv herunter und verbinde die beiden Datensätze mit einem left_join via station (bzw. stn)."
  },
  {
    "objectID": "infovis/Infovis2_Uebung_A.html#aufgabe-2",
    "href": "infovis/Infovis2_Uebung_A.html#aufgabe-2",
    "title": "Infovis 2: Übung A",
    "section": "Aufgabe 2",
    "text": "Aufgabe 2\nErstelle ein Scatterplot (time vs. temperature) wobei die Punkte aufgrund ihrer Meereshöhe eingefärbt werden sollen. Tiefe Werte sollen dabei blau eingefärbt werden und hohe Werte rot (scale_color_gradient). Verkleinere die Punkte um übermässiges Überplotten der Punkten zu vermeiden (size =). Weiter sollen auf der x-Achse im Abstand von 3 Monaten der jeweilige Monat vermerkt sein (date_breaks bzw. date_labels von scale_x_datetime()).\n\n\n\n\n\nAbbildung 11.1: Generierter Plot"
  },
  {
    "objectID": "infovis/Infovis2_Uebung_A.html#aufgabe-3",
    "href": "infovis/Infovis2_Uebung_A.html#aufgabe-3",
    "title": "Infovis 2: Übung A",
    "section": "Aufgabe 3",
    "text": "Aufgabe 3\nErstelle eine Zusatzvariabel Date mit dem Datum der jeweiligen Messung ( mit as.Date). Nutze diese Spalte um die Tagesmitteltemperatur pro Station zu berechnen (mit summarise()).\nUm die Metadaten (Name, Meereshoehe, x, y) nicht zu verlieren kannst du den Join aus der ersten Übung wieder ausführen. Alternativ (schneller aber auch schwerer zu verstehen) kannst du diese Variabeln innerhalb deines group_by verwenden."
  },
  {
    "objectID": "infovis/Infovis2_Uebung_A.html#aufgabe-4",
    "href": "infovis/Infovis2_Uebung_A.html#aufgabe-4",
    "title": "Infovis 2: Übung A",
    "section": "Aufgabe 4",
    "text": "Aufgabe 4\nWiederhole nun den Plot aus der ersten Aufgabe mit den aggregierten Daten aus der vorherigen Aufgabe. Um die labels korrekt zu setzen musst du scale_x_datetime mit scale_x_date ersetzen.\n\n\n\n\n\nAbbildung 11.2: Generierter Plot"
  },
  {
    "objectID": "infovis/Infovis2_Uebung_A.html#aufgabe-5",
    "href": "infovis/Infovis2_Uebung_A.html#aufgabe-5",
    "title": "Infovis 2: Übung A",
    "section": "Aufgabe 5",
    "text": "Aufgabe 5\nFüge am obigen Plot eine schwarze, gestrichelte Trendlinie hinzu.\n\n\n\n\n\nAbbildung 11.3: Generierter Plot"
  },
  {
    "objectID": "infovis/Infovis2_Uebung_A.html#aufgabe-6",
    "href": "infovis/Infovis2_Uebung_A.html#aufgabe-6",
    "title": "Infovis 2: Übung A",
    "section": "Aufgabe 6",
    "text": "Aufgabe 6\nPositioniere die Legende oberhalb des Plots (nutze dazu theme() mit legend.position).\n\n\n\n\n\nAbbildung 11.4: Generierter Plot"
  },
  {
    "objectID": "infovis/Infovis2_Uebung_A.html#aufgabe-7-optional-fortgeschritten",
    "href": "infovis/Infovis2_Uebung_A.html#aufgabe-7-optional-fortgeschritten",
    "title": "Infovis 2: Übung A",
    "section": "Aufgabe 7 (optional, fortgeschritten)",
    "text": "Aufgabe 7 (optional, fortgeschritten)\nFüge den Temperaturwerten auf der y-Ache ein °C hinzu (siehe unten und studiere diesen Tipp zur Hilfe).\n\n\n\n\n\nAbbildung 11.5: Generierter Plot"
  },
  {
    "objectID": "infovis/Infovis2_Uebung_A.html#aufgabe-8",
    "href": "infovis/Infovis2_Uebung_A.html#aufgabe-8",
    "title": "Infovis 2: Übung A",
    "section": "Aufgabe 8",
    "text": "Aufgabe 8\nJetzt verlassen wir den Scatterplot und machen einen Boxplot mit den Temperaturdaten. Färbe die Boxplots wieder in Abhängigkeit der Meereshöhe ein.\n\nBeachte den Unterschied zwischen colour = und fill =\nBeachte den Unterschied zwischen facet_wrap() und facet_grid()\nfacet_grid() braucht übrigens noch einen Punkt (.) zur Tilde (~).\nBeachte den Unterschied zwischen “.~” und “~.” bei facet_grid()\nverschiebe nach Bedarf die Legende\n\n\n\n\n\n\nAbbildung 11.6: Generierter Plot"
  },
  {
    "objectID": "infovis/Infovis2_Uebung_A.html#aufgabe-9",
    "href": "infovis/Infovis2_Uebung_A.html#aufgabe-9",
    "title": "Infovis 2: Übung A",
    "section": "Aufgabe 9",
    "text": "Aufgabe 9\nAls letzter wichtiger Plottyp noch zwei Übungen zum Histogramm. Erstelle ein Histogramm geom_histogram() mit den Temperaturwerten. Teile dazu die Stationen in verschiedene Höhenlagen ein (Tieflage [< 400 m], Mittellage [400 - 600 m] und Hochlage [> 600 m]). Vergleiche die Verteilung der Temperaturwerte in den verschiedenen Lagen mit einem Histogramm.\nTip: Nutze cut um die Stationen in die drei Gruppen aufzuteilen\n\n\n\n\n\nAbbildung 11.7: Generierter Plot"
  },
  {
    "objectID": "infovis/Infovis2_Uebung_B.html",
    "href": "infovis/Infovis2_Uebung_B.html",
    "title": "Infovis 2: Übung B",
    "section": "",
    "text": "In dieser Übung bauen wir einige etwas unübliche Plots aus der Vorlesung nach. Dafür verwenden wir Datensätze, die in R bereits integriert sind. Eine Liste dieser Datensätze findet man hier oder mit der Hilfe ?datasets.\nDazu verwenden wir nach wie vor ggplot2, aber mit einigen Tricks."
  },
  {
    "objectID": "infovis/Infovis2_Uebung_B.html#aufgabe-1-parallel-coordinate-plots",
    "href": "infovis/Infovis2_Uebung_B.html#aufgabe-1-parallel-coordinate-plots",
    "title": "Infovis 2: Übung B",
    "section": "Aufgabe 1: Parallel coordinate plots",
    "text": "Aufgabe 1: Parallel coordinate plots\nErstelle einen parallel coordinate plot. Dafür eignet sich der integrierte Datensatz mtcars. Extrahiere die Fahrzeugnamen mit rownames_to_column.\nZudem müssen die Werte jeweiles auf eine gemeinsame Skala normalisiert werden. Hierfür kannst du die Funktion scales::rescale verwenden.\n\nmtcars2 <- mtcars |>\n  tibble::rownames_to_column(\"car\") |>\n  pivot_longer(-car)\n\nmtcars2 <- mtcars2 |>\n  group_by(name) |>\n  mutate(value_scaled = scales::rescale(value))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\nMazda RX4\n21.0\n6\n160\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\nMazda RX4 Wag\n21.0\n6\n160\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\nDatsun 710\n22.8\n4\n108\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\nHornet 4 Drive\n21.4\n6\n258\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\nHornet Sportabout\n18.7\n8\n360\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n\n\nValiant\n18.1\n6\n225\n105\n2.76\n3.460\n20.22\n1\n0\n3\n1\n\n\n\n\n\nSo sieht der fertige Plot aus:\n\nmtcars2 <- mtcars2 |>\n  group_by(car) |>\n  mutate(gear = value[name == \"gear\"])\n\nggplot(mtcars2, aes(name, value_scaled, group = car, color = factor(gear))) +\n  geom_point() +\n  geom_line() +\n  theme_minimal() +\n  theme(legend.position = \"none\", axis.title.y = element_blank())\n\n\n\n\nAbbildung 12.1: Generierter Plot"
  },
  {
    "objectID": "infovis/Infovis2_Uebung_B.html#aufgabe-2-polar-plot-mit-biber-daten",
    "href": "infovis/Infovis2_Uebung_B.html#aufgabe-2-polar-plot-mit-biber-daten",
    "title": "Infovis 2: Übung B",
    "section": "Aufgabe 2: Polar Plot mit Biber Daten",
    "text": "Aufgabe 2: Polar Plot mit Biber Daten\nPolar Plots eignen sich unter anderem für Daten, die zyklischer Natur sind, wie zum Beispiel zeitlich geprägte Daten (Tages-, Wochen-, oder Jahresrhythmen). Aus den Beispiels-Datensätzen habe ich zwei Datensätze gefunden, die zeitlich geprägt sind:\n\nbeaver1 und beaver2\nAirPassenger\n\nBeide Datensätze müssen noch etwas umgeformt werden, bevor wir sie für einen Radialplot verwenden können. In Aufgabe 2 verwenden wir die Biber-Datensätze, in der nächsten Aufgabe (3) die Passagier-Daten.\nWenn wir die Daten von beiden Bibern verwenden wollen, müssen wir diese noch zusammenfügen:\n\nbeaver1_new <- beaver1 |>\n  mutate(beaver = \"nr1\")\n\nbeaver2_new <- beaver2 |>\n  mutate(beaver = \"nr2\")\n\nbeaver_new <- rbind(beaver1_new,beaver2_new)\n\nZudem müssen wir die Zeitangabe noch anpassen: Gemäss der Datenbeschreibung handelt es sich bei der Zeitangabe um ein sehr programmier-unfreundliches Format. 3:30 wird als “0330” notiert. Wir müssen diese Zeitangabe, noch in ein Dezimalsystem umwandeln:\n\nbeaver_new <- beaver_new |>\n  mutate(\n    hour_dec = (time/100)%/%1,         # Ganze Stunden (mittels ganzzaliger Division)\n    min_dec = (time/100)%%1/0.6,       # Dezimalminuten (15 min wird zu 0.25, via Modulo)\n    hour_min_dec = hour_dec+min_dec    # Dezimal-Zeitangabe (03:30 wird zu 3.5)\n    ) \n\nDer Datensatz:\n\n\n\n\n\nday\ntime\ntemp\nactiv\nbeaver\nhour_dec\nmin_dec\nhour_min_dec\n\n\n\n\n346\n840\n36.33\n0\nnr1\n8\n0.6666667\n8.666667\n\n\n346\n850\n36.34\n0\nnr1\n8\n0.8333333\n8.833333\n\n\n346\n900\n36.35\n0\nnr1\n9\n0.0000000\n9.000000\n\n\n346\n910\n36.42\n0\nnr1\n9\n0.1666667\n9.166667\n\n\n346\n920\n36.55\n0\nnr1\n9\n0.3333333\n9.333333\n\n\n346\n930\n36.69\n0\nnr1\n9\n0.5000000\n9.500000\n\n\n\n\n\nSo sieht der fertige Plot aus:\n\n# Lösung Aufgabe 2\n\nbeaver_new |>\n  ggplot(aes(hour_min_dec, temp, color = beaver)) +\n  geom_point() +\n  scale_x_continuous(breaks = seq(0,23,2)) +\n  coord_polar() +\n  theme_minimal() +\n  theme(axis.title =  element_blank())\n\n\n\n\nAbbildung 12.2: Generierter Plot"
  },
  {
    "objectID": "infovis/Infovis2_Uebung_B.html#aufgabe-3-raster-visualisierung-mit-flugpassagieren",
    "href": "infovis/Infovis2_Uebung_B.html#aufgabe-3-raster-visualisierung-mit-flugpassagieren",
    "title": "Infovis 2: Übung B",
    "section": "Aufgabe 3: Raster Visualisierung mit Flugpassagieren",
    "text": "Aufgabe 3: Raster Visualisierung mit Flugpassagieren\nAnalog Aufgabe 2, dieses Mal mit dem Datensatz AirPassanger\nAirPassengers kommt in einem Format daher, das ich selbst noch gar nicht kannte. Es sieht zwar aus wie ein data.frame oder eine matrix, ist aber von der Klasse ts.\n\nAirPassengers\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n1949 112 118 132 129 121 135 148 148 136 119 104 118\n1950 115 126 141 135 125 149 170 170 158 133 114 140\n1951 145 150 178 163 172 178 199 199 184 162 146 166\n1952 171 180 193 181 183 218 230 242 209 191 172 194\n1953 196 196 236 235 229 243 264 272 237 211 180 201\n1954 204 188 235 227 234 264 302 293 259 229 203 229\n1955 242 233 267 269 270 315 364 347 312 274 237 278\n1956 284 277 317 313 318 374 413 405 355 306 271 306\n1957 315 301 356 348 355 422 465 467 404 347 305 336\n1958 340 318 362 348 363 435 491 505 404 359 310 337\n1959 360 342 406 396 420 472 548 559 463 407 362 405\n1960 417 391 419 461 472 535 622 606 508 461 390 432\n\nclass(AirPassengers)\n\n[1] \"ts\"\n\n\nDamit wir den Datensatz verwenden können, müssen wir ihn zuerst in eine matrix umwandeln. Wie das geht habe ich hier erfahren.\n\nAirPassengers2 <- tapply(AirPassengers, list(year = floor(time(AirPassengers)), month = month.abb[cycle(AirPassengers)]), c)\n\nAirPassengers2\n\n      month\nyear   Apr Aug Dec Feb Jan Jul Jun Mar May Nov Oct Sep\n  1949 129 148 118 118 112 148 135 132 121 104 119 136\n  1950 135 170 140 126 115 170 149 141 125 114 133 158\n  1951 163 199 166 150 145 199 178 178 172 146 162 184\n  1952 181 242 194 180 171 230 218 193 183 172 191 209\n  1953 235 272 201 196 196 264 243 236 229 180 211 237\n  1954 227 293 229 188 204 302 264 235 234 203 229 259\n  1955 269 347 278 233 242 364 315 267 270 237 274 312\n  1956 313 405 306 277 284 413 374 317 318 271 306 355\n  1957 348 467 336 301 315 465 422 356 355 305 347 404\n  1958 348 505 337 318 340 491 435 362 363 310 359 404\n  1959 396 559 405 342 360 548 472 406 420 362 407 463\n  1960 461 606 432 391 417 622 535 419 472 390 461 508\n\n\nAus der matrix muss noch ein Dataframe her, zudem müssen wir aus der breiten Tabelle eine lange Tabelle machen:\n\nAirPassengers3 <- AirPassengers2 |>\n  as.data.frame() |>\n  tibble::rownames_to_column(\"year\") |>\n  pivot_longer(-year, names_to = \"month\", values_to = \"n\") |>\n  mutate(\n    # ich nutze einen billigen Trick um ausgeschriebene Monate in Nummern umzuwandeln\n    month = factor(month, levels = month.abb,ordered = T),\n    month_numb = as.integer(month),\n    year = as.integer(year)\n  )\n\nSo sieht der fertige Plot aus:\n\n\n\n\n\nAbbildung 12.3: Generierter Plot"
  },
  {
    "objectID": "Stat1-4.html",
    "href": "Stat1-4.html",
    "title": "Statistik 1 - 4",
    "section": "",
    "text": "Statistik 2\nIn Statistik 2 lernen die Studierenden die Idee, die Voraussetzungen und die praktische Anwendung „einfacher“ linearer Modelle in R (sowie teilweise ihrer „nicht-parametrischen“ bzw. „robusten“ Äquivalente). Am Anfang steht die Varianzanalyse (ANOVA) als Verallgemeinerung des t-Tests, einschliesslich post-hoc-Tests und mehrfaktorieller ANOVA. Dann geht es um die Voraussetzungen parametrischer (und nicht-parametrischer) Tests und Optionen, wenn diese verletzt sind. Dann beschäftigen wir uns mit Korrelationen, die auf einen linearen Zusammenhang zwischen zwei metrischen Variablen testen, ohne Annahme einer Kausalität. Es folgen einfache lineare Regressionen, die im Prinzip das Gleiche bei klarer Kausalität leisten. Abschliessend besprechen wir, was die grosse Gruppe linearer Modelle (Befehl lm in R) auszeichnet.\n\n\n\n\nStatistik 3\nStatistik 3 fassen wir zu Beginn den generellen Ablauf inferenzstatistischer Analysen in einem Flussdiagramm zusammen. Dann wird die ANCOVA als eine Technik vorgestellt, die eine ANOVA mit einer linearen Regression verbindet. Danach geht es um komplexere Versionen linearer Regressionen. Hier betrachten wir polynomiale Regressionen, die z. B. einen Test auf unimodale Beziehungen erlaubt, indem man dieselbe Prädiktorvariable linear und quadriert einspeist. Multiple Regressionen versuchen dagegen, eine abhängige Variable durch zwei oder mehr verschieden Prädiktorvariablen zu erklären. Wir thematisieren verschiedene dabei auftretende Probleme und ihre Lösung, insbesondere den Umgang mit korrelierten Prädiktoren und das Aufspüren des besten unter mehreren möglichen statistischen Modellen. Hieran wird auch der informatian theoretician-Ansatz der Statistik und die multimodel inference eingeführt.\n\n\nStatistik 4\nHeute geht es hauptsächlich um generalized linear models (GLMs), die einige wesentliche Limitierungen von linearen Modellen überwinden. Indem sie Fehler- und Varianzstrukturen explizit modellieren, ist man nicht mehr an Normalverteilung der Residuen und Varianzhomogenität gebunden. Bei generalized linear regressions muss man sich zwischen verschiedenen Verteilungen und link-Strukturen entscheiden. Spezifisch werden wir uns die Poisson-Regressionen für Zähldaten und die logistische Regression für ja/nein-Daten anschauen. Danach folgt ein Einstieg in nicht-lineare Regressionen, die es erlauben, etwa Potenzgesetze oder Sättigungsfunktionen direkt zu modellieren. Zum Abschluss gibt es einen Ausblick auf Glättungsverfahren (LOWESS) und general additive models (GAMs).\n\n\n\n\n\n\n\n   \n     \n     \n       Sortieren nach\n       Voreinstellung\n         \n          Datum - Datum (aufsteigend)\n        \n         \n          Datum - Neueste\n        \n         \n          Titel\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitel\n\n\nDatum\n\n\nLesson\n\n\nThema\n\n\n\n\n\n\nStat1: Demo\n\n\n2022-10-31\n\n\nStat1\n\n\nGrundlagen der Statistik\n\n\n\n\nStat1: NOVANIMAL\n\n\n2022-10-31\n\n\nStat1\n\n\nGrundlagen der Statistik\n\n\n\n\nStat1: Übung\n\n\n2022-10-31\n\n\nStat1\n\n\nGrundlagen der Statistik\n\n\n\n\nStat2: Demo\n\n\n2022-11-01\n\n\nStat2\n\n\nEinführung in lineare Modelle\n\n\n\n\nStat2: Übung\n\n\n2022-11-01\n\n\nStat2\n\n\nEinführung in lineare Modelle\n\n\n\n\nStat2: Lösung Beispiel\n\n\n2022-11-01\n\n\nStat2\n\n\nEinführung in lineare Modelle\n\n\n\n\nStat3: Demo\n\n\n2022-11-07\n\n\nStat3\n\n\nLineare Modelle II\n\n\n\n\nStat3: Übung\n\n\n2022-11-07\n\n\nStat3\n\n\nLineare Modelle II\n\n\n\n\nStat4: Demo\n\n\n2022-11-08\n\n\nStat4\n\n\nKomplexere Regressionsmethoden\n\n\n\n\nStat4: Übung\n\n\n2022-11-08\n\n\nStat4\n\n\nKomplexere Regressionsmethoden\n\n\n\n\n\n\nKeine Treffer"
  },
  {
    "objectID": "stat1-4/Statistik1_Demo.html",
    "href": "stat1-4/Statistik1_Demo.html",
    "title": "Stat1: Demo",
    "section": "",
    "text": "Download dieses Demoscripts via “</>Code” (oben rechts)"
  },
  {
    "objectID": "stat1-4/Statistik1_Demo.html#daten-genereieren-und-anschauen",
    "href": "stat1-4/Statistik1_Demo.html#daten-genereieren-und-anschauen",
    "title": "Stat1: Demo",
    "section": "Daten genereieren und anschauen",
    "text": "Daten genereieren und anschauen\n\na <- c(20, 19, 25, 10, 8, 15, 13, 18, 11, 14)\nb <- c(12, 15, 16, 7, 8, 10, 12, 11, 13, 10)\nblume <- data.frame(a,b)\nblume\n\n    a  b\n1  20 12\n2  19 15\n3  25 16\n4  10  7\n5   8  8\n6  15 10\n7  13 12\n8  18 11\n9  11 13\n10 14 10\n\nsummary(blume)\n\n       a               b        \n Min.   : 8.00   Min.   : 7.00  \n 1st Qu.:11.50   1st Qu.:10.00  \n Median :14.50   Median :11.50  \n Mean   :15.30   Mean   :11.40  \n 3rd Qu.:18.75   3rd Qu.:12.75  \n Max.   :25.00   Max.   :16.00  \n\nboxplot(blume$a, blume$b)\nboxplot(blume)\nhist(blume$a)\nhist(blume$b)\n\n\n\n\nAbbildung 13.1: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 13.2: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 13.3: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 13.4: Generierter Plot"
  },
  {
    "objectID": "stat1-4/Statistik1_Demo.html#zweiseitiger-t-test",
    "href": "stat1-4/Statistik1_Demo.html#zweiseitiger-t-test",
    "title": "Stat1: Demo",
    "section": "Zweiseitiger t-Test",
    "text": "Zweiseitiger t-Test\n\nt.test(blume$a, blume$b) # Zweiseitig \"Test auf a ≠ b\" (default)\n\n\n    Welch Two Sample t-test\n\ndata:  blume$a and blume$b\nt = 2.0797, df = 13.907, p-value = 0.05654\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.1245926  7.9245926\nsample estimates:\nmean of x mean of y \n     15.3      11.4"
  },
  {
    "objectID": "stat1-4/Statistik1_Demo.html#einseitiger-t-test",
    "href": "stat1-4/Statistik1_Demo.html#einseitiger-t-test",
    "title": "Stat1: Demo",
    "section": "Einseitiger t-Test",
    "text": "Einseitiger t-Test\n\nt.test(blume$a, blume$b, alternative = \"greater\") # Einseitig \"Test auf a > b\"\n\n\n    Welch Two Sample t-test\n\ndata:  blume$a and blume$b\nt = 2.0797, df = 13.907, p-value = 0.02827\nalternative hypothesis: true difference in means is greater than 0\n95 percent confidence interval:\n 0.5954947       Inf\nsample estimates:\nmean of x mean of y \n     15.3      11.4 \n\nt.test(blume$a, blume$b, alternative = \"less\") # Einseitig \"Test auf a < b\"\n\n\n    Welch Two Sample t-test\n\ndata:  blume$a and blume$b\nt = 2.0797, df = 13.907, p-value = 0.9717\nalternative hypothesis: true difference in means is less than 0\n95 percent confidence interval:\n     -Inf 7.204505\nsample estimates:\nmean of x mean of y \n     15.3      11.4"
  },
  {
    "objectID": "stat1-4/Statistik1_Demo.html#klassischer-t-test-vs.-welch-test",
    "href": "stat1-4/Statistik1_Demo.html#klassischer-t-test-vs.-welch-test",
    "title": "Stat1: Demo",
    "section": "Klassischer t-Test vs. Welch Test",
    "text": "Klassischer t-Test vs. Welch Test\n\n# Varianzen gleich, klassischer t-Test\nt.test(blume$a, blume$b, var.equal = TRUE) \n\n\n    Two Sample t-test\n\ndata:  blume$a and blume$b\nt = 2.0797, df = 18, p-value = 0.05212\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.03981237  7.83981237\nsample estimates:\nmean of x mean of y \n     15.3      11.4 \n\n# Varianzen ungleich, Welch's t-Test, der auch default Einstellung ist (siehe Titelzeile des R outputs)\nt.test(blume$a, blume$b) # dasselbe wie var.equal = FALSE\n\n\n    Welch Two Sample t-test\n\ndata:  blume$a and blume$b\nt = 2.0797, df = 13.907, p-value = 0.05654\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.1245926  7.9245926\nsample estimates:\nmean of x mean of y \n     15.3      11.4"
  },
  {
    "objectID": "stat1-4/Statistik1_Demo.html#gepaarter-t-test",
    "href": "stat1-4/Statistik1_Demo.html#gepaarter-t-test",
    "title": "Stat1: Demo",
    "section": "Gepaarter t-Test",
    "text": "Gepaarter t-Test\n\n# Gepaarter t-Test: erster Wert von  a wird mit erstem Wert von\n# b gepaart, zweiter Wert von a mit zweitem von b ect.\nt.test(blume$a, blume$b, paired = T)\n\n\n    Paired t-test\n\ndata:  blume$a and blume$b\nt = 3.4821, df = 9, p-value = 0.006916\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 1.366339 6.433661\nsample estimates:\nmean difference \n            3.9 \n\nt.test(blume$a, blume$b, paired = T, alternative = \"greater\")\n\n\n    Paired t-test\n\ndata:  blume$a and blume$b\nt = 3.4821, df = 9, p-value = 0.003458\nalternative hypothesis: true mean difference is greater than 0\n95 percent confidence interval:\n 1.846877      Inf\nsample estimates:\nmean difference \n            3.9 \n\n\nDas gleiche mit einem “long table”\n\ncultivar <- c(rep(\"a\", 10), rep(\"b\", 10))\nsize <- c(a, b)\nblume.long <- data.frame(cultivar, size)\n\nrm(size) #Befehl rm entfernt die nicht mehr benötitgten Objekte aus dem Workspace\nrm(cultivar)\n\nDas Gleiche in einer Zeile\n\nblume.long <- data.frame(cultivar = c(rep(\"a\", 10), rep(\"b\", 10)), size = c(a, b))\nsummary(blume.long)             \n\n   cultivar              size      \n Length:20          Min.   : 7.00  \n Class :character   1st Qu.:10.00  \n Mode  :character   Median :12.50  \n                    Mean   :13.35  \n                    3rd Qu.:15.25  \n                    Max.   :25.00  \n\nhead(blume.long)\n\n  cultivar size\n1        a   20\n2        a   19\n3        a   25\n4        a   10\n5        a    8\n6        a   15\n\nboxplot(size~cultivar, data = blume.long)\n\nt.test(size~cultivar, blume.long, var.equal = T)\n\n\n    Two Sample t-test\n\ndata:  size by cultivar\nt = 2.0797, df = 18, p-value = 0.05212\nalternative hypothesis: true difference in means between group a and group b is not equal to 0\n95 percent confidence interval:\n -0.03981237  7.83981237\nsample estimates:\nmean in group a mean in group b \n           15.3            11.4 \n\nt.test(size~cultivar, blume.long, paired = T)\n\n\n    Paired t-test\n\ndata:  size by cultivar\nt = 3.4821, df = 9, p-value = 0.006916\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 1.366339 6.433661\nsample estimates:\nmean difference \n            3.9 \n\n\n\n\n\nAbbildung 13.5: Generierter Plot"
  },
  {
    "objectID": "stat1-4/Statistik1_Demo.html#base-r-vs.-ggplot2",
    "href": "stat1-4/Statistik1_Demo.html#base-r-vs.-ggplot2",
    "title": "Stat1: Demo",
    "section": "Base R vs. ggplot2",
    "text": "Base R vs. ggplot2\n\nlibrary(tidyverse)\nggplot(blume.long, aes(cultivar, size)) + geom_boxplot()\nggplot(blume.long, aes(cultivar, size)) + geom_boxplot() + theme_classic()\nggplot(blume.long, aes(cultivar, size)) + geom_boxplot(size = 1) + theme_classic()+\ntheme(axis.line = element_line(size = 1)) + theme(axis.title = element_text(size = 14))+\ntheme(axis.text = element_text(size = 14))\nggplot(blume.long, aes(cultivar, size)) + geom_boxplot(size=1) + theme_classic()+\n  theme(axis.line = element_line(size = 1), axis.ticks = element_line(size = 1), \n       axis.text = element_text(size = 20), axis.title = element_text(size = 20))\n\n\n\n\nAbbildung 13.6: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 13.7: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 13.8: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 13.9: Generierter Plot\n\n\n\n\nDefinieren von mytheme mit allen gewünschten Settings, das man zu Beginn einer Sitzung einmal laden und dann immer wieder ausführen kann (statt des langen Codes)\n\nmytheme <- theme_classic() + \n  theme(axis.line = element_line(color = \"black\", size=1), \n        axis.text = element_text(size = 20, color = \"black\"), \n        axis.title = element_text(size = 20, color = \"black\"), \n        axis.ticks = element_line(size = 1, color = \"black\"), \n        axis.ticks.length = unit(.5, \"cm\"))\n\n\nggplot(blume.long, aes(cultivar, size)) + \n  geom_boxplot(size = 1) +\n  mytheme\n\nt_test <- t.test(size~cultivar, blume.long)\n\nggplot(blume.long, aes(cultivar, size)) + \n  geom_boxplot(size = 1) + \n  mytheme +\n  annotate(\"text\", x = \"b\", y = 24, \n  label = paste0(\"italic(p) == \", round(t_test$p.value, 3)), parse = TRUE, size = 8)\n\nggplot (blume.long, aes(cultivar,size)) + \n  geom_boxplot(size = 1) + \n  mytheme +\n  labs(x=\"Cultivar\",y=\"Size (cm)\")\n\n\n\n\nAbbildung 13.10: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 13.11: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 13.12: Generierter Plot"
  },
  {
    "objectID": "stat1-4/Statistik1_Demo.html#binomialtest",
    "href": "stat1-4/Statistik1_Demo.html#binomialtest",
    "title": "Stat1: Demo",
    "section": "Binomialtest",
    "text": "Binomialtest\nIn Klammern übergibt man die Anzahl der Erfolge und die Stichprobengrösse\n\nbinom.test(84, 200) # Anzahl Frauen im Nationalrat (≙ 42.0 %; Stand 2019) \n\n\n    Exact binomial test\n\ndata:  84 and 200\nnumber of successes = 84, number of trials = 200, p-value = 0.02813\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.3507439 0.4916638\nsample estimates:\nprobability of success \n                  0.42 \n\nbinom.test(116, 200) # Anzahl Männer im Nationalrat (≙ 58.0 %; Stand 2019) \n\n\n    Exact binomial test\n\ndata:  116 and 200\nnumber of successes = 116, number of trials = 200, p-value = 0.02813\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.5083362 0.6492561\nsample estimates:\nprobability of success \n                  0.58 \n\nbinom.test(3, 7) # Anzahl Frauen im Bundesrat (≙ 42.9 %; Stand 2019)\n\n\n    Exact binomial test\n\ndata:  3 and 7\nnumber of successes = 3, number of trials = 7, p-value = 1\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.09898828 0.81594843\nsample estimates:\nprobability of success \n             0.4285714"
  },
  {
    "objectID": "stat1-4/Statistik1_Demo.html#chi-quadrat-test-fishers-test",
    "href": "stat1-4/Statistik1_Demo.html#chi-quadrat-test-fishers-test",
    "title": "Stat1: Demo",
    "section": "Chi-Quadrat-Test & Fishers Test",
    "text": "Chi-Quadrat-Test & Fishers Test\nErmitteln des kritischen Wertes\n\nqchisq(0.95, 1)\n\n[1] 3.841459"
  },
  {
    "objectID": "stat1-4/Statistik1_Demo.html#direkter-test-in-r-dazu-werte-als-matrix-nötig",
    "href": "stat1-4/Statistik1_Demo.html#direkter-test-in-r-dazu-werte-als-matrix-nötig",
    "title": "Stat1: Demo",
    "section": "Direkter Test in R (dazu Werte als Matrix nötig)",
    "text": "Direkter Test in R (dazu Werte als Matrix nötig)\n\ncount <- matrix(c(38, 14, 11, 51), nrow = 2)\ncount\n\n     [,1] [,2]\n[1,]   38   11\n[2,]   14   51\n\nchisq.test(count)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  count\nX-squared = 33.112, df = 1, p-value = 8.7e-09\n\nfisher.test(count)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  count\np-value = 2.099e-09\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n  4.746351 34.118920\nsample estimates:\nodds ratio \n  12.22697"
  },
  {
    "objectID": "stat1-4/Statistik1_Novanimal.html",
    "href": "stat1-4/Statistik1_Novanimal.html",
    "title": "Stat1: NOVANIMAL",
    "section": "",
    "text": "Dazu wurde u.a. ein Experiment in zwei Hochschulmensen durchgeführt. Forschungsleitend war die Frage, wie die Gäste dazu bewogen werden können, häufiger vegetarische oder vegane Gerichte zu wählen. Konkret wurde untersucht, wie die Gäste auf ein verändertes Menü-Angebot mit einem höheren Anteil an vegetarischen und veganen Gerichten reagieren. Das Experiment fand während 12 Wochen statt und bestand aus zwei Mensazyklen à 6 Wochen. Über den gesamten Untersuchungszeitraum werden insgesamt 90 verschiedene Gerichte angeboten. In den 6 Referenz- bzw. Basiswochen wurden zwei fleisch- oder fischhaltige Menüs und ein vegetarisches Menü angeboten. In den 6 Interventionswochen wurde das Verhältnis umgekehrt und es wurden ein veganes, ein vegetarisches und ein fleisch- oder fischhaltiges Gericht angeboten. Basis- und Interventionsangebote wechselten wöchentlich ab. Während der gesamten 12 Wochen konnten die Gäste jeweils auf ein Buffet ausweichen und ihre Mahlzeit aus warmen und kalten Komponenten selber zusammenstellen. Die Gerichte wurden über drei vorgegebene Menülinien (F, K, W) randomisiert angeboten.\n\n\n\nDie Abbildung zeigt das Versuchsdesign der ersten 6 Experimentwochen (Kalenderwoche 40 bis 45).\n\n\nMehr Informationen über das Forschungsprojekt NOVANIMAL findet ihr auf der Webpage."
  },
  {
    "objectID": "stat1-4/Statistik1_Uebung.html",
    "href": "stat1-4/Statistik1_Uebung.html",
    "title": "Stat1: Übung",
    "section": "",
    "text": "Führt einen Assoziationstest zweier kategorialer Variablen (mit je zwei Ausprägungen) mit Chi-Quadrat und Fishers exaktem Test durch. Dazu erhebt ihr selbst die Daten (wozu ihr euch auch in Teams zusammenschliessen könnt). Ihr könnt z.B. eine Datenerhebung unter Mitstudierenden durchführen (etwa Nutzung Mac/Windows vs. männlich/weiblich). Bitte formuliert vor der Datenerhebung eine Hypothese, d.h. eine Erwartungshaltung, ob und welche Assoziation vorliegt und wenn ja warum. Beachtet, dass ihr für die Form des Assoziationstests genau zwei binäre Variablen benötigt. Wenn ihr also kategoriale Variablen mit mehr als zwei Ausprägungen habt, so könnt ihr entweder Ausprägungen sinnvoll zusammenfassen oder seltene Ausprägungen im Test unberücksichtigt lassen."
  },
  {
    "objectID": "stat1-4/Statistik1_Uebung.html#aufgabe-1.2-t-test",
    "href": "stat1-4/Statistik1_Uebung.html#aufgabe-1.2-t-test",
    "title": "Stat1: Übung",
    "section": "Aufgabe 1.2: t-Test",
    "text": "Aufgabe 1.2: t-Test\nT-Test mit Datensatz_novanimal_Uebung_Statistik1.2.csv\nWerden in den Basis- und Interventionswochen unterschiedlich viele Gerichte verkauft?\n\nDefiniere die Null- (\\(H_0\\)) und die Alternativhypothese (\\(H_1\\)).\nFühre einen t-Test durch.\nWelche Form von t-Test musst Du anwenden: einseitig/zweiseitig resp. gepaart/ungepaart?\nWie gut sind die Voraussetzungen für einen t-Test erfüllt (z.B. Normalverteilung der Residuen und Varianzhomogenität)?\nStelle deine Ergebnisse angemessen dar, d.h. Text mit Abbildung und/oder Tabelle"
  },
  {
    "objectID": "stat1-4/Statistik2_Demo.html",
    "href": "stat1-4/Statistik2_Demo.html",
    "title": "Stat2: Demo",
    "section": "",
    "text": "Demoscript als Download"
  },
  {
    "objectID": "stat1-4/Statistik2_Demo.html#t-test-als-anova",
    "href": "stat1-4/Statistik2_Demo.html#t-test-als-anova",
    "title": "Stat2: Demo",
    "section": "t-test als ANOVA",
    "text": "t-test als ANOVA\n\na <- c(20, 19, 25, 10, 8, 15, 13 ,18, 11, 14)\nb <- c(12, 15, 16, 7, 8, 10, 12, 11, 13, 10)\n\nblume <- data.frame(cultivar = c(rep(\"a\", 10), rep(\"b\" , 10)), size = c(a, b))\n\npar(mfrow=c(1,1))\nboxplot(size~cultivar, xlab = \"Sorte\", ylab = \"Bluetengroesse [cm]\", data = blume)\n\nt.test(size~cultivar, blume, var.equal = T)\n\n\n    Two Sample t-test\n\ndata:  size by cultivar\nt = 2.0797, df = 18, p-value = 0.05212\nalternative hypothesis: true difference in means between group a and group b is not equal to 0\n95 percent confidence interval:\n -0.03981237  7.83981237\nsample estimates:\nmean in group a mean in group b \n           15.3            11.4 \n\naov(size~cultivar, data = blume)\n\nCall:\n   aov(formula = size ~ cultivar, data = blume)\n\nTerms:\n                cultivar Residuals\nSum of Squares     76.05    316.50\nDeg. of Freedom        1        18\n\nResidual standard error: 4.193249\nEstimated effects may be unbalanced\n\nsummary(aov(size~cultivar, data = blume))\n\n            Df Sum Sq Mean Sq F value Pr(>F)  \ncultivar     1   76.0   76.05   4.325 0.0521 .\nResiduals   18  316.5   17.58                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary.lm(aov(size~cultivar, data = blume))\n\n\nCall:\naov(formula = size ~ cultivar, data = blume)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-7.300 -2.575 -0.350  2.925  9.700 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   15.300      1.326   11.54 9.47e-10 ***\ncultivarb     -3.900      1.875   -2.08   0.0521 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.193 on 18 degrees of freedom\nMultiple R-squared:  0.1937,    Adjusted R-squared:  0.1489 \nF-statistic: 4.325 on 1 and 18 DF,  p-value: 0.05212\n\n\n\n\n\nAbbildung 16.1: Generierter Plot"
  },
  {
    "objectID": "stat1-4/Statistik2_Demo.html#echte-anova",
    "href": "stat1-4/Statistik2_Demo.html#echte-anova",
    "title": "Stat2: Demo",
    "section": "Echte ANOVA",
    "text": "Echte ANOVA\n\nc <- c(30, 19, 31, 23, 18, 25, 26, 24, 17, 20)\n\nblume2 <- data.frame(cultivar = c(rep(\"a\", 10), rep(\"b\", 10), rep(\"c\", 10)), size = c(a, b, c))\nblume2$cultivar <- as.factor(blume2$cultivar)\n\nsummary(blume2)             \n\n cultivar      size      \n a:10     Min.   : 7.00  \n b:10     1st Qu.:11.25  \n c:10     Median :15.50  \n          Mean   :16.67  \n          3rd Qu.:20.00  \n          Max.   :31.00  \n\nhead(blume2)\n\n  cultivar size\n1        a   20\n2        a   19\n3        a   25\n4        a   10\n5        a    8\n6        a   15\n\npar(mfrow=c(1,1))\nboxplot(size~cultivar, xlab = \"Sorte\", ylab = \"Blütengrösse [cm]\", data = blume2)\n\naov(size~cultivar, data = blume2)\n\nCall:\n   aov(formula = size ~ cultivar, data = blume2)\n\nTerms:\n                cultivar Residuals\nSum of Squares  736.0667  528.6000\nDeg. of Freedom        2        27\n\nResidual standard error: 4.424678\nEstimated effects may be unbalanced\n\nsummary(aov(size~cultivar, data = blume2))\n\n            Df Sum Sq Mean Sq F value   Pr(>F)    \ncultivar     2  736.1   368.0    18.8 7.68e-06 ***\nResiduals   27  528.6    19.6                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary.lm(aov(size~cultivar, data=blume2))\n\n\nCall:\naov(formula = size ~ cultivar, data = blume2)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-7.300 -3.375 -0.300  2.700  9.700 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   15.300      1.399  10.935 2.02e-11 ***\ncultivarb     -3.900      1.979  -1.971 0.059065 .  \ncultivarc      8.000      1.979   4.043 0.000395 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.425 on 27 degrees of freedom\nMultiple R-squared:  0.582, Adjusted R-squared:  0.5511 \nF-statistic:  18.8 on 2 and 27 DF,  p-value: 7.683e-06\n\naov.1 <- aov(size~cultivar, data = blume2)\nsummary(aov.1)\n\n            Df Sum Sq Mean Sq F value   Pr(>F)    \ncultivar     2  736.1   368.0    18.8 7.68e-06 ***\nResiduals   27  528.6    19.6                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary.lm(aov.1)\n\n\nCall:\naov(formula = size ~ cultivar, data = blume2)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-7.300 -3.375 -0.300  2.700  9.700 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   15.300      1.399  10.935 2.02e-11 ***\ncultivarb     -3.900      1.979  -1.971 0.059065 .  \ncultivarc      8.000      1.979   4.043 0.000395 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.425 on 27 degrees of freedom\nMultiple R-squared:  0.582, Adjusted R-squared:  0.5511 \nF-statistic:  18.8 on 2 and 27 DF,  p-value: 7.683e-06\n\n#Berechnung Mittelwerte usw. zur Charakterisierung der Gruppen\naggregate(size~cultivar, blume2, function(x) c(Mean = mean(x), SD = sd(x), Min = min(x), Max = max(x)))\n\n  cultivar size.Mean   size.SD  size.Min  size.Max\n1        a 15.300000  5.207900  8.000000 25.000000\n2        b 11.400000  2.836273  7.000000 16.000000\n3        c 23.300000  4.854551 17.000000 31.000000\n\nlm.1 <- lm(size~cultivar, data = blume2)\nsummary(lm.1)\n\n\nCall:\nlm(formula = size ~ cultivar, data = blume2)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-7.300 -3.375 -0.300  2.700  9.700 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   15.300      1.399  10.935 2.02e-11 ***\ncultivarb     -3.900      1.979  -1.971 0.059065 .  \ncultivarc      8.000      1.979   4.043 0.000395 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.425 on 27 degrees of freedom\nMultiple R-squared:  0.582, Adjusted R-squared:  0.5511 \nF-statistic:  18.8 on 2 and 27 DF,  p-value: 7.683e-06\n\n\n\n\n\nAbbildung 16.2: Generierter Plot"
  },
  {
    "objectID": "stat1-4/Statistik2_Demo.html#tukeys-posthoc-test",
    "href": "stat1-4/Statistik2_Demo.html#tukeys-posthoc-test",
    "title": "Stat2: Demo",
    "section": "Tukeys Posthoc-Test",
    "text": "Tukeys Posthoc-Test\n\n#Load library\nif(!require(agricolae)){install.packages(\"agricolae\")}\nlibrary(agricolae)\n\nHSD.test(aov.1, \"cultivar\", group = FALSE, console = T)\n\n\nStudy: aov.1 ~ \"cultivar\"\n\nHSD Test for size \n\nMean Square Error:  19.57778 \n\ncultivar,  means\n\n  size      std  r Min Max\na 15.3 5.207900 10   8  25\nb 11.4 2.836273 10   7  16\nc 23.3 4.854551 10  17  31\n\nAlpha: 0.05 ; DF Error: 27 \nCritical Value of Studentized Range: 3.506426 \n\nComparison between treatments means\n\n      difference pvalue signif.        LCL       UCL\na - b        3.9 0.1388          -1.006213  8.806213\na - c       -8.0 0.0011      ** -12.906213 -3.093787\nb - c      -11.9 0.0000     *** -16.806213 -6.993787"
  },
  {
    "objectID": "stat1-4/Statistik2_Demo.html#beispiel-posthoc-labels-in-plot",
    "href": "stat1-4/Statistik2_Demo.html#beispiel-posthoc-labels-in-plot",
    "title": "Stat2: Demo",
    "section": "Beispiel Posthoc-Labels in Plot",
    "text": "Beispiel Posthoc-Labels in Plot\n\naov.2 <- aov(Sepal.Width ~ Species, data = iris)\nHSD.test(aov.2, \"Species\", console = T)\n\n\nStudy: aov.2 ~ \"Species\"\n\nHSD Test for Sepal.Width \n\nMean Square Error:  0.1153878 \n\nSpecies,  means\n\n           Sepal.Width       std  r Min Max\nsetosa           3.428 0.3790644 50 2.3 4.4\nversicolor       2.770 0.3137983 50 2.0 3.4\nvirginica        2.974 0.3224966 50 2.2 3.8\n\nAlpha: 0.05 ; DF Error: 147 \nCritical Value of Studentized Range: 3.348424 \n\nMinimun Significant Difference: 0.1608553 \n\nTreatments with the same letter are not significantly different.\n\n           Sepal.Width groups\nsetosa           3.428      a\nvirginica        2.974      b\nversicolor       2.770      c\n\nboxplot(Sepal.Width ~ Species, data = iris)\nboxplot(Sepal.Width ~ Species, ylim = c(2, 5), data = iris)\ntext(1, 4.8, \"a\")\ntext(2, 4.8, \"c\")\ntext(3, 4.8, \"b\")\n\n\n\n\nAbbildung 16.3: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 16.4: Generierter Plot\n\n\n\n\n\n#Load library\nlibrary(tidyverse)\n\nggplot(iris, aes(Species, Sepal.Width)) + geom_boxplot(size = 1) +\n  annotate(\"text\", y = 5, x = 1:3, label = c(\"a\", \"c\", \"b\"))\n\n\n\n\nAbbildung 16.5: Generierter Plot"
  },
  {
    "objectID": "stat1-4/Statistik2_Demo.html#klassische-tests-der-modellannahmen-nicht-empfohlen",
    "href": "stat1-4/Statistik2_Demo.html#klassische-tests-der-modellannahmen-nicht-empfohlen",
    "title": "Stat2: Demo",
    "section": "Klassische Tests der Modellannahmen (NICHT EMPFOHLEN!!!)",
    "text": "Klassische Tests der Modellannahmen (NICHT EMPFOHLEN!!!)\n\nshapiro.test(blume2$size[blume2$cultivar == \"a\"])\n\n\n    Shapiro-Wilk normality test\n\ndata:  blume2$size[blume2$cultivar == \"a\"]\nW = 0.97304, p-value = 0.9175\n\nvar.test(blume2$size[blume2$cultivar == \"a\"], blume2$size[blume2$cultivar == \"b\"])\n\n\n    F test to compare two variances\n\ndata:  blume2$size[blume2$cultivar == \"a\"] and blume2$size[blume2$cultivar == \"b\"]\nF = 3.3715, num df = 9, denom df = 9, p-value = 0.08467\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n  0.8374446 13.5738284\nsample estimates:\nratio of variances \n          3.371547 \n\n\n\n#Load library\nif(!require(car)){install.packages(\"car\")}\nlibrary(car)\nleveneTest(blume2$size[blume2$cultivar == \"a\"], blume2$size[blume2$cultivar == \"b\"], center=mean)\n\nLevene's Test for Homogeneity of Variance (center = mean)\n      Df    F value    Pr(>F)    \ngroup  7 2.2598e+30 < 2.2e-16 ***\n       2                         \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nwilcox.test(blume2$size[blume2$cultivar == \"a\"], blume2$size[blume2$cultivar == \"b\"])\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  blume2$size[blume2$cultivar == \"a\"] and blume2$size[blume2$cultivar == \"b\"]\nW = 73, p-value = 0.08789\nalternative hypothesis: true location shift is not equal to 0"
  },
  {
    "objectID": "stat1-4/Statistik2_Demo.html#nicht-parametrische-alternativen-wenn-modellannahmen-der-anvoa-massiv-verletzt-sind",
    "href": "stat1-4/Statistik2_Demo.html#nicht-parametrische-alternativen-wenn-modellannahmen-der-anvoa-massiv-verletzt-sind",
    "title": "Stat2: Demo",
    "section": "Nicht-parametrische Alternativen, wenn Modellannahmen der ANVOA massiv verletzt sind",
    "text": "Nicht-parametrische Alternativen, wenn Modellannahmen der ANVOA massiv verletzt sind"
  },
  {
    "objectID": "stat1-4/Statistik2_Demo.html#zum-vergleich-normale-anova-noch-mal",
    "href": "stat1-4/Statistik2_Demo.html#zum-vergleich-normale-anova-noch-mal",
    "title": "Stat2: Demo",
    "section": "Zum Vergleich normale ANOVA noch mal",
    "text": "Zum Vergleich normale ANOVA noch mal\n\nsummary(aov(size~cultivar, data = blume2))\n\n            Df Sum Sq Mean Sq F value   Pr(>F)    \ncultivar     2  736.1   368.0    18.8 7.68e-06 ***\nResiduals   27  528.6    19.6                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "stat1-4/Statistik2_Demo.html#bei-starken-abweichungen-von-der-normalverteilung-aber-ähnlichen-varianzen",
    "href": "stat1-4/Statistik2_Demo.html#bei-starken-abweichungen-von-der-normalverteilung-aber-ähnlichen-varianzen",
    "title": "Stat2: Demo",
    "section": "Bei starken Abweichungen von der Normalverteilung, aber ähnlichen Varianzen",
    "text": "Bei starken Abweichungen von der Normalverteilung, aber ähnlichen Varianzen"
  },
  {
    "objectID": "stat1-4/Statistik2_Demo.html#kruskal-wallis-test",
    "href": "stat1-4/Statistik2_Demo.html#kruskal-wallis-test",
    "title": "Stat2: Demo",
    "section": "Kruskal-Wallis-Test",
    "text": "Kruskal-Wallis-Test\n\nkruskal.test(size~cultivar, data = blume2)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  size by cultivar\nKruskal-Wallis chi-squared = 16.686, df = 2, p-value = 0.0002381\n\n\n\n#Load library\nif(!require(FSA)){install.packages(\"FSA\")} \nlibrary(FSA)\n\n#korrigierte p-Werte nach Bejamini-Hochberg\ndunnTest(size~cultivar, method = \"bh\", data = blume2) \n\n  Comparison         Z      P.unadj        P.adj\n1      a - b  1.526210 1.269575e-01 0.1269575490\n2      a - c -2.518247 1.179407e-02 0.0176911039\n3      b - c -4.044457 5.244459e-05 0.0001573338"
  },
  {
    "objectID": "stat1-4/Statistik2_Demo.html#bei-erheblicher-heteroskedastizität-aber-relative-normalsymmetrisch-verteilten-residuen",
    "href": "stat1-4/Statistik2_Demo.html#bei-erheblicher-heteroskedastizität-aber-relative-normalsymmetrisch-verteilten-residuen",
    "title": "Stat2: Demo",
    "section": "Bei erheblicher Heteroskedastizität, aber relative normal/symmetrisch verteilten Residuen",
    "text": "Bei erheblicher Heteroskedastizität, aber relative normal/symmetrisch verteilten Residuen"
  },
  {
    "objectID": "stat1-4/Statistik2_Demo.html#welch-test",
    "href": "stat1-4/Statistik2_Demo.html#welch-test",
    "title": "Stat2: Demo",
    "section": "Welch-Test",
    "text": "Welch-Test\n\noneway.test(size~cultivar, var.equal = F, data = blume2)\n\n\n    One-way analysis of means (not assuming equal variances)\n\ndata:  size and cultivar\nF = 21.642, num df = 2.000, denom df = 16.564, p-value = 2.397e-05"
  },
  {
    "objectID": "stat1-4/Statistik2_Demo.html#faktorielle-anova",
    "href": "stat1-4/Statistik2_Demo.html#faktorielle-anova",
    "title": "Stat2: Demo",
    "section": "2-faktorielle ANOVA",
    "text": "2-faktorielle ANOVA\n\nd <- c(10, 12, 11, 13, 10, 25, 12, 30, 26, 13)\ne <- c(15, 13, 18, 11, 14, 25, 39, 38, 28, 24)\nf <- c(10, 12, 11, 13, 10, 9, 2, 4, 7, 13)\n\nblume3 <- data.frame(cultivar=c(rep(\"a\", 20), rep(\"b\", 20), rep(\"c\", 20)),\n                   house = c(rep(c(rep(\"yes\", 10), rep(\"no\", 10)), 3)),\n                  size = c(a, b, c, d, e, f))\nblume3\n\n   cultivar house size\n1         a   yes   20\n2         a   yes   19\n3         a   yes   25\n4         a   yes   10\n5         a   yes    8\n6         a   yes   15\n7         a   yes   13\n8         a   yes   18\n9         a   yes   11\n10        a   yes   14\n11        a    no   12\n12        a    no   15\n13        a    no   16\n14        a    no    7\n15        a    no    8\n16        a    no   10\n17        a    no   12\n18        a    no   11\n19        a    no   13\n20        a    no   10\n21        b   yes   30\n22        b   yes   19\n23        b   yes   31\n24        b   yes   23\n25        b   yes   18\n26        b   yes   25\n27        b   yes   26\n28        b   yes   24\n29        b   yes   17\n30        b   yes   20\n31        b    no   10\n32        b    no   12\n33        b    no   11\n34        b    no   13\n35        b    no   10\n36        b    no   25\n37        b    no   12\n38        b    no   30\n39        b    no   26\n40        b    no   13\n41        c   yes   15\n42        c   yes   13\n43        c   yes   18\n44        c   yes   11\n45        c   yes   14\n46        c   yes   25\n47        c   yes   39\n48        c   yes   38\n49        c   yes   28\n50        c   yes   24\n51        c    no   10\n52        c    no   12\n53        c    no   11\n54        c    no   13\n55        c    no   10\n56        c    no    9\n57        c    no    2\n58        c    no    4\n59        c    no    7\n60        c    no   13\n\n\n\nboxplot(size~cultivar + house, data = blume3)\n\nsummary(aov(size~cultivar + house, data = blume3))\n\n            Df Sum Sq Mean Sq F value   Pr(>F)    \ncultivar     2  417.1   208.5   5.005     0.01 *  \nhouse        1  992.3   992.3  23.815 9.19e-06 ***\nResiduals   56 2333.2    41.7                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(aov(size~cultivar + house + cultivar:house, data = blume3)) \n\n               Df Sum Sq Mean Sq F value   Pr(>F)    \ncultivar        2  417.1   208.5   5.364   0.0075 ** \nhouse           1  992.3   992.3  25.520 5.33e-06 ***\ncultivar:house  2  233.6   116.8   3.004   0.0579 .  \nResiduals      54 2099.6    38.9                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#Kurzschreibweise: \"*\" bedeutet, dass Interaktion zwischen cultivar und house eingeschlossen wird\nsummary(aov(size~cultivar * house, data = blume3)) \n\n               Df Sum Sq Mean Sq F value   Pr(>F)    \ncultivar        2  417.1   208.5   5.364   0.0075 ** \nhouse           1  992.3   992.3  25.520 5.33e-06 ***\ncultivar:house  2  233.6   116.8   3.004   0.0579 .  \nResiduals      54 2099.6    38.9                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary.lm(aov(size~cultivar+house, data = blume3))\n\n\nCall:\naov(formula = size ~ cultivar + house, data = blume3)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-9.733 -4.696 -1.050  2.717 19.133 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    9.283      1.667   5.570 7.52e-07 ***\ncultivarb      6.400      2.041   3.135  0.00273 ** \ncultivarc      2.450      2.041   1.200  0.23509    \nhouseyes       8.133      1.667   4.880 9.19e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.455 on 56 degrees of freedom\nMultiple R-squared:  0.3766,    Adjusted R-squared:  0.3432 \nF-statistic: 11.28 on 3 and 56 DF,  p-value: 6.848e-06\n\ninteraction.plot(blume3$cultivar, blume3$house, blume3$size)\ninteraction.plot(blume3$house, blume3$cultivar, blume3$size)\n\nanova(lm(blume3$size~blume3$cultivar*blume3$house), lm(blume3$size~blume3$cultivar+blume3$house))\n\nAnalysis of Variance Table\n\nModel 1: blume3$size ~ blume3$cultivar * blume3$house\nModel 2: blume3$size ~ blume3$cultivar + blume3$house\n  Res.Df    RSS Df Sum of Sq      F  Pr(>F)  \n1     54 2099.6                              \n2     56 2333.2 -2   -233.63 3.0044 0.05792 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(lm(blume3$size~blume3$house), lm(blume3$size~blume3$cultivar * blume3$house))\n\nAnalysis of Variance Table\n\nModel 1: blume3$size ~ blume3$house\nModel 2: blume3$size ~ blume3$cultivar * blume3$house\n  Res.Df    RSS Df Sum of Sq      F   Pr(>F)   \n1     58 2750.3                                \n2     54 2099.6  4    650.73 4.1841 0.005045 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nAbbildung 16.6: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 16.7: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 16.8: Generierter Plot"
  },
  {
    "objectID": "stat1-4/Statistik2_Demo.html#korrelationen",
    "href": "stat1-4/Statistik2_Demo.html#korrelationen",
    "title": "Stat2: Demo",
    "section": "Korrelationen",
    "text": "Korrelationen\n\n#Load library\nlibrary(car)\n\nblume <- data.frame(a, b)\nscatterplot(a~b, blume)\n\ncor.test(a, b, method = \"pearson\", data = blume)\n\n\n    Pearson's product-moment correlation\n\ndata:  a and b\nt = 3.3678, df = 8, p-value = 0.009818\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.2628864 0.9414665\nsample estimates:\n      cor \n0.7657634 \n\ncor.test(a, b, method = \"spearman\", data = blume)\n\n\n    Spearman's rank correlation rho\n\ndata:  a and b\nS = 53.321, p-value = 0.03159\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.6768419 \n\ncor.test(a, b, method = \"kendall\", data = blume) \n\n\n    Kendall's rank correlation tau\n\ndata:  a and b\nz = 2.0738, p-value = 0.03809\nalternative hypothesis: true tau is not equal to 0\nsample estimates:\n      tau \n0.5228623 \n\n#Jetzt als Regression\nlm.2 <- lm(b~a)\nanova(lm.2)\n\nAnalysis of Variance Table\n\nResponse: b\n          Df Sum Sq Mean Sq F value   Pr(>F)   \na          1 42.455  42.455  11.342 0.009818 **\nResiduals  8 29.945   3.743                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(lm.2)\n\n\nCall:\nlm(formula = b ~ a)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.1897 -1.3388 -0.6067  1.3081  3.3933 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)   5.0193     1.9910   2.521  0.03575 * \na             0.4170     0.1238   3.368  0.00982 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.935 on 8 degrees of freedom\nMultiple R-squared:  0.5864,    Adjusted R-squared:  0.5347 \nF-statistic: 11.34 on 1 and 8 DF,  p-value: 0.009818\n\n#Model II-Regression\n\n\n\n\nAbbildung 16.9: Generierter Plot\n\n\n\n\n\n#Load library\nif(!require(lmodel2)){install.packages(\"lmodel2\")} \nlibrary(lmodel2)\n\nlmodel2(b~a)\n\n\nModel II regression\n\nCall: lmodel2(formula = b ~ a)\n\nn = 10   r = 0.7657634   r-square = 0.5863936 \nParametric P-values:   2-tailed = 0.009817588    1-tailed = 0.004908794 \nAngle between the two OLS regression lines = 12.78218 degrees\n\nRegression results\n  Method Intercept     Slope Angle (degrees) P-perm (1-tailed)\n1    OLS  5.019254 0.4170422        22.63820                NA\n2     MA  4.288499 0.4648040        24.92919                NA\n3    SMA  3.067471 0.5446097        28.57314                NA\n\nConfidence intervals\n  Method 2.5%-Intercept 97.5%-Intercept 2.5%-Slope 97.5%-Slope\n1    OLS      0.4280737        9.610435  0.1314843   0.7026001\n2     MA     -1.4843783        8.769024  0.1719592   0.8421162\n3    SMA     -2.3775157        6.360555  0.3293755   0.9004912\n\nEigenvalues: 32.37967 2.786995 \n\nH statistic used for computing C.I. of MA: 0.0684968"
  },
  {
    "objectID": "stat1-4/Statistik2_Demo.html#beispiele-modelldiagnostik",
    "href": "stat1-4/Statistik2_Demo.html#beispiele-modelldiagnostik",
    "title": "Stat2: Demo",
    "section": "Beispiele Modelldiagnostik",
    "text": "Beispiele Modelldiagnostik\n\npar(mfrow=c(2, 2)) #4 Plots in einem Fenster\nplot(lm(b~a))\n\n\n\n\nAbbildung 16.10: Generierter Plot\n\n\n\n\n\n#Load library\nif(!require(ggfortify)){install.packages(\"ggfortify\")}\nlibrary(ggfortify)\n\nautoplot(lm(b~a))\n\n# Modellstatistik nicht OK\ng <- c(20, 19, 25, 10, 8, 15, 13, 18, 11, 14, 25, 39, 38, 28, 24)\nh <- c(12, 15, 10, 7, 8, 10, 12, 11, 13, 10, 25, 12, 30, 26, 13)\npar(mfrow = c(1, 1))\n\nplot(h~g,xlim = c(0, 40), ylim = c(0, 30))\nabline(lm(h~g))\n\npar(mfrow = c(2, 2))\nplot(lm(h~g))\n\n# Modelldiagnostik mit ggplot\ndf <- data.frame(g, h)\nggplot(df, aes(x = g, y = h)) + \n    # scale_x_continuous(limits = c(0,25)) +\n    # scale_y_continuous(limits = c(0,25)) +\n    geom_point() +\n    geom_smooth( method = \"lm\", color = \"black\", size = .5, se = F) + \n    theme_classic()\n\npar(mfrow=c(2, 2))\nautoplot(lm(h~g))\n\n\n\n\nAbbildung 16.11: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 16.12: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 16.13: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 16.14: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 16.15: Generierter Plot"
  },
  {
    "objectID": "stat1-4/Statistik2_Uebung.html",
    "href": "stat1-4/Statistik2_Uebung.html",
    "title": "Stat2: Übung",
    "section": "",
    "text": "lauffähiges R-Skript\nbegründeter Lösungsweg (Kombination aus R-Code, R Output und dessen Interpretation)\nausformulierter Methoden- und Ergebnisteil (für eine wiss.Arbeit).\n\n\nBitte erklärt und begründet die einzelnen Schritte, die ihr unternehmt, um zu eurem Ergebnis zu kommen. Dazu erstellt bitte ein Word-Dokument, in dem ihr Schritt für Schritt den verwendeten R-Code, die dazu gehörigen Ausgaben von R, eure Interpretation derselben und die sich ergebenden Schlussfolgerungen für das weitere Vorgehen dokumentiert.\nDieser Ablauf sollte insbesondere beinhalten:\n\nÜberprüfen der Datenstruktur nach dem Einlesen, welches sind die abhängige(n) und welches die unabängige(n) Variablen etc.\nExplorative Datenanalyse, um zu sehen, ob evtl. Dateneingabefehler vorliegen oder Datentransformationen vorgenommen werden sollten\nAuswahl und Begründung eines statistischen Verfahrens\nBestimmung des vollständigen/maximalen Models\nSelektion des/der besten Models/Modelle\nDurchführen der Modelldiagnostik für dieses\nGenerieren aller Zahlen, Statistiken und Tabellen, die für eine wiss. Ergebnisdarstellung benötigt werden\n\nFormuliert abschliessend einen Methoden- und Ergebnisteil (ggf. incl. adäquaten Abbildungen/Tabellen) zu dieser Untersuchung in der Form einer wissenschaftlichen Arbeit (je einen ausformulierten Absatz von ca. 60-100 Worten bzw. 3-8 Sätzen). Alle wichtigen Informationen sollten enthalten sein, unnötige Redundanz dagegen vermieden werden.\n\n\nÜbung 2.1: Regression\nRegressionsanalyse mit SAR.csv\nDer Datensatz beschreibt die Zunahme der Artenzahlen (richness) von Pflanzen in Trockenrasen der Schweiz in Abhängigkeit von der Probeflächengrösse (area, hier in m²). Diese Beziehung bezeichnet man als Artenzahl-Areal-Kurve (Species-area relationship = SAR).\n\nLadet den Datensatz in R und macht eine explorative Datenanalyse.\nWählt unter den schon gelernten Methoden der Regressionsanalyse ein adäquates Vorgehen zur Analyse dieser Daten und führt diese dann durch.\nPrüft anhand der Residuen, ob die Modellvoraussetzungen erfüllt waren\nFalls die Modelldiagnostik negativ ausfällt, überlegt, welche Datentransformation helfen könnte, und rechnet neue Modelle mit einer oder ggf. mehreren Datentransformationen, bis ihr eine statistisch zufriedenstellende Lösung gefunden habt.\nStellt die erhaltenen Ergebnisse angemessen dar (Text, Abbildung und/oder Tabelle).\nKennt ihr ggf. noch eine andere geeignete Herangehensweise?\n\n\n\nÜbung 2.2: Einfaktorielle ANOVA\nANOVA mit Datensatz_novanimal_Uebung_Statistik2.2.csv\nFührt mit dem Datensatz novanimal.csv eine einfaktorielle ANOVA durch. Gibt es Unterschiede zwischen der Anzahl verkaufter Gerichte “tot_sold” (Buffet, Fleisch oder Vegetarisch) pro Woche?\n\n\nÜbung 2.3N: Mehrfaktorielle ANOVA (NatWis)\nANOVA mit kormoran.csv\nDer Datensatz enthält 40 Beobachtungen zu Tauchzeiten zweier Kormoranunterarten (C = Phalocrocorax carbo carbo und S = Phalacrocorax carbo sinensis) aus vier Jahreszeiten (F = Frühling, S = Sommer, H = Herbst, W = Winter).\n\nLest den Datensatz nach R ein und führt eine adäquate Analyse durch, um beantworten zu können, wie Unterart und Jahreszeit die Tauchzeit beeinflussen.\nStellt eure Ergebnisse dann angemessen dar (Text mit Abbildung und/oder Tabelle).\nGibt es eine Interaktion?\n\n\n\nÜbung 2.3S: Mehrfaktorielle ANOVA mit Interaktion (SozWis)\nMANOVA mit Datensatz_novanimal_Uebung_Statistik2.3.csv\nIn der Mensa gibt es zwei unterschiedliche Preisniveaus bzgl. den Gerichten: eine preisgünstigere Menülinie (“World” & “Favorite”) und eine teuere Menülinie (“Kitchen”). Gibt es Unterschiede zwischen dem Kauf von preisgünstigeren resp. teureren Menülinien betreffend Menüinhalt & Hochschulzugehörigkeit?"
  },
  {
    "objectID": "stat1-4/Statistik2_Loesung_Beispiel.html#musterlösung-beispiel",
    "href": "stat1-4/Statistik2_Loesung_Beispiel.html#musterlösung-beispiel",
    "title": "Stat2: Lösung Beispiel",
    "section": "Musterlösung Beispiel",
    "text": "Musterlösung Beispiel\n\nDatensatz decay.csv\nRCode als Download\nLösungstext Beispiel\n\n\nÜbungsaufgabe\n(hier so ausführlich formuliert, wie dies auch in der Klausur der Fall sein wird)\n\nLaden Sie den Datensatz decay.csv. Dieser enthält die Zahl radioaktiver Zerfälle pro Zeiteinheit (amount) für Zeitpunkte (time) nach dem Start des Experimentes.\nErmitteln Sie ein statistisches Modell, dass die Zerfallshäufigkeit in Abhängigkeit von der Zeit beschreibt.\nBitte erklären und begründen Sie die einzelnen Schritte, die Sie unternehmen, um zu diesem Ergebnis zu kommen. Dazu erstellen Sie bitte ein Word-Dokument, in das Sie Schritt für Schritt den verwendeten R-Code, die dazu gehörigen Ausgaben von R, Ihre Interpretation derselben und die sich ergebenden Schlussfolgerungen für das weitere Vorgehen dokumentieren.\nDieser Ablauf sollte insbesondere beinhalten:\n\nÜberprüfen der Datenstruktur nach dem Einlesen, welches sind die abhängige(n) und welches die unabängige(n) Variablen\nExplorative Datenanalyse, um zu sehen, ob evtl. Dateneingabefehler vorliegen oder Datentransformationen vorgenommen werden sollten\nAuswahl und Begründung eines statistischen Verfahrens (es gibt hier mehrere statistisch korrekte Möglichkeiten!)\nErmittlung eines Modells\nDurchführen der Modelldiagnostik für das gewählte Modell\nGenerieren aller Zahlen, Statistiken und Tabellen, die für eine wiss. Ergebnisdarstellung benötigt werden\nFormulieren Sie abschliessend einen Methoden- und Ergebnisteil (ggf. incl. adäquaten Abbildungen) zu dieser Untersuchung in der Form einer wissenschaftlichen Arbeit (ausformulierte schriftliche Zusammenfassung, mit je einem Absatz von ca. 60-100 Worten, resp. 3-8 Sätzen für den Methoden- und Ergebnisteil). D. h. alle wichtigen Informationen sollten enthalten sein, unnötige Redundanz dagegen vermieden werden.\nAbzugeben sind am Ende (a) Ein lauffähiges R-Skript; (b) begründeter Lösungsweg (Kombination aus R-Code, R Output und dessen Interpretation) und (c) ausformulierter Methoden- und Ergebnisteil (für eine wiss. Arbeit)."
  },
  {
    "objectID": "stat1-4/Statistik2_Loesung_Beispiel.html#kommentierter-lösungsweg",
    "href": "stat1-4/Statistik2_Loesung_Beispiel.html#kommentierter-lösungsweg",
    "title": "Stat2: Lösung Beispiel",
    "section": "Kommentierter Lösungsweg",
    "text": "Kommentierter Lösungsweg\n\nsummary(decay)\n\n      time          amount       \n Min.   : 0.0   Min.   :  8.196  \n 1st Qu.: 7.5   1st Qu.: 21.522  \n Median :15.0   Median : 35.015  \n Mean   :15.0   Mean   : 42.146  \n 3rd Qu.:22.5   3rd Qu.: 57.460  \n Max.   :30.0   Max.   :125.000  \n\nstr(decay)\n\n'data.frame':   31 obs. of  2 variables:\n $ time  : int  0 1 2 3 4 5 6 7 8 9 ...\n $ amount: num  125 100.2 70 83.5 100 ...\n\n\nMan erkennt, dass es 31 Beobachtungen für die Zeit als Integer von Zerfällen gibt, die als rationale Zahlen angegeben werden (dass die Zahl der Zerfälle nicht ganzzahlig ist, deutet darauf hin, dass sie möglicherweise nur in einem Teil des Zeitintervalls oder für einen Teil des betrachteten Raumes gemessen und dann hochgerechnet wurde.\n\nExplorative Datenanalyse\n\nboxplot(decay$time)\nboxplot(decay$amount)\nplot(amount~time, data=decay)\n\n\n\n\nAbbildung 18.1: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 18.2: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 18.3: Generierter Plot\n\n\n\n\nWährend der Boxplot für time wunderbar symmetrisch ohne Ausreisser ist, zeigt amount eine stark rechtsschiefe (linkssteile) Verteilung mit einem Ausreiser. Das deutet schon an, dass ein einfaches lineares Modell vermutlich die Modellannahmen verletzen wird. Auch der einfache Scatterplot zeigt, dass ein lineares Modell wohl nicht adäquat ist. Wir rechnen aber erst einmal weiter.\n\n\nEinfaches lineares Modell\n\nlm.1 <- lm(amount~time, data = decay)\nsummary(lm.1)\n\n\nCall:\nlm(formula = amount ~ time, data = decay)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.065 -10.029  -2.058   5.107  40.447 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  84.5534     5.0277   16.82  < 2e-16 ***\ntime         -2.8272     0.2879   -9.82 9.94e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.34 on 29 degrees of freedom\nMultiple R-squared:  0.7688,    Adjusted R-squared:  0.7608 \nF-statistic: 96.44 on 1 and 29 DF,  p-value: 9.939e-11\n\n\nDas sieht erst einmal nach einem Supermodell aus, höchstsignifikant und mit einem hohen R² von fast 77%. ABER: wir müssen uns noch die Modelldiagnostik ansehen…\n\n\nModelldiagnostik\n\npar(mfrow = c(2, 2))\nplot(lm.1)\n\n\n\n\nAbbildung 18.4: Generierter Plot\n\n\n\n\nHier zeigen die wichtigen oberen Plots beide massive Abweichungen vom „Soll“. Der Plot oben links zeigt eine „Banane“ und beim Q-Q-Plot oben rechts weichen die Punkte rechts der Mitte alle stark nach oben von der Solllinie ab. Wir haben unser Modell also offensichtlich falsch spezifiziert. Um eine Idee zu bekommen, was falsch ist, plotten wir noch, wie das Ergebnis dieses Modells aussähe:\n\n\nErgebnisplot\n\npar(mfrow = c(1, 1))\nplot(decay$time, decay$amount)\nabline(lm.1, col = \"red\")\n\n\n\n\nAbbildung 18.5: Generierter Plot\n\n\n\n\nDie Punkte links liegen alle über der Regressionslinie, die in der Mitte darunter und die ganz rechts wieder systematisch darüber (darum im Diagnostikplot oben die „Banane“). Es liegt also offensichtlich keine lineare Beziehung vor, sondern eine curvilineare.\nUm diese korrekt zu analysieren, gibt es im Prinzip drei Möglichkeiten, wovon am zweiten Kurstag nur eine hatten, während die zweite und dritte in Statistik 3 und 4 folgten. Im Folgenden sind alle drei nacheinander dargestellt (in der Klausur würde es aber genügen, eine davon darzustellen, wenn die Aufgabenstellung wie oben lautet)."
  },
  {
    "objectID": "stat1-4/Statistik2_Loesung_Beispiel.html#variante-1-lineares-modell-nach-transformation-der-abhängigen-variablen",
    "href": "stat1-4/Statistik2_Loesung_Beispiel.html#variante-1-lineares-modell-nach-transformation-der-abhängigen-variablen",
    "title": "Stat2: Lösung Beispiel",
    "section": "Variante (1): Lineares Modell nach Transformation der abhängigen Variablen",
    "text": "Variante (1): Lineares Modell nach Transformation der abhängigen Variablen\nDass die Verteilung der abhängigen Variable nicht normal ist, haben wir ja schon bei der explorativen Datenanalyse am Anfang gesehen. Da sie stark linkssteil ist, zugleich aber keine Nullwerte enthält, bietet sich eine Logarithmustransformation an, hier z. B. mit dem natürlichen Logarithmus.\n\nLösung 1: log-Transformation der abängigen Variablen\n\npar(mfrow = c(1, 2))\nboxplot(decay$amount)\nboxplot(log(decay$amount))\nhist(decay$amount)\nhist(log(decay$amount))\n\n\n\n\nAbbildung 18.6: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 18.7: Generierter Plot\n\n\n\n\nDie log-transformierte Variante rechts sieht sowohl im Boxplot als auch im #Histogramm viel symmetrischer/besser normalverteilt aus. Damit ergibt sich #dann folgendes lineares Modell\n\nlm.2 <- lm(log(amount)~time, data = decay)\nsummary(lm.2)\n\n\nCall:\nlm(formula = log(amount) ~ time, data = decay)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.5935 -0.2043  0.0067  0.2198  0.6297 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  4.547386   0.100295   45.34  < 2e-16 ***\ntime        -0.068528   0.005743  -11.93 1.04e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.286 on 29 degrees of freedom\nMultiple R-squared:  0.8308,    Adjusted R-squared:  0.825 \nF-statistic: 142.4 on 1 and 29 DF,  p-value: 1.038e-12\n\n\nJetzt ist der R²-Wert noch höher und der p-Wert noch niedriger als im ursprünglichen linearen Modell ohne Transformation. Das erlaubt aber keine Aussage, da wir Äpfel mit Birnen vergleichen, da die abhängige Variable einmal untransformiert und einmal log-transformiert ist. Entscheidend ist die Modelldiagnostik.\n\n\nModelldiagnostik\n\npar(mfrow = c(2, 2))\nplot(lm.2)\n\n\n\n\nAbbildung 18.8: Generierter Plot\n\n\n\n\nDer Q-Q-Plot sieht jetzt exzellent aus, der Plot rechts oben hat kaum noch eine Banane, nur noch einen leichten Keil. Insgesamt deutlich besser und auf jeden Fall ein statistisch korrektes Modell.\nLösungen 2 und 3 greifen auf Methoden von Statistik 3 und 4 zurück, sie sind hier nur zum Vergleich angeführt"
  },
  {
    "objectID": "stat1-4/Statistik2_Loesung_Beispiel.html#lösung-2-quadratische-regression-kommt-erst-in-statistik-3",
    "href": "stat1-4/Statistik2_Loesung_Beispiel.html#lösung-2-quadratische-regression-kommt-erst-in-statistik-3",
    "title": "Stat2: Lösung Beispiel",
    "section": "Lösung 2: quadratische Regression (kommt erst in Statistik 3)",
    "text": "Lösung 2: quadratische Regression (kommt erst in Statistik 3)\nkönnte für die Datenverteilung passen, entspricht aber nicht der physikalischen\n\nGesetzmässigkeit\n\nmodel.quad <- lm(amount~time + I(time^2), data=  decay)\nsummary(model.quad)\n\n\nCall:\nlm(formula = amount ~ time + I(time^2), data = decay)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-22.302  -6.044  -1.603   4.224  20.581 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 106.38880    4.65627  22.849  < 2e-16 ***\ntime         -7.34485    0.71844 -10.223 5.90e-11 ***\nI(time^2)     0.15059    0.02314   6.507 4.73e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.205 on 28 degrees of freedom\nMultiple R-squared:  0.908, Adjusted R-squared:  0.9014 \nF-statistic: 138.1 on 2 and 28 DF,  p-value: 3.122e-15\n\n\nHier können wir R² mit dem ursprünglichen Modell vergleichen (beide haben amount als abhängige Grösse) und es sieht viel besser aus. Sowohl der lineare als auch der quadratische Term sind hochsignifikant. Sicherheitshalber vergleichen wir die beiden Modelle aber noch mittels ANOVA.\n\n\nVergleich mit dem einfachen Modell mittels ANOVA (es ginge auch AICc)\n\nanova(lm.1, model.quad)\n\nAnalysis of Variance Table\n\nModel 1: amount ~ time\nModel 2: amount ~ time + I(time^2)\n  Res.Df    RSS Df Sum of Sq      F    Pr(>F)    \n1     29 5960.6                                  \n2     28 2372.6  1    3588.1 42.344 4.727e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn der Tat ist das komplexere Modell (jenes mit dem quadratischen Term) höchstsignifikant besser. Jetzt brauchen wir noch die Modelldiagnostik.\n\n\nModelldiagnostik\n\npar(mfrow = c(2, 2))\nplot(model.quad)\n\n\n\n\nAbbildung 18.9: Generierter Plot"
  },
  {
    "objectID": "stat1-4/Statistik2_Loesung_Beispiel.html#lösung-3-die-beste-hatten-wir-aber-am-2.-tag-noch-nicht-mit-startwerten-muss-man-ggf.-ausprobieren",
    "href": "stat1-4/Statistik2_Loesung_Beispiel.html#lösung-3-die-beste-hatten-wir-aber-am-2.-tag-noch-nicht-mit-startwerten-muss-man-ggf.-ausprobieren",
    "title": "Stat2: Lösung Beispiel",
    "section": "Lösung 3 (die beste, hatten wir aber am 2. Tag noch nicht; mit Startwerten muss man ggf. ausprobieren)",
    "text": "Lösung 3 (die beste, hatten wir aber am 2. Tag noch nicht; mit Startwerten muss man ggf. ausprobieren)\nmit Startwerten muss man ggf. ausprobieren\n\nmodel.nls <- nls(amount~a*exp(-b*time), start=(list(a = 100, b = 1)),data = decay)\nsummary(model.nls)\n\n\nFormula: amount ~ a * exp(-b * time)\n\nParameters:\n   Estimate Std. Error t value Pr(>|t|)    \na 1.081e+02  4.993e+00   21.66  < 2e-16 ***\nb 8.019e-02  5.833e-03   13.75 3.12e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.243 on 29 degrees of freedom\n\nNumber of iterations to convergence: 8 \nAchieved convergence tolerance: 7.966e-06\n\n\n\nModelldiagnostik\n\nif(!require(nlstools)){install.packages(\"nlstools\")}\nlibrary(nlstools)\nresiduals.nls <- nlsResiduals(model.nls)\nplot(residuals.nls)\n\n\n\n\nAbbildung 18.10: Generierter Plot\n\n\n\n\nFür nls kann man nicht den normalen Plotbefehl für die Residualdiagnostik nehmen, sondern verwendet das Äquivalent aus nlstools. Die beiden entscheidenden Plots sind jetzt links oben und rechts unten. Der QQ-Plot hat im unteren Bereich einen kleinen Schönheitsfehler, aber ansonsten ist alles OK.\nDa alle drei Lösungen zumindest statistisch OK waren, sollen jetzt noch die zugehörigen Ergebnisplots erstellt werden.\n\n\nErgebnisplots\n\npar(mfrow = c(1, 1))\nxv <- seq(0, 30, 0.1)\n\n\nlineares Modell mit log-transformierter Abhängiger\n\n\nplot(decay$time, decay$amount)\nyv1 <- exp(predict(lm.2, list(time = xv)))\nlines(xv, yv1, col = \"red\")\n\n\n\n\nAbbildung 18.11: Generierter Plot\n\n\n\n\n\nquadratisches Modell\n\n\nplot(decay$time, decay$amount)\nyv2 <- predict(model.quad, list(time = xv))\nlines(xv, yv2, col=  \"blue\")\n\n\n\n\nAbbildung 18.12: Generierter Plot\n\n\n\n\n\nnicht-lineares Modell\n\n\nplot(decay$time, decay$amount)\nyv3 <- predict(model.nls, list(time = xv))\nlines(xv, yv3, col = \"green\")\n\n\n\n\nAbbildung 18.13: Generierter Plot\n\n\n\n\nOptisch betrachtet, geben (2) und (3) den empirischen Zusammenhang etwas besser wieder als (1), da sie im linken Bereich die hohen Werte besser treffen. Man könnte sogar meinen, bei Betrachtung der Daten, dass die Werte ab time = 28 wieder leicht ansteigen, was die quadratische Funktion wiedergibt. Wer sich aber mit Physik etwas auskennt, weiss, dass Version (2) physikalisch nicht zutrifft, da die Zerfallsrate mit der Zeit immer weiter abfällt. Aufgrund der kurzen Messreihe wäre eine quadratische Funktion trotzdem eine statistisch korrekte Interpretation. Mit längeren Messreihen würde sich jedoch schnell zeigen, dass sie nicht zutrifft."
  },
  {
    "objectID": "stat1-4/Statistik3_Demo.html#ancova",
    "href": "stat1-4/Statistik3_Demo.html#ancova",
    "title": "Stat3: Demo",
    "section": "ANCOVA",
    "text": "ANCOVA\nExperiment zur Fruchtproduktion (“Fruit”) von Ipomopsis sp. (“Fruit”) in Abhängigkeit Ungrazedvon der Beweidung (Grazing mit 2 Levels: Grazed, Ungrazed) und korrigiert für die Pflanzengrösse vor der Beweidung (hier ausgedrückt als Durchmesser an der Spitze des Wurzelstock: “Root”)\n\ncompensation <- read.delim(\"data/ipomopsis.csv\", sep = \",\", stringsAsFactors = T)\n\n\n\n       X              Root            Fruit            Grazing  \n Min.   : 1.00   Min.   : 4.426   Min.   : 14.73   Grazed  :20  \n 1st Qu.:10.75   1st Qu.: 6.083   1st Qu.: 41.15   Ungrazed:20  \n Median :20.50   Median : 7.123   Median : 60.88                \n Mean   :20.50   Mean   : 7.181   Mean   : 59.41                \n 3rd Qu.:30.25   3rd Qu.: 8.510   3rd Qu.: 76.19                \n Max.   :40.00   Max.   :10.253   Max.   :116.05                \n\n\n  Grazed Ungrazed \n 67.9405  50.8805 \n\n\n             Df Sum Sq Mean Sq F value   Pr(>F)    \nRoot          1  16795   16795 359.968  < 2e-16 ***\nGrazing       1   5264    5264 112.832 1.21e-12 ***\nRoot:Grazing  1      5       5   0.103     0.75    \nResiduals    36   1680      47                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n             Df Sum Sq Mean Sq F value   Pr(>F)    \nGrazing       1   2910    2910  62.380 2.26e-09 ***\nRoot          1  19149   19149 410.420  < 2e-16 ***\nGrazing:Root  1      5       5   0.103     0.75    \nResiduals    36   1680      47                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nCall:\nlm(formula = Fruit ~ Grazing + Root, data = compensation)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17.1920  -2.8224   0.3223   3.9144  17.3290 \n\nCoefficients:\n                Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     -127.829      9.664  -13.23 1.35e-15 ***\nGrazingUngrazed   36.103      3.357   10.75 6.11e-13 ***\nRoot              23.560      1.149   20.51  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.747 on 37 degrees of freedom\nMultiple R-squared:  0.9291,    Adjusted R-squared:  0.9252 \nF-statistic: 242.3 on 2 and 37 DF,  p-value: < 2.2e-16\n\n\n\n\n\nAbbildung 19.1: ?(caption)\n\n\n\n\n\n\n\nAbbildung 19.2: ?(caption)\n\n\n\n\n\n\n\nAbbildung 19.3: ?(caption)\n\n\n\n\n\n\n\nAbbildung 19.4: ?(caption)"
  },
  {
    "objectID": "stat1-4/Statistik3_Demo.html#section",
    "href": "stat1-4/Statistik3_Demo.html#section",
    "title": "Stat3: Demo",
    "section": "",
    "text": "e <- c(20, 19, 25, 10, 8, 15, 13, 18, 11, 14, 25, 39, 38, 28, 24)\nf <- c(12, 15, 10, 7, 2, 10, 12, 11, 13, 10, 9, 2, 4, 7, 13)\n\nlm.1 <- lm(f~e)\nlm.quad <- lm(f~e + I(e^2))\n\nsummary(lm.1)\n\n\nCall:\nlm(formula = f ~ e)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.0549 -1.7015  0.5654  2.0617  5.6406 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  12.2879     2.4472   5.021 0.000234 ***\ne            -0.1541     0.1092  -1.412 0.181538    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.863 on 13 degrees of freedom\nMultiple R-squared:  0.1329,    Adjusted R-squared:  0.06622 \nF-statistic: 1.993 on 1 and 13 DF,  p-value: 0.1815\n\nsummary(lm.quad)\n\n\nCall:\nlm(formula = f ~ e + I(e^2))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.3866 -1.1018 -0.2027  1.3831  4.4211 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)   \n(Intercept) -2.239308   3.811746  -0.587  0.56777   \ne            1.330933   0.360105   3.696  0.00306 **\nI(e^2)      -0.031587   0.007504  -4.209  0.00121 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.555 on 12 degrees of freedom\nMultiple R-squared:  0.6499,    Adjusted R-squared:  0.5915 \nF-statistic: 11.14 on 2 and 12 DF,  p-value: 0.001842\n\npar(mfrow = c(1, 1))\n\n# 1. lineares Modell\nplot(f~e, xlim = c(0, 40), ylim = c(0, 20))\nabline(lm(f~e), col = \"blue\")\n\n# 2. quadratisches Modell\nxv <- seq(0, 40, 0.1)\nplot(f~e, xlim = c(0, 40), ylim = c(0, 20))\nyv2 <- predict(lm.quad, list(e = xv))\nlines(xv, yv2, col = \"red\")\n\n# Residualplots\npar(mfrow = c(2, 2))\nplot(lm.1)\nplot(lm.quad)\n\n\n\n\nAbbildung 19.5: ?(caption)\n\n\n\n\n\n\n\nAbbildung 19.6: ?(caption)\n\n\n\n\n\n\n\nAbbildung 19.7: ?(caption)\n\n\n\n\n\n\n\nAbbildung 19.8: ?(caption)"
  },
  {
    "objectID": "stat1-4/Statistik3_Demo.html#multiple-lineare-regression",
    "href": "stat1-4/Statistik3_Demo.html#multiple-lineare-regression",
    "title": "Stat3: Demo",
    "section": "Multiple lineare Regression",
    "text": "Multiple lineare Regression\n\nSimulation Overfitting\n\ntest <- data.frame(\"x\" = c(1, 2, 3, 4, 5, 6), \"y\" = c(34, 21, 70, 47, 23, 45))\n\npar(mfrow=c(1,1))\nplot(y~x, data = test)\n\nlm.0 <- lm(y~1, data = test)\nlm.1 <- lm(y~x, data = test)\nlm.2 <- lm(y~x+ I(x^2), data = test)\nlm.3 <- lm(y~x+ I(x^2) + I(x^3), data = test)\nlm.4 <- lm(y~x+ I(x^2) + I(x^3) + I(x^4), data = test)\nlm.5 <- lm(y~x+ I(x^2) + I(x^3) + I(x^4) + I(x^5), data = test)\nlm.6 <- lm(y~x+ I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6), data = test)\nsummary(lm.0)\nsummary(lm.1)\nsummary(lm.2)\nsummary(lm.3)\nsummary(lm.4)\nsummary(lm.5)\n\nxv <- seq(from = 0, to = 10, by = 0.1)\n\nplot(y~x, cex = 2, col = \"black\", lwd = 3, data = test)\nyv <- predict(lm.1, list(x = xv))\nlines(xv, yv, col = \"red\", lwd = 3)\nyv <- predict(lm.2, list(x = xv))\nlines(xv, yv, col = \"blue\", lwd = 3)\nyv<-predict(lm.3, list(x = xv))\nlines(xv, yv, col = \"green\", lwd =3)\nyv <- predict(lm.4, list(x = xv))\nlines(xv, yv, col = \"orange\", lwd = 3)\nyv <- predict(lm.5, list(x = xv))\nlines(xv, yv, col = \"black\", lwd = 3)\n\n\n\nMultiple lineare Regression basierend auf Logan, Beispiel 9A\n\nloyn <- read.delim(\"data/loyn.csv\", sep = \",\")\nsummary(loyn)\n\n       X             ABUND            AREA            YR.ISOL    \n Min.   : 1.00   Min.   : 1.50   Min.   :   0.10   Min.   :1890  \n 1st Qu.:14.75   1st Qu.:12.40   1st Qu.:   2.00   1st Qu.:1928  \n Median :28.50   Median :21.05   Median :   7.50   Median :1962  \n Mean   :28.50   Mean   :19.51   Mean   :  69.27   Mean   :1950  \n 3rd Qu.:42.25   3rd Qu.:28.30   3rd Qu.:  29.75   3rd Qu.:1966  \n Max.   :56.00   Max.   :39.60   Max.   :1771.00   Max.   :1976  \n      DIST            LDIST            GRAZE            ALT       \n Min.   :  26.0   Min.   :  26.0   Min.   :1.000   Min.   : 60.0  \n 1st Qu.:  93.0   1st Qu.: 158.2   1st Qu.:2.000   1st Qu.:120.0  \n Median : 234.0   Median : 338.5   Median :3.000   Median :140.0  \n Mean   : 240.4   Mean   : 733.3   Mean   :2.982   Mean   :146.2  \n 3rd Qu.: 333.2   3rd Qu.: 913.8   3rd Qu.:4.000   3rd Qu.:182.5  \n Max.   :1427.0   Max.   :4426.0   Max.   :5.000   Max.   :260.0  \n\n\n\n\nKorrelation zwischen den Prädiktoren\n\ncor <- cor(loyn[, 2:7])\nprint(cor, digits = 2)\n\n         ABUND    AREA YR.ISOL  DIST  LDIST  GRAZE\nABUND    1.000  0.2560  0.5034  0.24  0.087 -0.683\nAREA     0.256  1.0000 -0.0015  0.11  0.035 -0.310\nYR.ISOL  0.503 -0.0015  1.0000  0.11 -0.083 -0.636\nDIST     0.236  0.1083  0.1132  1.00  0.317 -0.256\nLDIST    0.087  0.0346 -0.0833  0.32  1.000 -0.028\nGRAZE   -0.683 -0.3104 -0.6356 -0.26 -0.028  1.000\n\ncor[abs(cor)<0.6] <- 0\ncor\n\n             ABUND AREA    YR.ISOL DIST LDIST      GRAZE\nABUND    1.0000000    0  0.0000000    0     0 -0.6825114\nAREA     0.0000000    1  0.0000000    0     0  0.0000000\nYR.ISOL  0.0000000    0  1.0000000    0     0 -0.6355671\nDIST     0.0000000    0  0.0000000    1     0  0.0000000\nLDIST    0.0000000    0  0.0000000    0     1  0.0000000\nGRAZE   -0.6825114    0 -0.6355671    0     0  1.0000000\n\nprint(cor, digits = 3)\n\n         ABUND AREA YR.ISOL DIST LDIST  GRAZE\nABUND    1.000    0   0.000    0     0 -0.683\nAREA     0.000    1   0.000    0     0  0.000\nYR.ISOL  0.000    0   1.000    0     0 -0.636\nDIST     0.000    0   0.000    1     0  0.000\nLDIST    0.000    0   0.000    0     1  0.000\nGRAZE   -0.683    0  -0.636    0     0  1.000\n\nlm.1 <- lm (ABUND ~ YR.ISOL + ALT + GRAZE, data = loyn)\nif(!require(car)){install.packages(\"car\")} \nlibrary(car)\nvif(lm.1)\n\n YR.ISOL      ALT    GRAZE \n1.679995 1.200372 1.904799 \n\ninfluence.measures(lm.1)\n\nInfluence measures of\n     lm(formula = ABUND ~ YR.ISOL + ALT + GRAZE, data = loyn) :\n\n      dfb.1_  dfb.YR.I   dfb.ALT  dfb.GRAZ     dffit cov.r   cook.d    hat inf\n1   0.128900 -0.136701 -2.25e-02  8.68e-02 -0.455383 0.663 4.64e-02 0.0286   *\n2  -0.046388  0.041396  1.50e-01 -2.15e-02 -0.222873 1.159 1.26e-02 0.0996    \n3  -0.178685  0.184085 -5.40e-02 -4.08e-02 -0.298379 1.108 2.23e-02 0.0901    \n4   0.054207 -0.053864 -2.43e-02 -4.06e-02 -0.085906 1.099 1.87e-03 0.0331    \n5   0.032249 -0.035235  3.34e-02  6.56e-02  0.138294 1.123 4.85e-03 0.0597    \n6   0.072550 -0.075381  3.68e-02 -3.40e-02 -0.129304 1.072 4.22e-03 0.0315    \n7   0.153153 -0.155477  8.56e-02 -1.78e-01 -0.263831 1.139 1.75e-02 0.0978    \n8  -0.044533  0.039741  1.44e-01 -2.07e-02 -0.213965 1.162 1.16e-02 0.0996    \n9   0.305330 -0.305810  1.16e-02 -2.93e-01 -0.412610 0.935 4.12e-02 0.0593    \n10 -0.134119  0.136978 -1.50e-02 -2.15e-02 -0.217402 1.140 1.19e-02 0.0876    \n11  0.145761 -0.154644  2.14e-01 -2.21e-02  0.300565 1.103 2.26e-02 0.0883    \n12 -0.246939  0.255702 -1.22e-01 -2.33e-02 -0.369735 1.161 3.42e-02 0.1318    \n13  0.071832 -0.068266 -1.34e-01 -3.53e-02 -0.191283 1.110 9.23e-03 0.0653    \n14  0.019281 -0.016626 -3.08e-01  1.90e-01 -0.597735 0.810 8.32e-02 0.0692    \n15  0.000311 -0.000315 -2.26e-05  2.96e-05  0.000496 1.184 6.27e-08 0.0874    \n16 -0.131537  0.136111 -5.26e-02 -3.46e-02 -0.223973 1.146 1.27e-02 0.0923    \n17 -0.098856  0.108184 -1.60e-01  1.44e-02  0.266285 0.984 1.75e-02 0.0393    \n18  0.238014 -0.243468  3.85e-02 -1.36e-01 -0.397451 0.753 3.65e-02 0.0293   *\n19 -0.031350  0.029292  5.78e-02  3.66e-02  0.081711 1.121 1.70e-03 0.0460    \n20 -0.024122  0.019709  7.59e-02  5.75e-02 -0.093805 1.170 2.24e-03 0.0829    \n21  0.036050 -0.033748 -7.79e-02 -2.15e-02 -0.102357 1.162 2.66e-03 0.0786    \n22 -0.015768  0.016959  4.26e-03 -6.28e-02 -0.127636 1.116 4.13e-03 0.0531    \n23  0.050368 -0.052333  2.55e-02 -2.36e-02 -0.089769 1.095 2.04e-03 0.0315    \n24 -0.012264  0.008841  5.20e-02  5.07e-02 -0.071851 1.209 1.31e-03 0.1091    \n25  0.145637 -0.146703  2.41e-02 -7.94e-02  0.157322 1.319 6.30e-03 0.1876   *\n26 -0.007372  0.007451  1.67e-03  5.11e-03  0.015793 1.106 6.36e-05 0.0232    \n27  0.043873 -0.045585  2.22e-02 -2.05e-02 -0.078194 1.100 1.55e-03 0.0315    \n28 -0.018037  0.016743  2.82e-02  2.63e-02  0.036688 1.224 3.43e-04 0.1175    \n29 -0.131935  0.133012 -2.20e-02  1.11e-01  0.164334 1.152 6.84e-03 0.0836    \n30  0.094249 -0.092478 -8.47e-02  1.81e-02  0.210983 1.127 1.12e-02 0.0790    \n31  0.118899 -0.130120  1.93e-01 -1.73e-02 -0.320276 0.928 2.49e-02 0.0393    \n32 -0.103130  0.098781  9.40e-02  1.33e-01  0.170699 1.126 7.37e-03 0.0690    \n33 -0.284839  0.290760 -1.33e-01  2.50e-01  0.433995 0.919 4.54e-02 0.0602    \n34 -0.213008  0.199453  2.95e-01  3.01e-01  0.408017 1.071 4.12e-02 0.1008    \n35  0.068874 -0.066760 -1.35e-01 -3.57e-03 -0.246916 1.008 1.51e-02 0.0407    \n36 -0.151383  0.159324 -1.23e-01  5.71e-02  0.283014 0.959 1.96e-02 0.0376    \n37  0.022901 -0.022520 -3.21e-02  3.25e-02  0.103312 1.136 2.71e-03 0.0605    \n38 -0.001488  0.001427  3.83e-03 -1.89e-04  0.006929 1.125 1.22e-05 0.0393    \n39 -0.299662  0.296262  7.86e-02  2.86e-01  0.365529 1.060 3.31e-02 0.0860    \n40  0.045779 -0.044212 -7.15e-02  3.70e-02  0.168859 1.126 7.21e-03 0.0685    \n41 -0.043463  0.037744  6.26e-02  1.22e-01 -0.153196 1.126 5.94e-03 0.0653    \n42 -0.067499  0.070133 -3.42e-02  3.16e-02  0.120302 1.078 3.66e-03 0.0315    \n43  0.002552 -0.002850 -1.05e-02  1.52e-02 -0.036428 1.143 3.38e-04 0.0558    \n44  0.011473 -0.009053 -3.51e-02 -3.92e-02  0.052676 1.192 7.07e-04 0.0953    \n45  0.002848  0.003165 -8.61e-02 -6.95e-02  0.137899 1.092 4.81e-03 0.0427    \n46 -0.116776  0.109111  2.15e-01  1.36e-01  0.304366 0.977 2.28e-02 0.0460    \n47  0.445830 -0.431209 -2.69e-01 -3.41e-01  0.629701 0.642 8.76e-02 0.0483   *\n48 -0.000133  0.004718  4.46e-02 -1.58e-01  0.302736 1.002 2.26e-02 0.0520    \n49  0.008724 -0.006876 -2.00e-02 -3.60e-02  0.048292 1.150 5.94e-04 0.0626    \n50  0.019369 -0.017688 -5.80e-03 -5.14e-02  0.069197 1.136 1.22e-03 0.0548    \n51 -0.122055  0.122805  7.02e-02  2.13e-02  0.231107 1.022 1.33e-02 0.0408    \n52  0.020580 -0.015671 -8.25e-02 -6.78e-02  0.099679 1.298 2.53e-03 0.1704   *\n53  0.014674 -0.013095 -8.87e-03 -4.28e-02  0.057249 1.139 8.35e-04 0.0549    \n54  0.138452 -0.137403  3.82e-02 -1.54e-01  0.204365 1.168 1.06e-02 0.1011    \n55 -0.000650  0.000535 -4.05e-03  6.97e-03 -0.014242 1.144 5.17e-05 0.0555    \n56  0.021139 -0.021938  2.56e-02 -1.62e-02  0.039541 1.363 3.98e-04 0.2077   *\n\n\n\n\nModellvereinfachung\n\nlm.1 <- lm(ABUND ~ YR.ISOL + ALT + GRAZE, data = loyn)\nsummary(lm.1)\n\n\nCall:\nlm(formula = ABUND ~ YR.ISOL + ALT + GRAZE, data = loyn)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-19.5498  -4.8951   0.6504   4.7798  20.2384 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -73.58185  107.24995  -0.686 0.495712    \nYR.ISOL       0.05143    0.05393   0.954 0.344719    \nALT           0.03285    0.02679   1.226 0.225618    \nGRAZE        -4.01692    0.99881  -4.022 0.000188 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.894 on 52 degrees of freedom\nMultiple R-squared:  0.4887,    Adjusted R-squared:  0.4592 \nF-statistic: 16.57 on 3 and 52 DF,  p-value: 1.106e-07\n\nlm.2 <- update(lm.1,~.-YR.ISOL)\nanova(lm.1, lm.2)\n\nAnalysis of Variance Table\n\nModel 1: ABUND ~ YR.ISOL + ALT + GRAZE\nModel 2: ABUND ~ ALT + GRAZE\n  Res.Df    RSS Df Sum of Sq      F Pr(>F)\n1     52 3240.4                           \n2     53 3297.1 -1   -56.662 0.9093 0.3447\n\nsummary(lm.2)\n\n\nCall:\nlm(formula = ABUND ~ ALT + GRAZE, data = loyn)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-19.1677  -4.8261   0.0266   4.6944  19.1054 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 28.55582    5.43245   5.257 2.67e-06 ***\nALT          0.03191    0.02675   1.193    0.238    \nGRAZE       -4.59679    0.79167  -5.806 3.67e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.887 on 53 degrees of freedom\nMultiple R-squared:  0.4798,    Adjusted R-squared:  0.4602 \nF-statistic: 24.44 on 2 and 53 DF,  p-value: 3.011e-08\n\nlm.3 <- update(lm.2,~.-ALT)\nanova(lm.2, lm.3)\n\nAnalysis of Variance Table\n\nModel 1: ABUND ~ ALT + GRAZE\nModel 2: ABUND ~ GRAZE\n  Res.Df    RSS Df Sum of Sq      F Pr(>F)\n1     53 3297.1                           \n2     54 3385.6 -1   -88.519 1.4229 0.2382\n\nsummary(lm.3)\n\n\nCall:\nlm(formula = ABUND ~ GRAZE, data = loyn)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-19.1066  -5.4097   0.0934   4.4856  18.2747 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  34.3692     2.4095  14.264  < 2e-16 ***\nGRAZE        -4.9813     0.7259  -6.862  6.9e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.918 on 54 degrees of freedom\nMultiple R-squared:  0.4658,    Adjusted R-squared:  0.4559 \nF-statistic: 47.09 on 1 and 54 DF,  p-value: 6.897e-09\n\npar(mfrow = c(2, 2))\nplot(lm.1)\n\n\n\n\nAbbildung 19.9: ?(caption)\n\n\n\n\n\n\nHierarchical partitioning\n\nif(!require(hier.part)){install.packages(\"hier.part\")}\nlibrary(hier.part)\n\nloyn.preds <-with(loyn, data.frame(YR.ISOL, ALT, GRAZE))\nhier.part(loyn$ABUND, loyn.preds, gof = \"Rsqu\")\n\n$gfs\n[1] 0.0000000 0.2533690 0.1488696 0.4658218 0.3297010 0.4739432 0.4797883\n[8] 0.4887284\n\n$IJ\n                 I          J     Total\nYR.ISOL 0.11892853 0.13444049 0.2533690\nALT     0.06960132 0.07926823 0.1488696\nGRAZE   0.30019854 0.16562324 0.4658218\n\n$I.perc\n        ind.exp.var\nYR.ISOL    24.33428\nALT        14.24131\nGRAZE      61.42441\n\n$params\n$params$full.model\n[1] \"y ~ YR.ISOL + ALT + GRAZE\"\n\n$params$family\n[1] \"gaussian\"\n\n$params$link\n[1] \"default\"\n\n$params$gof\n[1] \"Rsqu\"\n\n\n\n\n\nAbbildung 19.10: Generierter Plot\n\n\n\n\n\n\nPartial regressions\n\navPlots(lm.1, ask = F)\n\n\n\n\nAbbildung 19.11: ?(caption)"
  },
  {
    "objectID": "stat1-4/Statistik3_Demo.html#multimodel-inference",
    "href": "stat1-4/Statistik3_Demo.html#multimodel-inference",
    "title": "Stat3: Demo",
    "section": "Multimodel inference",
    "text": "Multimodel inference\n\nif(!require(MuMIn)){install.packages(\"MuMIn\")}\nlibrary(MuMIn)\n\nglobal.model <- lm(ABUND ~ YR.ISOL + ALT + GRAZE, data = loyn)\noptions(na.action = \"na.fail\")\n\nallmodels <- dredge(global.model)\nallmodels\n\nGlobal model call: lm(formula = ABUND ~ YR.ISOL + ALT + GRAZE, data = loyn)\n---\nModel selection table \n     (Int)     ALT    GRA  YR.ISO df   logLik  AICc delta weight\n3   34.370         -4.981          3 -194.315 395.1  0.00  0.407\n4   28.560 0.03191 -4.597          4 -193.573 395.9  0.84  0.267\n7  -62.750         -4.440 0.04898  4 -193.886 396.6  1.46  0.196\n8  -73.580 0.03285 -4.017 0.05143  5 -193.087 397.4  2.28  0.130\n6 -348.500 0.07006        0.18350  4 -200.670 410.1 15.03  0.000\n5 -392.300                0.21120  3 -203.690 413.8 18.75  0.000\n2    5.598 0.09515                 3 -207.358 421.2 26.09  0.000\n1   19.510                         2 -211.871 428.0 32.88  0.000\nModels ranked by AICc(x) \n\nsw(allmodels)\n\n                     GRAZE ALT  YR.ISOL\nSum of weights:      1.00  0.40 0.33   \nN containing models:    4     4    4   \n\navgmodel <- model.avg(allmodels, subset = TRUE)\nsummary(avgmodel)\n\n\nCall:\nmodel.avg(object = allmodels, subset = TRUE)\n\nComponent model call: \nlm(formula = ABUND ~ <8 unique rhs>, data = loyn)\n\nComponent models: \n       df  logLik   AICc delta weight\n2       3 -194.31 395.09  0.00   0.41\n12      4 -193.57 395.93  0.84   0.27\n23      4 -193.89 396.56  1.46   0.20\n123     5 -193.09 397.37  2.28   0.13\n13      4 -200.67 410.13 15.03   0.00\n3       3 -203.69 413.84 18.75   0.00\n1       3 -207.36 421.18 26.09   0.00\n(Null)  2 -211.87 427.97 32.88   0.00\n\nTerm codes: \n    ALT   GRAZE YR.ISOL \n      1       2       3 \n\nModel-averaged coefficients:  \n(full average) \n            Estimate Std. Error Adjusted SE z value Pr(>|z|)    \n(Intercept) -0.29874   77.23966    78.39113   0.004    0.997    \nGRAZE       -4.64605    0.89257     0.91048   5.103    3e-07 ***\nALT          0.01282    0.02311     0.02340   0.548    0.584    \nYR.ISOL      0.01631    0.03883     0.03941   0.414    0.679    \n \n(conditional average) \n            Estimate Std. Error Adjusted SE z value Pr(>|z|)    \n(Intercept) -0.29874   77.23966    78.39113   0.004    0.997    \nGRAZE       -4.64724    0.88957     0.90755   5.121    3e-07 ***\nALT          0.03224    0.02678     0.02741   1.176    0.240    \nYR.ISOL      0.05007    0.05421     0.05548   0.902    0.367    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "stat1-4/Statistik3_Uebung.html",
    "href": "stat1-4/Statistik3_Uebung.html",
    "title": "Stat3: Übung",
    "section": "",
    "text": "Übung 3: Multiple Regression\n\nBereiten Sie den Datensatz Ukraine_bearbeitet.csv für das Einlesen in R vor und lesen Sie ihn dann ein. Dieser enthält Pflanzenartenzahlen (Species.richness) von 199 10 m² grossen Plots (Vegetationsaufnahmen) von Steppenrasen in der Ukraine sowie zahlreiche Umweltvariablen, deren Bedeutung und Einheiten im Kopf der ExcelTabelle angegeben sind.\nErmitteln Sie ein minimal adäquates Modell, das den Artenreichtum in den Plots durch die Umweltvariablen erklärt.\nBitte erklären und begründen Sie die einzelnen Schritte, die Sie unternehmen, um zu diesem Ergebnis zu kommen. Dazu erstellen Sie bitte ein Word-Dokument, in das Sie Schritt für Schritt den verwendeten R-Code, die dazu gehörigen Ausgaben von R, Ihre Interpretation derselben und die sich ergebenden Schlussfolgerungen für das weitere Vorgehen dokumentieren.\nDieser Ablauf sollte insbesondere beinhalten:\n\nÜberprüfen der Datenstruktur nach dem Einlesen: welches sind die abhängige(n) und welches die unabängige(n) Variablen, sind alle Variablen für die Analyse geeignet?\nExplorative Datenanalyse, um zu sehen, ob die abhängige Variable in der vorliegenden Form für die Analyse geeignet ist\nDefinition eines globalen Modelles und dessen Reduktion zu einem minimal adäquaten Modell\nDurchführen der Modelldiagnostik für dieses\nGenerieren aller Zahlen, Statistiken und Tabellen, die für eine wiss. Ergebnisdarstellung benötigt werden\nFormulieren Sie abschliessend einen Methoden- und Ergebnisteil (ggf. incl. adäquaten Abbildungen) zu dieser Untersuchung in der Form einer wissenschaftlichen Arbeit (ausformulierte schriftliche Zusammenfassung, mit je einem Absatz von ca. 60-100 Worten, resp. 3-8 Sätzen für den Methoden- und Ergebnisteil). D. h. alle wichtigen Informationen sollten enthalten sein, unnötige Redundanz dagegen vermieden werden.\nAbzugeben sind am Ende (a) Ein lauffähiges R-Skript; (b) begründeter Lösungsweg (Kombination aus R-Code, R Output und dessen Interpretation) und (c) ausformulierter Methoden- und Ergebnisteil (für eine wiss. Arbeit)."
  },
  {
    "objectID": "stat1-4/Statistik4_Demo.html#von-lms-zu-glms",
    "href": "stat1-4/Statistik4_Demo.html#von-lms-zu-glms",
    "title": "Stat4: Demo",
    "section": "von LMs zu GLMs",
    "text": "von LMs zu GLMs\n\ntemp <- c(10, 12 ,16, 20, 24, 25, 30, 33, 37)\nbesucher <- c(40, 12, 50, 500, 400, 900, 1500, 900, 2000)\nstrand <- data.frame(\"Temperatur\" = temp, \"Besucher\" = besucher)\n\nplot(besucher~temp, data = strand)\n\nlm.strand <- lm(Besucher~Temperatur, data = strand)\nsummary(lm.strand)\n\n\nCall:\nlm(formula = Besucher ~ Temperatur, data = strand)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-476.41 -176.89   55.59  218.82  353.11 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -855.01     290.54  -2.943 0.021625 *  \nTemperatur     67.62      11.80   5.732 0.000712 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 311.7 on 7 degrees of freedom\nMultiple R-squared:  0.8244,    Adjusted R-squared:  0.7993 \nF-statistic: 32.86 on 1 and 7 DF,  p-value: 0.0007115\n\npar(mfrow = c(2, 2))\nplot(lm.strand)\n\npar(mfrow = c(1 ,1))\nxv <- seq(0, 40, by = .1)\nyv <- predict(lm.strand, list(Temperatur = xv))\nplot(strand$Temperatur, strand$Besucher, xlim = c(0, 40))\nlines(xv, yv, lwd = 3, col=  \"blue\")\n\nglm.gaussian <- glm(Besucher~Temperatur, family = gaussian, data = strand)\nglm.poisson <- glm(Besucher~Temperatur, family = poisson, data = strand)\n\nsummary(glm.gaussian)\n\n\nCall:\nglm(formula = Besucher ~ Temperatur, family = gaussian, data = strand)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-476.41  -176.89    55.59   218.82   353.11  \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -855.01     290.54  -2.943 0.021625 *  \nTemperatur     67.62      11.80   5.732 0.000712 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 97138.03)\n\n    Null deviance: 3871444  on 8  degrees of freedom\nResidual deviance:  679966  on 7  degrees of freedom\nAIC: 132.63\n\nNumber of Fisher Scoring iterations: 2\n\nsummary(glm.poisson)\n\n\nCall:\nglm(formula = Besucher ~ Temperatur, family = poisson, data = strand)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-13.577  -12.787   -4.491    9.515   15.488  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept) 3.500301   0.056920   61.49   <2e-16 ***\nTemperatur  0.112817   0.001821   61.97   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 6011.8  on 8  degrees of freedom\nResidual deviance: 1113.7  on 7  degrees of freedom\nAIC: 1185.1\n\nNumber of Fisher Scoring iterations: 5\n\n\n\n\n\nAbbildung 21.1: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 21.2: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 21.3: Generierter Plot\n\n\n\n\nRücktranformation der Werte auf die orginale Skale (Hier Exponentialfunktion da family=possion als Link-Funktion den natürlichen Logarithmus (log) verwendet) Besucher = exp(3.50 + 0.11 Temperatur/°C)\n\n#| label: fig-baseplots-glm\n#| fig-cap: \"Generierter Plot\"\n\nexp(3.500301) # Anzahl besucher bei 0°C\n\n[1] 33.12542\n\nexp(glm.poisson$coefficients[1]) # Werte aus Modell\n\n(Intercept) \n   33.12542 \n\nexp(3.500301 + 30*0.112817) # Anzahl besucher bei 30°C\n\n[1] 977.3169\n\nexp(glm.poisson$coeff[1] * glm.poisson$coeff[2]) #coefficients kann mit coeff abgekürzt werden\n\n(Intercept) \n   1.484225 \n\n# Test Overdispersion\nif(!require(AER)){install.packages(\"AER\")}\nlibrary(AER)\ndispersiontest(glm.poisson)\n\n\n    Overdispersion test\n\ndata:  glm.poisson\nz = 3.8576, p-value = 5.726e-05\nalternative hypothesis: true dispersion is greater than 1\nsample estimates:\ndispersion \n  116.5467 \n\nglm.quasi <- glm(Besucher~Temperatur, family = quasipoisson, data = strand)\nsummary(glm.quasi)\n\n\nCall:\nglm(formula = Besucher ~ Temperatur, family = quasipoisson, data = strand)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-13.577  -12.787   -4.491    9.515   15.488  \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)  3.50030    0.69639   5.026  0.00152 **\nTemperatur   0.11282    0.02227   5.065  0.00146 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasipoisson family taken to be 149.6826)\n\n    Null deviance: 6011.8  on 8  degrees of freedom\nResidual deviance: 1113.7  on 7  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 5\n\npar(mfrow = c(2,2))\nplot(glm.gaussian)\n\n\n\nplot(glm.poisson)\n\n\n\nplot(glm.quasi)\n\n\n\npar(mfrow = c(1, 1))\nplot(strand$Temperatur, strand$Besucher, xlim=c(0, 40))\nxv <- seq(0, 40, by = .1)\n\nyv <- predict(lm.strand, list(Temperatur = xv))\nlines(xv, yv, lwd = 3, col = \"blue\")\n\nyv2 <- predict(glm.poisson, list(Temperatur = xv))\nlines(xv, exp(yv2), lwd = 3, col = \"red\")\n\nyv3 <- predict(glm.quasi, list(Temperatur = xv))\nlines(xv, exp(yv3), lwd = 3, col = \"green\")"
  },
  {
    "objectID": "stat1-4/Statistik4_Demo.html#logistische-regression",
    "href": "stat1-4/Statistik4_Demo.html#logistische-regression",
    "title": "Stat4: Demo",
    "section": "Logistische Regression",
    "text": "Logistische Regression\n\nbathing <- data.frame(\n  \"temperature\" = c(1, 2, 5, 9, 14, 14, 15, 19, 22, 24, 25, 26, 27, 28, 29),\n  \"bathing\" = c(0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1))\nplot(bathing~temperature, data = bathing)\n\nglm.1<-glm(bathing~temperature, family = \"binomial\", data = bathing)\nsummary(glm.1)\n\n\nCall:\nglm(formula = bathing ~ temperature, family = \"binomial\", data = bathing)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.7408  -0.4723  -0.1057   0.5123   1.8615  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)  \n(Intercept)  -5.4652     2.8501  -1.918   0.0552 .\ntemperature   0.2805     0.1350   2.077   0.0378 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 20.728  on 14  degrees of freedom\nResidual deviance: 10.829  on 13  degrees of freedom\nAIC: 14.829\n\nNumber of Fisher Scoring iterations: 6\n\n# Modeldiagnostik (wenn nicht signifikant, dann OK)\n1 - pchisq (glm.1$deviance, glm.1$df.resid)\n\n[1] 0.6251679\n\n# Modellgüte (pseudo-R²)\n1 - (glm.1$dev / glm.1$null)\n\n[1] 0.4775749\n\n# Steilheit der Beziehung (relative Änderung der odds bei x + 1 vs. x)\nexp(glm.1$coefficients[2])\n\ntemperature \n   1.323807 \n\n# LD50 (also hier: Temperatur, bei der 50% der Touristen baden)\n-glm.1$coefficients[1] / glm.1$coefficients[2]\n\n(Intercept) \n   19.48311 \n\n# Vorhersagen\npredicted <- predict(glm.1, type = \"response\")\n\n# Konfusionsmatrix\nkm <- table(bathing$bathing, predicted > 0.5)\nkm\n\n   \n    FALSE TRUE\n  0     7    1\n  1     1    6\n\n# Missklassifizierungsrate\n1 - sum(diag(km) / sum(km))\n\n[1] 0.1333333\n\n#Plotting\nxs <- seq(0, 30, l = 1000)\nmodel.predict <- predict(glm.1, type = \"response\", se = T, \n                         newdata = data.frame(temperature = xs))\n\nplot(bathing~temperature, xlab = \"Temperature (°C)\", \n     ylab = \"% Bathing\", pch = 16, col = \"red\", data = bathing)\npoints(model.predict$fit ~ xs, type=\"l\")\nlines(model.predict$fit+model.predict$se.fit ~ xs, type = \"l\", lty = 2)\nlines(model.predict$fit-model.predict$se.fit ~ xs, type = \"l\", lty = 2)\n\n\n\n\nAbbildung 21.4: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 21.5: Generierter Plot"
  },
  {
    "objectID": "stat1-4/Statistik4_Demo.html#nicht-lineare-regression",
    "href": "stat1-4/Statistik4_Demo.html#nicht-lineare-regression",
    "title": "Stat4: Demo",
    "section": "Nicht-lineare Regression",
    "text": "Nicht-lineare Regression\n\nif(!require(AICcmodavg)){install.packages(\"AICcmodavg\")}\nif(!require(nlstools)){install.packages(\"nlstools\")}\nlibrary(AICcmodavg)\nlibrary(nlstools)\n\nloyn <- read.delim(\"data/loyn.csv\", sep = \",\") # Verzeichnis muss dort gesetzt sein wo Daten sind\n\n#Selbstdefinierte Funktion, hier Potenzfunktion\npower.model <- nls(ABUND~c*AREA^z, start = (list(c = 1, z = 0)), data = loyn)\nsummary(power.model)\n\n\nFormula: ABUND ~ c * AREA^z\n\nParameters:\n  Estimate Std. Error t value Pr(>|t|)    \nc 13.39418    1.30721  10.246 2.87e-14 ***\nz  0.16010    0.02438   6.566 2.09e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.995 on 54 degrees of freedom\n\nNumber of iterations to convergence: 12 \nAchieved convergence tolerance: 7.124e-06\n\nAICc(power.model)\n\n[1] 396.1723\n\n#Modeldiagnostik (in nlstools)\nplot(nlsResiduals(power.model))\n\n#Vordefinierte \"Selbststartfunktionen\"#\n?selfStart\nlogistic.model <- nls(ABUND~SSlogis(AREA, Asym, xmid, scal), data = loyn)\nsummary(logistic.model)\n\n\nFormula: ABUND ~ SSlogis(AREA, Asym, xmid, scal)\n\nParameters:\n     Estimate Std. Error t value Pr(>|t|)    \nAsym   31.306      2.207  14.182  < 2e-16 ***\nxmid    6.501      2.278   2.854  0.00614 ** \nscal    9.880      3.152   3.135  0.00280 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.274 on 53 degrees of freedom\n\nNumber of iterations to convergence: 8 \nAchieved convergence tolerance: 4.371e-06\n\nAICc(logistic.model)\n\n[1] 386.8643\n\n#Modeldiagnostik (in nlstools)\nplot(nlsResiduals(logistic.model))\n\n#Visualisierung\nplot(ABUND~AREA, data = loyn)\npar(mfrow = c(1, 1))\nxv <- seq(0, 2000, 0.01)\n\n# 1. Potenzfunktion\nyv1 <- predict(power.model, list(AREA = xv))\nlines(xv, yv1, col = \"green\")\n\n# 2. Logistische Funktion\nyv2 <- predict(logistic.model, list(AREA = xv))\nlines(xv, yv2, col = \"blue\")\n\n#Visualisierung II\nplot(ABUND~log10(AREA), data = loyn)\npar(mfrow = c(1, 1))\n\n# 1. Potenzfunktion\nyv1 <- predict(power.model, list(AREA = xv))\nlines(log10(xv), yv1, col = \"green\")\n\n# 2. Logistische Funktion\nyv2 <- predict(logistic.model, list(AREA = xv))\nlines(log10(xv), yv2, col = \"blue\")\n\n#Model seletkion zwischen den nicht-lineraen Modelen\ncand.models<-list()\ncand.models[[1]] <- power.model\ncand.models[[2]] <- logistic.model\n\nModnames <- c(\"Power\", \"Logistic\")\n\naictab(cand.set = cand.models, modnames = Modnames)\n\n\nModel selection based on AICc:\n\n         K   AICc Delta_AICc AICcWt Cum.Wt      LL\nLogistic 4 386.86       0.00   0.99   0.99 -189.04\nPower    3 396.17       9.31   0.01   1.00 -194.86\n\n\n\n\n\nAbbildung 21.6: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 21.7: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 21.8: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 21.9: Generierter Plot"
  },
  {
    "objectID": "stat1-4/Statistik4_Demo.html#smoother",
    "href": "stat1-4/Statistik4_Demo.html#smoother",
    "title": "Stat4: Demo",
    "section": "Smoother",
    "text": "Smoother\n\nloyn$log_AREA<-log10(loyn$AREA)       \nplot(ABUND~log_AREA, data = loyn)\nlines(lowess(loyn$log_AREA, loyn$ABUND, f = 0.25), lwd = 2, col = \"red\")\nlines(lowess(loyn$log_AREA, loyn$ABUND, f = 0.5), lwd = 2, col = \"blue\")\nlines(lowess(loyn$log_AREA, loyn$ABUND, f = 1), lwd = 2, col = \"green\")\n\n\n\n\nAbbildung 21.10: Generierter Plot"
  },
  {
    "objectID": "stat1-4/Statistik4_Demo.html#gams",
    "href": "stat1-4/Statistik4_Demo.html#gams",
    "title": "Stat4: Demo",
    "section": "GAMs",
    "text": "GAMs\n\nif(!require(mgcv)){install.packages(\"mgcv\")}\nlibrary(mgcv)\n\ngam.1 <- gam(ABUND~s(log_AREA), data = loyn)\ngam.1\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nABUND ~ s(log_AREA)\n\nEstimated degrees of freedom:\n2.88  total = 3.88 \n\nGCV score: 52.145     \n\nsummary(gam.1)\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nABUND ~ s(log_AREA)\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  19.5143     0.9309   20.96   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n              edf Ref.df     F p-value    \ns(log_AREA) 2.884  3.628 21.14  <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.579   Deviance explained = 60.1%\nGCV = 52.145  Scale est. = 48.529    n = 56\n\nplot(loyn$log_AREA, loyn$ABUND, pch = 16)\nxv <- seq(-1,4, by = 0.1)\nyv <- predict(gam.1, list(log_AREA = xv))\nlines(xv, yv, lwd = 2, col = \"red\")\n\nAICc(gam.1)\n\n[1] 383.2109\n\nsummary(gam.1)\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nABUND ~ s(log_AREA)\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  19.5143     0.9309   20.96   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n              edf Ref.df     F p-value    \ns(log_AREA) 2.884  3.628 21.14  <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.579   Deviance explained = 60.1%\nGCV = 52.145  Scale est. = 48.529    n = 56\n\n\n\n\n\nAbbildung 21.11: Generierter Plot"
  },
  {
    "objectID": "stat1-4/Statistik4_Uebung.html",
    "href": "stat1-4/Statistik4_Uebung.html",
    "title": "Stat4: Übung",
    "section": "",
    "text": "Datensatz Curonian_Spit.csv\nDieser enthält gemittelte Pflanzenartenzahlen (Species.richness) von geschachtelten Plots (Vegetationsaufnahmen) der Pflanzengesellschaft LolioCynosuretum im Nationalpark Kurische Nehrung (Russland) auf Flächengrössen (Area) von 0.0001 bis 900 m².\nErmittelt den funktionellen Zusammenhang (das beste Modell), der die Zunahme der Artenzahl mit der Flächengrösse am besten beschreibt.Berücksichtigt dabei mindestens die Potenzfunktion (power function, die logarithmische Funktion (logarithmic function,und eine Funktion mit Sättigung (saturation, asymptote) eurer Wahl."
  },
  {
    "objectID": "stat1-4/Statistik4_Uebung.html#aufgabe-4.2n-logistische-regression-natwis",
    "href": "stat1-4/Statistik4_Uebung.html#aufgabe-4.2n-logistische-regression-natwis",
    "title": "Stat4: Übung",
    "section": "Aufgabe 4.2N: Logistische Regression (NatWis)",
    "text": "Aufgabe 4.2N: Logistische Regression (NatWis)\nDatensatz polis.csv\nDer Datensatz polis.csv beschreibt für 19 Inseln im Golf von Kalifornien, ob Eidechsen der Gattung Uta vorkommen (presence/absence: PA) in Abhängigkeit von der Form der Inseln (Verhältnis Umfang zu Fläche: RATIO).\nBitte prüft mit einer logistischen Regression, ob und ggf. wie die Inselform die Präsenz der Eidechsen beinflusst"
  },
  {
    "objectID": "stat1-4/Statistik4_Uebung.html#aufgabe-4.2s-multiple-logistische-regression-sozwis",
    "href": "stat1-4/Statistik4_Uebung.html#aufgabe-4.2s-multiple-logistische-regression-sozwis",
    "title": "Stat4: Übung",
    "section": "Aufgabe 4.2S: Multiple logistische Regression (SozWis)",
    "text": "Aufgabe 4.2S: Multiple logistische Regression (SozWis)\nDatensatz Datensatz_novanimal_Uebung_Statistik4.2.csv\nFührt mit dem Datensatz der Gästebefragung eine logistische Regression durch. Kann der Mensabesuch durch die sozioökonomischen Variablen (Alter, Geschlecht, Hochschulzugehörigkeit), wahrgenommener Fleischkonsum und Umwelteinstellung vorhergesagt werden?"
  },
  {
    "objectID": "Stat5-8.html",
    "href": "Stat5-8.html",
    "title": "Statistik 5 - 8",
    "section": "",
    "text": "Statistik 6\nStatistik 6 führt in multivariat-deskriptive Methoden ein, die dazu dienen Datensätze mit multiplen abhängigen und multiplen unabhängigen Variablen effektiv zu analysieren. Dabei betonen Ordinationen kontinuierliche Gradienten und fokussieren auf zusammengehörende Variablen, während Cluster-Analysen Diskontinuitäten betonen und auf zusammengehörende Beobachtungen fokussieren. Es folgt eine konzeptionelle Einführung in die Idee von Ordinationen als einer Technik der deskriptiven Statistik, die Strukturen in multivariaten Datensätzen via Dimensionsreduktion visualisiert. Das Prinzip und die praktische Implementierung wird detailliert am Beispiel der Hauptkomponentenanalyse (PCA) erklärt. Danach folgen kurze Einführungen in weitere Ordinationstechniken für besondere Fälle, welche bestimmte Limitierungen der PCA überwinden, namentlich CA, DCA und NMDS.\n\n\nStatistik 7\nIn Statistik 7 beschäftigen wir uns zunächst damit, wie wir Ordinationsdiagramme informativer gestalten können, etwa durch die Beschriftung der Beobachtunge, post-hoc-Projektion der Prädiktorvariablen oder Response surfaces. Während wir bislang mit «unconstrained» Ordinationen gearbeitet haben, welche die Gesamtvariabilität in den Beobachtungen visualisieren, beschränken die jeweiligen «constrained»-Varianten derselben Ordinationsmethoden die Betrachtung auf den Teil der Variabilität, welcher durch eine Linearkombination der berücksichtigen Prädiktoren erklärt werden kann. Wir beschäftigen uns im Detail mit der Redundanz-Analyse (RDA), der «constrained»-Variante der PCA und gehen einen kompletten analytischen Ablauf mit Aufbereitung, Interpretation und Visualisierung der Ergebnisse am Beispiel eines gemeinschaftsökologischen Datensatzes (Fischgesellschaften und Umweltfaktoren im Jura-Fluss Doubs) durch\n\n\nStatistik 8\nIn Statistik 8 lernen die Studierenden Clusteranalysen/Klassifikationen als eine den Ordinationen komplementäre Technik der deskriptiven Statistik multivariater Datensätze kennen. Es gibt Partitionierungen (ohne Hierarchie), divisive und agglomerative Clusteranalysen (die jeweils eine Hierarchie produzieren). Etwas genauer gehen wir auf die k-means Clusteranalyse (eine Partitionierung) und eine Reihe von agglomerativen Clusterverfahren ein. Hierbei hat das gewählte Distanzmass und der Modus für die sukzessive Fusion von Clustern einen grossen Einfluss auf das Endergebnis. Wir besprechen ferner, wie man die Ergebnisse von Clusteranalysen adäquat visualisieren und mit anderen statistischen Prozeduren kombinieren kann. Im Abschluss von Statistik 8 werden wir dann die an den acht Statistiktagen behandelten Verfahren noch einmal rückblickend betrachten und thematisieren, welches Verfahren wann gewählt werden sollte. Ebenfalls ist Platz, um den adäquaten Ablauf statistischer Analysen vom Einlesen der Daten bis zur Verschriftlichung der Ergebnisse, einschliesslich der verschiedenen zu treffenden Entscheidungen, zu thematisieren.\n\n\n\n\n\n\n\n   \n     \n     \n       Sortieren nach\n       Voreinstellung\n         \n          Datum - Datum (aufsteigend)\n        \n         \n          Datum - Neueste\n        \n         \n          Titel\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitel\n\n\nDatum\n\n\nLesson\n\n\nThema\n\n\n\n\n\n\nStat5: Demo\n\n\n2022-11-14\n\n\nStat5\n\n\nVon linearen Modellen zu GLMMs\n\n\n\n\nStat5: Übung\n\n\n2022-11-14\n\n\nStat5\n\n\nVon linearen Modellen zu GLMMs\n\n\n\n\nStat6: Demo\n\n\n2022-11-15\n\n\nStat6\n\n\nEinführung in “multivariate” Methoden\n\n\n\n\nStat6: Übung\n\n\n2022-11-15\n\n\nStat6\n\n\nEinführung in “multivariate” Methoden\n\n\n\n\nStat7: Demo\n\n\n2022-11-21\n\n\nStat7\n\n\nOrdinationen II\n\n\n\n\nStat7: Übung\n\n\n2022-11-21\n\n\nStat7\n\n\nOrdinationen II\n\n\n\n\nStat8: Demo\n\n\n2022-11-22\n\n\nStat8\n\n\nClusteranalysen\n\n\n\n\nStat8: Übung\n\n\n2022-11-22\n\n\nStat8\n\n\nClusteranalysen\n\n\n\n\n\n\nKeine Treffer"
  },
  {
    "objectID": "stat5-8/Statistik5_Demo.html#split-plot-anova",
    "href": "stat5-8/Statistik5_Demo.html#split-plot-anova",
    "title": "Stat5: Demo",
    "section": "Split-plot ANOVA",
    "text": "Split-plot ANOVA\nBased on Logan (2010), Chapter 14\n\nspf <- read.delim(\"data/spf.csv\", sep = \";\") \nspf.aov <- aov(Reaktion~Signal * Messung + Error(VP), data = spf)\nsummary(spf.aov)\n\n\nError: VP\n          Df Sum Sq Mean Sq F value Pr(>F)\nSignal     1  3.125   3.125       2  0.207\nResiduals  6  9.375   1.562               \n\nError: Within\n               Df Sum Sq Mean Sq F value   Pr(>F)    \nMessung         3 194.50   64.83  127.89 2.52e-12 ***\nSignal:Messung  3  19.37    6.46   12.74 0.000105 ***\nResiduals      18   9.13    0.51                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ninteraction.plot(spf$Messung, spf$Signal, spf$Reaktion)\n\n# nun als LMM\nif(!require(nlme)){install.packages(\"nlme\")}\nlibrary(nlme)\n\n# mit random intercept (VP) und random slope (Messung)\nspf.lme.1 <- lme(Reaktion~Signal * Messung, random = ~Messung | VP, data = spf)\n# nur random intercept\nspf.lme.2 <- lme(Reaktion~Signal * Messung, random = ~1 | VP, data = spf)\n\nanova(spf.lme.1)\n\n               numDF denDF   F-value p-value\n(Intercept)        1    18 1488.1631  <.0001\nSignal             1     6    2.0808  0.1993\nMessung            3    18   70.7887  <.0001\nSignal:Messung     3    18   11.8592  0.0002\n\nanova(spf.lme.2)\n\n               numDF denDF  F-value p-value\n(Intercept)        1    18 591.6800  <.0001\nSignal             1     6   2.0000  0.2070\nMessung            3    18 127.8904  <.0001\nSignal:Messung     3    18  12.7397  0.0001\n\nsummary(spf.lme.1)\n\nLinear mixed-effects model fit by REML\n  Data: spf \n       AIC      BIC    logLik\n  97.63924 120.0223 -29.81962\n\nRandom effects:\n Formula: ~Messung | VP\n Structure: General positive-definite, Log-Cholesky parametrization\n            StdDev    Corr                \n(Intercept) 1.0801385 (Intr) MssnH2 MssnH3\nMessungH2   0.6455527 -0.717              \nMessungH3   0.6455528 -0.837  0.600       \nMessungH4   1.3229024 -0.816  0.390  0.878\nResidual    0.2886126                     \n\nFixed effects:  Reaktion ~ Signal * Messung \n                        Value Std.Error DF   t-value p-value\n(Intercept)              3.75 0.5590162 18  6.708213  0.0000\nSignalvisuell           -2.00 0.7905683  6 -2.529826  0.0447\nMessungH2                0.25 0.3818811 18  0.654654  0.5210\nMessungH3                3.25 0.3818811 18  8.510501  0.0000\nMessungH4                4.25 0.6922184 18  6.139681  0.0000\nSignalvisuell:MessungH2  1.00 0.5400614 18  1.851641  0.0806\nSignalvisuell:MessungH3  0.50 0.5400615 18  0.925821  0.3668\nSignalvisuell:MessungH4  4.00 0.9789446 18  4.086033  0.0007\n Correlation: \n                        (Intr) Sgnlvs MssnH2 MssnH3 MssnH4 Sg:MH2 Sg:MH3\nSignalvisuell           -0.707                                          \nMessungH2               -0.683  0.483                                   \nMessungH3               -0.781  0.552  0.571                            \nMessungH4               -0.808  0.571  0.394  0.788                     \nSignalvisuell:MessungH2  0.483 -0.683 -0.707 -0.404 -0.279              \nSignalvisuell:MessungH3  0.552 -0.781 -0.404 -0.707 -0.557  0.571       \nSignalvisuell:MessungH4  0.571 -0.808 -0.279 -0.557 -0.707  0.394  0.788\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-0.86583578 -0.26649517 -0.03263197  0.23603775  0.95285749 \n\nNumber of Observations: 32\nNumber of Groups: 8 \n\nsummary(spf.lme.2)\n\nLinear mixed-effects model fit by REML\n  Data: spf \n       AIC      BIC    logLik\n  89.64876 101.4293 -34.82438\n\nRandom effects:\n Formula: ~1 | VP\n        (Intercept)  Residual\nStdDev:   0.5137012 0.7120003\n\nFixed effects:  Reaktion ~ Signal * Messung \n                        Value Std.Error DF   t-value p-value\n(Intercept)              3.75 0.4389856 18  8.542422  0.0000\nSignalvisuell           -2.00 0.6208194  6 -3.221549  0.0181\nMessungH2                0.25 0.5034602 18  0.496564  0.6255\nMessungH3                3.25 0.5034602 18  6.455326  0.0000\nMessungH4                4.25 0.5034602 18  8.441580  0.0000\nSignalvisuell:MessungH2  1.00 0.7120003 18  1.404494  0.1772\nSignalvisuell:MessungH3  0.50 0.7120003 18  0.702247  0.4915\nSignalvisuell:MessungH4  4.00 0.7120003 18  5.617975  0.0000\n Correlation: \n                        (Intr) Sgnlvs MssnH2 MssnH3 MssnH4 Sg:MH2 Sg:MH3\nSignalvisuell           -0.707                                          \nMessungH2               -0.573  0.405                                   \nMessungH3               -0.573  0.405  0.500                            \nMessungH4               -0.573  0.405  0.500  0.500                     \nSignalvisuell:MessungH2  0.405 -0.573 -0.707 -0.354 -0.354              \nSignalvisuell:MessungH3  0.405 -0.573 -0.354 -0.707 -0.354  0.500       \nSignalvisuell:MessungH4  0.405 -0.573 -0.354 -0.354 -0.707  0.500  0.500\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-1.34519292 -0.63943480 -0.06164167  0.41510594  2.15199656 \n\nNumber of Observations: 32\nNumber of Groups: 8 \n\n\n\n\n\nAbbildung 24.1: Generierter Plot"
  },
  {
    "objectID": "stat5-8/Statistik5_Demo.html#glmm",
    "href": "stat5-8/Statistik5_Demo.html#glmm",
    "title": "Stat5: Demo",
    "section": "GLMM",
    "text": "GLMM\nBased on Zuur et al. (2009), chapter 13\n\nDeerEcervi <- read.delim(\"data/DeerEcervi.txt\", sep = \"\", stringsAsFactors = T)\n\n# Anzahl Larven hier in Presence/Absence übersetzt\nDeerEcervi$Ecervi.01 <- DeerEcervi$Ecervi\nDeerEcervi$Ecervi.01[DeerEcervi$Ecervi>0] <- 1\n\n#Numerische Geschlechtscodierung als Factor\nDeerEcervi$fSex <- as.factor(DeerEcervi$Sex)\n\nHirschlänge hier standardisiert, sonst würde der Achsenabschnitt im Modell für einen Hirsch der Länge 0 modelliert, was schlecht interpretierbar ist, jetzt ist der Achsenabschnitt für einen durschnittlich langen Hirsch\n\nDeerEcervi$CLength <- DeerEcervi$Length - mean(DeerEcervi$Length)\n\n# Zunächst als GLM\n# Interaktionen mit fFarm nicht berücksichtigt, da zu viele Freiheitsgrade verbraucht würden\nDE.glm <- glm(Ecervi.01 ~ CLength * fSex + Farm, family = binomial, data = DeerEcervi)\n\ndrop1(DE.glm, test = \"Chi\")\n\nSingle term deletions\n\nModel:\nEcervi.01 ~ CLength * fSex + Farm\n             Df Deviance     AIC     LRT  Pr(>Chi)    \n<none>            745.50  799.50                      \nFarm         23  1003.72 1011.72 258.225 < 2.2e-16 ***\nCLength:fSex  1   755.48  807.48   9.984  0.001579 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(DE.glm)\n\n\nCall:\nglm(formula = Ecervi.01 ~ CLength * fSex + Farm, family = binomial, \n    data = DeerEcervi)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.8400  -0.7576   0.3556   0.6431   2.2964  \n\nCoefficients:\n                Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   -1.796e+00  5.900e-01  -3.044 0.002336 ** \nCLength        4.062e-02  7.132e-03   5.695 1.24e-08 ***\nfSex2          6.280e-01  2.292e-01   2.740 0.006150 ** \nFarmAU         3.340e+00  7.841e-01   4.259 2.05e-05 ***\nFarmBA         3.510e+00  7.150e-01   4.908 9.19e-07 ***\nFarmBE         1.883e+01  6.216e+02   0.030 0.975831    \nFarmCB         3.012e+00  6.573e-01   4.583 4.58e-06 ***\nFarmCRC       -1.293e+01  2.400e+03  -0.005 0.995701    \nFarmHB        -2.364e-01  9.730e-01  -0.243 0.808045    \nFarmLN         3.831e+00  8.881e-01   4.314 1.60e-05 ***\nFarmMAN        1.046e+00  6.960e-01   1.503 0.132855    \nFarmMB         3.693e+00  8.152e-01   4.529 5.91e-06 ***\nFarmMO         9.722e-01  5.969e-01   1.629 0.103380    \nFarmNC         1.370e+00  6.904e-01   1.985 0.047169 *  \nFarmNV         2.098e+00  7.702e-01   2.725 0.006435 ** \nFarmPN         4.185e+00  8.584e-01   4.875 1.09e-06 ***\nFarmQM         3.975e+00  7.220e-01   5.506 3.68e-08 ***\nFarmRF         4.552e+00  1.050e+00   4.337 1.45e-05 ***\nFarmRN         8.706e-01  7.454e-01   1.168 0.242822    \nFarmRO         4.555e+00  9.556e-01   4.766 1.88e-06 ***\nFarmSAU       -1.545e+01  1.368e+03  -0.011 0.990986    \nFarmSE         2.785e+00  7.876e-01   3.536 0.000407 ***\nFarmTI         3.900e+00  1.166e+00   3.343 0.000828 ***\nFarmTN         3.102e+00  7.665e-01   4.046 5.21e-05 ***\nFarmVISO       3.720e+00  1.011e+00   3.679 0.000234 ***\nFarmVY         3.974e+00  1.257e+00   3.162 0.001565 ** \nCLength:fSex2  3.618e-02  1.168e-02   3.097 0.001953 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1073.1  on 825  degrees of freedom\nResidual deviance:  745.5  on 799  degrees of freedom\nAIC: 799.5\n\nNumber of Fisher Scoring iterations: 15\n\nanova(DE.glm)\n\nAnalysis of Deviance Table\n\nModel: binomial, link: logit\n\nResponse: Ecervi.01\n\nTerms added sequentially (first to last)\n\n             Df Deviance Resid. Df Resid. Dev\nNULL                           825    1073.13\nCLength       1   64.815       824    1008.31\nfSex          1    0.191       823    1008.12\nFarm         23  252.638       800     755.48\nCLength:fSex  1    9.984       799     745.50\n\n# Response curves für die einzelnen Farmen (Weibliche Tiere: fSex = \"1\" )\nplot(DeerEcervi$CLength, DeerEcervi$Ecervi.01,\n     xlab = \"Length\", ylab = \"Probability of \\\n     presence of E. cervi L1\")\n\nI <- order(DeerEcervi$CLength)\nAllFarms <- unique(DeerEcervi$Farm)\nfor (j in AllFarms){\n  mydata <- data.frame(CLength=DeerEcervi$CLength, fSex = \"1\",\n                       Farm = j)\n  n <- dim(mydata)[1]\n  if (n>10){\n    P.DE2 <- predict(DE.glm, mydata, type = \"response\")\n    lines(mydata$CLength[I], P.DE2[I])\n  }}\n\n\n\n\nAbbildung 24.2: Generierter Plot"
  },
  {
    "objectID": "stat5-8/Statistik5_Demo.html#glmm-1",
    "href": "stat5-8/Statistik5_Demo.html#glmm-1",
    "title": "Stat5: Demo",
    "section": "GLMM",
    "text": "GLMM\n\nif(!require(MASS)){install.packages(\"MASS\")}\nlibrary(MASS)\nDE.PQL <- glmmPQL(Ecervi.01 ~ CLength * fSex,\n                random = ~ 1 | Farm, family = binomial, data = DeerEcervi)\nsummary(DE.PQL)\n\nLinear mixed-effects model fit by maximum likelihood\n  Data: DeerEcervi \n  AIC BIC logLik\n   NA  NA     NA\n\nRandom effects:\n Formula: ~1 | Farm\n        (Intercept)  Residual\nStdDev:    1.462108 0.9620576\n\nVariance function:\n Structure: fixed weights\n Formula: ~invwt \nFixed effects:  Ecervi.01 ~ CLength * fSex \n                  Value Std.Error  DF  t-value p-value\n(Intercept)   0.8883697 0.3373283 799 2.633547  0.0086\nCLength       0.0378608 0.0065269 799 5.800768  0.0000\nfSex2         0.6104570 0.2137293 799 2.856216  0.0044\nCLength:fSex2 0.0350666 0.0108558 799 3.230228  0.0013\n Correlation: \n              (Intr) CLngth fSex2 \nCLength       -0.108              \nfSex2         -0.191  0.230       \nCLength:fSex2  0.092 -0.522  0.235\n\nStandardized Within-Group Residuals:\n       Min         Q1        Med         Q3        Max \n-6.3466592 -0.6387839  0.2978382  0.5218829  3.4912879 \n\nNumber of Observations: 826\nNumber of Groups: 24 \n\ng <- 0.8883697 + 0.0378608 * DeerEcervi$CLength\np.averageFarm1 <- exp(g)/(1 + exp(g))\nI <- order(DeerEcervi$CLength)  #Avoid spaghetti plot\nplot(DeerEcervi$CLength, DeerEcervi$Ecervi.01, xlab=\"Length\",\n     ylab = \"Probability of presence of E. cervi L1\")\nlines(DeerEcervi$CLength[I], p.averageFarm1[I],lwd = 3)\np.Upp <- exp(g + 1.96 * 1.462108)/(1 + exp(g + 1.96 * 1.462108))\np.Low <- exp(g - 1.96 * 1.462108)/(1 + exp(g - 1.96 * 1.462108))\nlines(DeerEcervi$CLength[I], p.Upp[I])\nlines(DeerEcervi$CLength[I], p.Low[I])\n\nif(!require(lme4)){install.packages(\"lme4\")}\nlibrary(lme4)\nDE.lme4 <- glmer(Ecervi.01 ~ CLength * fSex + (1|Farm), \n                 family = binomial, data = DeerEcervi)\nsummary(DE.lme4)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: Ecervi.01 ~ CLength * fSex + (1 | Farm)\n   Data: DeerEcervi\n\n     AIC      BIC   logLik deviance df.resid \n   832.6    856.1   -411.3    822.6      821 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-6.2678 -0.6090  0.2809  0.5022  3.4546 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n Farm   (Intercept) 2.391    1.546   \nNumber of obs: 826, groups:  Farm, 24\n\nFixed effects:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   0.938969   0.356004   2.638  0.00835 ** \nCLength       0.038964   0.006917   5.633 1.77e-08 ***\nfSex2         0.624487   0.222938   2.801  0.00509 ** \nCLength:fSex2 0.035859   0.011409   3.143  0.00167 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) CLngth fSex2 \nCLength     -0.107              \nfSex2       -0.189  0.238       \nCLngth:fSx2  0.091 -0.514  0.232\n\nif(!require(glmmML)){install.packages(\"glmmML\")}\nlibrary(glmmML)\nDE.glmmML <- glmmML(Ecervi.01 ~ CLength * fSex,\n                  cluster = Farm, family = binomial, data = DeerEcervi)\nsummary(DE.glmmML)\n\n\nCall:  glmmML(formula = Ecervi.01 ~ CLength * fSex, family = binomial,      data = DeerEcervi, cluster = Farm) \n\n                 coef se(coef)     z Pr(>|z|)\n(Intercept)   0.93968 0.357915 2.625 8.65e-03\nCLength       0.03898 0.006956 5.604 2.10e-08\nfSex2         0.62451 0.224251 2.785 5.35e-03\nCLength:fSex2 0.03586 0.011437 3.135 1.72e-03\n\nScale parameter in mixing distribution:  1.547 gaussian \nStd. Error:                              0.2975 \n\n        LR p-value for H_0: sigma = 0:  1.346e-41 \n\nResidual deviance: 822.6 on 821 degrees of freedom  AIC: 832.6 \n\n\n\n\n\nAbbildung 24.3: Generierter Plot"
  },
  {
    "objectID": "stat5-8/Statistik5_Uebung.html",
    "href": "stat5-8/Statistik5_Uebung.html",
    "title": "Stat5: Übung",
    "section": "",
    "text": "Datensatz splityield.csv\nVersuch zum Ernteertrag (yield) einer Kulturpflanze in Abhängigkeit der drei Faktoren Bewässerung (irrigated vs. control), Düngung (N, NP, P) und Aussaatdichten (low, medium, high). Es gab vier ganze Felder (block), die zwei Hälften mit den beiden Bewässerungstreatments, diese wiederum drei Drittel für die drei Saatdichten und diese schliesslich je drei Drittel für die drei Düngertreatments hatten.\n\n\n\nBestimmt das minimal adäquate Modell\nStellt die Ergebnisse da"
  },
  {
    "objectID": "stat5-8/Statistik5_Uebung.html#aufgabe-5.2-glmm",
    "href": "stat5-8/Statistik5_Uebung.html#aufgabe-5.2-glmm",
    "title": "Stat5: Übung",
    "section": "Aufgabe 5.2: GLMM",
    "text": "Aufgabe 5.2: GLMM\nDatensatz Datensatz_novanimal_Uebung_Statistik5.2.csv\nFührt mit dem novanimal Datensatz (inviduelle Daten) eine logistische Regression durch, wobei ihr die einzelnen Käufer (single campus_card holder) als weitere randomisierte Variable mitberücksichtigt. Kann der Fleischkonsum durch das Geschlecht, die Hochschulzugehörigkeit und das Alter erklärt werden? Vergleich die Ergebnisse mit der eurem multiplen logistische Modell von Aufgabe 4.2.\n\nAufgaben\n\nBestimmt das minimal adäquate Modell\nStellt die Ergebnisse dar"
  },
  {
    "objectID": "stat5-8/Statistik6_Demo.html#ordinationen-i",
    "href": "stat5-8/Statistik6_Demo.html#ordinationen-i",
    "title": "Stat6: Demo",
    "section": "Ordinationen I",
    "text": "Ordinationen I\nDemoscript als Download\n\nPCA\n\nif(!require(labdsv)){install.packages(\"labdsv\")}\nlibrary(labdsv)\n\n# Für Ordinationen benötigen wir Matrizen, nicht Data.frames\n# Generieren von Daten\nraw <- matrix(c(1, 2, 2.5, 2.5, 1, 0.5, 0, 1, 2, 4, 3, 1), nrow = 6)\ncolnames(raw) <- c(\"spec.1\", \"spec.2\")\nrownames(raw) <- c(\"r1\", \"r2\", \"r3\", \"r4\", \"r5\", \"r6\")\nraw\n\n   spec.1 spec.2\nr1    1.0      0\nr2    2.0      1\nr3    2.5      2\nr4    2.5      4\nr5    1.0      3\nr6    0.5      1\n\n# Originale Daten im zweidimensionalen Raum\nx1 <- raw[,1]\ny1 <- raw[,2]\nz <- c(rep(1:6))\n\n# Plot Abhängigkeit der Arten vom Umweltgradienten\nplot(c(x1, y1)~c(z, z), type = \"n\", axes = T, bty = \"l\", \n     las = 1, xlim = c(1,6), ylim = c(0,5),\n     xlab = \"Umweltgradient\", ylab = \"Deckung der Arten\")\npoints(x1~z, pch = 21, type = \"b\")\npoints(y1~z, pch = 16, type = \"b\")\n\n# zentrierte Daten\ncent <- scale(raw, scale = F)\nx2 <- cent[,1]\ny2 <- cent[,2]\n\n# rotierte Daten\no.pca <- pca(raw)\nx3 <- o.pca$scores[,1]\ny3 <- o.pca$scores[,2]\n\n# Visualisierung der Schritte im Ordinationsraum\nplot(c(y1, y2, y3)~c(x1, x2, x3), type = \"n\", axes = T, bty = \"l\", las = 1,\n     xlim = c(-4, 4), ylim = c(-4, 4), xlab = \"Art 1\", ylab=  \"Art 2\")\npoints(y1~x1, pch = 21, type = \"b\", col = \"green\", lwd = 2)\npoints(y2~x2, pch = 16, type = \"b\",col = \"red\", lwd = 2)\npoints(y3~x3, pch = 17, type = \"b\", col = \"blue\", lwd = 2)\n\n# Durchführung der PCA\no.pca <- pca(raw)\n\n# Koordinaten im Ordinationsraum\no.pca$scores\n\n          PC1         PC2\nr1 -1.9216223 -0.09357697\nr2 -0.6353776 -0.68143293\nr3  0.4762699 -0.80076373\nr4  2.3503705 -0.10237502\nr5  0.8895287  0.95400610\nr6 -1.1591692  0.72414255\n\n# Korrelationen der Variablen mit den Ordinationsachsen\no.pca$loadings\n\n             PC1        PC2\nspec.1 0.3491944 -0.9370503\nspec.2 0.9370503  0.3491944\n\n#Erklärte Varianz der Achsen\nE <- o.pca$sdev^2 / o.pca$totdev * 100\nE\n\n[1] 82.40009 17.59991\n\n# mit prcomp\npca.2 <- prcomp(raw, scale = F)\nsummary(pca.2)\n\nImportance of components:\n                         PC1    PC2\nStandard deviation     1.548 0.7154\nProportion of Variance 0.824 0.1760\nCumulative Proportion  0.824 1.0000\n\nplot(pca.2)\nbiplot(pca.2)\n\n# mit vegan\nif(!require(vegan)){install.packages(\"vegan\")}\nlibrary(\"vegan\")\n# Die Funktion rda führt ein PCA aus an wenn nicht Artdaten UND Umweltdaten definiert werden\npca.3 <- rda(raw, scale = FALSE)\n#scores(pca.3, display = c(\"sites\"))\n#scores(pca.3, display = c(\"species\"))\nsummary(pca.3, axes = 0)\n\n\nCall:\nrda(X = raw, scale = FALSE) \n\nPartitioning of variance:\n              Inertia Proportion\nTotal           2.908          1\nUnconstrained   2.908          1\n\nEigenvalues, and their contribution to the variance \n\nImportance of components:\n                        PC1    PC2\nEigenvalue            2.396 0.5119\nProportion Explained  0.824 0.1760\nCumulative Proportion 0.824 1.0000\n\nScaling 2 for species and site scores\n* Species are scaled proportional to eigenvalues\n* Sites are unscaled: weighted dispersion equal on all dimensions\n* General scaling constant of scores:  \n\nbiplot(pca.3)\n\n# Mit Beispieldaten aus Wildi\nif(!require(dave)){install.packages(\"dave\")}\nlibrary(dave)\ndata(sveg)\n\n\n\n\nAbbildung 25.1: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 25.2: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 25.3: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 25.4: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 25.5: Generierter Plot\n\n\n\n\n\nstr(sveg)\nsummary(sveg)\nnames(sveg)\n\n\n# PCA: Deckungen Wurzeltransformiert, cor=T erzwingt Nutzung der Korrelationsmatrix\npca.5 <- pca(sveg^0.25, cor = T)\n\n\n# Koordinaten im Ordinationsraum\npca.5$scores\n\n# Korrelationen der Variablen mit den Ordinationsachsen\npca.5$loadings\n\n\n# Erklärte Varianz der Achsen in Prozent (sdev ist die Wurzel daraus)\nE <- pca.5$sdev^2 / pca.5$totdev * 100\nE\n\n [1] 2.061885e+01 8.098205e+00 6.070537e+00 3.666650e+00 3.322363e+00\n [6] 3.128942e+00 3.003875e+00 2.634636e+00 2.605558e+00 2.449637e+00\n[11] 2.339344e+00 2.265430e+00 2.116464e+00 2.046578e+00 1.969912e+00\n[16] 1.871020e+00 1.777063e+00 1.693483e+00 1.524015e+00 1.503332e+00\n[21] 1.434245e+00 1.378271e+00 1.329404e+00 1.291336e+00 1.251895e+00\n[26] 1.186157e+00 1.109340e+00 1.068661e+00 1.044385e+00 9.891552e-01\n[31] 9.764586e-01 8.869747e-01 8.451212e-01 8.049318e-01 7.603242e-01\n[36] 7.311274e-01 6.945830e-01 6.339064e-01 6.063542e-01 5.502527e-01\n[41] 5.411059e-01 4.956931e-01 4.795188e-01 4.601244e-01 3.936176e-01\n[46] 3.477631e-01 3.402128e-01 3.165971e-01 2.951856e-01 2.728882e-01\n[51] 2.635725e-01 2.233500e-01 2.125542e-01 1.989449e-01 1.681852e-01\n[56] 1.555571e-01 1.485298e-01 1.271079e-01 9.164615e-02 7.880113e-02\n[61] 5.913306e-02 5.113452e-02 4.066351e-30\n\nE[1:5]\n\n[1] 20.618848  8.098205  6.070537  3.666650  3.322363\n\n# PCA-Plot der Lage der Beobachtungen im Ordinationsraum\nplot(pca.5$scores[,1], pca.5$scores[,2], type = \"n\", asp = 1, xlab = \"PC1\", ylab = \"PC2\")\npoints(pca.5$scores[,1], pca.5$scores[,2], pch = 18)\n\n# Subjektive Auswahl von Arten zur Darstellung\nsel.sp <- c(3, 11, 23, 39, 46, 72, 77, 96)\nsnames <- names(sveg[,sel.sp])\nsnames\n\n[1] \"Vaccinium.oxycoccos\" \"Carex.echinata\"      \"Arnica.montana\"     \n[4] \"Carex.pulicaris\"     \"Sphagnum.recurvum\"   \"Viola.palustris\"    \n[7] \"Galium.uliginosum\"   \"Stachys.officinalis\"\n\n# PCA-Plot der Korrelationen der Variablen (hier Arten) mit den Achsen (h)\nx <- pca.5$loadings[,1]\ny <- pca.5$loadings[,2]\nplot(x, y, type = \"n\", asp = 1)\narrows(0,0, x[sel.sp], y[sel.sp], length = 0.08)\ntext(x[sel.sp], y[sel.sp], snames, pos = 1, cex = 0.6)\n\n# Mit vegan\npca.6 <- rda(sveg^0.25, scale = TRUE)\n# Erklärte Varianz der Achsen\nsummary(pca.6, axes = 0)\n\n\nCall:\nrda(X = sveg^0.25, scale = TRUE) \n\nPartitioning of correlations:\n              Inertia Proportion\nTotal             119          1\nUnconstrained     119          1\n\nEigenvalues, and their contribution to the correlations \n\nImportance of components:\n                          PC1     PC2     PC3     PC4     PC5     PC6     PC7\nEigenvalue            24.5364 9.63686 7.22394 4.36331 3.95361 3.72344 3.57461\nProportion Explained   0.2062 0.08098 0.06071 0.03667 0.03322 0.03129 0.03004\nCumulative Proportion  0.2062 0.28717 0.34788 0.38454 0.41777 0.44906 0.47909\n                          PC8     PC9   PC10    PC11    PC12    PC13    PC14\nEigenvalue            3.13522 3.10061 2.9151 2.78382 2.69586 2.51859 2.43543\nProportion Explained  0.02635 0.02606 0.0245 0.02339 0.02265 0.02116 0.02047\nCumulative Proportion 0.50544 0.53150 0.5560 0.57939 0.60204 0.62320 0.64367\n                        PC15    PC16    PC17    PC18    PC19    PC20    PC21\nEigenvalue            2.3442 2.22651 2.11470 2.01524 1.81358 1.78896 1.70675\nProportion Explained  0.0197 0.01871 0.01777 0.01693 0.01524 0.01503 0.01434\nCumulative Proportion 0.6634 0.68208 0.69985 0.71679 0.73203 0.74706 0.76140\n                         PC22    PC23    PC24    PC25    PC26    PC27    PC28\nEigenvalue            1.64014 1.58199 1.53669 1.48976 1.41153 1.32011 1.27171\nProportion Explained  0.01378 0.01329 0.01291 0.01252 0.01186 0.01109 0.01069\nCumulative Proportion 0.77518 0.78848 0.80139 0.81391 0.82577 0.83687 0.84755\n                         PC29     PC30     PC31    PC32     PC33     PC34\nEigenvalue            1.24282 1.177095 1.161986 1.05550 1.005694 0.957869\nProportion Explained  0.01044 0.009892 0.009765 0.00887 0.008451 0.008049\nCumulative Proportion 0.85800 0.867887 0.877652 0.88652 0.894973 0.903022\n                          PC35     PC36     PC37     PC38     PC39     PC40\nEigenvalue            0.904786 0.870042 0.826554 0.754349 0.721562 0.654801\nProportion Explained  0.007603 0.007311 0.006946 0.006339 0.006064 0.005503\nCumulative Proportion 0.910626 0.917937 0.924883 0.931222 0.937285 0.942788\n                          PC41     PC42     PC43     PC44     PC45     PC46\nEigenvalue            0.643916 0.589875 0.570627 0.547548 0.468405 0.413838\nProportion Explained  0.005411 0.004957 0.004795 0.004601 0.003936 0.003478\nCumulative Proportion 0.948199 0.953156 0.957951 0.962552 0.966488 0.969966\n                          PC47     PC48     PC49     PC50     PC51     PC52\nEigenvalue            0.404853 0.376750 0.351271 0.324737 0.313651 0.265787\nProportion Explained  0.003402 0.003166 0.002952 0.002729 0.002636 0.002234\nCumulative Proportion 0.973368 0.976534 0.979486 0.982215 0.984851 0.987084\n                          PC53     PC54     PC55     PC56     PC57     PC58\nEigenvalue            0.252939 0.236744 0.200140 0.185113 0.176750 0.151258\nProportion Explained  0.002126 0.001989 0.001682 0.001556 0.001485 0.001271\nCumulative Proportion 0.989210 0.991199 0.992881 0.994436 0.995922 0.997193\n                           PC59     PC60      PC61      PC62\nEigenvalue            0.1090589 0.093773 0.0703683 0.0608501\nProportion Explained  0.0009165 0.000788 0.0005913 0.0005113\nCumulative Proportion 0.9981093 0.998897 0.9994887 1.0000000\n\nScaling 2 for species and site scores\n* Species are scaled proportional to eigenvalues\n* Sites are unscaled: weighted dispersion equal on all dimensions\n* General scaling constant of scores:  \n\n# PCA-Plot der Lage der Beobachtungen im Ordinationsraum\nbiplot(pca.6, display = \"sites\", type = \"points\", scaling = 1)\n# Subjektive Auswahl von Arten zur Darstellung\nsel.sp <- c(3, 11, 23, 39, 46, 72, 77, 96)\nsnames <- names(sveg[,sel.sp])\nsnames\n\n[1] \"Vaccinium.oxycoccos\" \"Carex.echinata\"      \"Arnica.montana\"     \n[4] \"Carex.pulicaris\"     \"Sphagnum.recurvum\"   \"Viola.palustris\"    \n[7] \"Galium.uliginosum\"   \"Stachys.officinalis\"\n\n# PCA-Plot der Korrelationen der Variablen (hier Arten) mit den Achsen (h)\nscores <- scores(pca.6, display = \"species\")\nx <- scores[,1]\ny <- scores[,2]\nplot(x, y, type = \"n\", asp = 1)\narrows(0,0, x[sel.sp], y[sel.sp], length = 0.08)\ntext(x[sel.sp], y[sel.sp], snames, pos = 1, cex = 0.6)\n\n# Mit angepassten Achsen\nplot(x, y, type = \"n\", asp = 1, xlim = c(-1, 1), ylim = c(-0.6, 0.6))\narrows(0,0, x[sel.sp], y[sel.sp], length = 0.08)\ntext(x[sel.sp], y[sel.sp], snames, pos = 1, cex = 0.6)\n\n\n\n\nAbbildung 25.6: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 25.7: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 25.8: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 25.9: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 25.10: Generierter Plot\n\n\n\n\n\n\nCA\n\nca.1 <- cca(sveg^0.5)\n# Arten (o) und Communities (+) plotten\nplot(ca.1)\n\n# Nur Arten plotten\nplot(ca.1, display = \"species\", type = \"points\")\n\n\n# Anteilige Varianz, die durch die ersten beiden Achsen erklärt wird\nca.1$CA$eig[1:2] / sum(ca.1$CA$eig)\n\n      CA1       CA2 \n0.1938717 0.0784178 \n\nsummary(eigenvals(ca.1))\n\nImportance of components:\n                         CA1     CA2     CA3     CA4     CA5     CA6     CA7\nEigenvalue            0.4248 0.17182 0.12995 0.09102 0.07954 0.07274 0.06705\nProportion Explained  0.1939 0.07842 0.05931 0.04154 0.03630 0.03320 0.03060\nCumulative Proportion 0.1939 0.27229 0.33160 0.37314 0.40944 0.44264 0.47324\n                          CA8     CA9    CA10    CA11    CA12    CA13   CA14\nEigenvalue            0.06245 0.05811 0.05348 0.05261 0.05133 0.04868 0.0480\nProportion Explained  0.02850 0.02652 0.02441 0.02401 0.02343 0.02222 0.0219\nCumulative Proportion 0.50174 0.52826 0.55267 0.57668 0.60010 0.62232 0.6442\n                         CA15    CA16    CA17    CA18    CA19    CA20    CA21\nEigenvalue            0.04421 0.04279 0.03913 0.03752 0.03699 0.03412 0.03309\nProportion Explained  0.02018 0.01953 0.01786 0.01712 0.01688 0.01557 0.01510\nCumulative Proportion 0.66440 0.68393 0.70179 0.71892 0.73580 0.75137 0.76647\n                         CA22    CA23    CA24    CA25    CA26    CA27    CA28\nEigenvalue            0.03253 0.03033 0.02963 0.02718 0.02621 0.02486 0.02372\nProportion Explained  0.01485 0.01384 0.01352 0.01241 0.01196 0.01135 0.01083\nCumulative Proportion 0.78132 0.79516 0.80869 0.82109 0.83305 0.84440 0.85523\n                         CA29     CA30     CA31     CA32     CA33     CA34\nEigenvalue            0.02262 0.021397 0.020274 0.018805 0.018216 0.017737\nProportion Explained  0.01032 0.009765 0.009253 0.008582 0.008314 0.008095\nCumulative Proportion 0.86555 0.875318 0.884571 0.893153 0.901467 0.909561\n                          CA35    CA36     CA37     CA38     CA39     CA40\nEigenvalue            0.016855 0.01422 0.014044 0.013002 0.011367 0.011185\nProportion Explained  0.007693 0.00649 0.006409 0.005934 0.005188 0.005105\nCumulative Proportion 0.917254 0.92374 0.930153 0.936087 0.941275 0.946379\n                          CA41     CA42     CA43     CA44     CA45     CA46\nEigenvalue            0.010417 0.010172 0.009513 0.009183 0.008162 0.007993\nProportion Explained  0.004754 0.004643 0.004342 0.004191 0.003725 0.003648\nCumulative Proportion 0.951133 0.955776 0.960118 0.964308 0.968033 0.971681\n                          CA47     CA48     CA49     CA50    CA51     CA52\nEigenvalue            0.006900 0.006684 0.006108 0.005493 0.00515 0.004995\nProportion Explained  0.003149 0.003051 0.002788 0.002507 0.00235 0.002279\nCumulative Proportion 0.974830 0.977881 0.980668 0.983176 0.98553 0.987805\n                          CA53     CA54     CA55     CA56     CA57     CA58\nEigenvalue            0.004426 0.004011 0.003517 0.003455 0.003059 0.002279\nProportion Explained  0.002020 0.001830 0.001605 0.001577 0.001396 0.001040\nCumulative Proportion 0.989825 0.991656 0.993261 0.994837 0.996233 0.997274\n                           CA59      CA60      CA61      CA62\nEigenvalue            0.0019296 0.0017784 0.0011904 0.0010752\nProportion Explained  0.0008807 0.0008116 0.0005433 0.0004907\nCumulative Proportion 0.9981544 0.9989660 0.9995093 1.0000000\n\n\n\n\n\nAbbildung 25.11: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 25.12: Generierter Plot\n\n\n\n\n\n\nDCA\n\nlibrary(vegan)\ndca.1 <- decorana(sveg, mk = 10)\nplot(dca.1, display = \"sites\", type = \"point\")\n\ndca.2 <- decorana(sveg, mk = 100)\nplot(dca.2, display = \"sites\", type = \"point\")\n\n\n\n\nAbbildung 25.13: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 25.14: Generierter Plot\n\n\n\n\n\n\nNMDS\n\n# Distanzmatrix als Start erzeugen\nmde <- vegdist(sveg, method = \"euclidean\")\n\n# Alternative mit einem für Vegetationsdaten häufig verwendeten Dissimilarity-index\nmde <- vegdist(sveg, method = \"bray\")\n\n#Z wei verschiedene NMDS-Methoden\nif(!require(MASS)){install.packages(\"MASS\")}\nlibrary(MASS)\nset.seed(1) # macht man, wenn man bei einer Wiederholung exakt die gleichen Ergebnisse will\nimds <- isoMDS(mde, k = 2)\n\ninitial  value 16.524491 \niter   5 value 12.518681\niter  10 value 12.025808\niter  10 value 12.020751\niter  10 value 12.020751\nfinal  value 12.020751 \nconverged\n\nset.seed(1)\nmmds <- metaMDS(mde, k = 2)\n\nRun 0 stress 0.1179909 \nRun 1 stress 0.1179909 \n... Procrustes: rmse 1.11122e-05  max resid 4.697213e-05 \n... Similar to previous best\nRun 2 stress 0.170918 \nRun 3 stress 0.1529993 \nRun 4 stress 0.1179909 \n... Procrustes: rmse 2.021269e-06  max resid 1.184555e-05 \n... Similar to previous best\nRun 5 stress 0.1252011 \nRun 6 stress 0.1583424 \nRun 7 stress 0.1181212 \n... Procrustes: rmse 0.006525662  max resid 0.04396629 \nRun 8 stress 0.1596312 \nRun 9 stress 0.1630026 \nRun 10 stress 0.1179909 \n... New best solution\n... Procrustes: rmse 3.475822e-06  max resid 2.360888e-05 \n... Similar to previous best\nRun 11 stress 0.1538119 \nRun 12 stress 0.1252011 \nRun 13 stress 0.1500845 \nRun 14 stress 0.1251634 \nRun 15 stress 0.1251634 \nRun 16 stress 0.1179909 \n... Procrustes: rmse 5.655652e-06  max resid 1.960818e-05 \n... Similar to previous best\nRun 17 stress 0.1179909 \n... Procrustes: rmse 7.036898e-06  max resid 2.755273e-05 \n... Similar to previous best\nRun 18 stress 0.1179909 \n... Procrustes: rmse 1.0129e-05  max resid 3.793497e-05 \n... Similar to previous best\nRun 19 stress 0.1251572 \nRun 20 stress 0.1179909 \n... Procrustes: rmse 5.011736e-06  max resid 2.261906e-05 \n... Similar to previous best\n*** Solution reached\n\nplot(imds$points)\nplot(mmds$points)\n\n#Stress = S² = Abweichung der zweidimensionalen NMDS-Lösung von der originalen Distanzmatrix\nstressplot(imds, mde)\nstressplot(mmds, mde)\n\n\n\n\nAbbildung 25.15: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 25.16: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 25.17: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 25.18: Generierter Plot"
  },
  {
    "objectID": "stat5-8/Statistik6_Uebung.html",
    "href": "stat5-8/Statistik6_Uebung.html",
    "title": "Stat6: Übung",
    "section": "",
    "text": "Datensatz: Doubs.RData\n\nLädt den Datensatz Doubs.RData mit dem folgenden Befehl ins R: load(“Doubs.RData”)\nDie Umweltvariablen findet ihr im data.frame env die Abundanzen im data.frame spe. Im data.frame fishtrait findet ihr die Vollständigen Namen der Fische\nDer Datensatz enthält Daten zum Vorkommen von Fischarten und den zugehörigen Umweltvariablen im Fluss Doubs (Jura). Es gibt 30 Probestellen (sites), an denen jeweils die Abundanzen von 27 Fischarten (auf einer Skalen von 0 bis 5) sowie 11 Umweltvariablen erhoben wurden:\n\ndfs = Distance from source (km)\nele = Elevation (m a.s.l.)\nslo = Slope (‰)\ndis = Mean annual discharge (m3 s-1)\npH = pH of water\nhar = Hardness (Ca concentration) (mg L-1)\npho = Phosphate concentration (mg L-1)\nnit = Nitrate concentration (mg L-1)\namm = Ammonium concentration (mg L-1)\noxy = Dissolved oxygen (mg L-1)\nbod = Biological oxygen demand (mg L-1)\n\nEure Aufgabe ist nun, in einem ersten Schritt eine PCA für die 11 Umweltvariablen zu rechnen. Da die einzelnen Variablen auf ganz unterschiedlichen Skalen gemessen wurden, ist dazu eine Standardisierung nötig (pca mit der Funktion rda, scale=TRUE). Überlegt, wie viele Achsen wichtig sind und für was sie jeweils stehen.\nIn einem zweiten Schritt sollen dann die vollständig unkorrelierten PCA-Achsen als Prädiktoren einer multiplen Regression zur Erklärung der Fischartenzahl (Anzahl kann z.B. kann mit dem Befehl specnumber(spe) ermittel werden) verwendet werden (wahlweise lm oder glm). Gebt das minimal adäquate Modell an und interpretiert dieses (wahlweise im frequentist oder information theoretician approach). (Wer noch mehr probieren möchte, kann zum Vergleich noch eine multiple Regression mit den Originaldaten rechnen)."
  },
  {
    "objectID": "stat5-8/Statistik7_Demo.html#ordinationen-ii",
    "href": "stat5-8/Statistik7_Demo.html#ordinationen-ii",
    "title": "Stat7: Demo",
    "section": "Ordinationen II",
    "text": "Ordinationen II\n\nDemoscript als Download\nDatensatz Doubs.RData\nFunktion triplot.rda.R\n\n\nInterpretation von Ordinationen\nWildi pp. 96 et seq.\n\n## Plot Arten\nif(!require(dave)){install.packages(\"dave\")}\nlibrary(dave)\nca <- cca(sveg^0.5)\n\n## Plot mit ausgewählten Arten\nsel.spec <- c(3, 11, 23, 31, 39, 46, 72, 77, 96)\nsnames <- names(sveg[,sel.spec])\nsnames\n\n[1] \"Vaccinium.oxycoccos\" \"Carex.echinata\"      \"Arnica.montana\"     \n[4] \"Festuca.rubra\"       \"Carex.pulicaris\"     \"Sphagnum.recurvum\"  \n[7] \"Viola.palustris\"     \"Galium.uliginosum\"   \"Stachys.officinalis\"\n\nscores <- scores(ca, display = \"species\", scaling = \"sites\")\nsx <- scores[sel.spec, 1]\nsy <- scores[sel.spec, 2]\nplot(ca, display = \"sites\", type = \"point\")\npoints(sx, sy, pch = 16)\nsnames <- make.cepnames(snames)\ntext(sx, sy, snames, pos = c(1,2,1,1,3,2,4,3,1), cex = 0.8)\n\n## Plot \"response surfaces\" in der CA\nplot(ca, display = \"sites\", type = \"point\")\nordisurf(ca, ssit$pH.peat, add = T)\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\ny ~ s(x1, x2, k = 10, bs = \"tp\", fx = FALSE)\n\nEstimated degrees of freedom:\n4.63  total = 5.63 \n\nREML score: 28.14791     \n\nplot(ca, display = \"sites\", type = \"points\")\nordisurf(ca, ssit$Waterlev.av, add = T, col = \"blue\")\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\ny ~ s(x1, x2, k = 10, bs = \"tp\", fx = FALSE)\n\nEstimated degrees of freedom:\n5.07  total = 6.07 \n\nREML score: 161.492     \n\n## Das gleiche für die DCA\ndca <- decorana(sveg)\nplot(dca, display = \"sites\", type = \"points\")\nordisurf(dca, ssit$pH.peat, add = T)\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\ny ~ s(x1, x2, k = 10, bs = \"tp\", fx = FALSE)\n\nEstimated degrees of freedom:\n2.61  total = 3.61 \n\nREML score: 29.47878     \n\nordisurf(dca, ssit$Waterlev.av, add = T, col = \"blue\")\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\ny ~ s(x1, x2, k = 10, bs = \"tp\", fx = FALSE)\n\nEstimated degrees of freedom:\n6.23  total = 7.23 \n\nREML score: 161.1293     \n\n## Das gleiche mit NMDS\nmde <- vegdist(sveg, method = \"euclidean\")\nmmds <- metaMDS(mde)\n\nRun 0 stress 0.1478603 \nRun 1 stress 0.1652625 \nRun 2 stress 0.1989437 \nRun 3 stress 0.1611976 \nRun 4 stress 0.1652625 \nRun 5 stress 0.1462813 \n... New best solution\n... Procrustes: rmse 0.02537661  max resid 0.1451257 \nRun 6 stress 0.1471495 \nRun 7 stress 0.1675598 \nRun 8 stress 0.1602715 \nRun 9 stress 0.1792628 \nRun 10 stress 0.1471495 \nRun 11 stress 0.4064439 \nRun 12 stress 0.1659453 \nRun 13 stress 0.1741626 \nRun 14 stress 0.1471495 \nRun 15 stress 0.1925595 \nRun 16 stress 0.1462959 \n... Procrustes: rmse 0.002078915  max resid 0.01276148 \nRun 17 stress 0.1471847 \nRun 18 stress 0.1476293 \nRun 19 stress 0.1864317 \nRun 20 stress 0.1766467 \n*** No convergence -- monoMDS stopping criteria:\n     1: no. of iterations >= maxit\n    13: stress ratio > sratmax\n     6: scale factor of the gradient < sfgrmin\n\nif(!require(MASS)){install.packages(\"MASS\")}\nlibrary(MASS)\nimds <- isoMDS(mde)\n\ninitial  value 21.981028 \niter   5 value 15.595142\niter  10 value 15.269201\nfinal  value 15.229997 \nconverged\n\nplot(mmds$points)\nordisurf(mmds, ssit$pH.peat, add = T)\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\ny ~ s(x1, x2, k = 10, bs = \"tp\", fx = FALSE)\n\nEstimated degrees of freedom:\n5.99  total = 6.99 \n\nREML score: 41.84463     \n\nordisurf(mmds, ssit$Waterlev.av,add = T, col = \"blue\")\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\ny ~ s(x1, x2, k = 10, bs = \"tp\", fx = FALSE)\n\nEstimated degrees of freedom:\n6.32  total = 7.32 \n\nREML score: 168.9827     \n\nplot(imds$points)\nordisurf(imds, ssit$pH.peat, add = T)\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\ny ~ s(x1, x2, k = 10, bs = \"tp\", fx = FALSE)\n\nEstimated degrees of freedom:\n7.06  total = 8.06 \n\nREML score: 37.68641     \n\nordisurf(imds, ssit$Waterlev.av, add = T, col = \"blue\")\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\ny ~ s(x1, x2, k = 10, bs = \"tp\", fx = FALSE)\n\nEstimated degrees of freedom:\n6.01  total = 7.01 \n\nREML score: 167.6801     \n\n\n\n\n\nAbbildung 27.1: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 27.2: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 27.3: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 27.4: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 27.5: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 27.6: Generierter Plot\n\n\n\n\n\n\nConstrained ordination\n\n## 5 Umweltvariablen gewählt, durch die die Ordination constrained werden soll\nssit\nsummary(ssit)\ns5 <- c(\"pH.peat\", \"P.peat\", \"Waterlev.av\", \"CEC.peat\", \"Acidity.peat\")\nssit5 <- ssit[s5]\n\ndata(sveg)\nsummary(sveg)\n\n\n## RDA = constrained PCA\nrda <- rda(sveg~., ssit5)\nplot(rda)\n\n## CCA = constrained CA\ncca <- cca(sveg~., ssit5)\nplot(cca)\n\n## Unconstrained and constrained variance\ntot <- cca$tot.chi\nconstr <- cca$CCA$tot.chi\nconstr / tot\n\n\n\nRedundancy analysis (RDA)\nMehr Details zu RDA aus Borcard et al. (Numerical ecology with R)\n\n## Datensatz Doubs\n## Doubs Datensatz in den workspace laden\nload(\"data/Doubs.RData\")  \n\n\nsummary(spe)\nsummary(env)\nsummary(spa)\n\n\n## Entfernen der Untersuchungsfläche ohne Arten\nspe <- spe[-8, ]\nenv <- env[-8, ]\nspa <- spa[-8, ]\n\n## Karten für 4 Fischarten\npar(mfrow = c(2, 2))\nplot(spa, asp = 1, col = \"brown\", cex = spe$Satr, xlab = \"x (km)\", ylab = \"y (km)\", main = \"Brown trout\")\nlines(spa, col = \"light blue\")\nplot(spa, asp = 1, col = \"brown\", cex = spe$Thth, xlab = \"x (km)\", ylab = \"y (km)\", main = \"Grayling\")\nlines(spa, col = \"light blue\")\nplot(spa, asp = 1, col = \"brown\", cex = spe$Alal, xlab = \"x (km)\", ylab = \"y (km)\", main = \"Bleak\")\nlines(spa, col = \"light blue\")\nplot(spa, asp = 1, col = \"brown\", cex = spe$Titi, xlab = \"x (km)\", ylab = \"y (km)\", main = \"Tench\")\nlines(spa, col = \"light blue\")\n\n## Set aside the variable 'dfs' (distance from the source) for \n## later use\ndfs <- env[, 1]\n## Remove the 'dfs' variable from the env data frame\nenv2 <- env[, -1]\n\n## Recode the slope variable (slo) into a factor (qualitative) \n## variable to show how these are handled in the ordinations\nslo2 <- rep(\".very_steep\", nrow(env))\nslo2[env$slo <= quantile(env$slo)[4]] <- \".steep\"\nslo2[env$slo <= quantile(env$slo)[3]] <- \".moderate\"\nslo2[env$slo <= quantile(env$slo)[2]] <- \".low\"\nslo2 <- factor(slo2, levels = c(\".low\", \".moderate\", \".steep\", \".very_steep\"))\ntable(slo2)\n\nslo2\n       .low   .moderate      .steep .very_steep \n          8           8           6           7 \n\n## Create an env3 data frame with slope as a qualitative variable\nenv3 <- env2\nenv3$slo <- slo2\n\n## Create two subsets of explanatory variables\n## Physiography (upstream-downstream gradient)\nenvtopo <- env2[, c(1 : 3)]\nnames(envtopo)\n\n[1] \"ele\" \"slo\" \"dis\"\n\n## Water quality\nenvchem <- env2[, c(4 : 10)]\nnames(envchem)\n\n[1] \"pH\"  \"har\" \"pho\" \"nit\" \"amm\" \"oxy\" \"bod\"\n\n## Hellinger-transform the species dataset\nlibrary(vegan)\nspe.hel <- decostand(spe, \"hellinger\")\n\n\n\n\nAbbildung 27.7: Generierter Plot\n\n\n\n\n\nspe.hel\n\n\n## Redundancy analysis (RDA)\n### RDA of the Hellinger-transformed fish species data, constrained\n### by all the environmental variables contained in env3\nspe.rda <- rda(spe.hel ~ ., env3) # Observe the shortcut formula\n\n\nspe.rda\nsummary(spe.rda)    # Scaling 2 (default)\n\n\n## Canonical coefficients from the rda object\ncoef(spe.rda)\n\n\n## Unadjusted R^2 und Adjusted R^2\n(R2 <- RsquareAdj(spe.rda))\n\n$r.squared\n[1] 0.7270922\n\n$adj.r.squared\n[1] 0.5224114\n\n### Triplots of the rda results (lc scores)\n### Site scores as linear combinations of the environmental variables\n## dev.new(title = \"RDA scaling 1 and 2 + lc\", width = 12, height = 6, noRStudioGD = TRUE)\npar(mfrow = c(1, 2))\n## Scaling 1\nplot(spe.rda,scaling = 1, display = c(\"sp\", \"lc\", \"cn\"), main = \"Triplot RDA spe.hel ~ env3 - scaling 1 - lc scores\")\nspe.sc1 <- scores(spe.rda, choices = 1:2, scaling = 1, display = \"sp\")\narrows(0, 0, spe.sc1[, 1] * 0.92, spe.sc1[, 2] * 0.92, length = 0, lty = 1, col = \"red\")\ntext(-0.75, 0.7, \"a\", cex = 1.5)\n## Scaling 2\nplot(spe.rda, display = c(\"sp\", \"lc\", \"cn\"), main = \"Triplot RDA spe.hel ~ env3 - scaling 2 - lc scores\")\nspe.sc2 <- scores(spe.rda, choices = 1:2, display = \"sp\")\narrows(0, 0, spe.sc2[, 1] * 0.92, spe.sc2[, 2] * 0.92,length = 0, lty = 1, col = \"red\")\ntext(-0.82, 0.55, \"b\", cex = 1.5)\n\n### Triplots of the rda results (wa scores)\n### Site scores as weighted averages (vegan's default)\n## Scaling 1 :  distance triplot\n##dev.new(title = \"RDA plot\", width = 12, height = 6, noRStudioGD = TRUE)\npar(mfrow = c(1, 2))\nplot(spe.rda, scaling = 1, main = \"Triplot RDA spe.hel ~ env3 - scaling 1 - wa scores\")\narrows(0, 0, spe.sc1[, 1] * 0.92, spe.sc1[, 2] * 0.92, length = 0, lty = 1, col = \"red\")\n## Scaling 2 (default) :  correlation triplot\nplot(spe.rda, main = \"Triplot RDA spe.hel ~ env3 - scaling 2 - wa scores\")\narrows(0, 0, spe.sc2[, 1] * 0.92, spe.sc2[, 2] * 0.92, length = 0, lty = 1, col = \"red\")\n\n## Select species with goodness-of-fit at least 0.6 in the \n## ordination plane formed by axes 1 and 2\nspe.good <- goodness(spe.rda)\nsel.sp <- which(spe.good[, 2] >= 0.6)\nsel.sp\n\nSatr Phph Chna Baba Albi Rham Legi Cyca Abbr Gyce Ruru Blbj Alal Anan \n   2    3    7   11   12   16   17   19   21   23   24   25   26   27 \n\n## Triplots with homemade function triplot.rda(), scalings 1 and 2\nsource(\"stat5-8/triplot.rda.R\")\n##dev.new(title = \"RDA plot with triplot.rda\", width = 12, height = 6, noRStudioGD = TRUE)\npar(mfrow = c(1, 2))\ntriplot.rda(spe.rda, site.sc = \"lc\", scaling = 1, cex.char2 = 0.7, pos.env = 3, \n            pos.centr = 1, mult.arrow = 1.1, mar.percent = 0.05, select.spe = sel.sp)\n\n\n-----------------------------------------------------------------------\nSite constraints (lc) selected. To obtain site scores that are weighted\nsums of species scores (default in vegan), argument site.sc must be set\nto wa.\n-----------------------------------------------------------------------\n\n\nError in if (class(mat) == \"matrix\") {: Bedingung hat Länge > 1\n\ntext(-0.92, 0.72, \"a\", cex = 2)\ntriplot.rda(spe.rda, site.sc = \"lc\", scaling = 2, cex.char2 = 0.7, pos.env = 3, \n            pos.centr = 1, mult.arrow = 1.1, mar.percent = 0.05, select.spe = sel.sp)\n\n\n-----------------------------------------------------------------------\nSite constraints (lc) selected. To obtain site scores that are weighted\nsums of species scores (default in vegan), argument site.sc must be set\nto wa.\n-----------------------------------------------------------------------\n\n\nError in if (class(mat) == \"matrix\") {: Bedingung hat Länge > 1\n\ntext(-2.82, 2, \"b\", cex = 2)\n\n## Global test of the RDA result\nanova(spe.rda, permutations = how(nperm = 999))\n\nPermutation test for rda under reduced model\nPermutation: free\nNumber of permutations: 999\n\nModel: rda(formula = spe.hel ~ ele + slo + dis + pH + har + pho + nit + amm + oxy + bod, data = env3)\n         Df Variance      F Pr(>F)    \nModel    12  0.36537 3.5523  0.001 ***\nResidual 16  0.13714                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n## Tests of all canonical axes\nanova(spe.rda, by = \"axis\", permutations = how(nperm = 999))\n\nPermutation test for rda under reduced model\nForward tests for axes\nPermutation: free\nNumber of permutations: 999\n\nModel: rda(formula = spe.hel ~ ele + slo + dis + pH + har + pho + nit + amm + oxy + bod, data = env3)\n         Df Variance       F Pr(>F)    \nRDA1      1 0.228083 26.6105  0.001 ***\nRDA2      1 0.053698  6.2649  0.003 ** \nRDA3      1 0.032119  3.7473  0.309    \nRDA4      1 0.023206  2.7074  0.762    \nRDA5      1 0.008699  1.0149  1.000    \nRDA6      1 0.007218  0.8421  1.000    \nRDA7      1 0.004869  0.5681  1.000    \nRDA8      1 0.002924  0.3412  1.000    \nRDA9      1 0.002141  0.2498  1.000    \nRDA10     1 0.001160  0.1353  1.000    \nRDA11     1 0.000914  0.1066  1.000    \nRDA12     1 0.000341  0.0397  1.000    \nResidual 16 0.137139                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n### Partial RDA: effect of water chemistry, holding physiography\n### constant\n\n## Simple syntax; X and W may be in separate tables of quantitative \n## variables\n(spechem.physio <- rda(spe.hel, envchem, envtopo))\n\nCall: rda(X = spe.hel, Y = envchem, Z = envtopo)\n\n              Inertia Proportion Rank\nTotal          0.5025     1.0000     \nConditional    0.2087     0.4152    3\nConstrained    0.1602     0.3189    7\nUnconstrained  0.1336     0.2659   18\nInertia is variance \n\nEigenvalues for constrained axes:\n   RDA1    RDA2    RDA3    RDA4    RDA5    RDA6    RDA7 \n0.09136 0.04590 0.00928 0.00625 0.00387 0.00214 0.00142 \n\nEigenvalues for unconstrained axes:\n    PC1     PC2     PC3     PC4     PC5     PC6     PC7     PC8 \n0.04643 0.02071 0.01746 0.01326 0.00975 0.00588 0.00512 0.00400 \n(Showing 8 of 18 unconstrained eigenvalues)\n\n\n\n\n\nAbbildung 27.8: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 27.9: Generierter Plot\n\n\n\n\n\nsummary(spechem.physio)\n\n\n## Formula interface; X and W variables must be in the same \n## data frame\n(spechem.physio2 <- rda(spe.hel ~ pH + har + pho + nit + amm + oxy + bod \n        + Condition(ele + slo + dis), data = env2))\n\nCall: rda(formula = spe.hel ~ pH + har + pho + nit + amm + oxy + bod +\nCondition(ele + slo + dis), data = env2)\n\n              Inertia Proportion Rank\nTotal          0.5025     1.0000     \nConditional    0.2087     0.4152    3\nConstrained    0.1602     0.3189    7\nUnconstrained  0.1336     0.2659   18\nInertia is variance \n\nEigenvalues for constrained axes:\n   RDA1    RDA2    RDA3    RDA4    RDA5    RDA6    RDA7 \n0.09136 0.04590 0.00928 0.00625 0.00387 0.00214 0.00142 \n\nEigenvalues for unconstrained axes:\n    PC1     PC2     PC3     PC4     PC5     PC6     PC7     PC8 \n0.04643 0.02071 0.01746 0.01326 0.00975 0.00588 0.00512 0.00400 \n(Showing 8 of 18 unconstrained eigenvalues)\n\n## Test of the partial RDA, using the results with the formula \n## interface to allow the tests of the axes to be run\nanova(spechem.physio2, permutations = how(nperm = 999))\n\nPermutation test for rda under reduced model\nPermutation: free\nNumber of permutations: 999\n\nModel: rda(formula = spe.hel ~ pH + har + pho + nit + amm + oxy + bod + Condition(ele + slo + dis), data = env2)\n         Df Variance      F Pr(>F)    \nModel     7  0.16023 3.0836  0.001 ***\nResidual 18  0.13362                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(spechem.physio2, permutations = how(nperm = 999), by = \"axis\")\n\nPermutation test for rda under reduced model\nForward tests for axes\nPermutation: free\nNumber of permutations: 999\n\nModel: rda(formula = spe.hel ~ pH + har + pho + nit + amm + oxy + bod + Condition(ele + slo + dis), data = env2)\n         Df Variance       F Pr(>F)    \nRDA1      1 0.091363 12.3078  0.001 ***\nRDA2      1 0.045904  6.1839  0.016 *  \nRDA3      1 0.009277  1.2497  0.962    \nRDA4      1 0.006250  0.8420  0.991    \nRDA5      1 0.003868  0.5210  0.999    \nRDA6      1 0.002145  0.2890  1.000    \nRDA7      1 0.001424  0.1919  0.995    \nResidual 18 0.133617                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n## Partial RDA triplots (with fitted site scores) \n## with function triplot.rda\n## Scaling 1\n##dev.new(title = \"Partial RDA\",width = 12, height = 6, noRStudioGD = TRUE)\npar(mfrow = c(1, 2))\ntriplot.rda(spechem.physio, site.sc = \"lc\", scaling = 1, \n            cex.char2 = 0.8, pos.env = 3, mar.percent = 0)\n\n\n-----------------------------------------------------------------------\nSite constraints (lc) selected. To obtain site scores that are weighted\nsums of species scores (default in vegan), argument site.sc must be set\nto wa.\n-----------------------------------------------------------------------\nNo factor, hence levels cannot be plotted with symbols; 'plot.centr' is set to FALSE\n\n\nError in if (class(mat) == \"matrix\") {: Bedingung hat Länge > 1\n\ntext(-0.58, 0.64, \"a\", cex = 2)\n\nError in text.default(-0.58, 0.64, \"a\", cex = 2): plot.new has not been called yet\n\n## Scaling 2\ntriplot.rda(spechem.physio, site.sc = \"lc\", scaling = 2, cex.char2 = 0.8, \n            pos.env = 3, mult.spe = 1.1, mar.percent = 0.04)\n\n\n-----------------------------------------------------------------------\nSite constraints (lc) selected. To obtain site scores that are weighted\nsums of species scores (default in vegan), argument site.sc must be set\nto wa.\n-----------------------------------------------------------------------\nNo factor, hence levels cannot be plotted with symbols; 'plot.centr' is set to FALSE\n\n\nError in if (class(mat) == \"matrix\") {: Bedingung hat Länge > 1\n\ntext(-3.34, 3.64, \"b\", cex = 2)\n\nError in text.default(-3.34, 3.64, \"b\", cex = 2): plot.new has not been called yet\n\n\n\n\nVariation partioning\n\n### Variation partitioning with two sets of explanatory variables\n\n## Explanation of fraction labels (two, three and four explanatory \n## matrices) with optional colours\npar(mfrow = c(1, 3), mar = c(1, 1, 1, 1))\nshowvarparts(2, bg = c(\"red\", \"blue\"))\nshowvarparts(3, bg = c(\"red\", \"blue\", \"yellow\"))\nshowvarparts(4, bg = c(\"red\", \"blue\", \"yellow\", \"green\"))\n\n### 1. Variation partitioning with all explanatory variables\n###    (except dfs)\n(spe.part.all <- varpart(spe.hel, envchem, envtopo))\n\n\nPartition of variance in RDA \n\nCall: varpart(Y = spe.hel, X = envchem, envtopo)\n\nExplanatory tables:\nX1:  envchem\nX2:  envtopo \n\nNo. of explanatory tables: 2 \nTotal variation (SS): 14.07 \n            Variance: 0.50251 \nNo. of observations: 29 \n\nPartition table:\n                     Df R.squared Adj.R.squared Testable\n[a+b] = X1            7   0.60579       0.47439     TRUE\n[b+c] = X2            3   0.41524       0.34507     TRUE\n[a+b+c] = X1+X2      10   0.73410       0.58638     TRUE\nIndividual fractions                                    \n[a] = X1|X2           7                 0.24131     TRUE\n[b]                   0                 0.23308    FALSE\n[c] = X2|X1           3                 0.11199     TRUE\n[d] = Residuals                         0.41362    FALSE\n---\nUse function 'rda' to test significance of fractions of interest\n\n## Plot of the partitioning results\npar(mfrow = c(1, 1))\nplot(spe.part.all, digits = 2, bg = c(\"red\", \"blue\"),\n     Xnames = c(\"Chemistry\", \"Physiography\"), \n     id.size = 0.7)\n\n\n\n\nAbbildung 27.10: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 27.11: Generierter Plot"
  },
  {
    "objectID": "stat5-8/Statistik7_Uebung.html",
    "href": "stat5-8/Statistik7_Uebung.html",
    "title": "Stat7: Übung",
    "section": "",
    "text": "Funktion triplot.rda.R\n\nMoordatensatz in library(dave) :\n\nsveg (Vegetationsdaten)\nssit (Umweltdaten)\n\nFührt eine RDA mit allen in der Vorlesung gezeigten Schritten durch und interpretiert die Ergebnisse.\nVon den Umweltvariablen entfallen x.axis & y.axis\nFür die partielle RDA und die Varianzpartitionierung bildet zwei Gruppen:\n\nPhysiographie (Waterlev.max, Waterlev.av, Waterlev.min, log.peat.lev, log slope.deg)\nChemie (alle übrigen)"
  },
  {
    "objectID": "stat5-8/Statistik8_Demo.html#k-means-clustering",
    "href": "stat5-8/Statistik8_Demo.html#k-means-clustering",
    "title": "Stat8: Demo",
    "section": "k-means clustering",
    "text": "k-means clustering\n\n# das Moordatenset aus Wildi...\nif(!require(dave)){install.packages(\"dave\")}\nlibrary(dave)\npca <- rda(sveg^0.25, scale = TRUE)\nca <- cca(sveg^0.5)\n\nkmeans.1 <- kmeans(sveg, 4)\n\n\nkmeans.1\n\n\nplot(ca, type = \"n\")\npoints(ca, display = \"sites\", col = kmeans.1[[1]])\n\nkmeans.2 <- kmeans(sveg, 3)\nplot(pca, type = \"n\")\npoints(pca, display = \"sites\", pch=19, col = kmeans.2[[1]])\n\nplot(pca, choices = c(1, 3), type = \"n\")\npoints(pca, choices = c(1, 3), display = \"sites\", pch = 19, col=kmeans.2[[1]])\n\n# k-means partitioning, 2 to 10 groups\nKM.cascade <- cascadeKM(sveg,  inf.gr = 2, sup.gr = 10, iter = 100, criterion = \"ssi\")\nsummary(KM.cascade)\n\n          Length Class  Mode     \npartition 567    -none- numeric  \nresults    18    -none- numeric  \ncriterion   1    -none- character\nsize       90    -none- numeric  \n\nKM.cascade$results\n\n      2 groups     3 groups    4 groups    5 groups     6 groups     7 groups\nSSE 1840.13571 1629.4399038 1488.296154 1378.336905 1286.5005411 1214.3219697\nssi    0.26103    0.2762098    0.361433    0.318487    0.4037824    0.4654111\n        8 groups     9 groups    10 groups\nSSE 1156.7314935 1101.5523810 1053.1476190\nssi    0.4051907    0.4991592    0.5099825\n\nKM.cascade$partition\n\n    2 groups 3 groups 4 groups 5 groups 6 groups 7 groups 8 groups 9 groups\n501        2        1        4        3        6        7        1        3\n502        2        1        3        2        3        2        4        4\n503        2        1        4        3        6        7        1        3\n504        2        1        4        3        6        7        1        3\n505        2        1        3        2        3        2        4        4\n506        2        1        4        3        3        2        4        4\n507        2        1        4        3        6        7        1        3\n508        2        1        4        3        6        7        1        3\n509        2        1        4        3        6        7        1        3\n510        2        1        3        2        1        1        5        1\n511        2        1        4        3        6        7        1        3\n512        2        1        3        2        3        2        4        4\n513        2        1        3        2        1        1        5        5\n514        2        1        3        2        3        2        4        4\n515        2        1        3        2        3        2        4        4\n516        2        1        3        2        3        2        4        4\n517        2        1        3        2        1        1        5        1\n518        1        3        1        4        4        3        6        2\n519        2        1        3        2        3        2        4        4\n520        2        1        3        2        1        1        5        1\n521        2        1        4        3        6        7        1        1\n522        2        1        3        2        3        2        4        4\n523        1        3        1        4        4        3        6        2\n524        2        1        4        3        6        7        1        1\n525        2        1        4        3        6        7        1        1\n526        1        2        2        4        4        3        6        2\n527        2        1        3        2        1        1        5        5\n528        2        1        3        2        1        1        5        5\n529        2        1        3        2        1        1        5        5\n530        2        1        3        2        1        1        5        1\n531        1        3        1        5        2        5        8        6\n532        2        1        3        2        1        1        5        5\n533        2        1        3        2        1        1        5        1\n534        2        3        1        5        2        5        8        6\n535        1        3        1        5        2        5        8        6\n536        2        1        3        2        1        1        5        5\n537        2        1        3        2        1        1        5        5\n538        1        3        1        4        4        3        6        2\n539        1        2        2        1        5        4        3        7\n540        2        1        4        3        6        7        1        3\n541        1        2        2        1        5        4        2        9\n542        1        2        2        1        5        4        3        7\n543        1        2        2        1        5        4        2        9\n544        2        3        1        5        2        5        8        6\n545        1        3        1        4        4        3        6        2\n546        1        3        1        5        2        5        8        6\n547        2        3        1        5        2        5        8        5\n548        1        2        2        1        5        4        2        9\n549        1        3        1        5        2        5        8        6\n550        1        3        1        5        2        5        8        6\n551        1        2        2        4        4        6        7        8\n552        1        2        2        1        5        4        2        9\n553        1        2        2        1        5        4        7        8\n554        1        2        2        4        4        6        7        8\n555        1        2        2        4        4        6        7        8\n556        1        2        2        1        5        4        2        9\n557        1        2        2        4        4        6        7        8\n558        1        2        2        4        4        6        7        8\n559        1        2        2        4        4        3        6        2\n560        1        2        2        4        4        3        6        2\n561        1        3        1        4        4        3        6        2\n562        1        2        2        1        5        4        3        7\n563        1        2        2        4        4        6        7        8\n    10 groups\n501        10\n502         1\n503        10\n504        10\n505         1\n506         1\n507        10\n508        10\n509        10\n510         7\n511         6\n512         1\n513         9\n514         1\n515         1\n516         1\n517         7\n518         6\n519         1\n520         7\n521         7\n522         1\n523         5\n524         7\n525         7\n526         5\n527         9\n528         9\n529         9\n530         7\n531         2\n532         9\n533         7\n534         2\n535         2\n536         9\n537         9\n538         6\n539         4\n540        10\n541         3\n542         4\n543         3\n544         2\n545         5\n546         2\n547         9\n548         3\n549         2\n550         2\n551         8\n552         3\n553         8\n554         8\n555         8\n556         3\n557         8\n558         8\n559         5\n560         5\n561         5\n562         4\n563         8\n\n# k-means visualisation\nplot(KM.cascade, sortg = TRUE)\n\n\n\n\nAbbildung 30.1: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 30.2: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 30.3: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 30.4: Generierter Plot"
  },
  {
    "objectID": "stat5-8/Statistik8_Demo.html#agglomarative-clusteranalyse",
    "href": "stat5-8/Statistik8_Demo.html#agglomarative-clusteranalyse",
    "title": "Stat8: Demo",
    "section": "Agglomarative Clusteranalyse",
    "text": "Agglomarative Clusteranalyse\nmit Daten und Skripten aus Borcard et al. (2018)\n\nload(\"data/Doubs.RData\")  \n\n\n# Remove empty site 8\nspe <- spe[-8, ]\nenv <- env[-8, ]\nspa <- spa[-8, ]\nlatlong <- latlong[-8, ]"
  },
  {
    "objectID": "stat5-8/Statistik8_Demo.html#dendogramme-berechnen-und-ploten",
    "href": "stat5-8/Statistik8_Demo.html#dendogramme-berechnen-und-ploten",
    "title": "Stat8: Demo",
    "section": "Dendogramme berechnen und ploten",
    "text": "Dendogramme berechnen und ploten\n\n## Hierarchical agglomerative clustering of the species abundance \n\n# Compute matrix of chord distance among sites\nspe.norm <- decostand(spe, \"normalize\")\nspe.ch <- vegdist(spe.norm, \"euc\")\n\n# Attach site names to object of class 'dist'\nattr(spe.ch, \"Labels\") <- rownames(spe)\n\npar(mfrow = c(1, 1))\n\n# Compute single linkage agglomerative clustering\nspe.ch.single <- hclust(spe.ch, method = \"single\")\n# Plot a dendrogram using the default options\nplot(spe.ch.single, labels = rownames(spe), main = \"Chord - Single linkage\")\n\n# Compute complete-linkage agglomerative clustering\nspe.ch.complete <- hclust(spe.ch, method = \"complete\")\nplot(spe.ch.complete, labels = rownames(spe), main = \"Chord - Complete linkage\")\n\n# Compute UPGMA agglomerative clustering\nspe.ch.UPGMA <- hclust(spe.ch, method = \"average\")\nplot(spe.ch.UPGMA, labels = rownames(spe), main = \"Chord - UPGMA\")\n\n# Compute centroid clustering\nspe.ch.centroid <- hclust(spe.ch, method = \"centroid\")\nplot(spe.ch.centroid, labels = rownames(spe),  main = \"Chord - Centroid\")\n\n# Compute Ward's minimum variance clustering\nspe.ch.ward <-hclust(spe.ch, method = \"ward.D2\")\nplot(spe.ch.ward, labels = rownames(spe),  main = \"Chord - Ward\")\n\n# Compute beta-flexible clustering using cluster::agnes()\n# beta = -0.1\nspe.ch.beta1 <- agnes(spe.ch, method = \"flexible\", par.method = 0.55)\n# beta = -0.25\nspe.ch.beta2 <- agnes(spe.ch, method = \"flexible\", par.method = 0.625)\n# beta = -0.5\nspe.ch.beta3 <- agnes(spe.ch, method = \"flexible\", par.method = 0.75)\n# Change the class of agnes objects\nclass(spe.ch.beta1)\n\n[1] \"agnes\" \"twins\"\n\nspe.ch.beta1 <- as.hclust(spe.ch.beta1)\nclass(spe.ch.beta1)\n\n[1] \"hclust\"\n\nspe.ch.beta2 <- as.hclust(spe.ch.beta2)\nspe.ch.beta3 <- as.hclust(spe.ch.beta3)\n\npar(mfrow = c(2, 2))\nplot(spe.ch.beta1, labels = rownames(spe), main = \"Chord - Beta-flexible (beta=-0.1)\")\nplot(spe.ch.beta2, labels = rownames(spe), main = \"Chord - Beta-flexible (beta=-0.25)\")\nplot(spe.ch.beta3,  labels = rownames(spe),  main = \"Chord - Beta-flexible (beta=-0.5)\")\n\n# Compute Ward's minimum variance clustering\nspe.ch.ward <- hclust(spe.ch, method = \"ward.D2\")\nplot(spe.ch.ward, labels = rownames(spe), main = \"Chord - Ward\")\n\n\n\n\nAbbildung 30.5: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 30.6: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 30.7: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 30.8: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 30.9: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 30.10: Generierter Plot"
  },
  {
    "objectID": "stat5-8/Statistik8_Demo.html#cophenetic-correlations",
    "href": "stat5-8/Statistik8_Demo.html#cophenetic-correlations",
    "title": "Stat8: Demo",
    "section": "Cophenetic correlations",
    "text": "Cophenetic correlations\n\n# Single linkage clustering\nspe.ch.single.coph <- cophenetic(spe.ch.single)\ncor(spe.ch, spe.ch.single.coph)\n\n[1] 0.5015116\n\n# Complete linkage clustering\nspe.ch.comp.coph <- cophenetic(spe.ch.complete)\ncor(spe.ch, spe.ch.comp.coph)\n\n[1] 0.7567998\n\n# Average clustering\nspe.ch.UPGMA.coph <- cophenetic(spe.ch.UPGMA)\ncor(spe.ch, spe.ch.UPGMA.coph)\n\n[1] 0.8537529\n\n# Ward clustering\nspe.ch.ward.coph <- cophenetic(spe.ch.ward)\ncor(spe.ch, spe.ch.ward.coph)\n\n[1] 0.7821555\n\n# Shepard-like diagrams\npar(mfrow = c(2, 2))\nplot(spe.ch, spe.ch.single.coph,\n  xlab = \"Chord distance\", ylab = \"Cophenetic distance\",\n  asp = 1, xlim = c(0, sqrt(2)), ylim = c(0, sqrt(2)),\n  main = c(\"Single linkage\", paste(\"Cophenetic correlation =\",\n                                   round(cor(spe.ch, spe.ch.single.coph), 3))))\nabline(0, 1)\nlines(lowess(spe.ch, spe.ch.single.coph), col = \"red\")\n\nplot(spe.ch, spe.ch.comp.coph,\n  xlab = \"Chord distance\", ylab = \"Cophenetic distance\",\n  asp = 1, xlim = c(0, sqrt(2)), ylim = c(0, sqrt(2)),\n  main = c(\"Complete linkage\", paste(\"Cophenetic correlation =\",\n                                     round(cor(spe.ch, spe.ch.comp.coph), 3))))\nabline(0, 1)\nlines(lowess(spe.ch, spe.ch.comp.coph), col = \"red\")\n\nplot(spe.ch, spe.ch.UPGMA.coph,\n  xlab = \"Chord distance\", ylab = \"Cophenetic distance\",\n  asp = 1, xlim = c(0, sqrt(2)), ylim = c(0, sqrt(2)),\n  main = c(\"UPGMA\", paste(\"Cophenetic correlation =\",\n                          round( cor(spe.ch, spe.ch.UPGMA.coph), 3))))\nabline(0, 1)\nlines(lowess(spe.ch, spe.ch.UPGMA.coph), col = \"red\")\n\nplot(spe.ch, spe.ch.ward.coph,\n  xlab = \"Chord distance\", ylab = \"Cophenetic distance\",\n  asp = 1, xlim = c(0, sqrt(2)), ylim = c(0, max(spe.ch.ward$height)),\n  main = c(\"Ward\", paste(\"Cophenetic correlation =\", \n                         round(cor(spe.ch, spe.ch.ward.coph), 3))))\nabline(0, 1)\nlines(lowess(spe.ch, spe.ch.ward.coph), col = \"red\")\n\n\n\n\nAbbildung 30.11: Generierter Plot"
  },
  {
    "objectID": "stat5-8/Statistik8_Demo.html#optimale-anzahl-cluster",
    "href": "stat5-8/Statistik8_Demo.html#optimale-anzahl-cluster",
    "title": "Stat8: Demo",
    "section": "Optimale Anzahl Cluster",
    "text": "Optimale Anzahl Cluster\n\n## Select a dendrogram (Ward/chord) and apply three criteria\n## to choose the optimal number of clusters\n\n# Choose and rename the dendrogram (\"hclust\" object)\nhc <- spe.ch.ward\n# hc <- spe.ch.beta2\n# hc <- spe.ch.complete\n\npar(mfrow = c(1, 2))\n\n# Average silhouette widths (Rousseeuw quality index)\nSi <- numeric(nrow(spe))\nfor (k in 2:(nrow(spe) - 1))\n{\n  sil <- silhouette(cutree(hc, k = k), spe.ch)\n  Si[k] <- summary(sil)$avg.width\n}\n\nk.best <- which.max(Si)\nplot(1:nrow(spe), Si, type = \"h\",\n  main = \"Silhouette-optimal number of clusters\",\n  xlab = \"k (number of clusters)\", ylab = \"Average silhouette width\")\naxis(1, k.best,paste(\"optimum\", k.best, sep = \"\\n\"), col = \"red\", \n     font = 2, col.axis = \"red\")\npoints(k.best,max(Si), pch = 16, col = \"red\",cex = 1.5)\n\n# Optimal number of clusters according to matrix correlation \n# statistic (Pearson)\n\n# Homemade function grpdist from Borcard et al. (2018)\ngrpdist <- function(X)\n{\n  require(cluster)\n  veg <- as.data.frame(as.factor(X))\n  distgr <- daisy(veg, \"gower\")\n  distgr\n} \n\nkt <- data.frame(k = 1:nrow(spe), r = 0)\nfor (i in 2:(nrow(spe) - 1)) \n{\n  gr <- cutree(hc, i)\n  distgr <- grpdist(gr)\n  mt <- cor(spe.ch, distgr, method = \"pearson\")\n  kt[i, 2] <- mt\n}\n\nk.best <- which.max(kt$r)\nplot(kt$k,kt$r, type = \"h\",\n  main = \"Matrix correlation-optimal number of clusters\",\n  xlab = \"k (number of clusters)\", ylab = \"Pearson's correlation\")\naxis(1, k.best, paste(\"optimum\", k.best, sep = \"\\n\"),\n  col = \"red\", font = 2, col.axis = \"red\")\npoints(k.best, max(kt$r), pch = 16, col = \"red\", cex = 1.5)\n\n# Optimal number of clusters according as per indicator species\n# analysis (IndVal, Dufrene-Legendre; package: labdsv)\nIndVal <- numeric(nrow(spe))\nng <- numeric(nrow(spe))\nfor (k in 2:(nrow(spe) - 1))\n{\n  iva <- indval(spe, cutree(hc, k = k), numitr = 1000)\n  gr <- factor(iva$maxcls[iva$pval <= 0.05])\n  ng[k] <- length(levels(gr)) / k\n  iv <- iva$indcls[iva$pval <= 0.05]\n  IndVal[k] <- sum(iv)\n}\n\nk.best <- which.max(IndVal[ng == 1]) + 1\ncol3 <- rep(1, nrow(spe))\ncol3[ng == 1] <- 3\n\npar(mfrow = c(1, 2))\nplot(1:nrow(spe), IndVal, type = \"h\",\n  main = \"IndVal-optimal number of clusters\",\n  xlab = \"k (number of clusters)\", ylab = \"IndVal sum\", col = col3)\naxis(1,k.best,paste(\"optimum\", k.best, sep = \"\\n\"),\n  col = \"red\", font = 2, col.axis = \"red\")\n\npoints(which.max(IndVal),max(IndVal),pch = 16,col = \"red\",cex = 1.5)\ntext(28, 15.7, \"a\", cex = 1.8)\n\nplot(1:nrow(spe),ng,\n  type = \"h\",\n  xlab = \"k (number of clusters)\",\n  ylab = \"Ratio\",\n  main = \"Proportion of clusters with significant indicator species\",\n  col = col3)\naxis(1,k.best,paste(\"optimum\", k.best, sep = \"\\n\"),\n     col = \"red\", font = 2, col.axis = \"red\")\npoints(k.best,max(ng), pch = 16, col = \"red\", cex = 1.5)\ntext(28, 0.98, \"b\", cex = 1.8)\n\n\n\n\nAbbildung 30.12: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 30.13: Generierter Plot"
  },
  {
    "objectID": "stat5-8/Statistik8_Demo.html#final-dendrogram-with-the-selected-clusters",
    "href": "stat5-8/Statistik8_Demo.html#final-dendrogram-with-the-selected-clusters",
    "title": "Stat8: Demo",
    "section": "Final dendrogram with the selected clusters",
    "text": "Final dendrogram with the selected clusters\n\n# Choose the number of clusters\nk <- 4\n# Silhouette plot of the final partition\nspech.ward.g <- cutree(spe.ch.ward, k = k)\nsil <- silhouette(spech.ward.g, spe.ch)\nrownames(sil) <- row.names(spe)\n\nplot(sil, main = \"Silhouette plot - Chord - Ward\", cex.names = 0.8, col = 2:(k + 1), nmax = 100)\n\n# Reorder clusters\nif(!require(gclus)){install.packages(\"gclus\")}\nlibrary(\"gclus\")\nspe.chwo <- reorder.hclust(spe.ch.ward, spe.ch)\n\n# Plot reordered dendrogram with group labels\npar(mfrow = c(1, 1))\nplot(spe.chwo, hang = -1, xlab = \"4 groups\", ylab = \"Height\", sub = \"\",\n  main = \"Chord - Ward (reordered)\", labels = cutree(spe.chwo, k = k))\nrect.hclust(spe.chwo, k = k)\n\n# Plot the final dendrogram with group colors (RGBCMY...)\n# Fast method using the additional hcoplot() function:\n# Usage:\n# hcoplot(tree = hclust.object,\n#   diss = dissimilarity.matrix,\n#   lab = object labels (default NULL),\n#   k = nb.clusters,\n#   title = paste(\"Reordered dendrogram from\",deparse(tree$call),\n#   sep=\"\\n\"))\nsource(\"stat5-8/hcoplot.R\")\nhcoplot(spe.ch.ward, spe.ch, lab = rownames(spe), k = 4)\n\n# Plot the Ward clusters on a map of the Doubs River\n# (see Chapter 2)\nsource(\"stat5-8/drawmap.R\")\ndrawmap(xy = spa, clusters = spech.ward.g, main = \"Four Ward clusters along the Doubs River\")\n\n\n\n\nAbbildung 30.14: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 30.15: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 30.16: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 30.17: Generierter Plot"
  },
  {
    "objectID": "stat5-8/Statistik8_Demo.html#miscellaneous-graphical-outputs",
    "href": "stat5-8/Statistik8_Demo.html#miscellaneous-graphical-outputs",
    "title": "Stat8: Demo",
    "section": "Miscellaneous graphical outputs",
    "text": "Miscellaneous graphical outputs\n\n# konvertieren von \"hclust\" Objekt in ein Dendogram Objekt\ndend <- as.dendrogram(spe.ch.ward)\n\n# Heat map of the dissimilarity matrix ordered with the dendrogram\nheatmap(as.matrix(spe.ch), Rowv = dend, symm = TRUE, margin = c(3, 3))\n\n# Ordered community table\n# Species are ordered by their weighted averages on site scores.\n# Dots represent absences.\nlibrary(vegan)\nor <- vegemite(spe, spe.chwo)\n\n                                    \n      32222222222  111111     1111  \n      098762105439598765064732213481\n Icme 5432121.......................\n Abbr 54332431.....1................\n Blbj 54542432.1...1................\n Anan 54432222.....111..............\n Gyce 5555443212...11...............\n Scer 522112221...21................\n Cyca 53421321.....1111.............\n Rham 55432333.....221..............\n Legi 35432322.1...1111.............\n Alal 55555555352..322..............\n Chna 12111322.1...211..............\n Titi 53453444...1321111.21.........\n Ruru 55554555121455221..1..........\n Albi 53111123.....2341.............\n Baba 35342544.....23322.........1..\n Eslu 453423321...41111..12.1....1..\n Gogo 5544355421..242122111......1..\n Pefl 54211432....41321..12.........\n Pato 2211.222.....3344.............\n Sqce 3443242312152132232211..11.1..\n Lele 332213221...52235321.1........\n Babl .1111112...32534554555534124..\n Teso .1...........11254........23..\n Phph .1....11...13334344454544455..\n Cogo ..............1123......2123..\n Satr .1..........2.12341345555355.3\n Thth .1............11.2......2134..\n30 sites, 27 species\n\n\n\n\n\nAbbildung 30.18: Generierter Plot"
  },
  {
    "objectID": "stat5-8/Statistik8_Uebung.html",
    "href": "stat5-8/Statistik8_Uebung.html",
    "title": "Stat8: Übung",
    "section": "",
    "text": "Datensatz crime2.csv\n\nRaten von 7 Kriminalitätsformen pro 100000 Einwohner und Jahr für die Bundesstaaten der USA\n\n(a) Führt eine k-means- und eine agglomerative Clusteranalyse eurer Wahl durch.\n(b) Überlegt in beiden Fällen, wie viele Cluster sinnvoll sind (k-means z. B.visuelle Betrachtung einer PCA, agglomerative Clusteranalyse z. B. SilhouettePlot).\n(c) Abschliessend entscheidet euch für eine Clusterung und vergleicht die erhaltenen Cluster bezüglich der Kriminalitätsformen mittels ANOVA und interpretiert die Cluster entsprechend.\n\nHinweis: Wegen der sehr ungleichen Varianzen muss auf jeden Fall eine Standardisierung stattfinden, damit Distanzen zwischen den verschiedenen Kriminalitätsraten sinnvoll berechnet werden können"
  },
  {
    "objectID": "StatKons.html",
    "href": "StatKons.html",
    "title": "Statistik Konsolidierung",
    "section": "",
    "text": "Statstik Konsolidierung 1\nIn diesem Block beschäftigen wir uns mit folgenden Inhalten:\nWarum Statistik? Warum mit R? Genereller Ablauf einer statistischen Analyse \\(Chi ^{2}/\\)-Test- bzw. Fishers Test (für kategoriale Daten) t-Test (für metrische Daten)\n\n\nStatistik Konsolidierung 2\nIn Statistik Konsolidierung 2 bekommen die Studierenden eine Einführung in das Thema der Ordinationen, eine Technik der deskriptiven Statistik. Diese Methoden visualisiert die Strukturen in multivariaten Datensätzen via Dimensionsreduktion. Das Prinzip und die praktische Implementierung wird detailliert am Beispiel der Hauptkomponentenanalyse (PCA) erklärt. Danach folgen kurze Einführungen in weitere Ordinationstechniken für besondere Fälle, welche bestimmte Limitierungen der PCA überwinden, namentlich NMDS.\n\n\nStatistik Konsolidierung 3\nIn Statistik Konsolidierung 3 lernen die Studierenden die Idee, die Voraussetzungen und die praktische Anwendung „einfacher“ linearer Modelle in R. Im Fokus steht die Varianzanalyse (ANOVA) als Verallgemeinerung des t-Tests, einschliesslich post-hoc-Tests und mehrfaktorieller ANOVA. Dann geht es um die Voraussetzungen parametrischer (und nicht-parametrischer) Tests und Optionen, wenn diese verletzt sind.\n\n\nStatistik Konsolidierung 4\nIn Statistik Konsolidierung 4 kennen die Studierende alles Rund um das Thema der lineare Regressionen (inkl. nicht-lineare Regressionen). Die Studierenden bekommen eine Einführung in die generalized linear models (GLMs), eine Methode die einige wesentliche Limitierungen von linearen Modellen überwindenwerden können. Spezifisch werden wir uns die Poisson-Regressionen für Zähldaten und logistische Regression für ja/nein-Daten anschauen.\n\n\n\n\n\n\n\n   \n     \n     \n       Sortieren nach\n       Voreinstellung\n         \n          Datum - Datum (aufsteigend)\n        \n         \n          Datum - Neueste\n        \n         \n          Titel\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitel\n\n\nDatum\n\n\nLesson\n\n\nThema\n\n\n\n\n\n\nStatKons1: Demo 1\n\n\n2022-11-14\n\n\nStatKons1\n\n\nStatistik Grundlagen\n\n\n\n\nStatKons1: Open Datasets\n\n\n2022-11-14\n\n\nStatKons1\n\n\nStatistik Grundlagen\n\n\n\n\nStatKons1: Suggested Datasets\n\n\n2022-11-14\n\n\nStatKons1\n\n\nStatistik Grundlagen\n\n\n\n\nStatKons2: Demo\n\n\n2022-11-15\n\n\nStatKons2\n\n\nPCA\n\n\n\n\nStatKons3: Demo\n\n\n2022-11-21\n\n\nStatKons3\n\n\nLM\n\n\n\n\nStatKons4: Demo\n\n\n2022-11-22\n\n\nStatKons4\n\n\nGLM\n\n\n\n\n\n\nKeine Treffer"
  },
  {
    "objectID": "statKons/StatKons1_Demo_assoziationen.html#grundlagen",
    "href": "statKons/StatKons1_Demo_assoziationen.html#grundlagen",
    "title": "StatKons1: Demo 1",
    "section": "Grundlagen",
    "text": "Grundlagen\n\n#lade Packages\n\nlibrary(tidyverse)\n\n#mytheme\nmytheme <- \n  theme_classic() + \n  theme(\n    axis.line = element_line(color = \"black\"), \n    axis.text = element_text(size = 20, color = \"black\"), \n    axis.title = element_text(size = 20, color = \"black\"), \n    axis.ticks = element_line(size = 1, color = \"black\"), \n    axis.ticks.length = unit(.5, \"cm\")\n  )\n\n\n#lade Daten\n# mehr Info darüber: https://cran.r-project.org/web/packages/explore/vignettes/explore_mtcars.html\ncars <- mtcars\n\n#neue kategoriale variable\ncars %<>% \n  as_tibble() %>% # da \"nur\" data frame kann glimplse nichts damit anfangen \n  mutate(vs_cat = if_else(.$vs == 0, \"normal\", \"v-type\")) %>% \n  mutate(am_cat = if_else(am == 0, \"automatic\", \"manual\"))\n\n# betrachte die Daten\nsummary(cars)\nglimpse(cars)\n\n#Assoziation zwischen Anzahl Zylinder und Motorentyp ()\ntable(cars$vs_cat, cars$am_cat) # Achtung: sieht aus, als gäbe es weniger V-Motoren bei den handgeschalteten Autos\n\n#lass und das überprüfen\n#achtung: bei chi-square test kommt es sehr auf das format drauf an (er erwartet entweder vektoren oder eine matrix!)\n\n#exkurs um in es in ein matrix form zu bringen\nchi_sq_matrix <- xtabs(~ vs_cat + am_cat, data = as.data.frame(cars)) # in diesem Spezialfall haben wir keine Kriteriumsvariable\n\n#1.version\nchi_sq <-chisq.test(chi_sq_matrix)\n\n#2. version\nchi_sq <- chisq.test(cars$am_cat, cars$vs_cat)\n\n#resp. fisher exacter test verwenden, da 2x2 table\nfisher.test(chi_sq_matrix)\n\n#fisher exakter test\nfisher.test(cars$am_cat, cars$vs_cat)\n\n#visualisieren: kudos goes to https://mgimond.github.io/Stats-in-R/ChiSquare_test.html#3_two_factor_classification\nOP <- par(mfrow=c(1,2), \"mar\"=c(1,1,3,1))\nmosaicplot(chi_sq$observed, cex.axis =1 , main = \"Observed counts\")\nmosaicplot(chi_sq$expected, cex.axis =1 , main = \"Expected counts\\n(if class had no influence)\")\npar(OP)\n\n\n\n\nAbbildung 31.1: Generierter Plot"
  },
  {
    "objectID": "statKons/StatKons1_Demo_assoziationen.html#möglicher-text-für-ergebnisse",
    "href": "statKons/StatKons1_Demo_assoziationen.html#möglicher-text-für-ergebnisse",
    "title": "StatKons1: Demo 1",
    "section": "Möglicher Text für Ergebnisse",
    "text": "Möglicher Text für Ergebnisse\nDer \\(\\chi^2\\)-Test sagt uns, dass das Art des Motors und Art des Fahrwerks statistisch nicht zusammenhängen. Es gibt keine signifikante Unterscheide zwischen den Variablen “VS” und “AM - Transmission” (\\(\\chi^2\\)(1) = 0.348, p = 0.556. Der Fisher exacter Test bestätigt diesen Befund. Die Odds Ratio (OR) sagt uns hingegen - unter der Prämisse, dass “normale” Motoren eher mit automatischen und V-Motoren eher mit handgeschalteten Fahrwerken ausgestattet sind - dass die Chance doppelt so hoch ist, dass ein Auto mit “normalem” Motor automatisch geschaltet ist, als dies bei einem Auto mit V-Motor der Fall wäre\n\n#define dataset\ncars <- mtcars\n\n#neue kategoriale variable\ncars %<>% \n  as_tibble() %>% # da \"nur\" data frame kann glimplse nichts damit anfangen \n  mutate(vs_cat = if_else(.$vs == 0, \"normal\", \"v-type\")) %>% \n  mutate(am_cat = if_else(am == 0, \"automatic\", \"manual\"))\n\n# bei t-Test immer zuerst visualisieren: in diesem Fall Boxplot mit Variablen Getriebe (v- vs. s-motor) und Anzahl Pferdestärke\nggplot2::ggplot(cars, aes(y = hp, x = vs_cat)) +\n  stat_boxplot(geom ='errorbar', width = .25) +\n  geom_boxplot() +\n  # geom_violin()+\n  labs(x = \"\\nBauform Motor\", y = \"Pferdestärke (PS)\\n\") +\n  mytheme\n  \n#alternativ     \nboxplot(cars$hp ~ cars$vs_cat) # sieht ganz ok aus, jedoch weist die variable \"normale Motoren\" deutlich eine grössere Streuung aus -> siehe aov.1 und deren Modelgüte-Plots\n\n# Definiere Model: t-Test, wobei die AV metrisch (in unserem Fall eine Zählvariable) sein muss\nttest <- t.test(cars$hp ~ cars$vs_cat)\naov.1 <- aov(cars$hp ~ cars$vs_cat)\n\n#schaue Modellgüte an\npar(mfrow = c(2,2))\nplot(aov.1)\n\n#zeige resultate\nttest\nsummary.lm(aov.1)\n\n#wie würdet ihr nun die Ergebnisse darstellen?\n\n\n\n\nAbbildung 31.2: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 31.3: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 31.4: Generierter Plot\n\n\n\n\n\n# für mehr infos here: https://cran.r-project.org/web/packages/datasauRus/vignettes/Datasaurus.html\n\nlibrary(datasauRus)\nif(requireNamespace(\"dplyr\")){\n  suppressPackageStartupMessages(library(dplyr))\n  dt <- datasaurus_dozen %>% \n    group_by(dataset) %>% \n    summarize(\n      mean_x    = mean(x),\n      mean_y    = mean(y),\n      std_dev_x = sd(x),\n      std_dev_y = sd(y),\n      corr_x_y  = cor(x, y)\n    )\n}\n\n# check data structure\nglimpse(dt)\n\n# plot two examples  \nif(requireNamespace(\"ggplot2\")){\n  library(ggplot2)\n  \n  dt = filter(datasaurus_dozen, dataset == \"dino\" | dataset == \"slant_up\")\n  \n  ggplot(dt, aes(x=x, y=y, colour=dataset))+\n    geom_point()+\n    theme_bw() +\n    theme(legend.position = \"none\") +\n    facet_wrap(~dataset) +\n    geom_smooth(method = \"lm\", se = FALSE)\n  \n}"
  },
  {
    "objectID": "statKons/StatKons1_Demo_open_datasets.html",
    "href": "statKons/StatKons1_Demo_open_datasets.html",
    "title": "StatKons1: Open Datasets",
    "section": "",
    "text": "In diesem Dokument findet ihr verschiedene Wege und Quellen, um an Datensätze zu gelangen."
  },
  {
    "objectID": "statKons/StatKons1_Demo_open_datasets.html#in-r",
    "href": "statKons/StatKons1_Demo_open_datasets.html#in-r",
    "title": "StatKons1: Open Datasets",
    "section": "in R",
    "text": "in R\nIn R gibt es vordefinierte Datensätze, welche gut abrufbar sind. Beispiele sind:\n\nsleep\nUSAccDeaths\nUSArrests\nTitanic\n\n\ndata() # erzeugt eine Liste mit den Datensätzen, welche in R verfügbaren sind\nhead(chickwts)\n\n  weight      feed\n1    179 horsebean\n2    160 horsebean\n3    136 horsebean\n4    227 horsebean\n5    217 horsebean\n6    168 horsebean\n\nstr(chickwts)\n\n'data.frame':   71 obs. of  2 variables:\n $ weight: num  179 160 136 227 217 168 108 124 143 140 ...\n $ feed  : Factor w/ 6 levels \"casein\",\"horsebean\",..: 2 2 2 2 2 2 2 2 2 2 ..."
  },
  {
    "objectID": "statKons/StatKons1_Demo_open_datasets.html#kaggle",
    "href": "statKons/StatKons1_Demo_open_datasets.html#kaggle",
    "title": "StatKons1: Open Datasets",
    "section": "Kaggle",
    "text": "Kaggle\nAuf Kaggle findet ihr öffentlich zugängliche Datensätze. Einzig was ihr tun müsst, ist euch registrieren. Beispiele sind:\n\n911\nfoodPreferences\nS.F. salaries\nTitanic\n…\n\n\n# Load packages and data\ndata_911 <- read_delim(here(\"data\",\"911.csv\"), delim = \",\")\n\nError in here(\"data\", \"911.csv\"): could not find function \"here\"\n\nstr(data_911)\n\nError in str(data_911): object 'data_911' not found"
  },
  {
    "objectID": "statKons/StatKons1_Demo_open_datasets.html#tidytuesday",
    "href": "statKons/StatKons1_Demo_open_datasets.html#tidytuesday",
    "title": "StatKons1: Open Datasets",
    "section": "Tidytuesday",
    "text": "Tidytuesday\nTidytuesday ist eine Plattform, in der wöchentlich - jeden Dienstag - einen öffentlich zugänglichen Datensatz publiziert. Dieses Projekt ist aus der R4DS Online Learning Community und dem R for Data Science Lehrbuch hervorgegangen. Beispiele sind:\n\nWomen in the Workplace\nDairy production & Consumption\nStar Wars Survey\nGlobal Coffee Chains\nMalaria Deaths\n…\n\nDownload via Github - 1. Möglichkeit\n\nGeht zum File, welches ihr herunterladen wollt\nKlickt auf das File (.csv, .xlsx etc.), um den Inhalt innerhalb der GitHub Benutzeroberfläche anzuzeigen\n\n\n\nKlickt mit der rechten Maustaste auf den Knopf “raw”\n\n\n\n(Ziel) Speichern unter…\n\nDownload via Github - 2. Möglichkeit\n\n# Beachtet dabei, dass ihr die URL zum originalen (raw) Datensatz habt \nstar_wars <- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2018/2018-05-14/week7_starwars.csv\", locale = readr::locale(encoding = \"latin1\")) #not working yet \n\nError in open.connection(structure(4L, class = c(\"curl\", \"connection\"), conn_id = <pointer: 0x249>), : Timeout was reached: [raw.githubusercontent.com] Connection timed out after 10001 milliseconds"
  },
  {
    "objectID": "statKons/StatKons1_Demo_open_datasets.html#opendata.swiss",
    "href": "statKons/StatKons1_Demo_open_datasets.html#opendata.swiss",
    "title": "StatKons1: Open Datasets",
    "section": "opendata.swiss",
    "text": "opendata.swiss\nAuf opendata.swiss sind offene, frei verfügbare Daten der Schweizerischen Behörden zu finden. opendata.swiss ist ein gemeinsames Projekt von Bund, Kantonen, Gemeinden und weiteren Organisationen mit einem staatlichen Auftrag. Beispiele sind:\n\nStatistik der Schweizer Städte\nBevölkerung nach Stadtquartier, Herkunft, Geschlecht und Alter\nAltpapiermengen\n…"
  },
  {
    "objectID": "statKons/StatKons1_Demo_open_datasets.html#open-data-katalog-stadt-zürich",
    "href": "statKons/StatKons1_Demo_open_datasets.html#open-data-katalog-stadt-zürich",
    "title": "StatKons1: Open Datasets",
    "section": "Open Data Katalog Stadt Zürich",
    "text": "Open Data Katalog Stadt Zürich\nAuf der Seite der Stadt Zürich Open Data findet ihr verschiedene Datensätze der Stadt Zürich. Spannend daran ist, dass die veröffentlichten Daten kostenlos und zur freien - auch kommerziellen - Weiterverwendung zur Verfügung. Beispiele sind:\n\nBevölkerung nach Bildungsstand, Jahr, Alter und Geschlecht seit 1970\nLuftqualitätsmessungen\nHäufigste Hauptsprachen\n…\n\n\n# lade die Datei \"Häufigste Sprachen\"\nurlfile = \"https://data.stadt-zuerich.ch/dataset/bfs_ste_bev_hauptsprachen_top50_od3011/download/BEV301OD3011.csv\"\n\ndat_lang <- read_delim(url(urlfile), delim = \",\", col_names = T)\n\nError in open.connection(structure(5L, class = c(\"url\", \"connection\"), conn_id = <pointer: 0x24d>), : cannot open the connection to 'https://data.stadt-zuerich.ch/dataset/bfs_ste_bev_hauptsprachen_top50_od3011/download/BEV301OD3011.csv'\n\nhead(dat_lang)\n\nError in head(dat_lang): object 'dat_lang' not found"
  },
  {
    "objectID": "statKons/StatKons1_Demo_suggest_datasets.html",
    "href": "statKons/StatKons1_Demo_suggest_datasets.html",
    "title": "StatKons1: Suggested Datasets",
    "section": "",
    "text": "R data sets\n\nchickwts\niris\nTitanic (achtung hat das “table” Datenformat)\nstarwars (dplyr package)\n\nResearch Methods data sets\n\nNOVANIMAL (Kassendaten oder Gästebefragung)\nUkraine (Demoskript 3)\nIpomopsis (Demoskript 3)"
  },
  {
    "objectID": "statKons/StatKons2_Demo_PCA.html#pca-mit-mtcars",
    "href": "statKons/StatKons2_Demo_PCA.html#pca-mit-mtcars",
    "title": "StatKons2: Demo",
    "section": "PCA mit mtcars",
    "text": "PCA mit mtcars\n\n#Beispiel inspiriert von Luke Hayden: https://www.datacamp.com/community/tutorials/pca-analysis-r\n\n#Ausgangslage: viel zusammenhängende Variablen\n#Ziel: Reduktion der Variablenkomplexität\n#WICHTIG hier: Datenformat muss Wide sein! Damit die Matrixmultiplikation gemacht werden kann\n\n# lade Datei\ncars <- mtcars\n\n# Korrelationen\ncor<- cor(cars[,c(1:7,10,11)])\ncor[abs(cor)<.7] <- 0\ncor\n\n#definiere Datei für PCA\ncars <- mtcars[,c(1:7,10,11)]\n\n# pca\n# achtung unterschiedliche messeinheiten, wichtig es muss noch einheitlich transfomiert werden\nlibrary(FactoMineR) # siehe Beispiel hier: https://www.youtube.com/watch?v=vP4korRby0Q\no.pca <- PCA(cars, scale.unit = TRUE) # entweder korrelations oder covarianzmatrix\n\n# schaue output an\nsummary(o.pca) # generiert auch automatische plots\n\n\n# plotte das ganze\nlibrary(devtools)\ninstall_github(\"vqv/ggbiplot\")\n\n\nlibrary(ggbiplot)\nggbiplot(o.pca,choices = c(1,2))\n\n# nehme noch die autonamen hinzu\nggbiplot(o.pca, labels=rownames(mtcars), choices = c(1,2)) # (+ mytheme) # choice gibt die axen an\n\n\n\n\nAbbildung 34.1: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 34.2: Generierter Plot"
  },
  {
    "objectID": "statKons/StatKons2_Demo_PCA.html#ca-mit-mtcars",
    "href": "statKons/StatKons2_Demo_PCA.html#ca-mit-mtcars",
    "title": "StatKons2: Demo",
    "section": "CA mit mtcars",
    "text": "CA mit mtcars\n\nlibrary(vegan)\n\n# ebenfalls mit transformierten daten\no.ca<-vegan::cca(cars)\no.ca1 <- FactoMineR::CA(cars) #blau: auots, rot: variablen\n\n# plotten (schwarz: autos, rot: variablen)\nplot(o.ca)\nsummary(o.ca)\nsummary(o.ca1)\n\n#Nur autos plotten; wieso?\nx<-o.ca$CA$u[,1]\ny<-o.ca$CA$u[,2]\nplot(x,y)\n\n#Anteilige Varianz, die durch die ersten beiden Achsen erklaert wird\no.ca$CA$eig[1:8]/sum(o.ca$CA$eig)\n\n\n\n\nAbbildung 34.3: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 34.4: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 34.5: Generierter Plot"
  },
  {
    "objectID": "statKons/StatKons2_Demo_PCA.html#nmds-mit-mtcars",
    "href": "statKons/StatKons2_Demo_PCA.html#nmds-mit-mtcars",
    "title": "StatKons2: Demo",
    "section": "NMDS mit mtcars",
    "text": "NMDS mit mtcars\n\n#Distanzmatrix als Start erzeugen\nlibrary(MASS)\n\nmde <-vegan::vegdist(cars,method=\"euclidean\")\nmdm <-vegan::vegdist(cars,method=\"manhattan\")\n\n#Zwei verschiedene NMDS-Methoden\nset.seed(1) #macht man, wenn man bei einer Wiederholung exakt die gleichen Ergebnisse will\no.mde.mass <- MASS::isoMDS(mde, k=2) # mit K = Dimensionen\no.mdm.mass <- MASS::isoMDS(mdm)\n\nset.seed(1)\no.mde.vegan <- vegan::metaMDS(mde,k=1) # scheint nicht mit 2 Dimensionen zu konvergieren\no.mdm.vegan <- vegan::metaMDS(mdm, k = 2)\n\n#plot euclidean distance\nplot(o.mde.mass$points)\nplot(o.mde.vegan$points)\n\n#plot manhattan distance\nplot(o.mdm.mass$points)\nplot(o.mdm.vegan$points)\n\n#Stress =  Abweichung der zweidimensionalen NMDS-Loesung von der originalen Distanzmatrix\nvegan::stressplot(o.mde.vegan, mde)\nvegan::stressplot(o.mde.mass, mde)\n\n\n\n\nAbbildung 34.6: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 34.7: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 34.8: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 34.9: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 34.10: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 34.11: Generierter Plot"
  },
  {
    "objectID": "statKons/StatKons2_Demo_PCA.html#pca-mit-sveg",
    "href": "statKons/StatKons2_Demo_PCA.html#pca-mit-sveg",
    "title": "StatKons2: Demo",
    "section": "PCA mit sveg",
    "text": "PCA mit sveg\n\n#Mit Beispieldaten aus Wildi (2013, 2017)\nlibrary(labdsv)\nlibrary(dave) # lade package für Daten sveg\nhead(sveg)\nstr(sveg)\n#View(sveg)\n\n#PCA-----------\n#Deckungen Wurzeltransformiert, cor=T erzwingt Nutzung der Korrelationsmatrix\no.pca <- labdsv::pca(sveg^0.25,cor=T)\no.pca2 <- stats::prcomp(sveg^0.25)\n\n#Koordinaten im Ordinationsraum => Y\nhead(o.pca$scores)\nhead(o.pca2$x)\n\n#Korrelationen der Variablen mit den Ordinationsachsen\nhead(o.pca$loadings)\nhead(o.pca2$rotation)\n\n#Erklaerte Varianz der Achsen (sdev ist die Wurzel daraus)\n# früher gabs den Befehl summary()\n# jetzt von hand: standardabweichung im quadrat/totale varianz * 100 (um prozentwerte zu bekommen)\nE<-o.pca$sdev^2/o.pca$totdev*100\nE[1:5] # erste fünf PCA\n\n#package stats funktioniert summary()\nsummary(o.pca2)\n\n#PCA-Plot der Lage der Beobachtungen im Ordinationsraum\nplot(o.pca$scores[,1],o.pca$scores[,2], type=\"n\", asp=1, xlab=\"PC1\", ylab=\"PC2\")\npoints(o.pca$scores[,1],o.pca$scores[,2],pch=18)\n\nplot(o.pca$scores[,1],o.pca$scores[,3],type=\"n\", asp=1, xlab=\"PC1\", ylab=\"PC3\")\npoints(o.pca$scores[,1],o.pca$scores[,3],pch=18)\n\n#Subjektive Auswahl von Arten zur Darstellung\nsel.sp <- c(3,11,23,39,46,72,77,96, 101, 119)\nsnames <- names(sveg[ , sel.sp])\nsnames\n\n#PCA-Plot der Korrelationen der Variablen (hier Arten) mit den Achsen\n#(hier reduction der observationen)\nx <- o.pca$loadings[,1]\ny <- o.pca$loadings[,2]\nplot(x,y,type=\"n\",asp=1)\narrows(0,0,x[sel.sp],y[sel.sp],length=0.08)\ntext(x[sel.sp],y[sel.sp],snames,pos=1,cex=0.6)\n\n# hier gehts noch zu weiteren Beispielen zu PCA's:\n# https://stats.stackexchange.com/questions/102882/steps-done-in-factor-analysis-compared-to-steps-done-in-pca/102999#102999\n# https://stats.stackexchange.com/questions/222/what-are-principal-component-scores\n# https://stats.stackexchange.com/questions/102882/steps-done-in-factor-analysis-compared-to-steps-done-in-pca/102999#102999\n\n\n\n\nAbbildung 34.12: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 34.13: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 34.14: Generierter Plot"
  },
  {
    "objectID": "statKons/StatKons2_Demo_PCA.html#pca-mit-beispiel-aus-vorlesung",
    "href": "statKons/StatKons2_Demo_PCA.html#pca-mit-beispiel-aus-vorlesung",
    "title": "StatKons2: Demo",
    "section": "PCA mit Beispiel aus Vorlesung",
    "text": "PCA mit Beispiel aus Vorlesung\n\n#Idee von Ordinationen aus Wildi p. 73-74\n\n#Für Ordinationen benötigen wir Matrizen, nicht Data.frames\n#Generieren von Daten\nraw <- matrix(c(1,2,2.5,2.5,1,0.5,0,1,2,4,3,1), nrow=6)\ncolnames(raw) <- c(\"spec.1\", \"spec.2\")\nrownames(raw) <- c(\"r1\",\"r2\",\"r3\",\"r4\",\"r5\",\"r6\")\nraw\n\n#originale Daten im zweidimensionalen Raum\nx1 <- raw[,1]\ny1 <- raw[,2]\nz <- c(rep(1:6))\n\n#Plot Abhängigkeit der Arten vom Umweltgradienten\nplot(c(x1, y1)~c(z,z), type=\"n\", axes=T, bty=\"l\", las=1, xlim=c(1,6), ylim=c(0,5),\n     xlab=\"Umweltgradient\",ylab=\"Deckung der Arten\")\npoints(x1~z, pch=21, type=\"b\")\npoints(y1~z, pch=16, type=\"b\")\n\n#zentrierte Daten\ncent <- scale(raw, scale=F)\nx2 <- cent[,1]\ny2 <- cent[,2]\n\n#rotierte Daten\no.pca <- pca(raw)\nx3 <- o.pca$scores[,1]\ny3 <- o.pca$scores[,2]\n\n#Visualisierung der Schritte im Ordinationsraum\nplot(c(y1,y2,y3)~c(x1,x2,x3), type=\"n\", axes=T, bty=\"l\", las=1, xlim=c(-4,4), \n     ylim=c(-4,4), xlab=\"Art 1\", ylab=\"Art 2\")\npoints(y1~x1, pch=21, type=\"b\", col=\"green\", lwd=2)\npoints(y2~x2, pch=16, type=\"b\",col=\"red\", lwd=2)\npoints(y3~x3, pch=17, type=\"b\", col=\"blue\", lwd=2)\n\n#zusammengefasst:-------\n\n#Durchführung der PCA\npca <- pca(raw)\n\n#Koordinaten im Ordinationsraum\npca$scores\n\n#Korrelationen der Variablen mit den Ordinationsachsen\npca$loadings\n\n#Erklärte Varianz der Achsen in Prozent\nE <- pca$sdev^2/pca$totdev*100\nE\n\n### excurs für weitere r-packages####\n\n#mit prcomp, ein weiteres Package für Ordinationen\npca.2 <- stats::prcomp(raw, scale=F)\nsummary(pca.2)\nplot(pca.2)\nbiplot(pca.2)\n\n#mit vegan, ein anderes Package für Ordinationen\npca.3 <- vegan::rda(raw, scale=FALSE) #Die Funktion rda führt ein PCA aus an wenn nicht Umwelt- und Artdaten definiert werden\n#scores(pca.3,display=c(\"sites\"))\n#scores(pca.3,display=c(\"species\"))\nsummary(pca.3, axes=0)\nbiplot(pca.3, scaling=2)\nbiplot(pca.3, scaling=\"species\")#scaling=species macht das selbe wie scaling=2\n\n\n\n\nAbbildung 34.15: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 34.16: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 34.17: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 34.18: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 34.19: Generierter Plot"
  },
  {
    "objectID": "statKons/StatKons2_Demo_PCA.html#ca-mit-sveg",
    "href": "statKons/StatKons2_Demo_PCA.html#ca-mit-sveg",
    "title": "StatKons2: Demo",
    "section": "CA mit sveg",
    "text": "CA mit sveg\n\nlibrary(vegan)\nlibrary(dave) #for the dataset sveg\nlibrary(FactoMineR)# siehe Beispiel hier: https://www.youtube.com/watch?v=vP4korRby0Q\n\n# ebenfalls mit transformierten daten\no.ca<-cca(sveg^0.5) #package vegan\no.ca1 <- CA(sveg^0.5) #package FactoMineR\n\n#Arten (o) und Communities (+) plotten\nplot(o.ca)\nsummary(o.ca1)\n\n#Nur Arten plotten\nx<-o.ca$CA$u[,1]\ny<-o.ca$CA$u[,2]\nplot(x,y)\n\n#Anteilige Varianz, die durch die ersten beiden Achsen erklaert wird\no.ca$CA$eig[1:63]/sum(o.ca$CA$eig)\n\n\n\n\nAbbildung 34.20: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 34.21: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 34.22: Generierter Plot"
  },
  {
    "objectID": "statKons/StatKons2_Demo_PCA.html#nmds-mit-sveg",
    "href": "statKons/StatKons2_Demo_PCA.html#nmds-mit-sveg",
    "title": "StatKons2: Demo",
    "section": "NMDS mit sveg",
    "text": "NMDS mit sveg\n\n#NMDS----------\n\n#Distanzmatrix als Start erzeugen\nlibrary(MASS)\nlibrary(vegan)\n\nmde <-vegdist(sveg,method=\"euclidean\")\nmdm <-vegdist(sveg,method=\"manhattan\")\n\n#Zwei verschiedene NMDS-Methoden\nset.seed(1) #macht man, wenn man bei einer Wiederholung exakt die gleichen Ergebnisse will\no.imds<-isoMDS(mde, k=2) # mit K = Dimensionen\nset.seed(1)\no.mmds<-metaMDS(mde,k=3) # scheint nicht mit 2 Dimensionen zu konvergieren\n\nplot(o.imds$points)\nplot(o.mmds$points)\n\n#Stress =  Abweichung der zweidimensionalen NMDS-Loesung von der originalen Distanzmatrix\nstressplot(o.imds,mde)\nstressplot(o.mmds,mde)"
  },
  {
    "objectID": "statKons/StatKons3_Demo_LM.html#einfaktorielle-anova",
    "href": "statKons/StatKons3_Demo_LM.html#einfaktorielle-anova",
    "title": "StatKons3: Demo",
    "section": "Einfaktorielle ANOVA",
    "text": "Einfaktorielle ANOVA\n\n# für mehr infos\n#https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/mtcars.html\n\ncars <- mtcars %>% \n    mutate(cyl = as.factor(cyl)) %>% \n    slice(-31) # lösch die 31ste Zeile\n\n#Alternativ ginge auch das\ncars[-31,]\n\n# schaue daten zuerst mal an\n#1. Responsevariable\nhist(cars$hp) # nur sinnvoll bei grossem n\nboxplot(cars$hp)\n\n\n#2. Responsevariable ~ Prediktorvariable\ntable(cars$cyl) # mögliches probel, da n's unterschiedlich gross\n\nboxplot(cars$hp ~ cars$cyl) # varianzheterogentität weniger das problem, \n# aber normalverteilung der residuen problematisch\n\n# definiere das modell für eine ein-faktorielle anova\naov.1 <- aov(log10(hp) ~ cyl, data = cars)\n\n#3. Schaue Modelgüte an\npar(mfrow = c(2,2))\nplot(aov.1)\n\n#4. Schaue output an und ordne es ein\nsummary.lm(aov.1)\n\n#5. bei meheren Kategorien wende einen post-hoc Vergleichstest an\nTukeyHSD(aov.1)\n\n#6. Ergebnisse passend darstellen\nlibrary(multcomp)\n\n#erstens die signifikanten Unterschiede mit Buchstaben versehen\nletters <- multcomp::cld(multcomp::glht(aov.1, linfct=multcomp::mcp(cyl=\"Tukey\"))) # Achtung die kategoriale\n#Variable (unsere unabhängige Variable \"cyl\") muss als Faktor\n#definiert sein z.B. as.factor()\n\n#einfachere Variante\nboxplot(hp ~ cyl, data = cars)\nmtext(letters$mcletters$Letters, at=1:3)\n\n#schönere Variante :)\nggplot(cars, aes(x = cyl, y = hp)) +\n    stat_boxplot(geom = \"errorbar\", width = .5) +\n  geom_boxplot(size = 1) +\n    annotate(\"text\", x = 1, y = 350, label = \"a\", size = 7)+\nannotate(\"text\", x = 2, y = 350, label = \"b\", size = 7)+\n  annotate(\"text\", x = 3, y = 350, label = \"c\", size = 7)\n  labs(x = \"\\nAnzahl Zylinder\", y = \"Pferdestärke\")  +\n  mytheme\n\n#Plot exportieren\nggsave(filename = \"statKons/distill-preview.png\",\n       device = \"png\") # hier kann man festlegen, was für ein Bildformat\n#exportiert werden möchte\n\n# Sind die Voraussetzungen für eine Anova verletzt, überprüfe alternative \n# nicht-parametische Tests z.B. oneway-Test mit Welch-korrektur für ungleiche\n# Varianzen (Achtung auch dieser Test hat Voraussetzungen -> siehe Skript XY)\nlibrary(rosetta)\nwelch1 <- oneway.test(hp ~ cyl, data = cars, var.equal = FALSE)\nrosetta::posthocTGH(cars$hp, cars$cyl, method = \"games-howell\")"
  },
  {
    "objectID": "statKons/StatKons3_Demo_LM.html#mehrfaktorielle-anova",
    "href": "statKons/StatKons3_Demo_LM.html#mehrfaktorielle-anova",
    "title": "StatKons3: Demo",
    "section": "Mehrfaktorielle ANOVA",
    "text": "Mehrfaktorielle ANOVA\n\n\n\n\n\nAbbildung 35.1: Generierter Plot"
  },
  {
    "objectID": "statKons/StatKons3_Demo_LM.html#einfache-regression",
    "href": "statKons/StatKons3_Demo_LM.html#einfache-regression",
    "title": "StatKons3: Demo",
    "section": "Einfache Regression",
    "text": "Einfache Regression\n\n# inspiriert von Simon Jackson: http s://drsimonj.svbtle.com/visualising-residuals\ncars <- mtcars %>% \n  #ändere die unabhängige Variable mpg in 100Km/L\n  mutate(kml = (235.214583/mpg)) # mehr Infos hier: https://www.asknumbers.com/mpg-to-L100km.aspx\n  # %>%  # klone data set\n  # slice(-31) # # lösche Maserrati und schaue nochmals Modelfit an\n\n#############\n##1.Daten anschauen\n############\n\n# Zusammenhang mal anschauen\n# Achtung kml = 100km pro Liter \nplot(hp ~ kml, data = cars)\n\n# Responsevariable anschauen\nboxplot(cars$hp)\n\n# Korrelationen uv + av anschauen\n# Reihenfolge spielt hier keine Rolle, wieso?\ncor(cars$kml, cars$hp) # hängen stark zusammen\n\n###################\n#2. Modell definieren: einfache regression\n##################\nmodel <- lm(hp ~ kml, data = cars)\nsummary.lm(model)\n\n###############\n#3.Modeldiagnostik und ggf. Anpassungen ans Modell oder ähnliches\n###############\n\n# semi schöne Ergebnisse\nlibrary(ggfortify)\nggplot2::autoplot(model) + mytheme # gitb einige Extremwerte => was tun? (Eingabe/Einlesen \n#überprüfen, Transformation, Extremwerte nur ausschliessen mit guter Begründung)\n\n# erzeuge vorhergesagte Werte und Residualwerte\ncars$predicted <- predict(model)   # bilde neue Variable mit geschätzten y-Werten\ncars$residuals <- residuals(model)\n\n# schaue es dir an, sieht man gut was die Residuen sind\nd <- cars %>%  \n    dplyr::select(hp, kml, predicted, residuals)\n\n# schauen wir es uns an\nhead(d, 4)\n\n#visualisiere residuen\nggplot(d, aes(x = kml, y = hp)) +\n  # verbinde beobachtete werte mit vorausgesagte werte\n  geom_segment(aes(xend = kml, yend = predicted)) + \n  geom_point() + # Plot the actual points\n  geom_point(aes(y = predicted), shape = 4) + # plot geschätzten y-Werten\n  # geom_line(aes(y = predicted), color = \"lightgrey\") # alternativ code\n  geom_smooth(method = \"lm\", se = FALSE, color = \"lightgrey\") +\n  # Farbe wird hier zu den redisuen gemapped, abs(residuals) wegen negativen zahlen  \n  geom_point(aes(color = abs(residuals))) + \n  # Colors to use here (für mehrere farben verwende color_gradient2)\n  scale_color_continuous(low = \"blue\", high = \"red\") +  \n  scale_x_continuous(limits = c(0, 40)) +\n  scale_y_continuous(limits = c(0, 300)) +\n  guides(color = \"none\") +  # Color legende entfernen\n  labs(x = \"\\nVerbraucht in Liter pro 100km\", y = \"Motorleistung in PS\\n\") +\n  mytheme\n\n##########\n#4. plotte Ergebnis\n##########\nggplot(d, aes(x = kml, y = hp)) +\n    geom_point(size = 4) +\n    # geom_point(aes(y = predicted), shape = 1, size = 4) +\n    # plot regression line\n    geom_smooth(method = \"lm\", se = FALSE, color = \"lightgrey\") +\n    #intercept\n    geom_line(aes(y = mean(hp)), color = \"blue\") +\n    mytheme\n\n\n\n\nAbbildung 35.2: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 35.3: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 35.4: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 35.5: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 35.6: Generierter Plot"
  },
  {
    "objectID": "statKons/StatKons3_Demo_LM.html#multiple-regression",
    "href": "statKons/StatKons3_Demo_LM.html#multiple-regression",
    "title": "StatKons3: Demo",
    "section": "Multiple regression",
    "text": "Multiple regression\n\n# Select data\ncars <- mtcars %>% \n    slice(-31) %>%\n    mutate(kml = (235.214583/mpg)) %>% \n    dplyr::select(kml, hp, wt, disp)\n\n################\n# 1. Multikollinearitüt überprüfen\n# Korrelation zwischen Prädiktoren kleiner .7\ncor <- cor(cars[, -2])\ncor[abs(cor)<0.7] <- 0  \ncor # \n\n##### info zu Variablen\n#wt = gewicht\n#disp = hubraum\n\n###############\n#2. Responsevariable + Kriteriumsvariable anschauen\n##############\n# was würdet ihr tun?\n\n############\n#3. Definiere das Model\n############\nmodel1 <- lm(hp ~ kml + wt + disp, data = cars) \nmodel2 <- lm(hp ~ kml + wt, data = cars)\nmodel3 <- lm(log10(hp) ~ kml + wt, data = cars)\n\n#############\n#4. Modeldiagnostik\n############\n\nlibrary(ggfortify)\nggplot2::autoplot(model1)\nggplot2::autoplot(model2) # besser, immernoch nicht ok => transformation? vgl. model3\nggplot2::autoplot(model3)\n\n############\n#5. Modellfit vorhersagen: wie gut sagt mein Modell meine Daten vorher\n############\n\n#es gibt 3 Mögliche Wege\n\n# gebe dir predicted values aus für model2 (für vorzeigebeispiel einfacher :)\n# gibts unterschidliche varianten die predicted values zu berechnen\n# 1. default funktion predict(model) verwenden\ncars$predicted <- predict(model2)\n\n# 2. datensatz selber zusammenstellen (nicht empfohlen): wichtig, die \n# prädiktoren müssen denselben\n# namen haben wie im Model\n# besser mit Traindata von Beginn an mehr Infos hier: https://www.r-bloggers.com/using-linear-regression-to-predict-energy-output-of-a-power-plant/\n\nnew.data <- tibble(kml = sample(seq(6.9384, 22.61, .3), 31),\n                   wt = sample(seq(1.513, 5.424, 0.01), 31),\n                   disp = sample(seq(71.1, 472.0, .1), 31)) \ncars$predicted_own <- predict(model2, newdata = new.data)\n\n# 3. train_test_split durchführen (empfohlen) muss jedoch von beginn an bereits \n# gemacht werden - Logik findet ihr hier: https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6 oder https://towardsdatascience.com/6-amateur-mistakes-ive-made-working-with-train-test-splits-916fabb421bb\n# beispiel hier: https://ijlyttle.github.io/model_cv_selection.html\ncars <- mtcars %>% \n  mutate(id = 1:nrow(.)) %>%  # für das mergen der Datensätze\n  mutate(kml = (235.214583/mpg)) %>% \n  dplyr::select(kml, hp, wt, disp, id)\n  \ntrain_data <- cars %>% \n  dplyr::sample_frac(.75) # für das Modellfitting\n\ntest_data  <- dplyr::anti_join(cars, train_data, by = 'id') # für den Test mit predict\n\n# erstelle das Modell und \"trainiere\" es auf den train Datensatz\nmodel2_train <- lm(hp ~ kml + wt, data = train_data)\n\n# mit dem \"neuen\" Datensatz wird das Model überprüft ob guter Modelfit\ntrain_data$predicted_test <- predict(model2_train, newdata = test_data)\n\n# Residuen\ntrain_data$residuals <- residuals(model2_train)\nhead(train_data)\n\n#weiterführende Infos zu \"machine learning\" Idee hier: https://stat-ata-asu.github.io/MachineLearningToolbox/regression-models-fitting-them-and-evaluating-their-performance.html\n#wichtigstes Packet in dieser Hinsicht ist \"caret\": https://topepo.github.io/caret/\n#beste Philosophie ist tidymodels: https://www.tidymodels.org\n\n#----------------\n# Schnelle variante mit broom\nd <- lm(hp ~ kml + wt+ disp, data = cars) %>% \n    broom::augment()\n\nhead(d)\n\nggplot(d, aes(x = kml, y = hp)) +\n    geom_segment(aes(xend = kml, yend = .fitted), alpha = .2) +\n    geom_point(aes(color = .resid)) +\n    scale_color_gradient2(low = \"blue\", mid = \"white\", high = \"red\") +\n    guides(color = \"none\") +\n    geom_point(aes(y = .fitted), shape = 4) +\n    scale_y_continuous(limits = c(0,350)) +\n    geom_smooth(method = \"lm\", se = FALSE, color = \"lightgrey\") +\n    mytheme\n\n############\n# 6. Modellvereinfachung\n############\n\n# Varianzpartitionierung\nlibrary(hier.part)\ncars <- mtcars %>% \n  mutate(kml = (235.214583/mpg)) %>% \n  select(-mpg)\n\nnames(cars) # finde \"position\" deiner Responsevariable\n\nX = cars[, -3] # definiere all die Prädiktorvariablen im Model (minus Responsevar)\n\n# dauert ein paar sekunden\nhier.part(cars$hp, X, gof = \"Rsqu\")\n\n# alle Modelle miteinander vergleichen mit dredge Befehl: geht nur bis \n# maximal 15 Variablen\nmodel2 <- lm(hp ~ ., data = cars)\nlibrary(MuMIn)\noptions(na.action = \"na.fail\")\nallmodels <- dredge(model2)\nhead(allmodels)\n\n# Wichtigkeit der Prädiktoren\nMuMIn::importance(allmodels)\n\n# mittleres Model\navgmodel<- MuMIn::model.avg(get.models(allmodels, subset=TRUE))\nsummary(avgmodel)\n\n# adäquatest model gemäss multimodel inference\nmodel_ad <- lm(hp ~ carb + disp + wt, data = mtcars)\nsummary(model_ad)\n\n\n\n\nAbbildung 35.7: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 35.8: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 35.9: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 35.10: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 35.11: Generierter Plot"
  },
  {
    "objectID": "statKons/StatKons4_Demo_GLM.html#poisson-regression",
    "href": "statKons/StatKons4_Demo_GLM.html#poisson-regression",
    "title": "StatKons4: Demo",
    "section": "Poisson Regression",
    "text": "Poisson Regression\n\n############\n# quasipoisson regression\n############\n\ncars <- mtcars %>% \n   mutate(kml = (235.214583/mpg))\n\nglm.poisson <- glm(hp ~ kml, data = cars, family = \"poisson\")\n\nsummary(glm.poisson) # klare overdisperion\n## \n## Call:\n## glm(formula = hp ~ kml, family = \"poisson\", data = cars)\n## \n## Deviance Residuals: \n##    Min      1Q  Median      3Q     Max  \n## -6.438  -2.238  -1.159   2.457  10.576  \n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(>|z|)    \n## (Intercept) 3.894293   0.050262   77.48   <2e-16 ***\n## kml         0.081666   0.003414   23.92   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for poisson family taken to be 1)\n## \n##     Null deviance: 958.27  on 31  degrees of freedom\n## Residual deviance: 426.59  on 30  degrees of freedom\n## AIC: 645.67\n## \n## Number of Fisher Scoring iterations: 4\n\n# deshalb quasipoisson\nglm.quasipoisson <- glm(hp ~ kml, data = cars, family = quasipoisson(link = log))\n\nsummary(glm.quasipoisson)\n## \n## Call:\n## glm(formula = hp ~ kml, family = quasipoisson(link = log), data = cars)\n## \n## Deviance Residuals: \n##    Min      1Q  Median      3Q     Max  \n## -6.438  -2.238  -1.159   2.457  10.576  \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  3.89429    0.19508  19.963  < 2e-16 ***\n## kml          0.08167    0.01325   6.164 8.82e-07 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for quasipoisson family taken to be 15.06438)\n## \n##     Null deviance: 958.27  on 31  degrees of freedom\n## Residual deviance: 426.59  on 30  degrees of freedom\n## AIC: NA\n## \n## Number of Fisher Scoring iterations: 4\n\n# visualisiere\nggplot2::ggplot(cars, aes(x = kml, y = hp)) + \n    geom_point(size = 8) + \n    geom_smooth(method = \"glm\", method.args = list(family = \"poisson\"), se = F,\n                color = \"green\", size = 2) + \n    scale_x_continuous(limits = c(0,35)) + \n    scale_y_continuous(limits = c(0,400)) + \n    theme_classic()\n\n#Rücktransformation meines Outputs für ein besseres Verständnis\nglm.quasi.back <- exp(coef(glm.quasipoisson))\n\n#für ein schönes ergebnis\nglm.quasi.back %>%\n  broom::tidy() %>% \n  knitr::kable(digits = 3)\n\n\n\n\n\nnames\nx\n\n\n\n\n(Intercept)\n49.121\n\n\nkml\n1.085\n\n\n\nAbbildung 36.1: Generierter Plot\n\n\n\n#for more infos, also for posthoc tests\n#here: https://rcompanion.org/handbook/J_01.html\n\n\n\n\nAbbildung 36.2: Generierter Plot"
  },
  {
    "objectID": "statKons/StatKons4_Demo_GLM.html#logistische-regression",
    "href": "statKons/StatKons4_Demo_GLM.html#logistische-regression",
    "title": "StatKons4: Demo",
    "section": "logistische Regression",
    "text": "logistische Regression\n\n############\n# logistische regression\n############\ncars <- mtcars\n\n# erstelle das modell\nglm.binar <- glm(vs ~ hp, data = cars, family = binomial(link = logit)) \n\n#achtung Model gibt Koeffizienten als logit() zurück\nsummary(glm.binar)\n## \n## Call:\n## glm(formula = vs ~ hp, family = binomial(link = logit), data = cars)\n## \n## Deviance Residuals: \n##      Min        1Q    Median        3Q       Max  \n## -2.12148  -0.20302  -0.01598   0.51173   1.20083  \n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(>|z|)   \n## (Intercept)  8.37802    3.21593   2.605  0.00918 **\n## hp          -0.06856    0.02740  -2.502  0.01234 * \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 43.860  on 31  degrees of freedom\n## Residual deviance: 16.838  on 30  degrees of freedom\n## AIC: 20.838\n## \n## Number of Fisher Scoring iterations: 7\n\n# überprüfe das modell\ncars$predicted <- predict(glm.binar, type = \"response\")\n\n# visualisiere\nggplot(cars, aes(x = hp, y = vs)) +    \n    geom_point(size = 8) +\n    geom_point(aes(y = predicted), shape  = 1, size = 6) +\n    guides(color = \"none\") +\n    geom_smooth(method = \"glm\", method.args = list(family = 'binomial'), \n                se = FALSE,\n                size = 2) +\n    # geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n    mytheme\n\n#Modeldiagnostik (wenn nicht signifikant, dann OK)\n1 - pchisq(glm.binar$deviance,glm.binar$df.resid)  \n## [1] 0.9744718\n\n#Modellgüte (pseudo-R²)\n1 - (glm.binar$dev / glm.binar$null)  \n## [1] 0.6161072\n\n#Steilheit der Beziehung (relative Änderung der odds von x + 1 vs. x)\nexp(glm.binar$coefficients[2])\n##        hp \n## 0.9337368\n\n#LD50 (wieso negativ: weil zweiter koeffizient negative steigung hat)\nabs(glm.binar$coefficients[1]/glm.binar$coefficients[2])\n## (Intercept) \n##    122.1986\n\n# kreuztabelle (confusion matrix): fasse die ergebnisse aus predict und \n# \"gegebenheiten, realität\" zusammen\ntab1 <- table(cars$predicted>.5, cars$vs)\ndimnames(tab1) <- list(c(\"M:S-type\",\"M:V-type\"), c(\"T:S-type\", \"T:V-type\"))\ntab1 \n##          T:S-type T:V-type\n## M:S-type       15        2\n## M:V-type        3       12\n\nprop.table(tab1, 2) \n##           T:S-type  T:V-type\n## M:S-type 0.8333333 0.1428571\n## M:V-type 0.1666667 0.8571429\n#was könnt ihr daraus ablesen? Ist unser Modell genau?\n\n# Funktion die die logits in Wahrscheinlichkeiten transformiert\n# mehr infos hier: https://sebastiansauer.github.io/convert_logit2prob/\n# dies ist interessant, falls ihr mal ein kategorialer Prädiktor habt\nlogit2prob <- function(logit){\n  odds <- exp(logit)\n  prob <- odds / (1 + odds)\n  return(prob)\n}\n\n\n\n\nAbbildung 36.3: Generierter Plot"
  },
  {
    "objectID": "statKons/StatKons4_Demo_GLM.html#gams",
    "href": "statKons/StatKons4_Demo_GLM.html#gams",
    "title": "StatKons4: Demo",
    "section": "GAM’s",
    "text": "GAM’s\n\n###########\n# LOESS & GAM\n###########\n\nggplot2::ggplot(mtcars, aes(x = mpg, y = hp)) + \n  geom_point(size = 8) + \n  geom_smooth(method = \"gam\", se = F, color = \"green\", size = 2, formula = y ~ s(x, bs = \"cs\")) + \n  geom_smooth(method = \"loess\", se = F, color = \"red\", size = 2) + \n  geom_smooth(method = \"glm\", size = 2, color = \"blue\", se = F) + \n  scale_x_continuous(limits = c(0,35)) + \n    scale_y_continuous(limits = c(0,400)) + \n    mytheme\n\nggplot2::ggplot(mtcars, aes(x = mpg, y = hp)) + \n  geom_point(size = 8) + \n  geom_smooth(method = \"gam\", se = F, color = \"green\", size = 2, formula = y ~ s(x, bs = \"cs\")) + \n    # geom_smooth(method = \"loess\", se = F, color = \"red\", size = 2) + \n  geom_smooth(method = \"glm\", size = 2, color = \"grey\", se = F) + \n  scale_x_continuous(limits = c(0,35)) + \n  scale_y_continuous(limits = c(0,400)) + \n  mytheme\n\n\n\n\nAbbildung 36.4: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 36.5: Generierter Plot"
  },
  {
    "objectID": "RaumAn.html",
    "href": "RaumAn.html",
    "title": "Räumliche Analysen",
    "section": "",
    "text": "Die erste Übung zur Raumanalyse illustriert das einfache Laden und Anzeigen von Geodaten im Vektor- und Raster-Datenformat. Zusätzlich veranschaulicht die Übung den Umgang mit Koordinatensystemen sowie die Vektor-Raster-Konvertierung. Einfach erste Analysen umfassen den Spatial Join (Annotieren von Punkten mit Attributen von die Punkte einbettenden Vektordaten) sowie Puffer-Operationen. Zum Abschluss thematisiert die Übung die Aggregationsabhängigkeit räumlicher Daten durch die Illustration des Modifiable Areal Unit Problem (MAUP). Inhaltlich orientiert sich die Übung an Bodeneigenschaften für den Untersuchungsraum Schweiz."
  },
  {
    "objectID": "RaumAn.html#teil-2",
    "href": "RaumAn.html#teil-2",
    "title": "Räumliche Analysen",
    "section": "Teil 2",
    "text": "Teil 2\nIn dieser zweiten Übung wirst Du wiederum Geodatensätze verarbeiten und darstellen. Wir starten mit einem Punktdatensatz zu einem Messnetz zur Erhebung der Luftqualität in der Schweiz (Stickstoffdioxid NO2 um genau zu sein). Im Gegensatz zum Punktdatensatz zur Wasserverfügbarkeit aus der vorherigen Übung, sind die Messstellen des Messnetzes zur Luftqualität sehr unregelmässig im Raum verteilt. Trotzdem möchten wir versuchen ein kontinuierliches Raster von Luftqualitätswerten für die ganze Schweiz zu interpolieren. Wir starten mit der einfachen Interpolations-Methode Inverse Distance Weighting IDW. Danach wollen wir für den gleichen Datensatz nach dem Ansatz der nächsten Nachbarn die Thiessen Polygone konstruieren. Im zweiten Teil der Übung wollen wir Dichteverteilung untersuchen. Dabei untersuchen wir einen Datensatz mit Bewegungsdaten eines Rotmilans in der Schweiz. Mittels einer Kernel Density Estimation (KDE) berechnen wir eine kontinuierliche Dichteverteilung, über die wir eine Annäherung an das Habitat des untersuchten Greifvogels berechnen können. Bevor wir aber starten, schauen wir uns die Punktdatensätze genauer an indem wir die G-Function berechnen und plotten."
  },
  {
    "objectID": "RaumAn.html#teil-3",
    "href": "RaumAn.html#teil-3",
    "title": "Räumliche Analysen",
    "section": "Teil 3",
    "text": "Teil 3\nZum Abschluss des Themenblockes Spatial Data Science berechnen wir mit dem Moran’s I einen Index zur Berechnung der räumlichen Autokorrelation einer Choroplethenkarte. Wir verwenden nochmals die aggregierten Choroplethenkarten zur Wasserverfügbarkeit aus der ersten Übung und schauen uns an, wie stark die Werte für die Kantone und die Bezirke autokorreliert sind. Anstatt einfach eine Funktion zur Berechnung von Moran’s I aufzurufen und diese dann wie eine Black Box anzuwenden, wollen wir Formel für die Berechnung des Index in Ihre Bausteine zerlegen und diese Schritt für Schritt selber nachrechnen. So seht Ihr, wie Moran’s I wirklich funktioniert und könnte dabei erst noch die zuvor gelernten Data Science Techniken repetieren.\nAusserdem zeigen wir Euch ganz einfache Verfahren, um die bereits erstellten Karten interaktiv zu machen.\nAuf geht’s!"
  },
  {
    "objectID": "rauman/Rauman1_Uebung_A.html",
    "href": "rauman/Rauman1_Uebung_A.html",
    "title": "Rauman 1: Übung A",
    "section": "",
    "text": "Es gibt bereits eine Vielzahl von Packages um in R mit räumlichen Daten zu arbeiten, die ihrerseits wiederum auf weiteren Packages basieren (Stichwort dependencies). Für Vektordaten dominierte lange das Package sp, welches nun durch sf abgelöst wurde. Wir werden wenn immer möglich mit sf arbeiten und nur in Ausnahmefällen auf andere Packages zurück greifen.\nFür die kommenden Übungen könnt ihr folgende Packages installieren bzw. laden:"
  },
  {
    "objectID": "rauman/Rauman1_Uebung_A.html#aufgabe-1-vektor-daten-runterladen-und-importieren",
    "href": "rauman/Rauman1_Uebung_A.html#aufgabe-1-vektor-daten-runterladen-und-importieren",
    "title": "Rauman 1: Übung A",
    "section": "Aufgabe 1: Vektor Daten runterladen und importieren",
    "text": "Aufgabe 1: Vektor Daten runterladen und importieren\nLade zunächst die Datensätze unter folgenden Links herunter:\n\nkantone.gpkg\ngemeinden.gpkg\n\nEs handelt sich um Geodatensätze im Format Geopackage (“*.gpkg”), eine alternatives Datenformat zum bekannteren Format “Shapefiles”. Importiere die Datensätze wie folgt:\n\nkantone <- read_sf(\"datasets/rauman/kantone.gpkg\")\ngemeinden <- read_sf(\"datasets/rauman/gemeinden.gpkg\") \n\nSchau Dir die importierten Datensätze an. Am meisten Informationen zu sf Objekten bekommst du, wenn du dir den Datensatz in der Konsole anschaust (in dem du den Variabel-Name in der Konsole eintippst). Mit dem RStudio Viewer werden sf Objekte nur sehr langsam geladen und die Metadaten werden nicht angezeigt."
  },
  {
    "objectID": "rauman/Rauman1_Uebung_A.html#aufgabe-2-daten-visualisieren",
    "href": "rauman/Rauman1_Uebung_A.html#aufgabe-2-daten-visualisieren",
    "title": "Rauman 1: Übung A",
    "section": "Aufgabe 2: Daten Visualisieren",
    "text": "Aufgabe 2: Daten Visualisieren\nVektordaten (sf Objekte) lassen sich teilweise sehr schön in die bekannten Tidyverse workflows integrieren. Das merkt man schnell, wenn man die Daten visualisieren möchte. In InfoVis 1 & 2 haben wir intensiv mit ggplot2 gearbeitet und dort die Layers geom_point() und geom_line() kennen gelernt. Zusätzlich beinhaltet ggplot die Möglichkeit, mit geom_sf() Vektordaten direkt und sehr einfach zu plotten. Die nachfolgende Aufgabe werden wir aber mit plot abbilden (Warum wird gleich erläutert). Führe die angegebenen R-Befehle aus und studiere die entstehenden Plots. Welche Unterschiede findest Du? Wie erklärst Du diese Unterschiede?\n\nplot(gemeinden, max.plot = 1)\n\nplot(kantone, max.plot = 1)\n\n\n\n\nAbbildung 37.1: Generierter Plot\n\n\n\n\n\n\n\nAbbildung 37.2: Generierter Plot"
  },
  {
    "objectID": "rauman/Rauman1_Uebung_A.html#input-koodinatensysteme",
    "href": "rauman/Rauman1_Uebung_A.html#input-koodinatensysteme",
    "title": "Rauman 1: Übung A",
    "section": "Input: Koodinatensysteme",
    "text": "Input: Koodinatensysteme\nIn der obigen visualierung fällt folgendes auf:\n\ndie X/Y Achsen weisen zwei ganz unterschiedlichen Zahlenbereiche auf (vergleiche die Achsenbeschriftungen)\nder Umriss der Schweiz sieht in den beiden Datensätzen unterschiedlich aus (kantone ist gegenüber gemeinden gestaucht)\n\nDies hat natürlich damit zu tun, dass die beiden Datensätze in unterschiedlichen Koordinatensystemen erfasst wurden. Koordinatensysteme werden mit CRS (Coordinate Reference System) abgekürzt. Mit st_crs() können die zugewiesenen Koordinatensysteme abgefragt werden.\n\nst_crs(kantone)\n\nCoordinate Reference System:\n  User input: Undefined Cartesian SRS \n  wkt:\nENGCRS[\"Undefined Cartesian SRS\",\n    EDATUM[\"\"],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"Meter\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"Meter\",1]]]\n\nst_crs(gemeinden)\n\nCoordinate Reference System:\n  User input: Undefined Cartesian SRS \n  wkt:\nENGCRS[\"Undefined Cartesian SRS\",\n    EDATUM[\"\"],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"Meter\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"Meter\",1]]]\n\n\nLeider sind in unserem Fall keine Koordinatensysteme zugewiesen. Mit etwas Erfahrung kann man das Koordinatensystem aber erraten, so viele kommen nämlich gar nicht in Frage. Am häufigsten trifft man hierzulande eines der drei folgenden Koordinatensysteme an:\n\nCH1903 LV03: das alte Koordinatensystem der Schweiz\nCH1903+ LV95: das neue Koordinatensystem der Schweiz\nWGS84: ein häufig genutztes weltumspannendes geodätisches Koordinatensystem, sprich die Koordinaten werden in Länge und Breite angegeben (Lat/Lon).\n\nNun gilt es, anhand der Koordinaten die in der Spalte geometry ersichtlich sind das korrekte Koordinatensystem festzustellen. Wenn man sich auf epsg.io/map die Schweiz anschaut, kann man die Koordinaten in verschiedenen Koordinatensystem betrachten.\nBedienungshinweise:\n \n\nKoordinanten (des Fadenkreuzes) werden im ausgewählten Koordinatensystem dargestellt\n\n\n \n\nDas Koordinatensystem, in welchem die Koordinaten dargestellt werden sollen, kann mit “Change” angepasst werden\n\n\n \n\nFür Enthusiasten: Schau Dir die Schweiz in verschiedenen Koordinatensystemen an, in dem Du auf “Reproject Map” klickst\n\n\nWenn man diese Koordinaten mit den Koordinaten unserer Datensätze vergleicht, dann ist schnell klar, dass es sich beim Datensatz kantone um das Koordinatensystem WGS84 handelt und bei gemeinden das Koordinatensystem CH1903+ LV95. Diese Koordinatensyteme weisen wir nun mit st_set_crs() und dem entsprechenden EPSG-Code (siehe die jeweiligen Links) zu.\n\nkantone <- st_set_crs(kantone, 4326)\ngemeinden <- st_set_crs(gemeinden, 2056)\n\n# zuweisen mit st_set_crs(), abfragen mit st_crs()\nst_crs(kantone)\n\nCoordinate Reference System:\n  User input: EPSG:4326 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]\n\n\nAuch wenn das CRS der Datensätze bekannt ist, nutzt ggplot immer noch EPSG 4326 um die Achsen zu beschriften. Wenn das stört, kann man coord_sf(datum = 2056) in einem weiteren Layer spezifizieren. Oder aber man blendet die Achsenbeschriftung mit theme_void() komplett aus. Versuche beide Varianten.\n\nggplot() + \n  geom_sf(data = kantone) +\n  coord_sf(datum = 2056)\n\n\n\n\nAbbildung 37.3: Generierter Plot"
  },
  {
    "objectID": "rauman/Rauman1_Uebung_A.html#aufgabe-3-koordinatensyteme-transformieren",
    "href": "rauman/Rauman1_Uebung_A.html#aufgabe-3-koordinatensyteme-transformieren",
    "title": "Rauman 1: Übung A",
    "section": "Aufgabe 3: Koordinatensyteme transformieren",
    "text": "Aufgabe 3: Koordinatensyteme transformieren\nIn der vorherigen Übung haben wir das bestehende Koordinatensystem zugewiesen. Dabei haben wir die bestehenden Koordinaten (in der Spalte geom) nicht manipuliert. Ganz anders ist eine Transformation der Daten von einem Koordinatensystem in das andere. Bei einer Transformation werden die Koordinaten in das neue Koordinatensystem umgerechnet und somit manipuliert. Aus praktischen Gründen wollen  wir all unsere Daten ins neue Schweizer Koordinatensystem CH1903+ LV95 transfomieren. Transformiere den Datensatz kantone mit st_transform()in CH1903+ LV95, nutze dafür den korrekten EPSG-Code.\nVor der Transformation (betrachte die Attribute Bounding box sowie Geodetic CRS):\n\nkantone\n\nSimple feature collection with 51 features and 6 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 5.955902 ymin: 45.81796 xmax: 10.49217 ymax: 47.80845\nGeodetic CRS:  WGS 84\n# A tibble: 51 × 7\n   NAME       KANTON…¹ SEE_F…² KANTO…³ KT_TEIL EINWO…⁴                      geom\n * <chr>         <int>   <dbl>   <dbl> <chr>     <int>             <POLYGON [°]>\n 1 Graubünden       18      NA  710530 0        197888 ((8.877053 46.81291, 8.8…\n 2 Bern              2   11897  595952 1       1031126 ((7.153522 46.98628, 7.1…\n 3 Valais           23    1060  522463 0        341463 ((8.477625 46.52762, 8.4…\n 4 Vaud             22   39097  321201 1        793129 ((6.779825 46.85296, 6.7…\n 5 Ticino           21    7147  281216 0        353709 ((8.477625 46.52762, 8.4…\n 6 St. Gallen       17    7720  202820 1        504686 ((8.808609 47.22009, 8.7…\n 7 Zürich            1    6811  172894 0       1504346 ((8.410084 47.24837, 8.4…\n 8 Fribourg         10    7818  167142 1        315074 ((7.040344 46.97952, 7.0…\n 9 Luzern            3    6438  149352 0        406506 ((8.468167 46.99652, 8.4…\n10 Aargau           19     870  140380 1        670988 ((8.410084 47.24837, 8.4…\n# … with 41 more rows, and abbreviated variable names ¹​KANTONSNUM, ²​SEE_FLAECH,\n#   ³​KANTONSFLA, ⁴​EINWOHNERZ\n\n\n\nkantone <- st_transform(kantone, 2056)\n\nNach der Transformation (betrachte die Attribute Bounding box sowie Projected CRS):\n\nkantone\n\nSimple feature collection with 51 features and 6 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 5.955902 ymin: 45.81796 xmax: 10.49217 ymax: 47.80845\nGeodetic CRS:  WGS 84\n# A tibble: 51 × 7\n   NAME       KANTON…¹ SEE_F…² KANTO…³ KT_TEIL EINWO…⁴                      geom\n * <chr>         <int>   <dbl>   <dbl> <chr>     <int>             <POLYGON [°]>\n 1 Graubünden       18      NA  710530 0        197888 ((8.877053 46.81291, 8.8…\n 2 Bern              2   11897  595952 1       1031126 ((7.153522 46.98628, 7.1…\n 3 Valais           23    1060  522463 0        341463 ((8.477625 46.52762, 8.4…\n 4 Vaud             22   39097  321201 1        793129 ((6.779825 46.85296, 6.7…\n 5 Ticino           21    7147  281216 0        353709 ((8.477625 46.52762, 8.4…\n 6 St. Gallen       17    7720  202820 1        504686 ((8.808609 47.22009, 8.7…\n 7 Zürich            1    6811  172894 0       1504346 ((8.410084 47.24837, 8.4…\n 8 Fribourg         10    7818  167142 1        315074 ((7.040344 46.97952, 7.0…\n 9 Luzern            3    6438  149352 0        406506 ((8.468167 46.99652, 8.4…\n10 Aargau           19     870  140380 1        670988 ((8.410084 47.24837, 8.4…\n# … with 41 more rows, and abbreviated variable names ¹​KANTONSNUM, ²​SEE_FLAECH,\n#   ³​KANTONSFLA, ⁴​EINWOHNERZ"
  },
  {
    "objectID": "rauman/Rauman1_Uebung_A.html#aufgabe-4-chloroplethen-karte",
    "href": "rauman/Rauman1_Uebung_A.html#aufgabe-4-chloroplethen-karte",
    "title": "Rauman 1: Übung A",
    "section": "Aufgabe 4: Chloroplethen Karte",
    "text": "Aufgabe 4: Chloroplethen Karte\nNun wollen wir die Gemeinden respektive die Kantone nach ihrer Einwohnerzahl einfärben. Dafür verwenden wir wie gewohnt die Methode aes(fill = ...) von ggplot.\nTips:\n\num die scientific notation (z.B. 3e+03) zu verhindern, könnt ihr den Befehl options(scipen = 999) ausführen\num die Darstellung der Gemeinde- (bzw. Kantons-) Grenzen zu verhindern, könnt ihr im entsprechenden Layer color = NA setzen. Alternativ könnt ihr die Linienbreite mit size = verändern.\n\n\nggplot(kantone, aes(fill = EINWOHNERZ/1e6)) +\n  geom_sf(color= \"white\",size = .05) +\n  labs(title = \"Anzahl Einwohner pro Kanton\",\n       subtitle = \"in Millionen\") +\n  theme(legend.position = \"bottom\",\n        legend.title = element_blank(),\n        legend.key.width = unit(0.15, 'npc'),\n        legend.key.height = unit(0.02, 'npc'),\n        legend.text = element_text(angle = 90,hjust = 0.5),\n        legend.text.align = 1)\n\nggplot(gemeinden,aes(fill = EINWOHNERZ/1e6)) +\n  geom_sf(color= \"white\",size = .05) +\n  scale_fill_continuous(\"Einwohner (in Mio)\") +\n  labs(title = \"Anzahl Einwohner pro Gemeinde\",\n       subtitle = \"in Millionen\") +\n  theme(legend.position = \"bottom\",\n        legend.title = element_blank(),\n        legend.key.width = unit(0.15, 'npc'),\n        legend.key.height = unit(0.02, 'npc'),\n        legend.text = element_text(angle = 90,hjust = 0.5),\n        legend.text.align = 1)\n\n\n\n\nAbbildung 37.4: Der Vergleich dieser beiden Darstellungen veranschaulicht die MAUP Problematik sehr deutlich\n\n\n\n\n\n\n\nAbbildung 37.5: Der Vergleich dieser beiden Darstellungen veranschaulicht die MAUP Problematik sehr deutlich"
  },
  {
    "objectID": "rauman/Rauman1_Uebung_B.html",
    "href": "rauman/Rauman1_Uebung_B.html",
    "title": "Rauman 1: Übung B",
    "section": "",
    "text": "Für die kommende Übung arbeiten wir mit nachstehendem Datensatz. Lade diesen Herunter und importiere ihn in R.\nZudem brauchen wir die folgenden libraries:\n\nlibrary(dplyr)\nlibrary(sf)\nlibrary(ggplot2)\n\n\nAufgabe 1: Geopackage “Layers”\nAllenfalls ist euch beim Importieren des Geopackage gruental.pgkg folgende Warnmeldung aufgefallen:\nWarning message:\nIn evalq((function (..., call. = TRUE, immediate. = FALSE, noBreaks. = FALSE,  :\n  automatically selected the first layer in a data source containing more than one.\nDiese Warnmeldung weist darauf hin, dass das Geopackage gruental.gpkg mehrere Layers (rep. Datensätze) enthält und nur der erste Layer importiert wurde. Bringe mit dem Befehl st_layers die Layer Namen in Erfahrung und nutze diese im Anschluss in st_read (als Argument layer =) um die layers einzeln zu importieren und in variablen zu speichern (zB in als Variable wiesen und baeume).\n\n\nAufgabe 2: Datensätze erkunden\nNimm dir etwas Zeit und erkunde die beiden Datensätze. Nutze dafür auch die Visualisierungsmöglichkeiten von ggplot (insbesondere geom_sf).\n\n\n\n\n\nAbbildung 38.1: Beispielsweise kannst du die Daten in dieser Weise visualisieren.\n\n\n\n\n\n\nAufgabe 3: Spatial Join mit Punkten\nWir wollen nun für jeden Baum wissen, ob er sich in einer Wiese befindet oder nicht. Dazu nutzen wir die GIS-Technik Spatial Join, die in der Vorlesung beschrieben wurde. In sf können wir Spatial Joins mit der Funktion st_join durchführen, dabei gibt es nur left sowie inner-Joins (vgl. PrePro 1 & 2). So müssen die Punkte “Links”, also an erste Stelle aufgeführt werden, da wir ja Attribute an die Punkte anheften wollen.\nBeachte, dass der Output eine neue Spalte flaechen_typ aufweist. Diese ist leer (NA) wenn sich der entsprechende Baum nicht in einer Wiese befindet. Wie viele Bäume befinden sich in einer Wiese, wie viele nicht?\n\n\n\n\n\nAufgabe 4: Spatial Join mit Flächen\nAnalog der Vorlesung wollen wir nun in Erfahrung bringen, wie hoch der Wiesen-Anteil im Umkreis von 20m um jeden Baum ist. Dazu sind folgende Schritte nötig:\n\nAls erster Schritt müssen wir jeden Baum mit einem 20m Puffer verstehen. Nutze dazu st_buffer um speichere den Output als baeume_20m. Schau dir baeume_20m nun genau an. Um welchen Geometrietyp handelt es sich dabei nun?\nBerechnen nun die Schnittmenge aus baeume_20m und wiesen mit der Funktion st_intersection und speichere den Output als baeume_wiesen. Exploriere nun baeume_wiesen, auch mit ggplot(). Was ist passiert? Überprüfe die Anzahl Zeilen pro Datensatz. Haben die sich verändert? Wenn ja, warum?\nBerechnen nun die Flächengrösse pro Geometrie mit der Funktion st_area(). Speichere den Output in einer neuen Spalte von baeume_wiesen (z.B. mit dem Namen wiesen_flaeche). Tipp: Konvertiere den Output aus st_area einen nummerischen Vektor mit as.numeric().\nBerechne nun aus wiesen_flaeche den wiesen_anteil. Tipp: 100% ist die Kreisfläche aus \\(r^2\\times \\pi\\), wobei in unserem Fall \\(r = 20\\) entspricht.\n\nUm die berechneten Werte in den Datensatz baeume zu überführen braucht es noch folgende Schritte:\n\nKonvertiere baeume_wiesen in eine data.frame mit st_drop_geometry und speichere diese als baeume_wiesen_df\nNutze die Spalte baum_id in baeume_wiesen_df um den berechneten wiesen_anteil in den Datenatz baeume zu überführen. Tipp: Nutze dafür einen left_join\nErsetze alle NA Werte in der Spalte wiesen_anteil mit 0.\n\n\n\n\n\n\n\n\n\n\nAbbildung 38.2: Nach dieser Übung kannst du das Resultat optional in dieser Weise visualisieren."
  },
  {
    "objectID": "rauman/Rauman2_Uebung_A.html#aufgabe-1",
    "href": "rauman/Rauman2_Uebung_A.html#aufgabe-1",
    "title": "Rauman 2: Übung A",
    "section": "Aufgabe 1",
    "text": "Aufgabe 1\nFür die heutige Übung benötigen wir nachstehende Datensätze. Lade diese herunter und importiere sie in R. Prüfe, ob das CRS korrekt gesetzt wurde, setze es wenn nötig. Mache dich mit den Daten vertraut (visualieren, durchscrollen usw).\n\n\n\n\nDer Datensatz rotmilan.gpkg stammt aus einem grösseren Forschungsprojekt der Vogelwarte Sempach Mechanismen der Populationsdynamik beim Rotmilan. Der Datensatz wurde über die Plattform movebank zur Verfügung gestellt. Es handelt sich dabei um ein einzelnes Individuum, welches seit 2017 mit einem Sender versehen ist und über ganz Mitteleuropa zieht. Wir arbeiten in dieser Übung nur mit denjenigen Datenpunkten, die in der Schweiz erfasst wurden. Wer den ganzen Datensatz analysieren möchte, kann sich diesen über den Movebank-Link runterladen.\nDer Datensatz luftqualitaet.gpkg beinhaltet Messungen von Stickstoffdioxid \\(NO_2\\) aus dem Jahr 2015 für 97 Messstellen in der Schweiz. Stickstoffdioxid entstehen beim Verbrennen von Brenn- und Treibstoffen, insbesondere bei hohen Verbrennungstemperaturen, wobei der Strassenverkehr als Hauptquelle gilt. Mehr Informationen dazu findet ihr hier.\nEbenfalls benötigt ihr den Datensatz schweiz.gpkg\n\n\n\n\n\n\n\n\n\nAbbildung 39.1: Eine solche Visualisierung zeigt dir beispielsweise die räumliche Ausdehnung der Datenpunkte"
  },
  {
    "objectID": "rauman/Rauman2_Uebung_A.html#aufgabe-2",
    "href": "rauman/Rauman2_Uebung_A.html#aufgabe-2",
    "title": "Rauman 2: Übung A",
    "section": "Aufgabe 2",
    "text": "Aufgabe 2\nAls erstes berechnen wir die G-Function für die Rotmilanpositionen:\n\nSchritt 1\nMit st_distance() können Distanzen zwischen zwei sf Datensätze berechnet werden. Wird nur ein Datensatz angegeben, wird eine Kreuzmatrix erstellt wo die Distanzen zwischen allen Features zu allen anderen Features dargestellt werden. Wir nützen diese Funktion zur Berechnung der nächsten Nachbarn.\n\nrotmilan_distanzmatrix <- st_distance(rotmilan)\n\nnrow(rotmilan_distanzmatrix)\n\n[1] 2305\n\nncol(rotmilan_distanzmatrix)\n\n[1] 2305\n\n# zeige die ersten 6 Zeilen und Spalten der Matrix\n# jeder Wert ist 2x vorhanden (vergleiche Wert [2,1] mit [1,2])\n# die Diagonale ist die Distanz zu sich selber (gleich 0)\nrotmilan_distanzmatrix[1:6,1:6] \n\nUnits: [m]\n         1         2         3        4        5        6\n1     0.00 14362.044 20272.492 35596.07 52519.10 64156.67\n2 14362.04     0.000  8149.486 29752.74 44809.10 53775.25\n3 20272.49  8149.486     0.000 22580.04 36848.93 45662.55\n4 35596.07 29752.737 22580.037     0.00 17223.26 31439.57\n5 52519.10 44809.096 36848.926 17223.26     0.00 16499.19\n6 64156.67 53775.250 45662.554 31439.57 16499.19     0.00\n\n\n\n\nSchritt 2\nNun wollen wir wissen, wie gross die kürzeste Distanz von jedem Punkt zu seinem nächsten Nachbarn beträgt, also die kürzeste Distanz pro Zeile. Bevor wir diese ermitteln müssen wir die diagonalen Werte noch entfernen, denn diese stellen ja jeweils die Distanz zu sich selber dar und sind immer 0. Danach kann mit apply() eine Funktion (FUN = min) über die Zeilen (MARGIN = 1) einer Matrix (X = rotmilan_distanzmatrix) gerechnet werden. Zusätzlich müssen wir noch na.rm = TRUE setzen, damit NA Werte von der Berechnung ausgeschlossen werden. Das Resultat ist ein Vektor mit gleich vielen Werten wie Zeilen in der Matrix.\n\ndiag(rotmilan_distanzmatrix) <- NA # entfernt alle diagonalen Werte\n\nrotmilan_distanzmatrix[1:6,1:6] \n\nUnits: [m]\n         1         2         3        4        5        6\n1       NA 14362.044 20272.492 35596.07 52519.10 64156.67\n2 14362.04        NA  8149.486 29752.74 44809.10 53775.25\n3 20272.49  8149.486        NA 22580.04 36848.93 45662.55\n4 35596.07 29752.737 22580.037       NA 17223.26 31439.57\n5 52519.10 44809.096 36848.926 17223.26       NA 16499.19\n6 64156.67 53775.250 45662.554 31439.57 16499.19       NA\n\nrotmilan_mindist <- apply(rotmilan_distanzmatrix,1,min, na.rm = TRUE)\n\n\n\nSchritt 3\nNun müssen wir die Distanzen nach ihrer Grösse sortieren\n\nrotmilan_mindist <- sort(rotmilan_mindist) \n\n\n\nSchritt 4\nJetzt berechnen wir die kummulierte Häufigkeit von jeder Distanz berechnen. Die kummulierte Häufikgeit vom ersten Wert ist 1 (der Index des ersten Wertes) dividiert durch die Anzahl Werte insgesamt. Mit seq_along erhalten wir die Indizes aller Werte, mit lenth die Anzahl Werte insgesamt.\n\nkumm_haeufgikeit <- seq_along(rotmilan_mindist) / length(rotmilan_mindist)\n\n\n\nSchritt 5\nNun wollen wir die kumulierte Häufigkeit der Werte in einer Verteilungsfunktion (engl: Empirical Cumulative Distribution Function, ECDF) darstellen. Dafür müssen wir die beiden Vektoren zuerst noch in einen Dataframe packen, damit ggplot damit klar kommt.\n\nrotmilan_mindist_df <- data.frame(distanzen = rotmilan_mindist,\n                                  kumm_haeufgikeit = kumm_haeufgikeit)\n\np <- ggplot() + \n  geom_line(data = rotmilan_mindist_df, aes(distanzen, kumm_haeufgikeit)) +\n  labs(x = \"Distanz (Meter)\", y = \"Häufigkeit (kummuliert)\")\n\np\n\n\n\n\nAbbildung 39.2: Generierter Plot\n\n\n\n\nLesehilfe:\n\n\n\n\n\nAbbildung 39.3: Generierter Plot"
  },
  {
    "objectID": "rauman/Rauman2_Uebung_A.html#aufgabe-3",
    "href": "rauman/Rauman2_Uebung_A.html#aufgabe-3",
    "title": "Rauman 2: Übung A",
    "section": "Aufgabe 3",
    "text": "Aufgabe 3\nFühre nun die gleichen Schritte mit luftqualitaet durch und vergleiche die ECDF-Plots.\n\n\n\n\n\nAbbildung 39.4: Generierter Plot"
  },
  {
    "objectID": "rauman/Rauman2_Uebung_B.html",
    "href": "rauman/Rauman2_Uebung_B.html",
    "title": "Rauman 2: Übung B",
    "section": "",
    "text": "In dieser Übung geht es darum, zwei verschiedene Interpolationsverfahren in R umzusetzen. Im ersten Interpolationsverfahren verwenden wir die inverse distance weighted interpolation, später verwenden wir die nearest neighbour methode. Dazu braucht ihr die folgenden Packages:\n\nlibrary(sf)\nlibrary(dplyr)\nlibrary(ggplot2)\n\nlibrary(gstat) # <- ggf. installieren!\n\nWeiter benötigt ihr die nachstehenden Datensätze:\nluftqualitaet <- read_sf(\"datasets/rauman/luftqualitaet.gpkg\")\nschweiz <- read_sf(\"datasets/rauman/schweiz.gpkg\") \nDie Library gstat bietet verschiedene Möglichkeiten, Datenpunkte zu interpolieren, unter anderem auch die inverse distance weighted Methode. Leider ist das Package noch nicht so benutzerfreundlich wie sf: Das Package wird aber aktuell überarbeitet und in mittlerer Zukunft sollte es ebenso einfach zugänglich sein. Damit Ihr Euch nicht mit den Eigenheiten dieser Library umschlagen müsst, haben wir eine Function vorbereitet, die Euch die Verwendung der IDW-Interpolation erleichtern soll.\nWir nehmen Euch damit etwas Komplexität weg und liefern Euch ein pfannenfertiges Werkzeug. Das hat auch Nachteile und wir ermutigen alle, die dafür Kapazität haben, unsere Function eingehend zu studieren und allenfalls ganz auf die Function zu verzichten und stattdessen direkt gstat zu verwenden. Wenn ihr mit unserer Function arbeiten möchtet, müsst ihr den unten stehenden Code in euer Skript kopieren und ausführen.\n\nmy_idw <- function(groundtruth,column,cellsize, nmax = Inf, maxdist = Inf, idp = 2, extent = NULL){\n  library(gstat)\n  library(sf)\n  \n  if(is.null(extent)){\n    extent <- groundtruth\n  }\n  \n  samples <- st_make_grid(extent,cellsize,what = \"centers\")\n  my_formula <- formula(paste(column,\"~1\"))\n  idw_sf <- gstat::idw(formula = my_formula,groundtruth, newdata = samples, nmin = 1, nmax = nmax, maxdist = maxdist, idp = idp)\n  \n  idw_matrix <- cbind(as.data.frame(st_coordinates(idw_sf)),pred = st_drop_geometry(idw_sf)[,1])\n  idw_matrix\n}\n\nNun könnt Ihr mit my_idw() den Datensatz luftqualitaet folgendermassen interpolieren.\n\nmy_idw(groundtruth = luftqualitaet,column = \"value\",cellsize = 10000, extent = schweiz)\n\nFolgende Parameter stehen Euch zur Verfügung:\n\nNotwendige Parameter:\n\ngroundtruth: Punktdatensatz mit den Messwerten (sf-Objekt)\ncolumn: Name der Spalte mit den Messwerten (in Anführungs- und Schlusszeichen)\ncellsize: Zellgrösse des output Rasters\n\nOptionale Parameter\n\nnmax: Maximale Anzahl Punkte, die für die Interpolation berücksichtigt werden sollen. Default: Inf (alle Werte im gegebenen Suchradius)\nmaxdist: Suchradius, welcher für die Interpolation verwendet werden soll. Default Inf (alle Werte bis nmax)\nidp: Inverse Distance Power: die Potenz, mit der der Nenner gesteigert werden soll. Default: 2. Werte werden im Kehrwert des Quadrates gewichtet: \\(\\frac{1}{dist^{idp}}\\).\nextent: Gebiet, für welches die Interpolation durchgeführt werden soll. Wenn nichts angegeben wird (Default NULL), wird die Ausdehnung von groundtruth verwendet.\n\nOuput\n\nder Output der Funktion ist eine data.frame mit 3 Spalten:\n\nX, Y Koordinaten der interpolierten Werte\npred: der Interpolierte Wert\n\n\n\nBeim Output handelt sich hier um einen Raster-ähnlichen Datentyp (siehe Vorlesung Spatial DataScience 1). Diesen können wir mit geom_raster mit ggplot visualisieren. Dafür müsst ihr in aes die X und Y Koordinaten angeben, und der interpolierte Wert mit fill einfärben.\n\nAufgabe 1: Raeumliche Interpolation mit IDW\nRechnet so den IDW für die Luftqualitätsmessungen mit verschiedenen Parametern und visualisiert jeweils die Resultate. Experimentiert mit nmax sowie maxdist. Was stellt ihr fest?\nTips: - Was für Distanzen bei maxdist Sinn machen, könnt ihr dem Output aus der G-Funktion (vorherige Übung) entnehmen - Wählt am Anfang eine etwas Konvervative (grosse) cellsize und verringert diesen nur wenn euer Rechner damit gut klar kommt - Da der Output aus der Interpolation im gleichen Koordinatenbezugssystem sind wie schweiz.gpkg kann man diese beiden Datensätze im gleichen ggplot darstellen. Dafür müsst ihr die aesthetics (aes()) für jeden Layer einzeln setzen, und nicht auf der Ebene von ggplot().\n\n\n\n[inverse distance weighted interpolation]\n[inverse distance weighted interpolation]\n[inverse distance weighted interpolation]\n[inverse distance weighted interpolation]\n\n\n\n\n\nAbbildung 40.1: Stickstoffdioxid (NO2) in μg/m3, Interpoliert über die ganze Schweiz mit der Inverse Distance Weighted Methode. Die verschiedenen Plots zeigen die Veränderung der Interpolation bei steigendem IDP-Wert\n\n\n\n\n\n\n\nAufgabe 2: Interpolation mit Nearest Neighbour\nEine weitere einfache Möglichkeit zur Interpolation bietet die Erstellung eines Voronoi-Diagrammes, auch als Thiessen-Polygone oder Dirichlet-Zerlegung bekannt. sf liefert dazu die Funktion st_voronoi(), die einen Punktdatensatz annimmt und eben um die Punkte die Thiessenpolygone konstruiert. Dazu braucht es lediglich einen kleinen Vorverarbeitungsschritt: sf möchte für jedes Feature, also für jede Zeile in unserem Datensatz, ein Voronoidiagramm. Das macht bei uns wenig Sinn, weil jede Zeile nur aus einem Punkt besteht. Deshalb müssen wir vorher luftqualitaet mit st_union() von einem POINT in ein MULTIPOINT Objekt konvertieren, in welchem alle Punkte in einer Zeile zusammengefasst sind.\n\nluftqualitaet_union <- st_union(luftqualitaet)\n\nthiessenpolygone <- st_voronoi(luftqualitaet_union)\n\n\n\n\n\n\nAbbildung 40.2: Generierter Plot\n\n\n\n\nst_voronoi hat die Thiessenpolygone etwas weiter gezogen als wir sie wollen. Dies ist allerdings eine schöne Illustration der Randeffekte von Thiessenpolygonen, die zum Rand hin (wo es immer weniger Punkte hat) sehr gross werden können. Wir können die Polygone auf die Ausdehnung der Schweiz mit st_intersection() clippen. Auch hier braucht es zwei kleine Vorverarbeitungsschritte:\n\nwie vorher müssen wir die einzelnen Kantons-Polygone miteinander verschmelzen. Dies erreichen wir mit st_union(). Wir speichern den Output als schweiz, was als Resultat ein einzelnes Polygon der Schweizergrenze retourniert.\nfür die Thiessen-Polygone machen wir genau das Umgekehrte: st_voronoi() liefert ein einzelnes Feature mit allen Polygonen, welches sich nicht gerne clippen lässt. Mit st_cast() wird die GEOMETRYCOLLECTION in Einzelpolygone aufgeteilt.\n\n\nthiessenpolygone <- st_cast(thiessenpolygone)\n\nthiessenpolygone_clip <- st_intersection(thiessenpolygone,schweiz)\n\n\n\n\n\n\nAbbildung 40.3: Generierter Plot\n\n\n\n\nJetzt müssen wir nur noch den jeweiligen Wert für jedes Polygon ermitteln. Dies erreichen wir wieder durch st_join. Auch hier ist noch ein kleiner Vorverarbeitungsschritt nötig: Wir konvertieren das sfc Objekt (nur Geometrien) in ein sf Objekt (Geometrien mit Attributtabelle).\n\nthiessenpolygone_clip <- st_as_sf(thiessenpolygone_clip)\nthiessenpolygone_clip <- st_join(thiessenpolygone_clip,luftqualitaet)\n\n\nggplot() + \n  geom_sf(data = schweiz) +\n  geom_sf(data = thiessenpolygone_clip, aes(fill = value)) +\n  geom_sf(data = luftqualitaet) +\n  scale_fill_gradientn(colours = rev(RColorBrewer::brewer.pal(11,\"RdYlBu\"))) +\n  theme_void() +\n  theme(legend.position = \"bottom\", legend.title = element_blank(),\n      legend.key.width = unit(0.10, 'npc'),\n      legend.key.height = unit(0.02, 'npc'))\n\n\n\n\nAbbildung 40.4: Stickstoffdioxid (NO2) in μg/m3, Interpoliert über die ganze Schweiz nach der Nearest Neighbour Methode."
  },
  {
    "objectID": "rauman/Rauman2_Uebung_C.html",
    "href": "rauman/Rauman2_Uebung_C.html",
    "title": "Rauman 2: Übung C",
    "section": "",
    "text": "Nun wollen wir für die bereits verwendeten Datensätze luftqualitaet.gpkg und rotmilan.gpkg Dichteschätzungen durchführen. Ladet dafür die notwendigen Package und ladet bei Bedarf die Datensätze herunter."
  },
  {
    "objectID": "rauman/Rauman2_Uebung_C.html#aufgabe-1-rotmilan-bewegungsdaten-visualisieren",
    "href": "rauman/Rauman2_Uebung_C.html#aufgabe-1-rotmilan-bewegungsdaten-visualisieren",
    "title": "Rauman 2: Übung C",
    "section": "Aufgabe 1: Rotmilan Bewegungsdaten visualisieren",
    "text": "Aufgabe 1: Rotmilan Bewegungsdaten visualisieren\nDie erste Frage, die bei solchen Bewegungsstudien typischerweise gestellt wird, lautet: Wo hält sich das Tier hauptsächlich auf? Um diese Frage zu beantworten, kann man als erstes einfach die Datenpunkte in einer einfachen Karte visualisieren. Erstellt zur Beantwortung dieser Frage nachstehende Karte.\n\n\n\n\n\nAbbildung 41.1: Generierter Plot"
  },
  {
    "objectID": "rauman/Rauman2_Uebung_C.html#aufgabe-2-kernel-density-estimation-berechnen",
    "href": "rauman/Rauman2_Uebung_C.html#aufgabe-2-kernel-density-estimation-berechnen",
    "title": "Rauman 2: Übung C",
    "section": "Aufgabe 2: Kernel Density Estimation berechnen",
    "text": "Aufgabe 2: Kernel Density Estimation berechnen\nIn einer ersten Annäherung funktioniert dies, doch wir sehen hier ein klassisches Problem des “Overplotting”. Das heisst, dass wir durch die Überlagerung vieler Punkte in den dichten Regionen nicht abschätzen können, wie viele Punkte dort effektiv liegen und ggf. übereinander liegen. Es gibt hier verschiedene Möglichkeiten, die Punktdichte klarer zu visualisieren. Eine unter Biologen sehr beliebte Methode ist die Dichteverteilung mit einer Kernel Density Estimation (KDE). Dies v.a. darum, weil mit KDE das Habitat (Streifgebiet) eines Tieres abgeschätzt werden kann. Homeranges werden oft mit KDE95 und Core Areas mit KDE50 definiert (Fleming C., Calabrese J., 2016).\nÄhnlich wie beim IDW sind auch die verfügbaren KDE-Funktionen in R etwas kompliziert in der Handhabung. Damit wir dieses Verfahren aber dennoch auf unsere Rotmilan-Daten anwenden können, haben wir eine eigene KDE-Funktion erstellt, die wir Euch zur Verfügung stellen.\nHier gilt das gleiche wie schon bei der Funktion my_idw(): Wir ermutigen alle, die dafür Kapazität haben, unsere Function eingehend zu studieren und allenfalls ganz auf die Funktion zu verzichten und stattdessen direkt MASS zu verwenden. Wenn ihr mit unserer Funktion arbeiten möchtet, müsst ihr den unten stehenden Code in euer Skript kopieren und ausführen.\n\nmy_kde <- function(points,cellsize, bandwith, extent = NULL){\n  library(MASS)\n  library(sf)\n  library(tidyr)\n  if(is.null(extent)){\n    extent_vec <- st_bbox(points)[c(1,3,2,4)]\n  } else{\n    extent_vec <- st_bbox(extent)[c(1,3,2,4)]\n  }\n  \n  n_y <- ceiling((extent_vec[4]-extent_vec[3])/cellsize)\n  n_x <- ceiling((extent_vec[2]-extent_vec[1])/cellsize)\n  \n  extent_vec[2] <- extent_vec[1]+(n_x*cellsize)-cellsize\n  extent_vec[4] <- extent_vec[3]+(n_y*cellsize)-cellsize\n\n  coords <- st_coordinates(points)\n  mat <- kde2d(coords[,1],coords[,2],h = bandwith,n = c(n_x,n_y),lims = extent_vec)\n\n  mydf <- as.data.frame(mat[[3]])\n  \n  colnames(mydf) <- mat[[2]]\n  mydf$X <- mat[[1]]\n  \n  pivot_longer(mydf, -X,names_to = \"Y\",names_transform = list(Y = as.numeric))\n}\n\nDie Parameter der Funktion sollten relativ klar sein:\n\npoints: Ein Punktdatensatz aus der Class sf\ncellsize: Die Zellgrösse des output-Rasters\nbandwith: Der Suchradius für die Dichteberechnung\nextent (optional): Der Perimeter, in dem die Dichteverteilung berechnet werden soll. Wenn kein Perimeter angegeben wird, wird die “bounding box” von points genutzt.\n\nWenn wir nun mit my_kde() die Dichteverteilung berechnen, erhalten wir ein data.frame mit X und Y Koordinaten sowie eine Spalte value zurück. Nutzt diese drei Spalten mit geom_raster() um eure Daten mit ggplot zu visualisieren (aes(x = X, y = Y, fill = value).\n\nrotmilan_kde <- my_kde(points = rotmilan,cellsize = 1000, bandwith = 10000, extent = schweiz)\n\nrotmilan_kde\n\n# A tibble: 77,129 × 3\n          X        Y value\n      <dbl>    <dbl> <dbl>\n 1 2485410. 1075268.     0\n 2 2485410. 1076268.     0\n 3 2485410. 1077268.     0\n 4 2485410. 1078268.     0\n 5 2485410. 1079268.     0\n 6 2485410. 1080268.     0\n 7 2485410. 1081268.     0\n 8 2485410. 1082268.     0\n 9 2485410. 1083268.     0\n10 2485410. 1084268.     0\n# … with 77,119 more rows\n\n\n\n\n\n\n\nAbbildung 41.2: Generierter Plot\n\n\n\n\nDie Kernel Density Estimation ist nun sehr stark von den tiefen Werten dominiert, da die Dichte in den meisten Zellen unseres Untersuchungsgebiets nahe bei Null liegt. Wie erwähnt sind Wissenschaftler häufig nur an den höchsten 95% der Werte interessiert. Folge folgende Schritte um das Resultat etwas besser zu verantschaulichen:\n\nBerechne die 95. Perzentile aller Werte mit der Funktion quantile und benne diesen q25\nErstelle eine neue Spalte in rotmilan_kde, wo alle Werte tiefer als q25 NA entsprechen\n(Optional): Transformiere die Werte mit log10, um einen differenzierteren Farbverlauf zu erhalten\n\nWir können die tiefen Werte ausblenden, indem wir nur die höchsten 5% der Werte darstellen. Dafür berechnen wir mit raster::quantile die 95. Perzentile aller Werte und nutzen diesen Wert als “Grenzwert” für die Darstellung.\nZusätzlich hilft eine logarithmische Transformation der Werte, die Farbskala etwas sichtbarer zu machen.\n\n\n\n\n\nAbbildung 41.3: Generierter Plot"
  },
  {
    "objectID": "rauman/Rauman2_Uebung_C.html#aufgabe-3-dichteverteilung-mit-thiessen-polygonen",
    "href": "rauman/Rauman2_Uebung_C.html#aufgabe-3-dichteverteilung-mit-thiessen-polygonen",
    "title": "Rauman 2: Übung C",
    "section": "Aufgabe 3: Dichteverteilung mit Thiessen Polygonen",
    "text": "Aufgabe 3: Dichteverteilung mit Thiessen Polygonen\nThiessen Polygone bieten eine spannende Alternative um Unterschiede in der Dichteverteilung von Punktdatensätzen zu visualisieren. Wir wollen dies nun ausprobieren und konstruieren zum Schluss die Thiessenpolygone für die Rotmilan-Daten für das Untersuchungsgebiet Schweiz. Nutze die Anleitung für das Erstellen von Thiessenpolygonen aus der Übung B um Thiessenpolygone für die Rotmilanpositionen zu erstellen.\n\n\n\n\n\n\n\n\nAbbildung 41.4: Wenn wir jetzt die Thiessenpolygone (ohne Punkte) darstellen, wird deutlicher, wie die Dichteverteilung im Innern des Clusters aussieht."
  },
  {
    "objectID": "rauman/Rauman3_Uebung_A.html",
    "href": "rauman/Rauman3_Uebung_A.html",
    "title": "Rauman 3: Übung",
    "section": "",
    "text": "Für die Berechnung von Morans \\(I\\) benutzen wir kein externes Package, sondern erarbeiten uns alles selber, basierend auf der Formel von Moran’s \\(I\\):\n\\[I = \\frac{n}{\\sum_{i=1}^n (y_i - \\bar{y})^2} \\times \\frac{\\sum_{i=1}^n \\sum_{j=1}^n w_{ij}(y_i - \\bar{y})(y_j - \\bar{y})}{\\sum_{i=1}^n \\sum_{j=1}^n w_{ij}}\\]\nDiese sieht sehr beeindruckend aus, aber wenn wir die Formel in ihre Einzelbestandteile aufteilen, sehen wir, dass diese in sich gar nicht so komplex sind.\nAls erster Schritt müssen wir die notwendigen Libraries und Geodaten laden:"
  },
  {
    "objectID": "rauman/Rauman3_Uebung_A.html#aufgabe-1-herleitung-der-formel",
    "href": "rauman/Rauman3_Uebung_A.html#aufgabe-1-herleitung-der-formel",
    "title": "Rauman 3: Übung",
    "section": "Aufgabe 1: Herleitung der Formel",
    "text": "Aufgabe 1: Herleitung der Formel\nIn der ersten Übung wollen wir Moran’s \\(I\\) für eine gegebene Choroplethenkarte nachrechnen. Dazu nehmen wir die Formel für Moran’s \\(I\\) und zerlegen sie in Einzelteile, die wir dann Schritt für Schritt für unsere Daten berechnen. So teilen wir ein vermeintlich komplexes Problem in überschaubare Einzelteile. Dieses Vorgehen illustriert ausserdem sehr schön ein generelles Data Science Prinzip. Divide and Conquer - Teile und Herrsche: Teile ein komplexes Problem in kleinere, beherrschbare Unterprobleme. Wir beginnen mit dem ersten Bruch und berechnen dabei zuerst den Zähler, dann dem Nenner. So können wir den Bruch auflösen und uns danach dem zweiten Bruch zuwenden:\n\\[I = \\frac{n}{\\sum_{i=1}^n (y_i - \\bar{y})^2} \\times \\frac{\\sum_{i=1}^n \\sum_{j=1}^n w_{ij}(y_i - \\bar{y})(y_j - \\bar{y})}{\\sum_{i=1}^n \\sum_{j=1}^n w_{ij}}\\]\n\nBruch 1\nWidmen wir uns dem ersten Bruch:\n\\[\\frac{n}{\\sum_{i=1}^n (y_i - \\bar{y})^2}\\]\n\nZähler (von Bruch 1)\nBeginnen wir mit dem Zähler, \\(n\\). Dies ist lediglich die Anzahl Messwerte in unserem Datensatz, also die Anzahl Kantone.\n\nn <- nrow(zweitwohnung_kanton)\nn\n## [1] 26\n\n\n\nNenner (von Bruch 1)\nDer Nenner des ersten Bruches (\\({\\sum_{i=1}^n (y_i - \\bar{y})^2}\\)) ist sehr ähnlich der Berechnung der Varianz:\n\nBerechne den Durchschnitt aller Messwerte (\\(\\bar{y}\\))\nBerechne für jeden Messwert die Differenz zum Durchschnitt (\\(y_i - \\bar{y}\\))\nQuadriere diese Werte \\((y_i - \\bar{y})^2\\)\nSummiere die Quadrierten Werte \\(\\sum_{i=1}^n\\)\n\nAlso berechnen wir zuerst diese Differenzwerte (Messwert minus Mittelwert):\n\n# Die Werte aller Kantone:\ny <- zweitwohnung_kanton$ja_in_percent\n\n# Der Durchschnittswert aller Kantone\nybar <- mean(y, na.rm = TRUE)\n\n# von jedem Wert den Durchschnittswert abziehen:\ndy <- y - ybar\n\nWelche dieser Zwischenresultate sind Einzelwerte und welche Vektoren? Nun quadrieren wir die Differenzen:\n\ndy_2 <- dy^2\n\nund summieren die Differenzen:\n\ndy_sum <- sum(dy_2, na.rm = TRUE)\n\n\n\nAuflösung (Bruch 1)\nBeschliessen wir die Bearbeitung des ersten Bruchs indem wir den Zähler durch den Nennen dividieren: n durch dy_sum.\n\nvr <- n/dy_sum\n\n\n\n\nBruch 2\nWenden wir uns nun also dem Bruches der Formel zu.\n\\[\\frac{\\sum_{i=1}^n \\sum_{j=1}^n w_{ij}(y_i - \\bar{y})(y_j - \\bar{y})}{\\sum_{i=1}^n \\sum_{j=1}^n w_{ij}}\\]\nHier berechnen wir die Summe aller Gewichte sowie die gewichteten Covarianzen. Wir betrachten immer Messwertpaare, sprich paarweise Vergleiche zweier Raumeinheiten (hier Kantone). Deshalb haben die zwei Summenzeichen die beiden unterschiedlichen Laufvariablen (\\(i\\) und \\(j\\)). Solche paarweise Vergleiche von Werten mit allen anderen Werten können wir elegant mit Kreuzmatrizen abbilden. In der Kreuzmatrix vergleichen wir jeden Messwert mit allen anderen Messwerten. Dabei gibt es zwei Kreuzmatrizen: (\\(w_{ij}\\) ist die erste Kreuzmatrix, \\((y_i - \\bar{y})(y_j - \\bar{y})\\) ist die zweite Kreuzmatrix).\n\nZähler (Bruch 2)\nDer erste Term, \\(w_{ij}\\), beschreibt die räumlichen Gewichte aller Kantone. Sind die Kantone benachbart, dann gilt ein Gewicht von 1, sind sie nicht benachbart, gilt ein Gewicht von 0. Dies entspricht dem Schalter aus der Vorlesung.\nWie wir “benachbart” definieren ist nicht festgelegt. Denkbar wären zum Beispiel folgende Optionen:\n\nDie Kantone müssen sich berühren (dürfen sich aber nicht überlappen): st_touches()\nDie Kantone müssen innerhalb einer bestimmten Distanz zueinander liegen: st_is_within_distance()\nDie Kantone müssen überlappen: st_overlaps()\n\nEgal für welche Variante Ihr Euch entscheidet, setzt sparse = FALSE damit eine Kreuzmatrix erstellt wird.\n\nw <- st_touches(zweitwohnung_kanton, sparse = FALSE)\n\nw[1:6, 1:6]\n##       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]\n## [1,] FALSE FALSE FALSE FALSE  TRUE FALSE\n## [2,] FALSE FALSE  TRUE  TRUE FALSE  TRUE\n## [3,] FALSE  TRUE FALSE FALSE  TRUE  TRUE\n## [4,] FALSE  TRUE FALSE FALSE  TRUE  TRUE\n## [5,]  TRUE FALSE  TRUE  TRUE FALSE FALSE\n## [6,] FALSE  TRUE  TRUE  TRUE FALSE FALSE\n\n(Lasst Euch nicht davon beirren, dass wir nun TRUE und FALSE statt 1 und 0 haben. In R sind TRUE und 1 äquivalent, sowie auch FALSE und 0).\n\n\n\nZur Überprüfung unserer Operation: Mit w[1,] bekommt ihr ein Vektor, wo bei allen Kantone, die den ersten kanton (Zürich) berühren TRUE steht und bei allen anderen FALSE. Nun können wir überprüfen, ob die räumliche Operation funktioniert hat.\n\nberuehrt_1 <- w[1, ]\n\nggplot(zweitwohnung_kanton[beruehrt_1, ]) +\n  geom_sf(aes(fill = KANTONSNAME)) +\n  labs(title = \"Welche Kanton berühren den Kanton Zürich (st_touches)\")\n\n\n\n\nAbbildung 42.2: Generierter Plot\n\n\n\n\nDer nächste Teil sollte Euch nun bekannt vorkommen. Die Differenz aller Werte vom Mittelwert aller Werte \\((y_i - \\bar{y})\\) kennen wir schon vom ersten Bruch und haben wir auch bereits gelöst. Nun gilt es paarweise das Produkt der Abweichungen vom Mittelwert (die Covarianz) zu berechnen \\((y_i - \\bar{y})(y_j - \\bar{y})\\). DAzu müssen wir das Produkt aller Wertekombinationen berechnen. Dies erreichen wir mit der Funktion tcrossprod():\n\npm <- tcrossprod(dy)\npm[1:6,1:6]\n##               [,1]         [,2]          [,3]         [,4]         [,5]\n## [1,]  0.0008726497  0.001597812 -0.0006495424 -0.003249747 -0.001984912\n## [2,]  0.0015978120  0.002925576 -0.0011893051 -0.005950251 -0.003634352\n## [3,] -0.0006495424 -0.001189305  0.0004834762  0.002418896  0.001477437\n## [4,] -0.0032497469 -0.005950251  0.0024188956  0.012102055  0.007391811\n## [5,] -0.0019849120 -0.003634352  0.0014774366  0.007391811  0.004514842\n## [6,] -0.0023882557 -0.004372870  0.0017776588  0.008893862  0.005432280\n##              [,6]\n## [1,] -0.002388256\n## [2,] -0.004372870\n## [3,]  0.001777659\n## [4,]  0.008893862\n## [5,]  0.005432280\n## [6,]  0.006536145\n\nNun multiplizieren wir die Covarianzen mit den Gewichten \\(w\\) (Schalter), damit wir nur noch die Werte von den Kantonen haben, die auch effektiv benachbart sind (und eliminieren nicht-benachbarte Werte). Beachtet dass wir hier nun eine Matrix mit einer Matrix multiplizieren.\n\npmw <- pm * w\nw[1:6,1:6]\n##       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]\n## [1,] FALSE FALSE FALSE FALSE  TRUE FALSE\n## [2,] FALSE FALSE  TRUE  TRUE FALSE  TRUE\n## [3,] FALSE  TRUE FALSE FALSE  TRUE  TRUE\n## [4,] FALSE  TRUE FALSE FALSE  TRUE  TRUE\n## [5,]  TRUE FALSE  TRUE  TRUE FALSE FALSE\n## [6,] FALSE  TRUE  TRUE  TRUE FALSE FALSE\npmw[1:6,1:6]\n##              [,1]         [,2]         [,3]         [,4]         [,5]\n## [1,]  0.000000000  0.000000000  0.000000000  0.000000000 -0.001984912\n## [2,]  0.000000000  0.000000000 -0.001189305 -0.005950251  0.000000000\n## [3,]  0.000000000 -0.001189305  0.000000000  0.000000000  0.001477437\n## [4,]  0.000000000 -0.005950251  0.000000000  0.000000000  0.007391811\n## [5,] -0.001984912  0.000000000  0.001477437  0.007391811  0.000000000\n## [6,]  0.000000000 -0.004372870  0.001777659  0.008893862  0.000000000\n##              [,6]\n## [1,]  0.000000000\n## [2,] -0.004372870\n## [3,]  0.001777659\n## [4,]  0.008893862\n## [5,]  0.000000000\n## [6,]  0.000000000\n\nDen Zähler des ersten Bruches können wir nun fertig berechnen, indem wir die Summe aller gewichten (sprich eingeschalteten) Werten bilden:\n\nspmw <- sum(pmw, na.rm = TRUE)\nspmw\n## [1] 0.2007517\n\n\n\nNenner (Bruch 2)\nFür den Nenner des zweiten Teils der Formal (des zweiten Bruchs) müssen wir nun nur noch alle Gewichte summieren. Diese Summer entspricht der Anzahl effektiv benachbarter Kantone und kann Anzahl der \\(TRUE\\)-Werte in \\(w\\) bestimmt werden.\n\nsmw <- sum(w, na.rm = TRUE)\n\n\n\nAuflösung (Bruch 2)\nSo können wir den zweiten Bruch auflösen und berechnen:\n\nsw  <- spmw / smw\n\n\n\n\nAuflösung der Formel\nDer allerletzte Schritt besteht darin, die Werte aus den beiden Brüche miteinander zu multiplizieren.\n\nMI <- vr * sw\nMI\n## [1] 0.3148631\n\nDer Global Morans \\(I\\) für die Abstimmungsdaten beträgt auf Kantonsebene also 0.3148631. Wie interpretiert ihr dieses Resultate? Was erwartet ihr für eine Resultat auf Gemeinde- oder Bezirksebene?"
  },
  {
    "objectID": "rauman/Rauman3_Uebung_A.html#aufgabe-2-morans-i-für-gemeinde-oder-bezirke-berechnen",
    "href": "rauman/Rauman3_Uebung_A.html#aufgabe-2-morans-i-für-gemeinde-oder-bezirke-berechnen",
    "title": "Rauman 3: Übung",
    "section": "Aufgabe 2: Morans I für Gemeinde oder Bezirke berechnen",
    "text": "Aufgabe 2: Morans I für Gemeinde oder Bezirke berechnen\nNun könnt ihr Morans \\(I\\) auf der Ebene der Gemeine oder Bezirke und untersuchen, ob und wie sich Morans \\(I\\) verändert. Wenn ihr einen wenig leistungsfähigen Rechner habt, berechnet verwendet besser die Ebene “Berzirke”. Importiert dazu den Layer bezrik oder gemeinde aus dem Datensatz zweitwohnungsinitiative.gpkg. Visualisiert in einem ersten Schritt die Abstimmungsresultate.\n\nzweitwohnung_gemeinde <- read_sf(\"data/zweitwohnungsinitiative.gpkg\", \"gemeinde\")\n## Error: Cannot open \"data/zweitwohnungsinitiative.gpkg\"; The file doesn't seem to exist.\n\nggplot(zweitwohnung_gemeinde) +\n  geom_sf(aes(fill = ja_in_percent), colour = \"white\",lwd = 0.2) +\n  scale_fill_gradientn(\"Ja Anteil\",colours = RColorBrewer::brewer.pal(11, \"RdYlGn\"), limits = c(0,1)) +\n  theme(legend.position = \"bottom\")\n## Error in ggplot(zweitwohnung_gemeinde): Objekt 'zweitwohnung_gemeinde' nicht gefunden"
  },
  {
    "objectID": "rauman/Rauman4_Uebung_A_ahp.html",
    "href": "rauman/Rauman4_Uebung_A_ahp.html",
    "title": "Rauman 4: Übung A",
    "section": "",
    "text": "Nachdem Sie nun die Theorie kennengelernt haben, werden Sie ein konkretes Beispiel für einen Analytischen Hierarchieprozess (AHP) durchführen. Dies ist ein manueller Ansatz, um Ihnen die Grundlagen eines AHP zu zeigen. Wenn Sie einen komplexeren AHP erstellen möchten, können Sie spezielle R AHP-Pakete verwenden, wie zum Beispiel ahpsurvey package."
  },
  {
    "objectID": "rauman/Rauman4_Uebung_A_ahp.html#übung-1-definieren-sie-die-ausgangssituation",
    "href": "rauman/Rauman4_Uebung_A_ahp.html#übung-1-definieren-sie-die-ausgangssituation",
    "title": "Rauman 4: Übung A",
    "section": "Übung 1: Definieren Sie die Ausgangssituation",
    "text": "Übung 1: Definieren Sie die Ausgangssituation\nDenken Sie zunächst an eine aktuelle Entscheidung, vor der Sie gerade stehen oder gestanden haben (z. B. Fahrradkauf oder Wohnungsmiete), und definieren Sie die folgenden Punkte.\n\nEin Ziel für Ihren AHP (z.B. Wohnungsmiete)\n4 Kriterien, auf die Sie Ihre Entscheidung stützen wollen (z. B. Preis, Entfernung zur Arbeit/Schule, Grösse, Schönheit der Landschaft)\n3 verschiedene Optionen/Alternativen (z. B. 3 verschiedene Wohnungen)"
  },
  {
    "objectID": "rauman/Rauman4_Uebung_A_ahp.html#übung-2-paarweiser-vergleich-1-2",
    "href": "rauman/Rauman4_Uebung_A_ahp.html#übung-2-paarweiser-vergleich-1-2",
    "title": "Rauman 4: Übung A",
    "section": "Übung 2: Paarweiser Vergleich 1 & 2",
    "text": "Übung 2: Paarweiser Vergleich 1 & 2\nn einem ersten Schritt muss jedes Kriterium paarweise mit einem anderen Kriterium verglichen werden. Verwenden Sie die folgende Skala zur Gewichtung der Kriterien (siehe Tabelle 43.1).\n\n\n\n\nTabelle 43.1: AHP Skala Gewichtungen\n\n\nWert\nDefinition\n\n\n\n\n1\nDie beiden Merkmale sind gleich wichtig\n\n\n3\nKriterium A ist etwas wichtiger als Kriterium B\n\n\n5\nKriterium A ist mässig wichtiger als Kriterium B\n\n\n7\nKriterium A ist deutlich wichtiger als Kriterium B\n\n\n9\nKriterium A ist absolut wichtiger als Kriterium B\n\n\n2, 4, 6, 8\nZwischenwerte\n\n\n\n\n\n\nSie können den folgenden Code verwenden, um Ihre Gewichtungsmatrix zu erstellen. In der Matrix werden zwei Kriterien immer zweimal verglichen, und diese beiden Vergleiche sollten den Kehrwert des jeweils anderen darstellen. Zur Veranschaulichung haben wir einen Vergleich hinzugefügt, der wie folgt lautet:\n\nZeile 1, Spalte 2: Kriterium 1 ist etwas wichtiger als Kriterium 2\nZeile 2, Spalte 1: Kriterium 2 ist etwas weniger wichtig als Kriterium 1\n\nErstellen Sie diesen Matrixvergleich, der Ihren Kriterien entspricht, und ersetzen Sie die “0”-Werte durch Ihre Gewichte gemäss der Tabelle 43.1. Beachten Sie, dass alle Diagonalwerte gleich “1” sein sollten\n\npairwise_comparison <- c(\n  1,   3, 0, 0,\n  1/3, 1, 0, 0,\n  0,   0, 1, 0,\n  0,   0, 0, 1\n) |> matrix(ncol = 4, byrow = TRUE) \n\n\n\n\nTipp: Fügen Sie Spalten- und Zeilennamen hinzu, damit Ihre Matrix besser lesbar ist.\n\ncriterias <- c(\"price\", \"distance\", \"size\", \"beauty\")\n\nrownames(pairwise_comparison) <- criterias\ncolnames(pairwise_comparison) <- criterias"
  },
  {
    "objectID": "rauman/Rauman4_Uebung_A_ahp.html#übung-3-berechnung-der-kriteriengewichte",
    "href": "rauman/Rauman4_Uebung_A_ahp.html#übung-3-berechnung-der-kriteriengewichte",
    "title": "Rauman 4: Übung A",
    "section": "Übung 3: Berechnung der Kriteriengewichte",
    "text": "Übung 3: Berechnung der Kriteriengewichte\n\nÜbung 3.1: Normalisierung der Matrix (Berechnung der Kritiriengewichte 1)\nIm nächsten Schritt muss die Matrix normalisiert werden (siehe Abbildung 43.1). Dies kann in den folgenden zwei Schritten erfolgen:\n\nBerechnen Sie die Summe jeder Spalte mit colSums. Speichern Sie die Ausgabe in einer Variablen (z.B. ahp_colsums).\nTeilen Sie jeden Wert in der Matrix durch die entsprechende Spaltensumme. Um dies zu erreichen, können Sie die Funktion sweep() auf die Matrix anwenden, die der Funktion apply sehr ähnlich ist (verwenden Sie MARGIN = 2 (Spalten), STATS = ahp_colsums und FUN = \"/\").\n\n\n\n\n\n\n\nAbbildung 43.1: Normalisierung der Kriterien\n\n\n\n\nÜbung 3.2: Gewichtung der Kriterien (Berechnung der Kritiriengewichte 2)\nDies ist der letzte Schritt zur Berechnung der Gewichtung der einzelnen Kriterien (siehe Abbildung 43.2). Dies geschieht wie folgt:\n\nBerechne die Summe jeder Zeile und speichere das Ergebnis in einer Variablen (z.B. criteria_sum).\nDividiere die Summe der Kriterien durch die Summe der Kriterien und speichere das Ergebnis in einer Variablen (z.B. criteria_weight).\n\nHinweis: Die Summe von criteria_weight sollte 1 sein.\n\n\n\nAbbildung 43.2: Gewichtung der Kriterien"
  },
  {
    "objectID": "rauman/Rauman4_Uebung_A_ahp.html#übung-4-konsistenzanalyse-konsistenzanalyse-1-2",
    "href": "rauman/Rauman4_Uebung_A_ahp.html#übung-4-konsistenzanalyse-konsistenzanalyse-1-2",
    "title": "Rauman 4: Übung A",
    "section": "Übung 4: Konsistenzanalyse (Konsistenzanalyse 1 & 2)",
    "text": "Übung 4: Konsistenzanalyse (Konsistenzanalyse 1 & 2)\nNachdem der paarweise Vergleich durchgeführt wurde, muss eine Konsistenzanalyse durchgeführt werden, um zu prüfen, ob die paarweisen Vergleiche konsistent sind oder dieser Widersprüche enthalten. Eine gewisse Inkonsistenz ist im Rahmen eines AHP zulässig, sie sollte aber nicht zu gross sein.\nUm die Konsistenz zu berechnen, sollten Sie wie in Folie 30 (Konsistenzanalyse 1) und den folgenden Schritten vorgehen:\n\nFühren Sie eine Matrixmultiplikation (%*%) zwischen pairwise_comparison und criteria_weight durch.\n\n\n\n\n\nDividiere das Ergebnis von 1) durch criteria_weight\n\n\n\n\n\nBerechnen Sie \\(\\lambda_{max}\\), indem Sie die Summe des in 2) erhaltenen Ergebnisses durch die Anzahl der Kriterien dividieren\n\n\n\n\n\nBerechnung von \\(CI\\) (\\(CI = \\frac{\\lambda_{max} - n}{n-1}\\)), wobei “n” der Anzahl der Kriterien entspricht\n\n\n\n\n\nErmitteln Sie \\(RI\\) durch Nachschlagen in der Abbildung 43.3\n\n\n\n\n\nBerechnen Sie \\(CR\\) (\\(CR = CI / RI\\))\n\n\n\n\n\nWenn CR > 0,1 ist, müssen Sie Ihre paarweisen Vergleiche neu bewerten.\n\n\n\n\nAbbildung 43.3: AHP Random Index by Saaty"
  },
  {
    "objectID": "rauman/Rauman4_Uebung_A_ahp.html#gratuliere",
    "href": "rauman/Rauman4_Uebung_A_ahp.html#gratuliere",
    "title": "Rauman 4: Übung A",
    "section": "Gratuliere!",
    "text": "Gratuliere!\nSie haben nun die Gewichte bestimmt, auf denen Sie Ihre Entscheidung aufbauen können, und haben festgestellt, ob diese Gewichte konsistent sind oder nicht. Diese nächsten Schritte sind technisch gesehen sehr ähnlich zu dem, was Sie in der obigen Übung getan haben, daher überlassen wir es Ihnen, ob Sie diese Schritte ausführen wollen oder nicht. Der Vollständigkeit halber sei gesagt, dass der nächsten Schritte folgende wären:\n\nVergleichen Sie Ihre Optionen/Alternativen in einem paarweisen Vergleich miteinander (ähnlich wie Sie die Kriterien miteinander verglichen haben). Dies tun Sie für jedes Kriterium\nNormalisieren Sie die paarweisen Vergleiche Ihrer Optionen (ähnlich wie Sie die paarweisen Vergleiche der Kriterien normalisiert haben)\nVerwenden Sie die Gewichte, die Sie in der obigen Übung ermittelt haben, um Ihre Ergebnisse aus 2) zu gewichten\nBestimmen Sie die beste Entscheidung auf der Grundlage des Ergebnisses aus 3)"
  },
  {
    "objectID": "rauman/Rauman4_Uebung_B_raster.html",
    "href": "rauman/Rauman4_Uebung_B_raster.html",
    "title": "Rauman 4: Übung B",
    "section": "",
    "text": "Im dieser Übung werden wir terra verwenden, um zu zeigen, wie wir einen Rasterdatensatz importieren können. Unter dem untenstehenden Link können Sie eine tif-Datei herunterladen, die das “Digitale Höhenmodell” (DHM)* des Kantons Schwyz in der Schweiz darstellt. Laden Sie den Datensatz herunter und führen Sie den angegebenen Code aus.\n\nDatensatz: dhm25m.tif\n\n\nlibrary(terra)\n\nImportieren Sie Ihr Raster mit der Funktion rast\n\ndhm_schwyz <- rast(\"datasets/rauman/dhm25m.tif\")\n\nSie erhalten einige wichtige Metadaten über den Rasterdatensatz, wenn Sie den Variablennamen in die Konsole eingeben.\n\ndhm_schwyz \n\nclass       : SpatRaster \ndimensions  : 1496, 1861, 1  (nrow, ncol, nlyr)\nresolution  : 25, 25  (x, y)\nextent      : 672187.5, 718712.5, 193662.5, 231062.5  (xmin, xmax, ymin, ymax)\ncoord. ref. : CH1903 / LV03 \nsource      : dhm25m.tif \nname        : dhm25m \n\n\nUm einen schnellen Überblick eines Rasterdatensatz zu erhalten, können wir einfach die plot() Funktion verwenden.\n\nplot(dhm_schwyz)\n\n\n\n\nAbbildung 44.1: Generierter Plot\n\n\n\n\nLeider ist das Verwenden von Rastern in ggplot nicht sehr einfach. Da ggplot ein universelles Plot-Framework ist, stossen wir schnell an die Grenzen des Möglichen, wenn wir etwas so Spezielles wie Karten erstellen. Aus diesem Grund werden wir ein neues Plot-Framework einführen, das auf Karten spezialisiert ist und in einem sehr ähnlichen Design wie ggplot gebaut wurde: tmap. Installieren und laden Sie dieses Paket jetzt.\n\nlibrary(tmap)\n\nGenau wie ggplot basiert tmap auf der Idee von “Ebenen”, die durch ein + verbunden sind. Jede Ebene hat zwei Komponenten:\n\neine Datensatzkomponente, die immer tm_shape(dataset) ist (ersetzen Sie dataset durch Ihre Variable)\neine Geometriekomponente, die beschreibt, wie das vorangegangene tm_shape() visualisiert werden soll. Dies kann tm_dots() für Punkte, tm_polygons() für Polygone, tm_lines() für Linien usw. sein. Für Einzelbandraster (was bei dhm_schwyz der Fall ist) ist es tm_raster()\n\n\ntm_shape(dhm_schwyz) + \n  tm_raster() \n\n\n\n\nAbbildung 44.2: Generierter Plot\n\n\n\n\nBeachten Sie, dass tm_shape() und tm_raster() (in diesem Fall) zusammengehören. Das eine kann nicht ohne das andere leben.\nWenn Sie die Hilfe von ?tm_raster konsultieren, werden Sie eine Vielzahl von Optionen sehen, mit denen Sie die Visualisierung Ihrer Daten verändern können. Zum Beispiel ist der Standardstil von tm_raster() die Erstellung von “Bins” mit einer diskreten Farbskala. Wir können dies mit style = \"cont\" ausser Kraft setzen.\n\ntm_shape(dhm_schwyz) + \n  tm_raster(style = \"cont\") \n\n\n\n\nAbbildung 44.3: Generierter Plot\n\n\n\n\nDas sieht schon ziemlich toll aus, aber vielleicht wollen wir die Standard-Farbpalette ändern. Glücklicherweise ist das in tmap viel einfacher als in ggplot2. Um sich die verfügbaren Paletten anzusehen, geben Sie tmaptools::palette_explorer() oder RColorBrewer::display.brewer.all() in der Konsole ein (für Ersteres müssen Sie möglicherweise zusätzliche Pakete installieren, z.B. shinyjs).\n\ntm_shape(dhm_schwyz) + \n  tm_raster(style = \"cont\", palette = \"Spectral\") \n\n\n\n\nAbbildung 44.4: Generierter Plot\n\n\n\n\nSie können Layout-Anpassungen mit tm_layout() vornehmen, prüfen Sie ?tm_layout, um alle verfügbaren Optionen zu sehen!\n\n\ntm_shape(dhm_schwyz) + \n  tm_raster(style = \"cont\", palette = \"Spectral\", legend.is.portrait = FALSE, title = \"\") +\n  tm_layout(legend.outside = TRUE, legend.outside.position = \"bottom\",frame = FALSE)\n\n\n\n\nAbbildung 44.5: Generierter Plot"
  },
  {
    "objectID": "rauman/Rauman5_Uebung_mce.html",
    "href": "rauman/Rauman5_Uebung_mce.html",
    "title": "Rauman 5: Übung",
    "section": "",
    "text": "Die folgende Übung mag einigen von Ihnen bekannt vorkommen, wenn Sie unseren Bachelorstudiengang besucht haben. Dieses MCE wurde bereits im Bachelormodul “GIS” unter Verwendung von ArcGIS Pro und dessen ModelBuilder durchgeführt (siehe Abbildung 45.1).\nDas Ziel dieser Übung ist es, das gleiche MCE nur mit R durchzuführen. Wir werden hauptsächlich Funktionen aus den R-Paketen sf und raster verwenden. Bitte schauen Sie sich das Prozessmodell (siehe Abbildung 45.1) an, das in ArcGIS Pro erstellt wurde, und versuchen Sie herauszufinden, welche Funktionen in den beiden R-Paketen denen im Modell entsprechen. Sie werden sehen, dass es einige ähnliche Funktionen gibt, aber für einige Berechnungen ist ein anderer Ansatz erforderlich."
  },
  {
    "objectID": "rauman/Rauman5_Uebung_mce.html#übungen-1-daten-laden-und-anzeigen",
    "href": "rauman/Rauman5_Uebung_mce.html#übungen-1-daten-laden-und-anzeigen",
    "title": "Rauman 5: Übung",
    "section": "Übungen 1: Daten laden und anzeigen",
    "text": "Übungen 1: Daten laden und anzeigen\nLaden Sie die nachfolgenden Rasterdatensätze herunter:\n\n\n\nLayer\nKoord. System\nBeschreibung\nTyp\nAusschlussgebiet\n\n\n\n\ndhm25m.tif\nCH1903/LV03\nGeländemodell (m)\nRaster (25m)\nNein\n\n\neis25m.tif\nCH1903/LV03\nHäufigkeit der Vereisung (Tage/Jahr)\nRaster (25m)\nNein\n\n\nwind25m.tif\nCH1903/LV03\nDurchschnittliche Windgeschwindigkeit (dm/s)\nRaster (25m)\nNein\n\n\n\nLaden Sie das File windkraft_geodata.gpkg von Moodle herunter. Dieses beinhaltet die folgenden Layers:\n\n\n\n\n\n\n\n\n\n\nLayer\nKoord. System\nBeschreibung\nTyp\nAusschlussgebiet\n\n\n\n\nBewohnte_Flaeche\nCH1903/LV03\nSiedlungen (inkl. Puffer 200m)\nPolygon\n(Ja)/Distanz\n\n\nNationale_Schutzgebiete\nCH1903/LV03\nNationale Schutzgebiete\nPolygon\n(Ja)/Distanz\n\n\nSeeflaechen\nCH1903/LV03\nSeegebiete\nPolygon\nJa\n\n\nStrassen\nCH1903/LV03\nStrassen\nLinie\nNein/Distanz\n\n\nUntersuchungsgebiet_Schwyz\nCH1903/LV03\nUntersuchungsgebiet, Kanton Schwyz\nPolygon\nNein\n\n\nWaldgebiete\nCH1903/LV03\nWaldgebiete\nPolygon\n(Ja)/Distanz\n\n\n\nDie Rasterdaten können mit der Funktion terra::rast und die Vektorebenen mit sf::read_sf geladen werden. Zeigen Sie die verfügbaren Layers an und plotten Sie sie auf ansprechende Weise. Zur Visualisierung können Sie die Funktionen plot für Rasterdaten und ggplot für Vektordaten verwenden.\n\nWir werden die folgenden Pakete in dieser Übung verwenden:\n\nlibrary(sf)\nlibrary(terra)\nlibrary(dplyr)\nlibrary(tmap)"
  },
  {
    "objectID": "rauman/Rauman5_Uebung_mce.html#übung-2-ausschlusskriterien-zusammenführen",
    "href": "rauman/Rauman5_Uebung_mce.html#übung-2-ausschlusskriterien-zusammenführen",
    "title": "Rauman 5: Übung",
    "section": "Übung 2: Ausschlusskriterien zusammenführen",
    "text": "Übung 2: Ausschlusskriterien zusammenführen\nFühren Sie die Ausschlusskriterien Siedlungsgebiete, nationale Schutzgebiete, Seeflächen und Waldgebiete zusammen. Diese Vektordatensätze sind als Data Frames strukturiert und können daher durch einfaches Kombinieren zusammengeführt werden. Beachten Sie dabei, dass die Data Frames unterschiedliche Grössen haben. Zusätzlich müssen wir aus dem neu erstellten Vektordatensatz (Ausschlussgebiet) ein Raster erstellen. Dazu können Sie die Funktion rasterize verwenden. Die Ausgabe soll ein Raster mit 0 und 1 sein, wobei die Felder des Ausschlussbereichs den Wert 0 und die restlichen Felder den Wert 1 haben (siehe Abbildung 45.2).\n\nTipp: Um ein Raster mit nur 0 und 1 zu erhalten, verwenden Sie die Optionen “rasterize” field = 0 und background = 1.\nTipp: Um Vektordaten zu rastern, müssen Sie vorher ein leeres Raster erstellen. Dieses Raster sollte die gleichen Grenzen (extent), die gleiche Auflösung und das gleiche Koordinatensystem (crs) haben wie die anderen Rastersätze. Verwenden Sie dazu den folgenden Code.*\n\n\nr <- terra::rast(ext(kt_schwyz), \n          resolution = c(250, 250), \n          crs = \"EPSG:21781\")\n\n\n\n\nAbbildung 45.2: Ausschlussgebiete"
  },
  {
    "objectID": "rauman/Rauman5_Uebung_mce.html#übung-3-neigung-berechnen",
    "href": "rauman/Rauman5_Uebung_mce.html#übung-3-neigung-berechnen",
    "title": "Rauman 5: Übung",
    "section": "Übung 3: Neigung berechnen",
    "text": "Übung 3: Neigung berechnen\nAls nächstes berechnen Sie die Neigung in Grad auf der Grundlage des Geländemodells (dhm25m). Das terra-Paket bietet Ihnen eine sehr hilfreiche Funktion namens terrain.\n\nTipp: Wenn Sie die Terrain-Funktion verwenden, benutzen Sie die folgenden Optionen: v=“slope”, unit=“degrees”, neighbors=8.*"
  },
  {
    "objectID": "rauman/Rauman5_Uebung_mce.html#übung-4-entfernungen-zu-kriterien-berechnen",
    "href": "rauman/Rauman5_Uebung_mce.html#übung-4-entfernungen-zu-kriterien-berechnen",
    "title": "Rauman 5: Übung",
    "section": "Übung 4: Entfernungen zu Kriterien berechnen",
    "text": "Übung 4: Entfernungen zu Kriterien berechnen\nBei der Bewertung geeigneter Standorte für Windkraftanlagen ist die Entfernung zu Strassen, Waldgebieten, nationalen Schutzgebieten und bewohnten Gebieten relevant. Je nach Kriterium hat eine geringe oder grosse Entfernung einen positiven Einfluss auf die Bewertung der potenziellen Standorte. Führen Sie zu diesem Zweck eine Abstandsanalyse mit den ausgewählten Kriterien-Rasterlayern durch. Verwenden Sie für diese Berechnung die Rasterfunktion distance.\n\nTipp: Um die Funktion distance durchführen zu können, müssen Sie die Kriterien ebenfalls “rastern”. Sie können den gleichen Befehl wie in Übung 2 verwenden, aber nur mit der Option field = 1.\nTipp: Verwenden Sie erneut die Funktion “Zuschneiden”, um nur relevante Daten aus dem Untersuchungsgebiet zu erhalten."
  },
  {
    "objectID": "rauman/Rauman5_Uebung_mce.html#übung-5-kriterien-standardisieren-und-bewerten-grading",
    "href": "rauman/Rauman5_Uebung_mce.html#übung-5-kriterien-standardisieren-und-bewerten-grading",
    "title": "Rauman 5: Übung",
    "section": "Übung 5: Kriterien standardisieren und bewerten (grading)",
    "text": "Übung 5: Kriterien standardisieren und bewerten (grading)\nDie Datenebenen Neigung, Windgeschwindigkeit, Vereisungshäufigkeit und die in Übung 4 berechneten Entfernungsebenen haben unterschiedliche Einheiten (dm/s, Grad, d/yr und m). Diese Einheiten können nicht direkt miteinander verrechnet werden. Daher müssen die verschiedenen Ebenen durch eine lineare Abstufung operationalisiert werden. Die lineare Abstufung wird mit der Funktion reclassify durchgeführt. Verwenden Sie die Standards für die Neuklassifizierung aus Abbildung 45.3 (a) und Abbildung 45.3 (b)).\n\nTipp: Beachten Sie die Minimal- und Maximalwerte der einzelnen Rasterebenen.\nTipp: Hier ist ein Beispielcode zur Neuklassifizierung der Entfernungen zu Siedlungen.\n\n\nsettlements_max <- minmax(settlements_ed)[2]\n\nreclass_settlements <- c(0,80,0,\n                        80,160,0.1,\n                        160,240,0.2,\n                        240,320,0.3,\n                        320,400,0.4,\n                        400,480,0.5,\n                        480,560,0.6,\n                        560,640,0.7,\n                        640,720,0.8,\n                        720,800,0.9,\n                        800,settlements_max,1.0) |> matrix(ncol = 3, byrow = TRUE)\nreclass_settlements_ed <- terra::classify(settlements_ed, reclass_settlements)\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n(b)\n\n\n\n\nAbbildung 45.3: MCE Reclassify"
  },
  {
    "objectID": "rauman/Rauman5_Uebung_mce.html#übung-6-gewichtung-der-kriterien-mit-ahp",
    "href": "rauman/Rauman5_Uebung_mce.html#übung-6-gewichtung-der-kriterien-mit-ahp",
    "title": "Rauman 5: Übung",
    "section": "Übung 6: Gewichtung der Kriterien mit AHP",
    "text": "Übung 6: Gewichtung der Kriterien mit AHP\nFühren Sie einen AHP durch, um die Kriterien zu gewichten, die dem MCE zugrunde liegen. Vergleichen Sie zunächst die Kriterien paarweise und berechnen Sie dann die Gewichte - wie Sie es in der letzten Woche gelernt haben. Am Ende sollten Sie eine Liste mit 7 Gewichten erhalten, wie unten dargestellt.\n\nTipp: Sehen Sie sich die Übungen 2 und 3 aus der letzten Woche an. Verwenden Sie den vorbereiteten R-Code, um Ihre ahp-Matrix zu erstellen.\n\n\n\n\n\n\n\n\n\n\n\n\n\nWind\nStreets\nIce\nSettlements\nForest\nSlope\nProtected areas\n\n\n\n\n0.33862692\n0.09816760\n0.06166626\n0.24969460\n0.03515759\n0.18043000\n0.03625702\n\n\n\n\nahp_matrix <- c(\n  1, 0, 0, 0, 0, 0, 0, #Wind\n  0, 1, 0, 0, 0, 0, 0, #Distance to streets\n  0, 0, 1, 0, 0, 0, 0, #Ice\n  0, 0, 0, 1, 0, 0, 0, #Distance to settlements\n  0, 0, 0, 0, 1, 0, 0, #Distance to forests\n  0, 0, 0, 0, 0, 1, 0, #Slope\n  0, 0, 0, 0, 0, 0, 1  #Distance to protected areas\n) |> matrix(ncol = 7, byrow = TRUE)"
  },
  {
    "objectID": "rauman/Rauman5_Uebung_mce.html#übung-7-gewichtetes-overlay",
    "href": "rauman/Rauman5_Uebung_mce.html#übung-7-gewichtetes-overlay",
    "title": "Rauman 5: Übung",
    "section": "Übung 7: Gewichtetes Overlay",
    "text": "Übung 7: Gewichtetes Overlay\nDie linear gewichteten Kriterien (Übung 5) sollen nun unter Berücksichtigung der mit dem AHP ermittelten Gewichtung (Übung 6) miteinander kombiniert werden. Diese gewichtete Überlagerung kann mit Hilfe von Rasterberechnungen durchgeführt werden, indem einfach jedes Kriterium mit seinem Gewicht multipliziert und addiert wird (siehe Abbildung 45.4).\n\nTipp: Da die Rastersätze leicht unterschiedliche Ursprünge haben, erhöhen Sie die Toleranz, indem Sie rasterOptions(tolerance = 0.5) verwenden.\nTipp: Achten Sie bei der Multiplikation auch auf die Reihenfolge der Gewichte in Ihrer Liste.\n\n\n\n\nAbbildung 45.4: Gewichteter Overlay"
  },
  {
    "objectID": "rauman/Rauman5_Uebung_mce.html#übung-8-potenzielle-gebiete-mit-ausschlusskriterien-überschneiden",
    "href": "rauman/Rauman5_Uebung_mce.html#übung-8-potenzielle-gebiete-mit-ausschlusskriterien-überschneiden",
    "title": "Rauman 5: Übung",
    "section": "Übung 8: Potenzielle Gebiete mit Ausschlusskriterien überschneiden",
    "text": "Übung 8: Potenzielle Gebiete mit Ausschlusskriterien überschneiden\nDurch einfache Multiplikation des Ausschluss-Layers (Ergebnis aus Übung 2) mit dem gewichteten Overlay-Layer (Ergebnis aus Übung 7) schliessen wir alle Flächen mit dem Wert 0 (Ausschlussflächen) aus und behalten alle anderen Flächen mit dem Wert 1 (z.B. 0x3=0, 1x3=3). Erstellen Sie als Abschluss der Studie eine endgültige Darstellung der Potenzialflächen für Windkraftanlagen im Kanton Schwyz (wie Abbildung 45.5). Diskutieren Sie die Ergebnisse und bestimmen Sie drei mögliche Standorte, an denen eine konkrete Planung von Windkraftanlagen möglich wäre.\n\n\n\nAbbildung 45.5: Potenzielle Standorte"
  },
  {
    "objectID": "fallstudie_s/1_Einleitung.html",
    "href": "fallstudie_s/1_Einleitung.html",
    "title": "KW40: Einleitung",
    "section": "",
    "text": "Das rund 1100 ha grosse Naturschutzgebiet Wildnispark Zürich Sihlwald, welches im periurbanen Raum südlich von Zürich liegt, gilt seit dem 1. Januar 2010 als erster national anerkannter Naturerlebnispark. Er ist Teil des Wildnisparks Zürich und wichtiges Naherholungsgebiet für die Stadt Zürich.\nDas Schutzgebiet befindet sich im Spannungsfeld zwischen Schutz und Nutzen, denn einerseits sollen die Besuchenden den Wald erleben dürfen, andererseits soll sich dieser, in der Kernzone, frei entwickeln dürfen. Im Perimeter gelten darum verschiedene Regeln. So darf z. B. nur auf bestimmten Wegen mit den Velo gefahren werden.\n\nDas Management braucht solide, empirisch erhobene Daten zur Natur und zu den Besuchenden damit die Ziele von Nutzen und Schürzen erreicht werden können. Das Besuchermonitoring deckt den zweiten Teil dieser notwendigen Daten ab. Im Wildnispark Zürich sind dazu mehrere automatische Zählstellen in Betrieb. Die Zählstellen erfassen stundenweise die Besuchenden auf den Wegen. Einige Zählstellen erfassen richtungsgetrennt und / oder können zwischen verschiedenen Nutzergruppen wie Personen, die zu Fuss gehen, und Velofahrenden unterscheiden.\nIm Rahmen des Moduls Research Methods werden in dieser Fallstudie mehrere dieser automatischen Zählstellen genauer untersucht. Die Daten, welche im Besitz des WPZ sind, wurden bereits kalibriert. Das heisst, Zählungen während Wartungsarbeiten, bei Felhbetrieb o.ä. wurden bereits ausgeschlossen. Dies ist eine zeitintensive Arbeit und wir dürfen hier mit einem sauber aufbereiteten “Datenschatz” arbeiten.\nPerimeter des Wildnispark Zürichs mit den ungefähren Standorten von zwei ausgewählten automatischen Zählstellen.\n\n\n\n\nHinweis:\n\nDie Zähler 211 und 502 erfassen sowohl Fussgänger:innen als auch Fahrräder. Die Erfassung erfolgt richtungsgetrennt.\n\nDer Wildnispark wertet die Zahlen auf verschiedene Weise aus. So sind z. B. Jahresgänge (an welchen Monaten herrscht besonders viel Betrieb?) und die absoluten Nutzungszahlen bekannt. Vertiefte Auswertungen, die beispielsweise den Zusammenhang zwischen Besuchszahlen und dem Wetter untersuchen, werden nicht gemacht.\nUnsere Analysen in diesem Modul helfen dem Management, ein besseres Verständnis zum Verhalten der Besuchenden zu erlangen und bilden Grundlagen für Managemententscheide in der Praxis."
  },
  {
    "objectID": "fallstudie_s/1_Einleitung.html#ziel",
    "href": "fallstudie_s/1_Einleitung.html#ziel",
    "title": "KW40: Einleitung",
    "section": "Ziel",
    "text": "Ziel\nIn dieser Fallstudie zeigen wir, welche Einflüsse die Covid19-Pandemie im Frühjahr 2020 auf die täglichen Besuchszahlen im Wildnispark Zürich hatte. Dabei setzen wir den Fokus auf die Dämmerung und die Nacht, den in diesen Zeiten sind Wildtiere besonders sensibel gegenüber Störungen.\nIn unsere Analysen ziehen wir auch weitere erklärende Faktoren wie Wetter, Wochentag, Kalenderwoche und Schulferien mit ein. Die statistischen Auswertungen erlauben und somit klare Rückschlüsse auf die Effekte der Faktoren und deren Stärke zu ziehen."
  },
  {
    "objectID": "fallstudie_s/1_Einleitung.html#grundlagen",
    "href": "fallstudie_s/1_Einleitung.html#grundlagen",
    "title": "KW40: Einleitung",
    "section": "Grundlagen",
    "text": "Grundlagen\nZur Verfügung stehen:\n\ndie stündlichen Zählungen von Fussgänger:innen und Velos an den Zählstellen\nMeteodaten (Temperatur, Sonnenscheindauer, Niederschlagssumme)\nR-Skripte mit Hinweisen zur Auswertung"
  },
  {
    "objectID": "fallstudie_s/1_Einleitung.html#aufbau-der-fallstudie",
    "href": "fallstudie_s/1_Einleitung.html#aufbau-der-fallstudie",
    "title": "KW40: Einleitung",
    "section": "Aufbau der Fallstudie",
    "text": "Aufbau der Fallstudie\nIn dieser Fallstudie erheben wir zuerst selbst Daten auf dem Grüntal, welche wir dann deskriptiv auswerten. Anschliessend beschäftigen wir uns mit den Daten aus dem Wildnispark Zürich, welche wir ebenfalls deskriptiv auswerten und auch sttistische Modelle damit programmieren. Diese Ergebnisse werden dann im Abschlussbericht dokumentiert."
  },
  {
    "objectID": "fallstudie_s/2_Felderhebung.html",
    "href": "fallstudie_s/2_Felderhebung.html",
    "title": "KW 40 - KW 42: Felderhebung",
    "section": "",
    "text": "Es gibt eine Vielzahl an möglichen Methoden zur Erfassung der Besuchszahlen. Automatische Zählgeräte bieten die Möglichkeit lange und durchgehende Zeitreihen zu erfassen. Inputs dazu, wie diese ausgewertet werden können, erhält ihr in dieser Aufgabe."
  },
  {
    "objectID": "fallstudie_s/2_Felderhebung.html#ziele",
    "href": "fallstudie_s/2_Felderhebung.html#ziele",
    "title": "KW 40 - KW 42: Felderhebung",
    "section": "Ziele",
    "text": "Ziele\n\nDie Studierenden können das eingesetzte Zählgerät installieren und kennen die Vor- und Nachteile verschiedener Methoden.\nDie Studierenden können die Daten auslesen und explorativ analysieren."
  },
  {
    "objectID": "fallstudie_s/2_Felderhebung.html#grundlagen",
    "href": "fallstudie_s/2_Felderhebung.html#grundlagen",
    "title": "KW 40 - KW 42: Felderhebung",
    "section": "Grundlagen",
    "text": "Grundlagen\nDie Geräte werden innerhalb der unten eingezeichneten Elipsen platziert. Damit soll überprüft werden, wie stark frequentiert die Waldränder der ökologisch aufgewerteten Seeparzelle sind.\n\nDatenschutz ist ein wichtiges Thema. Die Besuchenden werden über den Zweck der Kameras informiert, die Daten nach der Bearbeitung wieder gelöscht und nicht weitergegeben."
  },
  {
    "objectID": "fallstudie_s/2_Felderhebung.html#auswertung",
    "href": "fallstudie_s/2_Felderhebung.html#auswertung",
    "title": "KW 40 - KW 42: Felderhebung",
    "section": "Auswertung",
    "text": "Auswertung\nAUFGABE ab dem 17. 10. 2022\nNachdem die Kameras für zwei Wochen im Einsatz standen, sichten wir zusammen die Ergebnisse.\nDa die Anzahl Passagen auf der Seeparzelle keine schöne Auswertung erlauben, arbeiten wir ab jetzt mit einem Datensatz aus dem WPZ. Die Vorteile für euch sind:\n\nihr habt genügend Daten für die Auswertung mit R und\ndie Daten sind im selben Format wir für die späteren Aufgaben.\n\n\nAufgabe 1: Vorarbeiten\n\nÜberlegt euch mögliche Darstellungsformen für die Anzahl Passagen und die beobachteten Aktivitäten an den untersuchten Standorten.\nSkizziert eure Ideen mittels Stift und Papier.\n\n\n\nAufgabe 2: Darstellung in R\nR bietet sehr viele Optionen zur Analyse und zur Darstellung der Daten. Nehmt bitte den bereitgestellten Datensatz zur Hand und visualisiert eure Ideen mit R.\nUntenstehend sind einige Ideen zur Umsetzung.\n\n### Bibliothek laden\n\n# zuerst muss sie installiert sein:\n# install.packages(\"tidyverse\")\nlibrary(tidyverse) # Arbeiten mit Datumsformaten\n\n### Datensatz einlesen\n\n# dabei speichere ich ihn gleich unter der Variable \"depo\" ab.\ndepo <- read.csv(\"data/Bsp_Data.csv\", sep = \";\")\n\n### Datum und Uhrzeit\n# das Datum und die Uhrzeit sind in einer Spalte. R liest das als \"Buchstaben\" ein. Wir definieren es als Datum:\ndepo$DatumUhrzeit <- as.POSIXct(depo$DatumUhrzeit, format = \"%d.%m.%Y %H:%M\")\n\n\n### Kennzahlen\n\n#zuerst schaue ich mir jeweils den Aufau und die Kennzahlen zum Datensaz an:\nstr(depo)\n\n# hat es im Datensatz noch fehlende Werte?\nsum(is.na(depo))\n\n# wie viele Personen sind IN das Gebiet gegangen?\nsum(depo$Fuss_IN)\n\n# wie viele insgesamt?\n# dafür erstellen wir zuerst eine neue Spalte mit der Totalen Anzahl pro Datum und Zeitstempel:\ndepo$Total = depo$Fuss_IN + depo$Fuss_OUT + depo$Velo_IN + depo$Velo_OUT\n# und berechnen nachher die Summe dieser neuen Spalte\nsum(depo$Total)\n\n\n# Darstellen der Anzahl Passagen pro Stunde und Tag\nplot(x = depo$DatumUhrzeit, y = depo$Total,\n     pch = 21,  # Form\n     cex = 1.5, # Grösse\n     bg=\"blue\") # Füllung\n\n\n\n\n\n# Darstellung der verschiedenen Nutzergruppen\nslieces <- c(sum(depo$Fuss_IN), sum(depo$Fuss_OUT), sum(depo$Velo_IN), sum(depo$Velo_OUT))\nlbls <- c(\"Fuss_IN\", \"Fuss_OUT\", \"Velo_IN\", \"Velo_OUT\") \npie(slieces, labels = lbls)\n\n\n\n\n\n\nAufgabe 3: für Fortgeschrittene\nDie Anzahl Passagen pro Zählstelle können nicht nur als statische Diagramme dargestellt werden. R ist auch ein GIS! Hier seht ihr, wie mit R interaktive Karten gestaltet werden können.\n\n# zuerst berechnen wir mit der Bibliothek tidyverse das Total pro Standort\ntotal_Standort <- depo |> \n  group_by(Standort, lon, lat) |> \n  summarise(Total = sum(Total))\n\n# dann überführen wir den Datensatz in ein räumliches Format \n# (Hinweis: dafür muss die Bibliothes \"sf\" installiert sein)\ntotal_Standort <- sf::st_as_sf(total_Standort, \n                         coords = c(\"lon\", \"lat\"),\n                         crs = 2056)\n\n# Transformiere die CH1903 Koordinaten in WGS84\ntotal_Standort <- sf::st_transform(total_Standort, crs = 4326)\n\n# Plotte nun eine interaktive Karte\n# install.packages(\"tmap\")\nlibrary(tmap)\n\n# setze den Modus auf Interaktiv\ncurrent.mode <- tmap_mode(\"view\") \n\n\n# plotte\ntm_basemap(server = c(Topo = \"Esri.WorldTopoMap\",\n                      Ortho = \"Esri.WorldImagery\",\n                      OSM = \"OpenStreetMap.HOT\"))+\n  tm_shape(total_Standort)+\n  tm_bubbles(size = \"Total\", border.col = \"black\", col = \"red\", scale = 5, alpha = 0.5)\n\n\n\n\n\n\nInteraktive Karte von zwei ausgewählten Zählstellen im Untersuchungsgebiet mit fiktiven Besuchszahlen."
  },
  {
    "objectID": "fallstudie_s/3_Aufgabenstellung.html",
    "href": "fallstudie_s/3_Aufgabenstellung.html",
    "title": "KW 42: Aufgabenstellung WPZ",
    "section": "",
    "text": "Hinweis: Bitte bearbeitet dieses Skript am 18.10.2022 erst nach der Präsentation / Diskussion eurer Visualisierungen aus der Aufgabenstellung [Einführung und Installation].\n\n\nIhr habt selbst ein (kleines) Besuchermonitoring auf dem Grüental durchgeführt und euch bereits mit dem WPZ beschäftigt. De Aufgaben im Zusammenhang mit dem Grüental sind nun abgeschlossen und wir beschäftigen uns ausschliesslich mit dem WPZ.\nIm Rahmen unserer Analyse programmieren wir multivariate Modelle, welche den Zusammenhang zwischen der Anzahl Besuchenden und verschiedenen Einflussfaktoren beschreiben. Dank den Modellen können wir sagen, wie die Besucher:innen auf die untersuchten Faktoren reagiert haben (siehe dazu auch [Einleitung], Ziele).\nKonkret sollen folgende Fragestellungen beantwortet werden:\n\n\nWelchen Einfluss haben neben den Phasen der Covid-Pandemie auch die Wetterparameter (Sonnenscheindauer, Tageshöchsttemperatur, Niederschlagssumme) sowie der Wochentag, die Ferien, die Kalenderwoche und das Jahr auf die Besuchszahlen am Tag, in der Dämmerung und in der Nacht?\nWie stark sind die jeweiligen Einflüsse, welche Effektrichtungen sind beobachtbar und welche der untersuchten Parameter sind signifikant?\nKönnen deutliche Unterschiede zwischen den “normalen”, vor-Covid19-Jahren und danach bei der Tageszeitliche Nutzung, den Wochen-, und / oder Saisongängen sowie den wichtigsten, deskriptiven Kennzahlen gefunden werden?\n\n\n\nJede Gruppe wertet Daten von einem Zähler aus. Sprecht miteinander ab, wer welchen Zähler behandelt (211 oder 502; Spezifikationen siehe [Einleitung], Hinweis). Jeder Zähler soll nur von einer Gruppe ausgewertet werden!\nBezieht in eure Auswertungen den gesamten zur Verfügung stehenden Zeitraum ein.\nFür euren Zähler stehen Zahlen zu Fussgänger:innen und Velos zur Verfügung (siehe [Einleitung], Hinweis). Entscheidet euch selbst, ob ihr Fussgänger:innen ODER Velos auswerten wollt. Die anderen Daten dürft ihr vernachlässigen.\nIm Bericht sollen die Informationen und Erfahrungen aus dem gesamten Verlauf der Fallstudie in geeigneter Weise einfliessen. Bezüglich der Felderhebung Grüntal erwarten wir keine Angaben.\n\n\n\n\n\n\nStruktur / Aufbau\n\n\nFragestellung (siehe oben; die Fragestellung ist vorgegeben, darf aber natürlich für den Bericht geschärft und optimal formuliert und konkretisiert werden.)\nMethoden (kurzes Kapitel mit den statistischen Analysen)\nResultate (deskriptive Statistik, multivariates Modell; kurzer Fliesstext sowie die notwendigen Tabellen und eine Auswahl möglichst informativer Grafiken)\nDiskussion (Diskussion der deskriptiven Analysen und der Modellergebnisse; dieser Abschnitt sollte die eigenen Resultate auch im Zusammenhang mit aktueller Fachliteratur reflektieren.)\nLiteraturverzeichnis (Tipp: Das Literaturverzeichnis sollte vollständig sein, sowie formal korrekt und einheitlich daherkommen. Wir erwarten speziell in der Diskussion eine Abstützung auf aktuelle Fachliteratur. Auf Moodle haben wir Euch eine Auswahl relevanter Papers bereitgestellt.)\nAnhang (für alle Auswertungen relevanter R-Code in geeigneter Form)\n\n\nGesamtumfang max. 7500 Zeichen (inkl. Leerzeichen; exkl. Einleitung, Tabellen, Literaturverzeichnis und Anhang)\nAbgabe am 8.1.2023 per Mail an hoce@zhaw.ch\n\n\n\n\n\nIst die Methode klar und verständlich formuliert?\nSind die deskriptiven Analysen klar beschrieben und geeignet visualisiert?\nIst die Variablenselektion klar beschrieben, plausibel und nachvollziehbar?\nSind die Modellresultate in Text- und Tabellenform korrekt beschrieben und geeignet visualisiert?\nIst die Diskussion klar formuliert und inhaltlich schlüssig?\nWie gut ist die Diskussion auf relevante und aktuelle Fachliteratur abgestützt?\nZusätzliche bewerten wir die inhaltliche Dichte der Arbeit und die formale Qualität (Sprache, Struktur, Aufbau, Darstellung, Literaturverzeichnis, Umgang mit Literatur im Text)\n\nZusammensetzung der Fallstudiennote:\n\nFallstudie-Leistungsnachweis 1 - Forschungsplan: 30 %\nFallstudie-Leistungsnachweis 2 - Multivariate Analyse: 70 %"
  },
  {
    "objectID": "fallstudie_s/4_Projektierung.html",
    "href": "fallstudie_s/4_Projektierung.html",
    "title": "KW 42: Projektierung",
    "section": "",
    "text": "Hinweis: Bitte bearbeitet dieses Skript am 18.10.2022 erst nach der Einführung [Multivariate Analyse: Abschlussbericht].\n\n\nVor den eigentlichen Auswertungen müssen einige Vorbereitungen unternommen werden. Die Zeit, die man hier investiert, wird in der späteren Projektphase um ein Mehrfaches eingespart.\nIch empfehle generell mit Projekten zu arbeiten, da diese sehr einfach ausgetauscht (auf verschiedene Rechner) und somit auch reproduziert werden können. Wichtig ist, dass es keine absoluten Arbeitspfade sondern nur relative gibt. Der Datenimport (und -export) kann mithilfe dieser relativen Pfade stark vereinfacht werden. –> Kurz gesagt: Projekte helfen alles am richtigen Ort zu behalten (mehr zur Arbeit mit Projekten: https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects).\n\nErstellt an einem passenden Speicherort ein neues Projekt mit einem treffenden Namen:\n\n–> File / New Project\n\n\n\nHinweise:\nNutzt für allen Text, welcher nicht im Code integriert ist, das Symbol #. Wenn ihr den Text als Titel definieren wollt, so dass er in der Übersicht erscheint, müssen vor dem Wort # und nach dem Wort #### eingefügt werden.\n\n# Texte, vor denen ein # und nach denen #### stehen, sind Überschriften\n\n# Ich bin eine Überschrift ####\n\n# Texte, vor denen ein # steht, erklaeren den Ablauf\n\n# Dann folgen die Arbeitsschritte\n1+1\n\n# Wenn man auf \"Outline\" klickt (oder CTRL + SHIFT + O), \n# öffnet sich die Übersicht zu den Überschriften\n\nTipps:\n\nAlt + - = <-\nCtrl + Shift + C = # vor der ausgewaehlten Zeile\n\nAufbau eines Skripts\nZuerst immer den Titel des Projekts sowie den Autor/ die Autorin des Skripts nennen. Hier soll auch die Herkunft der Daten ersichtlich sein und falls externe Daten verwendet werden, sollte geklärt werden, wer Dateneigentümer ist (WPZ und Meteo Schweiz).\nIm Skript soll immer die Ordnerstruktur des Projekts genannt werden. So kann der Arbeitsvorgang auf verschiedenen Rechnern einfach reproduziert werden (ich verwende hier ein Projektordner mit den Unterordnern __skripts, data, results).\nBeschreibt zudem folgendes die verwendete Meteodaten (siehe dazu Metadata Meteodaten, –> order_XXX_legend.txt)\nEin Skript soll in R eigentlich immer (mehr oder weniger) nach dem selbem Schema aufgebaut sein. Dieses Schema enthällt (nach den bereits erwähnten Definitionen) 4 Kapitel:\n\nMetadaten und Definitionen\nDatenimport,\nVorbereitung,\nDeskriptive Analyse und Visualisierung und\nMultifaktorielle Analyse und Visualisierung.\n\nBereitet euer Skript mit diesen Kapitel vor.\n\n#.###########################################################################################\n# Einfluss von COVID19 auf das Naherholungsverhalten in WPZ ####\n# Fallstudie Modul Research Methods, HS22. Autor/in ####\n#.##########################################################################################\n\n#.##########################################################################################\n# METADATA UND DEFINITIONEN ####\n#.##########################################################################################\n\n# Datenherkunft ####\n# ...\n\n#.##########################################################################################\n# 1. DATENIMPORT #####\n#.##########################################################################################\n\nIn einem Bericht sollen die Abbildung einheitlich sein Dafür braucht es u.a. eine Farbpalette. Ich definiere meine Auswahl bereits hier; das hat den Vorteil, dass man die Farbnamen nur einmal schreiben muss und später die selbst definierte Palette unter der Variable “mycolors” abrufen kann.\n\nmycolors <- c(\"orangered\",\"gold\", \"mediumvioletred\", \"darkblue\")\n\n\n\n\n\nGeplottet wird mit ggplot, daher wird tidyverse geladen. Diese Bibliothek ergaenzt BASE R in vielerlei Hinsicht uns ist eigentlich fast immer nötig.\nDa wir es bei Besucherdaten immer mit einem zeitlichen Bezug zu tun haben, benoetigen wir eine passende Bibliothek. Ich arbeite mit lubridate, POSIXct waere natuerlich auch moeglich.\nggpubr brauchen wir für das Darstellen von mehreren verschiedenen Plots in nur einem. - PerformanceAnalytics, MuMIn, AICcmodavg, fitdistrplus, lme4 und sjPlot werden fuer die spaeteren multivariaten Analysen benoetigt. -Die Modellguete werden wir mittels lattice, blmeco und lattice pruefen.\n\n–> Lädt nun die benoetigten Bibliotheken.\n\nDiese müssen zuerst mit install.packages(“NAME”) installiert werden.\n\n\n# Benoetigte Bibliotheken ####\nlibrary(tidyverse) # Data wrangling und piping\nlibrary(lubridate) # Arbeiten mit Datumsformaten\nlibrary(ggpubr)    # to arrange multiple plots in one graph\nlibrary(PerformanceAnalytics) # Plotte Korrelationsmatrix\nlibrary(MuMIn)     # Multi-Model Inference\nlibrary(AICcmodavg)# Modellaverageing\nlibrary(fitdistrplus)# Prueft die Verteilung in Daten\nlibrary(lme4)      # Multivariate Modelle\nlibrary(blmeco)    # Bayesian data analysis using linear models\nlibrary(sjPlot)    # Plotten von Modellergebnissen (tab_model)\nlibrary(lattice)   # einfaches plotten von Zusammenhängen zwischen Variablen\n\n\n\n\nWIr lesen später zwei verschiedene Datensätze ein. Beide sollen exakt denselben Zeitraum umfassen. Definiert dazu den ersten und letzten Tag gemäss den vorhandenen Zähldaten.\n\ndepo_start <- as.Date(\"2017-01-01\")\ndepo_end <- as.Date(\"2022-7-31\")\n\nWichtiger Teil unserer Auswertungen ist der Einfluss des Lockdown auf das Besuchsverhalten.\n-Wir müssen also Start und Ende der beiden Lockdowns in der Schweiz definieren:\n\nlock_1_start_2020 <- as.Date(\"2020-03-16\")\nlock_1_end_2020 <- as.Date(\"2020-05-11\")\n\nlock_2_start_2021 <- as.Date(\"2020-12-22\")\nlock_2_end_2021 <- as.Date(\"2021-03-01\")\n\nEbenfalls müssen die erste und letzte Kalenderwoche der Untersuchungsfrist definiert werden. Diese werden bei wochenweisen Analysen ausgeklammert da sie i.d.R. unvollstaendig sind (das ist ein späterer Arbeitsschritt). Geht wie oben vor. Tipp: der Befehl isoweek() liefert euch die Kalenderwoche.\nFerienzeiten können einen grossen Einfluss auf das Besucheraufkommen haben. Die relevanten Ferienzeiträume (je nach dem müsst ihr das anpassen) müssen daher bekannt sein. Zur Definition der Ferien kann z.B. folgend vorgegangen werden:\n\n# (https://www.schulferien.org/schweiz/ferien/2020/)\nWinterferien_2016_start <- as.Date(\"2017-01-01\") \nWinterferien_2016_ende <- as.Date(\"2017-01-08\")\n\nFruehlingsferien_2017_start <- as.Date(\"2017-04-15\") \nFruehlingsferien_2017_ende <- as.Date(\"2017-04-30\") \nSommerferien_2017_start <- as.Date(\"2017-07-15\") \nSommerferien_2017_ende <- as.Date(\"2017-08-20\") \nHerbstferien_2017_start <- as.Date(\"2017-10-07\") \nHerbstferien_2017_ende <- as.Date(\"2017-10-22\") \nWinterferien_2017_start <- as.Date(\"2017-12-23\") \nWinterferien_2017_ende <- as.Date(\"2018-01-07\") \n\nFruehlingsferien_2018_start <- as.Date(\"2018-04-21\") \nFruehlingsferien_2018_ende <- as.Date(\"2018-05-06\") \nSommerferien_2018_start <- as.Date(\"2018-07-14\") \nSommerferien_2018_ende <- as.Date(\"2018-08-19\") \nHerbstferien_2018_start <- as.Date(\"2018-10-06\") \nHerbstferien_2018_ende <- as.Date(\"2018-10-21\") \nWinterferien_2018_start <- as.Date(\"2018-12-22\") \nWinterferien_2018_ende <- as.Date(\"2019-01-06\") \n\nFruehlingsferien_2019_start <- as.Date(\"2019-04-20\") \nFruehlingsferien_2019_ende <- as.Date(\"2019-05-05\") \nSommerferien_2019_start <- as.Date(\"2019-07-13\") \nSommerferien_2019_ende <- as.Date(\"2019-08-18\") \nHerbstferien_2019_start <- as.Date(\"2019-10-05\") \nHerbstferien_2019_ende <- as.Date(\"2019-10-20\") \nWinterferien_2019_start <- as.Date(\"2019-12-21\") \nWinterferien_2019_ende <- as.Date(\"2020-01-05\")\n\nFruehlingsferien_2020_start <- as.Date(\"2020-04-11\")\nFruehlingsferien_2020_ende <- as.Date(\"2020-04-26\")\nSommerferien_2020_start <- as.Date(\"2020-07-11\")\nSommerferien_2020_ende <- as.Date(\"2020-08-16\")\nHerbstferien_2020_start <- as.Date(\"2020-10-03\")\nHerbstferien_2020_ende <- as.Date(\"2020-10-18\")\nWinterferien_2020_start <- as.Date(\"2020-12-19\")\nWinterferien_2020_ende <- as.Date(\"2021-01-03\")\n\nFruehlingsferien_2021_start <- as.Date(\"2021-04-24\")\nFruehlingsferien_2021_ende <- as.Date(\"2021-05-09\")\nSommerferien_2021_start <- as.Date(\"2021-07-17\")\nSommerferien_2021_ende <- as.Date(\"2021-08-22\")\nHerbstferien_2021_start <- as.Date(\"2021-10-09\")\nHerbstferien_2021_ende <- as.Date(\"2021-10-24\")\nWinterferien_2021_start <- as.Date(\"2021-12-18\")\nWinterferien_2021_ende <- as.Date(\"2022-01-02\")\n\nFruehlingsferien_2022_start <- as.Date(\"2022-04-16\")\nFruehlingsferien_2022_ende <- as.Date(\"2022-05-01\")\nSommerferien_2022_start <- as.Date(\"2022-07-16\")\nSommerferien_2022_ende <- as.Date(\"2022-08-21\")\nHerbstferien_2022_start <- as.Date(\"2022-10-08\")\nHerbstferien_2022_ende <- as.Date(\"2022-10-23\")\nWinterferien_2022_start <- as.Date(\"2022-12-24\")\nWinterferien_2022_ende <- as.Date(\"2023-01-08\")\n\nNun sind alle Vorbereitungen gemacht, die Projektstruktur aufgebaut und die eigentliche Arbeit kann beginnen."
  },
  {
    "objectID": "fallstudie_s/5_Vorverarbeitung_Uebung.html",
    "href": "fallstudie_s/5_Vorverarbeitung_Uebung.html",
    "title": "KW 42+43: Übung Datenvorverarbeitung",
    "section": "",
    "text": "Die Projektstruktur steht. Nun können die Daten eingelesen und die nötigen Datentypen definiert werden.\nLädt die Daten zuerst von Moodle herunter.\nHinweise:\n\nSiehe [Einleitung] für den Standort der Zähler 211 und 502.\nDie Daten sind auf Moodle unter ReMe HS22 MSc ENR / Fallstudie Biodiversity & Ecosystems / S_Daten abgelegt.\n\n\nZähldaten zu eurem Standort (211_sihlwaldstrasse_2017_2022.csv, 502_sihluferweg_2016_2022.csv)\nMeteodaten (order_105742_data.txt)\n\nDie Zähldaten des Wildnispark Zürich wurden vorgängig bereinigt (z.B. wurden Stundenwerte entfernt, an denen am Zähler Wartungsarbeiten stattgefunden haben). Das macht es für uns einfach, denn wir können die Daten ohne vorgängige Bereinigung einlesen. Behaltet aber im Hinterkopf, dass die Datenaufbereitung, die Datenbereinigung mit viel Aufwand verbunden ist.\n\nLest die Zählaten ein, speichert ihn unter der Variable depo und sichtet den Datensatz (z.B. str(), head(), view() usw.).\n\n\ndepo <- read.csv(\"./HIER RELATIVEN DATEIPFAD EINGEBEN\", sep = \"HIER SEPERATOR EINGEBEN\") \n# Speicherort sowie Dateiname anpassen\n\nHinweis: Im Stundenformat zeigen die Werte bei 11:00 die Zähldaten zwischen 11:00 bis 12:00 Uhr."
  },
  {
    "objectID": "fallstudie_s/5_Vorverarbeitung_Uebung.html#a",
    "href": "fallstudie_s/5_Vorverarbeitung_Uebung.html#a",
    "title": "KW 42+43: Übung Datenvorverarbeitung",
    "section": "1a)",
    "text": "1a)\n\nIm Datensatz des Wildnisparks sind Datum und Uhrzeit in einer Spalte. Diese müssen getrennt werden (Ich schlage hier den Ansatz des piping ( |> ) vor. Damit können in einem “Rutsch” mehrere Operationen ausgeführt werden).\nEbenfalls muss das Datum als solches definiert werden. Welches Format hat es (im Code: format = “HIER DATUMSFORMAT”)?\n\n\nstr(depo)\n\ndepo <- depo |>\n  mutate(Datum = as.character(Datum)) |> \n  mutate(Datum = as.Date(Datum, format = \"HIER DATUMSFORMAT\")) # hier wird Text zum Datum"
  },
  {
    "objectID": "fallstudie_s/5_Vorverarbeitung_Uebung.html#b",
    "href": "fallstudie_s/5_Vorverarbeitung_Uebung.html#b",
    "title": "KW 42+43: Übung Datenvorverarbeitung",
    "section": "1b)",
    "text": "1b)\nIhr könnt selbst wählen, ob ihr Fussgänger:innen oder Velos untersuchen wollt (je nachdem ob sie in eurem Datensatz vorhanden sind).\n\nEntfernt die überflüssigen Spalten aus dem Datensatz.Ich schlage vor, dass ihr dafuer den Befehl dplyr::select() verwendet."
  },
  {
    "objectID": "fallstudie_s/5_Vorverarbeitung_Uebung.html#c",
    "href": "fallstudie_s/5_Vorverarbeitung_Uebung.html#c",
    "title": "KW 42+43: Übung Datenvorverarbeitung",
    "section": "1c)",
    "text": "1c)\n\nBerechnen des Totals (IN + OUT), da dieses in den Daten nicht vorhanden ist (wiederum mit piping).\n\nTipp: Wenn man R sagt: “addiere mir Spalte x mit Spalte y”, dann macht R das für alle Zeilen in diesen zwei Spalten. Wenn man nun noch sagt: “speichere mir das Ergebnis dieser Addition in einer neuen Spalte namens Total”, dann hat man die Aufgabe bereits gelöst. Arbeitet mit mutate()).\nHinweis: Ihr habt das auch schon in Kapitel [Einführung und Installation] gemacht.\n\nEntfernt nun alle NA-Werte mit na.omit()."
  },
  {
    "objectID": "fallstudie_s/5_Vorverarbeitung_Uebung.html#a-1",
    "href": "fallstudie_s/5_Vorverarbeitung_Uebung.html#a-1",
    "title": "KW 42+43: Übung Datenvorverarbeitung",
    "section": "2a)",
    "text": "2a)\n\nLest die Meteodaten ein und speichert sie unter meteo."
  },
  {
    "objectID": "fallstudie_s/5_Vorverarbeitung_Uebung.html#b-1",
    "href": "fallstudie_s/5_Vorverarbeitung_Uebung.html#b-1",
    "title": "KW 42+43: Übung Datenvorverarbeitung",
    "section": "2b)",
    "text": "2b)\n\nAuch hier müssen die Datentypen manuell gesetzt werden.\n\nTipp: Das Datum wird als Integer erkannt. Zuerst muss es in Text umgewandelt werden aus dem dann das eigentliche Datum herausgelesen werden kann. Das ist mühsam - darum hier der Code.\n\nmeteo <- transform(meteo, time = as.Date(as.character(time), \"%Y%m%d\"))\n\nHinweis Was ist eigentlich Niederschlag:\nhttps://www.meteoschweiz.admin.ch/home/wetter/wetterbegriffe/niederschlag.html\n\nWerden den anderen Spalten die richtigen Typen zugewiesen? Falls nicht, ändert die Datentypen.\nNun schneiden wir den Datensatz auf die Untersuchungsdauer zu."
  },
  {
    "objectID": "fallstudie_s/5_Vorverarbeitung_Uebung.html#c-1",
    "href": "fallstudie_s/5_Vorverarbeitung_Uebung.html#c-1",
    "title": "KW 42+43: Übung Datenvorverarbeitung",
    "section": "2c)",
    "text": "2c)\n\nJetzt müssen auch hier alle nicht verfügbare Werte (NA’s) herausgefiltert werden.\n\nTipp: Entweder geht das mit na.omit() für alle Spalten oder, etwas konservativer, können mit filter() die zu filternden Spalten definiert werden. Mit folgendem Codeblock können z.B. alle Werte gefiltert werden, die in der Spalte stn nicht gleich NA sind (es werden also die Werte behalten, die vorhanden sind). Der Code muss für die anderen relevanten Spalten noch ergänzt werden.\n\nmeteo <- meteo |>\n  filter(!is.na(stn))|>\n  ...|>\n  ...\n\nHinweis: … steht im Code für folgende oder vorhergehende Zeilen im Code (in einer Pipe)\n\nPrüft nun, wie die Struktur des data.frame (df) aussieht und ob alle NA Werte entfernt wurden (sum(is.na(df$Variable))). Stimmen alle Datentypen?"
  },
  {
    "objectID": "fallstudie_s/5_Vorverarbeitung_Uebung.html#a-2",
    "href": "fallstudie_s/5_Vorverarbeitung_Uebung.html#a-2",
    "title": "KW 42+43: Übung Datenvorverarbeitung",
    "section": "3a)",
    "text": "3a)\nJetzt fügen wir viele Convinience Variabeln hinzu. Wir brauchen:\n\nWochentag; der Befehl dazu ist weekdays()\n\nTipp: R sortiert die Levels alphabetisch. Da das in unserem Fall aber sehr unpraktisch ist, müssen die Levels manuell bestimmt werden\n\n  ...\n  mutate(Wochentag = base::factor(Wochentag, \n                            levels = c(\"Montag\", \"Dienstag\", \"Mittwoch\", \n                                       \"Donnerstag\", \"Freitag\", \"Samstag\", \"Sonntag\")))\n  ...\n\nFrage: Was bedeutet base:: vor den eigentlichen Befehl?\n\nWerktag oder Wochenende?\n\n\n  ...\n  mutate(Wochenende = if_else(Wochentag == \"Montag\" | Wochentag == \"Dienstag\" | \n                           Wochentag == \"Mittwoch\" | Wochentag == \"Donnerstag\" | \n                           Wochentag == \"Freitag\", \"Werktag\", \"Wochenende\"))\n  ...\n\nFrage: Was bedeuten die | (zu erstellen mit AltGr + 7)? Welches ist das if Argument, welches das else?\n\nKalenderwoche: isoweek()\nMonat: month()\nJahr: year()\nPhase Covid (Code untenstehend). Wir definieren fünf Phasen:\n\n\nvon Anfang Untersuchungsperiode bis 1 Jahr vor Lockdown 1 (pre)\n1 Jahr vor Corona (normal)\nLockdown 1\nLockdown 2\nEnde 2. Lockdown bis Ende Untersuchungsperiode\n\nHinweis:\n\nIch mache den letzten Punkt nachgelagert, da zu viele Operationen in einem Schritt auch schon mal etwas durcheinander erzeugen können.\nWir packen alle Phasen (normal, die beiden Lockdowns und Covid aber ohne Lockdown) in eine Spalte –> long-format ist schöner (und praktischer für das plotten) als wide-format.\n\n\ndepo <- depo |>\n  mutate(Phase = if_else(Datum >= lock_1_start_2020 & Datum <= lock_1_end_2020,\n                         \"Lockdown_1\",\n                         if_else(Datum >= lock_2_start_2021 & Datum <= lock_2_end_2021,\n                                 \"Lockdown_2\",\n                                 if_else(Datum>= (lock_1_start_2020 - years(1)) & Datum < lock_1_start_2020,\n                                         \"Normal\", \n                                         if_else(Datum > lock_2_end_2021,\n                                                 \"Post\", \"Pre\")))))\n\n# hat das gepklappt?!\nunique(depo$Phase)\n\nFrage: Welches ist das if Argument, welches das else?\n\nÄndert die Datentypen der Spalten Wochenende, KW, Phase zu factor und sortiert die Levels, so dass diese Sinn machen (z.B. in Phase = Pre, Normal, Lockdown 1, Lockdown 2, Post)."
  },
  {
    "objectID": "fallstudie_s/5_Vorverarbeitung_Uebung.html#b-2",
    "href": "fallstudie_s/5_Vorverarbeitung_Uebung.html#b-2",
    "title": "KW 42+43: Übung Datenvorverarbeitung",
    "section": "3b)",
    "text": "3b)\n\nNun soll noch die volle Stunde als Integer im Datensatz stehen. Diese Angabe muss etwas mühsam aus den Daten gezogen werden (darum hier der fertige Code dazu):\n\n\ndepo$Stunde <- as.numeric(format(as.POSIXct(depo$Zeit,format=\"%H:%M:%S\"),\"%H\"))\n\n# ersetze 0 Uhr mit 24 Uhr (damit wir besser rechnen können)\ndepo$Stunde[depo$Stunde == 0] <- 24\nunique(depo$Stunde)\ntypeof(depo$Stunde)"
  },
  {
    "objectID": "fallstudie_s/5_Vorverarbeitung_Uebung.html#c-2",
    "href": "fallstudie_s/5_Vorverarbeitung_Uebung.html#c-2",
    "title": "KW 42+43: Übung Datenvorverarbeitung",
    "section": "3c)",
    "text": "3c)\nDie Daten wurden durch den WPZ kalibriert (Kommastellen).\n\nRundet sie auf 0 Nachkommastellen (Ganzzahl; unser Modell kann nicht mit Kommazahlen in der ahbängigen Variable umgehen).\nDefiniert sie sicherheitshalber als Integer\nMacht das für IN, OUT und Total.\n\n\ndepo$... <- round(..., digits = 0)\ndepo$... <- as.integer(...)"
  },
  {
    "objectID": "fallstudie_s/5_Vorverarbeitung_Uebung.html#d-tageszeit",
    "href": "fallstudie_s/5_Vorverarbeitung_Uebung.html#d-tageszeit",
    "title": "KW 42+43: Übung Datenvorverarbeitung",
    "section": "3d) Tageszeit",
    "text": "3d) Tageszeit\nWir setzen den Fokus unserer Untersuchung auf die Veränderung der Besuchszahlen in der Abend- und Morgendämmerung sowie der Nacht. Dafür müssen wir diese tageszeitliche Einteilung der Daten erst machen. Da dies über den Umfang dieser Fallstudie geht, liefere ich euch hier den Code dazu.\nDie wichtigsten Punkte:\n\nDie Tageslänge wurde für den Standort Zürich (Zeitzone CET) mit dem Package “suncalc” berechnet. Dabei wurden Sommer- und Winterzeit berücksichtigt.\nDie Einteilung der Tageszeit beruht auf dem Start und dem Ende der astronomischen Dämmerung sowie der Golden Hour. Der Morgen und der Abend wurden nach dieser Definition berechnet und um je eine Stunde Richtung Tag verlängert.\n\n\n# Einteilung Standort Zuerich\nLatitude <- 47.38598\nLongitude <- 8.50806\n\n# Zur Berechnung der Tageslaege muessen wir zuerst den Start und das Ende der Sommer-\n# zeit definieren\n# https://www.schulferien.org/schweiz/zeit/zeitumstellung/\n\nSo_start_2017 <- as.Date(\"2017-03-26\") \nSo_end_2017 <- as.Date(\"2017-10-29\") \nSo_start_2018 <- as.Date(\"2018-03-25\") \nSo_end_2018 <- as.Date(\"2018-10-28\") \nSo_start_2019 <- as.Date(\"2019-03-31\") \nSo_end_2019 <- as.Date(\"2019-10-27\") \nSo_start_2020 <- as.Date(\"2020-03-29\")\nSo_end_2020 <- as.Date(\"2020-10-25\")\nSo_start_2021 <- as.Date(\"2021-03-28\")\nSo_end_2021 <- as.Date(\"2021-10-31\")\nSo_start_2022 <- as.Date(\"2022-03-27\")\nSo_end_2022 <- as.Date(\"2022-10-30\")\n\n# Welche Zeitzone haben wir eigentlich?\n# Switzerland uses Central European Time (CET) during the winter as standard time, \n# which is one hour ahead of Coordinated Universal Time (UTC+01:00), and \n# Central European Summer Time (CEST) during the summer as daylight saving time, \n# which is two hours ahead of Coordinated Universal Time (UTC+02:00).\n# https://en.wikipedia.org/wiki/Time_in_Switzerland\n\n# Was sind Astronomische Dämmerung und Golden Hour ueberhaupt?\n# https://sunrisesunset.de/sonne/schweiz/zurich-kreis-1-city/\n# https://www.rdocumentation.org/packages/suncalc/versions/0.5.0/topics/getSunlightTimes\n\n# Wir arbeiten mit folgenden Variablen:\n# \"nightEnd\" : night ends (morning astronomical twilight starts)\n# \"goldenHourEnd\" : morning golden hour (soft light, best time for photography) ends\n# \"goldenHour\" : evening golden hour starts\n# \"night\" : night starts (dark enough for astronomical observations)\n\nlumidata <-\n  getSunlightTimes(\n    date = seq.Date(depo_start, depo_end, by = 1),\n    keep = c(\"nightEnd\", \"goldenHourEnd\", \"goldenHour\", \"night\"),\n    lat = Latitude,\n    lon = Longitude,\n    tz = \"CET\")\n\nlumidata <- lumidata |> \n  mutate(Jahreszeit = ifelse(date >= So_start_2017 & date <=  So_end_2017 |\n                               date >= So_start_2018 & date <=  So_end_2018 |\n                               date >= So_start_2019 & date <=  So_end_2019 |\n                               date >= So_start_2020 & date <= So_end_2020 |\n                               date >= So_start_2021 & date <= So_end_2021 |\n                               date >= So_start_2022 & date <= So_end_2022, \n                               \"Sommerzeit\", \"Winterzeit\"))\n\n# CH ist im Im Sommer CET + 1. \n# Darum auf alle relevanten Spalten eine Stunde addieren\n# hinweis: ich verzichte hier auf ifelse, da es einfacher und nachvollziehbarer scheint,\n# hier mit einem filter die betreffenden Spalten zu waehlen\nlumidata_So <- lumidata |> \n  filter(Jahreszeit==\"Sommerzeit\") |> \n  mutate(nightEnd = nightEnd + hours(1),\n         goldenHourEnd =  goldenHourEnd + hours(1),\n         goldenHour = goldenHour + hours(1),\n         night = night + hours(1))\n\nlumidata_Wi <- lumidata |> \n  filter(Jahreszeit==\"Winterzeit\") \n# verbinde sommer- und winterzeit wieder\nlumidata <- rbind(lumidata_So, lumidata_Wi) |> \n  arrange(date)\n\n# change data type\nlumidata$date <- as.Date(lumidata$date, format= \"%Y-%m-%d\")\n\n# drop unnecessary cols\nlumidata <- lumidata |> dplyr::select(-lat, -lon)\n\n# jetzt haben wir alle noetigen Angaben zu Sonnenaufgang, Tageslaenge usw. \n# diese Angaben koennen wir nun mit unseren Zaehldaten verbinden:\ndepo <- left_join(depo,lumidata, by = c(\"Datum\" =\"date\"))\n\n# aendere alle Zeit- und Datumsangaben so, dass sie gleich sind und miteinander verrechnet werden können.\ndepo <- depo |> \n  mutate(datetime = paste(Datum, Zeit)) |> \n  mutate(datetime = as.POSIXct(datetime, format = \"%Y-%m-%d  %H:%M:%S\"))|> \n  mutate(nightEnd = as.POSIXct(nightEnd)) |> \n  mutate(goldenHourEnd = as.POSIXct(goldenHourEnd)) |> \n  mutate(goldenHourEnd = goldenHourEnd + hours(1)) |> \n  mutate(goldenHour = as.POSIXct(goldenHour)) |> \n  mutate(goldenHour = goldenHour - hours(1)) |> \n  mutate(night = as.POSIXct(night))\n\n# im naechsten Schritt weise ich den Stunden die Tageszeiten Morgen, Tag, Abend und Nacht zu.\n# diese Zuweisung basiert auf der Einteilung gem. suncalc und eigener Definition.\ndepo <- depo|>\n  mutate(Tageszeit = if_else(datetime >= nightEnd & datetime <= goldenHourEnd, \"Morgen\",\n                             ifelse(datetime > goldenHourEnd & datetime < goldenHour, \"Tag\",\n                                    ifelse(datetime >= goldenHour & datetime <= night,\n                                           \"Abend\",\n                                           \"Nacht\")))) |>\n  mutate(Tageszeit = factor(Tageszeit, levels = c(\n    \"Morgen\", \"Tag\", \"Abend\", \"Nacht\")))\n\n# # behalte die relevanten Var\ndepo <- depo |> dplyr::select(-nightEnd, -goldenHourEnd, -goldenHour, -night)\n\n#Plotte zum pruefn ob das funktioniert hat\np <- ggplot(depo, aes(y = Datum, color = Tageszeit, x = Stunde))+\n  geom_jitter()+\n  scale_color_manual(values=mycolors)\n\nplotly::ggplotly(p)\n\nBei mir hat der Zusatz der Tageszeit noch zu einigen NA-Wertren geführt. Diese lösche ich einfach:\n\ndepo <- na.omit(depo)\n# hat das funktioniert?\nsum(is.na(depo))"
  },
  {
    "objectID": "fallstudie_s/5_Vorverarbeitung_Uebung.html#a-3",
    "href": "fallstudie_s/5_Vorverarbeitung_Uebung.html#a-3",
    "title": "KW 42+43: Übung Datenvorverarbeitung",
    "section": "4a)",
    "text": "4a)\nUnsere Daten liegen im Stundenformat vor. Für einige Auswertungen müssen wir aber auf ganze Tage zurückgreifen können.\n\nDie Stundendaten müssen zu ganzen Tagen aggregiert werden. Macht das wiederum einer Pipe. Bezieht folgende Gruppierungen (group_by()) mit ein: Datum, Wochentag, Wochenende, KW, Monat, Jahr, Phase. Summiert die Zählmengen separat (Total, IN, OUT) auf und speichert das Resultat unter depo_d.\n\nTipp: Wenn man die Convinience Variablen als grouping variable einspeisst, dann werden sie in das neue df übernommen und müssen nicht nochmals hinzugefügt werden\n\ndepo_d <- depo |> \n  group_by(VARIABLE1, VARIABLE2, ...) |>   # Gruppieren nach den Variablen\n  summarise(Total = sum(Fuss_IN + Fuss_OUT),# Berechnen der gewünschten Werte\n            Fuss_IN = sum(Fuss_IN),\n            ...\n\n\nErstellt nun einen Datensatz depo_daytime, in welchem ihr obrigen Schritt wiederholt aber zusätzlich noch die Gruppierung “Tageszeit” nutzt."
  },
  {
    "objectID": "fallstudie_s/5_Vorverarbeitung_Uebung.html#b-3",
    "href": "fallstudie_s/5_Vorverarbeitung_Uebung.html#b-3",
    "title": "KW 42+43: Übung Datenvorverarbeitung",
    "section": "4b)",
    "text": "4b)\n\nAggregiere die Stundenwerte nach dem Monat (Gruppierungen Monat, Jahr) und speichert das neue df unter depo_m.\n\nTipp: Braucht wiederum group_by() und summarise(). Nun brauchen wir nur noch das Total, keine Richtungstrennung mehr.\n\nFügt den neu erstellten df eine Spalte mit Jahr + Monat hinzu. Das ist etwas mühsam, darum hier der fertige Code dazu:\n\n\n# vergewissere, dass sicher df\ndepo_m <- as.data.frame(depo_m)\n# sortiere das df anhand zwei Spalten aufsteigend (damit die Reihenfolge sicher stimmt)\ndepo_m[\n  with(depo_m, order(Jahr, Monat)),]\n\n# Speichere dann Jahr und Monat in einer Spalte und formatiere diese als Datum \ndepo_m <- depo_m |> \n  mutate(Ym = paste(Jahr, Monat)) |>\n  mutate(Ym= lubridate::ym(Ym)) \n\n\nWiederholt diesen Schritt, diesmal aber mit der Gruppierung “Tageszeit” neben “Jahr” und “Monat” und speichert das Resultat unter “depo_m_daytime”."
  },
  {
    "objectID": "fallstudie_s/5_Vorverarbeitung_Uebung.html#c-3",
    "href": "fallstudie_s/5_Vorverarbeitung_Uebung.html#c-3",
    "title": "KW 42+43: Übung Datenvorverarbeitung",
    "section": "4c)",
    "text": "4c)\nMacht euch mit den Daten vertraut. Plottet sie, seht euch die df’s an, versteht, was sie repräsentieren.\nZ.B. sind folgende Befehle und Plots wichtig:\n\nstr()\nsummarize()\nhead()\nScatterplot, x = Datum, y = Anzahl pro Zeiteinheit\nHistrogram\nusw.\n\nHinweis: Geht noch nicht zu weit mit euren Plots. Die Idee ist, dass man sich einen Überblick über die Daten verschafft und noch keine “analysierenden” Plots erstellt.\n–> Erklärt dem Plenum am 25.10.2021 was ihr gemacht habt, was eure Daten zeigen und präsentiert diese einfachen Plots. \nNachdem nun alle Daten vorbereitet sind folgt im nächsten Schritt die deskriptive Analyse."
  },
  {
    "objectID": "fallstudie_s/5_Vorverarbeitung_Loesung.html",
    "href": "fallstudie_s/5_Vorverarbeitung_Loesung.html",
    "title": "KW 42+43: Lösung Datenvorverarbeitung",
    "section": "",
    "text": "Aufgabe 2: Meteodaten\n\n# Einlesen\nmeteo <- read.csv(\"./data/order_105742_data.txt\", sep = \";\")\n\n# Datentypen setzen\n# Das Datum wird als Integer erkannt. Zuerst muss es in Text umgewaldelt werden aus dem dann\n# das eigentliche Datum herausgelesen werden kann\nmeteo <- transform(meteo, time = as.Date(as.character(time), \"%Y%m%d\"))\n\n# Die eigentlichen Messwerte sind alle nummerisch\nmeteo <- meteo|>\n  mutate(tre200jx = as.numeric(tre200jx))|>\n  mutate(rre150j0 = as.numeric(rre150j0))|>\n  mutate(sremaxdv = as.numeric(sremaxdv)) |> \n  filter(time >= depo_start, time <=  depo_end) # schneide dann auf Untersuchungsdauer\n\n# Was ist eigentlich Niederschlag:\n# https://www.meteoschweiz.admin.ch/home/wetter/wetterbegriffe/niederschlag.html\n\n# Filtere Werte mit NA\nmeteo <- meteo |>\n  filter(!is.na(stn)) |>\n  filter(!is.na(time))|>\n  filter(!is.na(tre200jx))|>\n  filter(!is.na(rre150j0))|>\n  filter(!is.na(sremaxdv))\n# Pruefe ob alles funktioniert hat\nstr(meteo)\n\n'data.frame':   2025 obs. of  7 variables:\n $ stn     : chr  \"WAE\" \"WAE\" \"WAE\" \"WAE\" ...\n $ time    : Date, format: \"2017-01-01\" \"2017-01-02\" ...\n $ tre200nx: chr  \"-2.5\" \"1.0\" \"-0.6\" \"0.9\" ...\n $ tre200jx: num  -1.2 1.4 -1.4 1.6 -0.8 -4.5 -4.7 0.3 0.4 -0.6 ...\n $ rre150n0: chr  \"0.0\" \"3.0\" \"0.0\" \"1.8\" ...\n $ rre150j0: num  0 0 0 3.2 5.2 0 0 1.5 0 3.8 ...\n $ sremaxdv: num  1 28 2 0 0 75 6 0 6 0 ...\n\nsum(is.na(meteo)) # zeigt die Anzahl NA's im data.frame an\n\n[1] 0\n\n\n\n\nAufgabe 3: Datenvorverarbeitung (Mutationen)\n\n#.################################################################################################\n# 2. VORBEREITUNG DER DATEN #####\n#.################################################################################################\n\n# 2.1 Convinience Variablen ####\n# fuege dem Dataframe (df) die Wochentage hinzu\ndepo <- depo |> \n  mutate(Wochentag = weekdays(Datum)) |> \n  # R sortiert die Levels aplhabetisch. Da das in unserem Fall aber sehr unpraktisch ist,\n  # muessen die Levels manuell manuell bestimmt werden\n  mutate(Wochentag = base::factor(Wochentag, \n                            levels = c(\"Montag\", \"Dienstag\", \"Mittwoch\", \n                                       \"Donnerstag\", \"Freitag\", \"Samstag\", \"Sonntag\"))) |> \n  # Werktag oder Wochenende hinzufuegen\n  mutate(Wochenende = if_else(Wochentag == \"Montag\" | Wochentag == \"Dienstag\" | \n                           Wochentag == \"Mittwoch\" | Wochentag == \"Donnerstag\" | \n                           Wochentag == \"Freitag\", \"Werktag\", \"Wochenende\"))|>\n  #Kalenderwoche hinzufuegen\n  mutate(KW= isoweek(Datum))|>\n  # monat und Jahr\n  mutate(Monat = month(Datum)) |> \n  mutate(Jahr = year(Datum))\n\n#Lockdown \n# Hinweis: ich mache das nachgelagert, da ich die Erfahrung hatte, dass zu viele \n# Operationen in einem Schritt auch schon mal durcheinander erzeugen koennen.\n# Hinweis II: Wir packen alle Phasen (normal, die beiden Lockdowns und Covid aber ohne Lockdown)\n# in eine Spalte --> long ist schoener als wide\ndepo <- depo |>\n  mutate(Phase = if_else(Datum >= lock_1_start_2020 & Datum <= lock_1_end_2020,\n                         \"Lockdown_1\",\n                         if_else(Datum >= lock_2_start_2021 & Datum <= lock_2_end_2021,\n                                 \"Lockdown_2\",\n                                 if_else(Datum>= (lock_1_start_2020 - years(1)) & Datum < lock_1_start_2020,\n                                         \"Normal\", \n                                         if_else(Datum > lock_2_end_2021,\n                                                 \"Post\", \"Pre\")))))\n\n# hat das gepklappt?!\nunique(depo$Phase)\n\n[1] \"Pre\"        \"Normal\"     \"Lockdown_1\" \"Lockdown_2\" \"Post\"      \n\n# aendere die Datentypen\ndepo <- depo |> \n  mutate(Wochenende = as.factor(Wochenende)) |> \n  mutate(KW = factor(KW)) |> \n  # mit factor() koennen die levels direkt einfach selbst definiert werden.\n  # wichtig: speizfizieren, dass aus R base, ansonsten kommt es zu einem \n  # mix-up mit anderen packages\n  mutate(Phase = base::factor(Phase, levels = c(\"Pre\", \"Normal\", \"Lockdown_1\", \"Lockdown_2\", \"Post\")))\n\nstr(depo)\n\n'data.frame':   46330 obs. of  12 variables:\n $ Datum       : Date, format: \"2017-01-01\" \"2017-01-01\" ...\n $ Zeit        : chr  \"00:00:00\" \"01:00:00\" \"02:00:00\" \"03:00:00\" ...\n $ Fuss_IN     : int  0 0 0 0 0 0 0 0 0 0 ...\n $ Fuss_OUT    : int  0 0 0 0 0 0 0 0 0 0 ...\n $ DatumUhrzeit: chr  \"2017-01-01 00:00:00\" \"2017-01-02 00:00:00\" \"2017-01-03 00:00:00\" \"2017-01-04 00:00:00\" ...\n $ Total       : int  0 0 0 0 0 0 0 0 0 0 ...\n $ Wochentag   : Factor w/ 7 levels \"Montag\",\"Dienstag\",..: 7 7 7 7 7 7 7 7 7 7 ...\n $ Wochenende  : Factor w/ 2 levels \"Werktag\",\"Wochenende\": 2 2 2 2 2 2 2 2 2 2 ...\n $ KW          : Factor w/ 53 levels \"1\",\"2\",\"3\",\"4\",..: 52 52 52 52 52 52 52 52 52 52 ...\n $ Monat       : num  1 1 1 1 1 1 1 1 1 1 ...\n $ Jahr        : num  2017 2017 2017 2017 2017 ...\n $ Phase       : Factor w/ 5 levels \"Pre\",\"Normal\",..: 1 1 1 1 1 1 1 1 1 1 ...\n - attr(*, \"na.action\")= 'omit' Named int [1:2582] 2473 2474 2475 2476 2477 2478 2479 2480 2481 2482 ...\n  ..- attr(*, \"names\")= chr [1:2582] \"2473\" \"2474\" \"2475\" \"2476\" ...\n\n# Fuer einige Auswertungen muss auf die Stunden als nummerischer Wert zurueckgegriffen werden\ndepo$Stunde <- as.numeric(format(as.POSIXct(depo$Zeit,format=\"%H:%M:%S\"),\"%H\"))\n\n# ersetze 0 Uhr mit 24 Uhr (damit wir besser rechnen können)\ndepo$Stunde[depo$Stunde == 0] <- 24\nunique(depo$Stunde)\n\n [1] 24  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n\ntypeof(depo$Stunde)\n\n[1] \"double\"\n\n# Die Daten wurden kalibriert. Wir runden sie fuer unserer Analysen auf Ganzzahlen\ndepo$Total <- round(depo$Total, digits = 0)\ndepo$Fuss_IN <- round(depo$Fuss_IN, digits = 0)\ndepo$Fuss_OUT <- round(depo$Fuss_OUT, digits = 0)\n\n# 2.2 Tageszeit hinzufuegen ####\n\n# Einteilung Standort Zuerich\nLatitude <- 47.38598\nLongitude <- 8.50806\n\n# Zur Berechnung der Tageslaege muessen wir zuerst den Start und das Ende der Sommer-\n# zeit definieren\n# https://www.schulferien.org/schweiz/zeit/zeitumstellung/\n\nSo_start_2017 <- as.Date(\"2017-03-26\") \nSo_end_2017 <- as.Date(\"2017-10-29\") \nSo_start_2018 <- as.Date(\"2018-03-25\") \nSo_end_2018 <- as.Date(\"2018-10-28\") \nSo_start_2019 <- as.Date(\"2019-03-31\") \nSo_end_2019 <- as.Date(\"2019-10-27\") \nSo_start_2020 <- as.Date(\"2020-03-29\")\nSo_end_2020 <- as.Date(\"2020-10-25\")\nSo_start_2021 <- as.Date(\"2021-03-28\")\nSo_end_2021 <- as.Date(\"2021-10-31\")\nSo_start_2022 <- as.Date(\"2022-03-27\")\nSo_end_2022 <- as.Date(\"2022-10-30\")\n\n# Welche Zeitzone haben wir eigentlich?\n# Switzerland uses Central European Time (CET) during the winter as standard time, \n# which is one hour ahead of Coordinated Universal Time (UTC+01:00), and \n# Central European Summer Time (CEST) during the summer as daylight saving time, \n# which is two hours ahead of Coordinated Universal Time (UTC+02:00).\n# https://en.wikipedia.org/wiki/Time_in_Switzerland\n\n# Was sind Astronomische Dämmerung und Golden Hour ueberhaupt?\n# https://sunrisesunset.de/sonne/schweiz/zurich-kreis-1-city/\n# https://www.rdocumentation.org/packages/suncalc/versions/0.5.0/topics/getSunlightTimes\n\n# Wir arbeiten mit folgenden Variablen:\n# \"nightEnd\" : night ends (morning astronomical twilight starts)\n# \"goldenHourEnd\" : morning golden hour (soft light, best time for photography) ends\n# \"goldenHour\" : evening golden hour starts\n# \"night\" : night starts (dark enough for astronomical observations)\n\nlumidata <-\n  getSunlightTimes(\n    date = seq.Date(depo_start, depo_end, by = 1),\n    keep = c(\"nightEnd\", \"goldenHourEnd\", \"goldenHour\", \"night\"),\n    lat = Latitude,\n    lon = Longitude,\n    tz = \"CET\")\n\nlumidata <- lumidata |> \n  mutate(Jahreszeit = ifelse(date >= So_start_2017 & date <=  So_end_2017 |\n                               date >= So_start_2018 & date <=  So_end_2018 |\n                               date >= So_start_2019 & date <=  So_end_2019 |\n                               date >= So_start_2020 & date <= So_end_2020 |\n                               date >= So_start_2021 & date <= So_end_2021 |\n                               date >= So_start_2022 & date <= So_end_2022, \n                               \"Sommerzeit\", \"Winterzeit\"))\n\n# CH ist im Im Sommer CET + 1. \n# Darum auf alle relevanten Spalten eine Stunde addieren\n# hinweis: ich verzichte hier auf ifelse, da es einfacher und nachvollziehbarer scheint,\n# hier mit einem filter die betreffenden Spalten zu waehlen\nlumidata_So <- lumidata |> \n  filter(Jahreszeit==\"Sommerzeit\") |> \n  mutate(nightEnd = nightEnd + hours(1),\n         goldenHourEnd =  goldenHourEnd + hours(1),\n         goldenHour = goldenHour + hours(1),\n         night = night + hours(1))\n\nlumidata_Wi <- lumidata |> \n  filter(Jahreszeit==\"Winterzeit\") \n# verbinde sommer- und winterzeit wieder\nlumidata <- rbind(lumidata_So, lumidata_Wi) |> \n  arrange(date)\n\n# change data type\nlumidata$date <- as.Date(lumidata$date, format= \"%Y-%m-%d\")\n\n# drop unnecessary cols\nlumidata <- lumidata |> dplyr::select(-lat, -lon)\n\n# jetzt haben wir alle noetigen Angaben zu Sonnenaufgang, Tageslaenge usw. \n# diese Angaben koennen wir nun mit unseren Zaehldaten verbinden:\ndepo <- left_join(depo,lumidata, by = c(\"Datum\" =\"date\"))\n\n# aendere alle Zeit- und Datumsangaben so, dass sie gleich sind und miteinander verrechnet werden können.\ndepo <- depo |> \n  mutate(datetime = paste(Datum, Zeit)) |> \n  mutate(datetime = as.POSIXct(datetime, format = \"%Y-%m-%d  %H:%M:%S\"))|> \n  mutate(nightEnd = as.POSIXct(nightEnd)) |> \n  mutate(goldenHourEnd = as.POSIXct(goldenHourEnd)) |> \n  mutate(goldenHourEnd = goldenHourEnd + hours(1)) |> \n  mutate(goldenHour = as.POSIXct(goldenHour)) |> \n  mutate(goldenHour = goldenHour - hours(1)) |> \n  mutate(night = as.POSIXct(night))\n\n# im naechsten Schritt weise ich den Stunden die Tageszeiten Morgen, Tag, Abend und Nacht zu.\n# diese Zuweisung basiert auf der Einteilung gem. suncalc und eigener Definition.\ndepo <- depo|>\n  mutate(Tageszeit = if_else(datetime >= nightEnd & datetime <= goldenHourEnd, \"Morgen\",\n                             ifelse(datetime > goldenHourEnd & datetime < goldenHour, \"Tag\",\n                                    ifelse(datetime >= goldenHour & datetime <= night,\n                                           \"Abend\",\n                                           \"Nacht\")))) |>\n  mutate(Tageszeit = factor(Tageszeit, levels = c(\n    \"Morgen\", \"Tag\", \"Abend\", \"Nacht\")))\n\n# # behalte die relevanten Var\ndepo <- depo |> dplyr::select(-nightEnd, -goldenHourEnd, -goldenHour, -night)\n\n#Plotte zum pruefn ob das funktioniert hat\np <- ggplot(depo, aes(y = Datum, color = Tageszeit, x = Stunde))+\n  geom_jitter()+\n  scale_color_manual(values=mycolors)\n\nplotly::ggplotly(p)\n\n\n\n\n\n\n# bei mir hat der Zusatz der Tageszeit noch zu einigen NA-Wertren gefueht. \n# Diese loesche ich einfach:\ndepo <- na.omit(depo)\n# hat das funktioniert?\nsum(is.na(depo))\n\n[1] 0\n\n\n\n\nAufgabe 4: Aggregierung der Stundendaten\n\n# 2.4 Aggregierung der Stundendaten zu ganzen Tagen ####\n# Zur Berechnung von Kennwerten ist es hilfreich, wenn neben den Stundendaten auch auf Ganztagesdaten\n# zurueckgegriffen werden kann\n# hier werden also pro Nutzergruppe und Richtung die Stundenwerte pro Tag aufsummiert\ndepo_d <- depo |> \n  group_by(Datum, Wochentag, Wochenende, KW, Monat, Jahr, Phase) |> \n  summarise(Total = sum(Fuss_IN + Fuss_OUT), \n            Fuss_IN = sum(Fuss_IN),\n            Fuss_OUT = sum(Fuss_OUT)) \n# Wenn man die Convinience Variablen als grouping variable einspeisst, dann werden sie in \n# das neue df uebernommen und muessen nicht nochmals hinzugefuegt werden\n# pruefe das df\nhead(depo_d)\n\n# A tibble: 6 × 10\n# Groups:   Datum, Wochentag, Wochenende, KW, Monat, Jahr [6]\n  Datum      Wochentag  Wochenende KW    Monat  Jahr Phase Total Fuss_IN Fuss_…¹\n  <date>     <fct>      <fct>      <fct> <dbl> <dbl> <fct> <dbl>   <dbl>   <dbl>\n1 2017-01-01 Sonntag    Wochenende 52        1  2017 Pre      17      11       6\n2 2017-01-02 Montag     Werktag    1         1  2017 Pre      29      18      11\n3 2017-01-03 Dienstag   Werktag    1         1  2017 Pre       9       7       2\n4 2017-01-04 Mittwoch   Werktag    1         1  2017 Pre      11       4       7\n5 2017-01-05 Donnerstag Werktag    1         1  2017 Pre       2       1       1\n6 2017-01-06 Freitag    Werktag    1         1  2017 Pre      30      22       8\n# … with abbreviated variable name ¹​Fuss_OUT\n\n# nun gruppieren wir nicht nur nach Tag sondern auch noch nach Tageszeit\ndepo_daytime <- depo |> \n  group_by(Datum, Wochentag, Wochenende, KW, Monat, Jahr, Phase, Tageszeit) |> \n  summarise(Total = sum(Fuss_IN + Fuss_OUT), \n            Fuss_IN = sum(Fuss_IN),\n            Fuss_OUT = sum(Fuss_OUT)) \n\n\n# Gruppiere die Werte nach Monat\ndepo_m <- depo |> \n  group_by(Jahr, Monat) |> \n  summarise(Total = sum(Total)) \n# sortiere das df aufsteigend (nur das es sicher stimmt)\ndepo_m <- as.data.frame(depo_m)\ndepo_m[\n  with(depo_m, order(Jahr, Monat)),]\n\n   Jahr Monat Total\n1  2017     1   377\n2  2017     2   528\n3  2017     3   904\n4  2017     4   855\n5  2017     5  1009\n6  2017     6   729\n7  2017     7   878\n8  2017     8   997\n9  2017     9   854\n10 2017    10   951\n11 2017    11   444\n12 2017    12   309\n13 2018     1   346\n14 2018     2   314\n15 2018     3   487\n16 2018     4  1255\n17 2018     5  1369\n18 2018     6   999\n19 2018     7  1032\n20 2018     8   968\n21 2018     9   925\n22 2018    10   819\n23 2018    11   576\n24 2018    12   341\n25 2019     1   437\n26 2019     2   479\n27 2019     3   832\n28 2019     4  1050\n29 2019     5  1385\n30 2019     6  1658\n31 2019     7  1162\n32 2019     8  1115\n33 2019     9   865\n34 2019    10   790\n35 2019    11   635\n36 2019    12   460\n37 2020     1   633\n38 2020     2   623\n39 2020     3  1473\n40 2020     4  3038\n41 2020     5  2905\n42 2020     6  1333\n43 2020     7  1185\n44 2020     8  1335\n45 2020     9  1197\n46 2020    10  1650\n47 2020    11  1353\n48 2020    12   935\n49 2021     1   862\n50 2021     2   992\n51 2021     3  1002\n52 2021     4  1888\n53 2021     5  1962\n54 2021     6  1304\n55 2021     7   833\n56 2021     8   410\n57 2021     9  1038\n58 2021    10   855\n59 2021    11   420\n60 2021    12   177\n61 2022     1   615\n62 2022     2   618\n63 2022     3  1039\n64 2022     4   395\n65 2022     5   624\n66 2022     6  1100\n67 2022     7  1413\n\ndepo_m <- depo_m |> \n  mutate(Ym = paste(Jahr, Monat)) |> # und mache eine neue Spalte, in der Jahr und\n  mutate(Ym= lubridate::ym(Ym)) # formatiere als Datum\n\n# Gruppiere die Werte nach Monat und TAGESZEIT\ndepo_m_daytime <- depo |> \n  group_by(Jahr, Monat, Tageszeit) |> \n  summarise(Total = sum(Total)) \n# sortiere das df aufsteigend (nur das es sicher stimmt)\ndepo_m_daytime <- as.data.frame(depo_m_daytime)\ndepo_m_daytime[\n  with(depo_m_daytime, order(Jahr, Monat)),]\n\n    Jahr Monat Tageszeit Total\n1   2017     1    Morgen    54\n2   2017     1       Tag   267\n3   2017     1     Abend    43\n4   2017     1     Nacht    13\n5   2017     2    Morgen   133\n6   2017     2       Tag   362\n7   2017     2     Abend    27\n8   2017     2     Nacht     6\n9   2017     3    Morgen    63\n10  2017     3       Tag   762\n11  2017     3     Abend    50\n12  2017     3     Nacht    29\n13  2017     4    Morgen    68\n14  2017     4       Tag   785\n15  2017     4     Abend     2\n16  2017     4     Nacht     0\n17  2017     5    Morgen    68\n18  2017     5       Tag   940\n19  2017     5     Abend     0\n20  2017     5     Nacht     1\n21  2017     6    Morgen    53\n22  2017     6       Tag   671\n23  2017     6     Abend     4\n24  2017     6     Nacht     1\n25  2017     7    Morgen    80\n26  2017     7       Tag   797\n27  2017     7     Abend     1\n28  2017     7     Nacht     0\n29  2017     8    Morgen   132\n30  2017     8       Tag   855\n31  2017     8     Abend     8\n32  2017     8     Nacht     2\n33  2017     9    Morgen   104\n34  2017     9       Tag   744\n35  2017     9     Abend     1\n36  2017     9     Nacht     5\n37  2017    10    Morgen   175\n38  2017    10       Tag   756\n39  2017    10     Abend    17\n40  2017    10     Nacht     3\n41  2017    11    Morgen    83\n42  2017    11       Tag   296\n43  2017    11     Abend    50\n44  2017    11     Nacht    15\n45  2017    12    Morgen    63\n46  2017    12       Tag   183\n47  2017    12     Abend    59\n48  2017    12     Nacht     4\n49  2018     1    Morgen    61\n50  2018     1       Tag   238\n51  2018     1     Abend    43\n52  2018     1     Nacht     4\n53  2018     2    Morgen    36\n54  2018     2       Tag   241\n55  2018     2     Abend    33\n56  2018     2     Nacht     4\n57  2018     3    Morgen    69\n58  2018     3       Tag   403\n59  2018     3     Abend    15\n60  2018     3     Nacht     0\n61  2018     4    Morgen   121\n62  2018     4       Tag  1126\n63  2018     4     Abend     7\n64  2018     4     Nacht     1\n65  2018     5    Morgen   170\n66  2018     5       Tag  1194\n67  2018     5     Abend     5\n68  2018     5     Nacht     0\n69  2018     6    Morgen    80\n70  2018     6       Tag   904\n71  2018     6     Abend     7\n72  2018     6     Nacht     8\n73  2018     7    Morgen    84\n74  2018     7       Tag   943\n75  2018     7     Abend     4\n76  2018     7     Nacht     1\n77  2018     8    Morgen   239\n78  2018     8       Tag   718\n79  2018     8     Abend     9\n80  2018     8     Nacht     2\n81  2018     9    Morgen   146\n82  2018     9       Tag   759\n83  2018     9     Abend    15\n84  2018     9     Nacht     5\n85  2018    10    Morgen   148\n86  2018    10       Tag   647\n87  2018    10     Abend    19\n88  2018    10     Nacht     5\n89  2018    11    Morgen    90\n90  2018    11       Tag   402\n91  2018    11     Abend    78\n92  2018    11     Nacht     6\n93  2018    12    Morgen    76\n94  2018    12       Tag   189\n95  2018    12     Abend    66\n96  2018    12     Nacht    10\n97  2019     1    Morgen    82\n98  2019     1       Tag   283\n99  2019     1     Abend    66\n100 2019     1     Nacht     6\n101 2019     2    Morgen    70\n102 2019     2       Tag   368\n103 2019     2     Abend    41\n104 2019     2     Nacht     0\n105 2019     3    Morgen   111\n106 2019     3       Tag   695\n107 2019     3     Abend    25\n108 2019     3     Nacht     1\n109 2019     4    Morgen   163\n110 2019     4       Tag   875\n111 2019     4     Abend    10\n112 2019     4     Nacht     2\n113 2019     5    Morgen   127\n114 2019     5       Tag  1083\n115 2019     5     Abend     5\n116 2019     5     Nacht   170\n117 2019     6    Morgen   256\n118 2019     6       Tag  1376\n119 2019     6     Abend     9\n120 2019     6     Nacht    17\n121 2019     7    Morgen   142\n122 2019     7       Tag  1013\n123 2019     7     Abend     6\n124 2019     7     Nacht     1\n125 2019     8    Morgen   194\n126 2019     8       Tag   883\n127 2019     8     Abend    11\n128 2019     8     Nacht    27\n129 2019     9    Morgen   132\n130 2019     9       Tag   704\n131 2019     9     Abend     9\n132 2019     9     Nacht    20\n133 2019    10    Morgen   157\n134 2019    10       Tag   551\n135 2019    10     Abend    29\n136 2019    10     Nacht    53\n137 2019    11    Morgen    70\n138 2019    11       Tag   441\n139 2019    11     Abend    90\n140 2019    11     Nacht    34\n141 2019    12    Morgen    95\n142 2019    12       Tag   258\n143 2019    12     Abend    99\n144 2019    12     Nacht     8\n145 2020     1    Morgen   120\n146 2020     1       Tag   398\n147 2020     1     Abend    97\n148 2020     1     Nacht    18\n149 2020     2    Morgen    61\n150 2020     2       Tag   490\n151 2020     2     Abend    66\n152 2020     2     Nacht     6\n153 2020     3    Morgen    83\n154 2020     3       Tag  1306\n155 2020     3     Abend    74\n156 2020     3     Nacht    10\n157 2020     4    Morgen   294\n158 2020     4       Tag  2725\n159 2020     4     Abend    17\n160 2020     4     Nacht     2\n161 2020     5    Morgen   111\n162 2020     5       Tag  2789\n163 2020     5     Abend     4\n164 2020     5     Nacht     1\n165 2020     6    Morgen    77\n166 2020     6       Tag  1252\n167 2020     6     Abend     4\n168 2020     6     Nacht     0\n169 2020     7    Morgen    72\n170 2020     7       Tag  1104\n171 2020     7     Abend     9\n172 2020     7     Nacht     0\n173 2020     8    Morgen   190\n174 2020     8       Tag  1129\n175 2020     8     Abend    16\n176 2020     8     Nacht     0\n177 2020     9    Morgen   182\n178 2020     9       Tag  1000\n179 2020     9     Abend    11\n180 2020     9     Nacht     4\n181 2020    10    Morgen   244\n182 2020    10       Tag  1322\n183 2020    10     Abend    71\n184 2020    10     Nacht    13\n185 2020    11    Morgen   136\n186 2020    11       Tag  1018\n187 2020    11     Abend   191\n188 2020    11     Nacht     8\n189 2020    12    Morgen   153\n190 2020    12       Tag   522\n191 2020    12     Abend   241\n192 2020    12     Nacht    19\n193 2021     1    Morgen   134\n194 2021     1       Tag   574\n195 2021     1     Abend   141\n196 2021     1     Nacht    13\n197 2021     2    Morgen    98\n198 2021     2       Tag   820\n199 2021     2     Abend    71\n200 2021     2     Nacht     3\n201 2021     3    Morgen   107\n202 2021     3       Tag   848\n203 2021     3     Abend    43\n204 2021     3     Nacht     4\n205 2021     4    Morgen   152\n206 2021     4       Tag  1717\n207 2021     4     Abend    10\n208 2021     4     Nacht     9\n209 2021     5    Morgen    98\n210 2021     5       Tag  1862\n211 2021     5     Abend     2\n212 2021     5     Nacht     0\n213 2021     6    Morgen   109\n214 2021     6       Tag  1193\n215 2021     6     Abend     2\n216 2021     6     Nacht     0\n217 2021     7    Morgen    40\n218 2021     7       Tag   791\n219 2021     7     Abend     2\n220 2021     7     Nacht     0\n221 2021     8    Morgen    27\n222 2021     8       Tag   383\n223 2021     8     Abend     0\n224 2021     8     Nacht     0\n225 2021     9    Morgen   118\n226 2021     9       Tag   914\n227 2021     9     Abend     4\n228 2021     9     Nacht     2\n229 2021    10    Morgen   177\n230 2021    10       Tag   654\n231 2021    10     Abend     7\n232 2021    10     Nacht    17\n233 2021    11    Morgen    61\n234 2021    11       Tag   310\n235 2021    11     Abend    43\n236 2021    11     Nacht     6\n237 2021    12    Morgen    38\n238 2021    12       Tag   105\n239 2021    12     Abend    30\n240 2021    12     Nacht     4\n241 2022     1    Morgen   100\n242 2022     1       Tag   448\n243 2022     1     Abend    59\n244 2022     1     Nacht     8\n245 2022     2    Morgen    55\n246 2022     2       Tag   527\n247 2022     2     Abend    29\n248 2022     2     Nacht     7\n249 2022     3    Morgen    93\n250 2022     3       Tag   908\n251 2022     3     Abend    35\n252 2022     3     Nacht     3\n253 2022     4    Morgen    67\n254 2022     4       Tag   323\n255 2022     4     Abend     2\n256 2022     4     Nacht     3\n257 2022     5    Morgen    28\n258 2022     5       Tag   594\n259 2022     5     Abend     2\n260 2022     5     Nacht     0\n261 2022     6    Morgen   116\n262 2022     6       Tag   978\n263 2022     6     Abend     6\n264 2022     6     Nacht     0\n265 2022     7    Morgen   114\n266 2022     7       Tag  1295\n267 2022     7     Abend     4\n268 2022     7     Nacht     0\n\ndepo_m_daytime <- depo_m_daytime |> \n  mutate(Ym = paste(Jahr, Monat)) |> # und mache eine neue Spalte, in der Jahr und\n  mutate(Ym= lubridate::ym(Ym)) # formatiere als Datum"
  },
  {
    "objectID": "fallstudie_s/6_Deskr_Analysen_Uebung.html#a",
    "href": "fallstudie_s/6_Deskr_Analysen_Uebung.html#a",
    "title": "KW 43+44: Übung Deskriptiv",
    "section": "1a)",
    "text": "1a)\nNachdem wir die Projektstruktur aufgebaut haben und die Daten vorbereitet (inkl. aggregiert) sind, machen wir uns an die deskriptive Analyse. Dies macht eigentlich immer Sinn. Bevor mach sich an die schliessende Statistik macht, muss man ein “Gefühl” für die Daten bekommen. Dies funktioniert am einfachsten mit explorativen Analysen.\nWir interessieren uns in den Analysen für 5 Zeitabschnitte:\n\nvon Anfang Untersuchungsperiode bis 1 Jahr vor Lockdown 1 (pre)\n1 Jahr vor Corona (normal)\nLockdown 1\nLockdown 2\nEnde 2. Lockdown bis Ende Untersuchungsperiode\n\n\nPlottet den Verlauf der monatlichen Besuchszahlen an eurer Zählstelle. Auf der x-Achse steht dabei dabei Jahr und Monat (gespeichert im df depo_m), auf der y-Achse die monatlichen Besuchszahlen. Zeichnet auch die beiden Lockdown ein (Hinweis: rundet das Start- und Enddatum der Lockdowns auf den Monat, da im Plot die monatlichen Zahlen gezeigt werden).\n\nHaltet euch dabei an untenstehenden Plot:\n\n\n\n\n\nHinweis: - Nutzt zum plotten ggplot() - folgende Codeschnipsel helfen euch:\n\nggplot(data = depo_m, mapping = aes(Ym, Total, group = 1))+ # group 1 braucht R, dass aus den Einzelpunkten ein Zusammenhang hergestellt wird\n  # zeichne Lockdown 1; ein einfaches Rechteck. bestimme mit min und max die Dimensionen\n  geom_rect(mapping = aes(xmin=\"2020 3\", xmax=\"2020 5\",\n                          ymin =0, ymax=max(Total+(Total/100*10))), # das Rechteck soll 10 % grösser als die maximale Besuchszahl sein \n            fill = \"lightskyblue\", alpha = 0.4, colour = NA)+\n  # zeichne Lockdown 2\n    ...+\n  # zeichne die Linie\n  geom_line(...)+\n  theme_linedraw(base_size = 15)+\n  ...\n\n\nExportiert euren Plot mit ggsave() nach results. Breite = 20, Höhe = 10, Einheiten = cm, dpi = 1000"
  },
  {
    "objectID": "fallstudie_s/6_Deskr_Analysen_Uebung.html#b",
    "href": "fallstudie_s/6_Deskr_Analysen_Uebung.html#b",
    "title": "KW 43+44: Übung Deskriptiv",
    "section": "1b)",
    "text": "1b)\nNachdem wir wissen, wie sich die Besuchszahlen allgemein entwickelt haben, untersuchen wir wie sich diese während den unterschiedlichen Tageszeiten entwickelten.\n\nWie benötigen dazu wieder denselben Datensatz, dieselben x- und y-Achsen. Allerdings ergänzen wir den area-plot mit dem “fill”-Argument:\nSpeichert auch diesen Plot.\n\n\nggplot(depo_m_daytime, aes(Ym, Total, fill = Tageszeit)) + \n  geom_area(position = \"fill\")+\n  ..."
  },
  {
    "objectID": "fallstudie_s/6_Deskr_Analysen_Uebung.html#aufgabe-2-wochengang",
    "href": "fallstudie_s/6_Deskr_Analysen_Uebung.html#aufgabe-2-wochengang",
    "title": "KW 43+44: Übung Deskriptiv",
    "section": "Aufgabe 2: Wochengang",
    "text": "Aufgabe 2: Wochengang\nNachdem wir nun wissen, wie sich die Besuchszahlen während der Untersuchungsdauer monatlich entwickelt haben, möchten wir genauer untersuchen, wie sich die Zahlen je nach Phase (Pre, Normal, Lockdown 1, Lockdown 2 und Covid) auf die Wochentage verteilen.\n\n2a)\n\nBerechnet zuerst die Totale Anzahl pro Wochentag pro Phase.\n\n\nmean_phase_wd <- depo_d %>% \n  group_by(...) %>% \n  ...\n\n\nSpeichert das als .csv\n\n\nwrite.csv(mean_phase_wd, \"results/mean_phase_wd.csv\")\n\n\n\n2b)\n\nErstellt einen Boxplot nach untenstehender Vorgabe:\n\n\n\n\n\n\nHinweis: - Nutzt zum plotten ggplot() - folgende Codeschnipsel helfen euch:\n\nggplot(data = depo_d)+\n  geom_boxplot(mapping = aes(x= Wochentag, y = Total, fill = Phase))+\n  ...\n\n\nExportiert auch diesen Plot mit ggsave(). Welche Breite und Höhe passt hier?\n\n\n\n2c)\nSind die Unterschiede zwischen Werktag und Wochenende wirklich signifikant? Falls ja, in allen Phasen oder nur während bestimmter?\n\nPrüft das pro Phase mit einem einfachen t.test."
  },
  {
    "objectID": "fallstudie_s/6_Deskr_Analysen_Uebung.html#aufgabe-3-tagesgang",
    "href": "fallstudie_s/6_Deskr_Analysen_Uebung.html#aufgabe-3-tagesgang",
    "title": "KW 43+44: Übung Deskriptiv",
    "section": "Aufgabe 3: Tagesgang",
    "text": "Aufgabe 3: Tagesgang\nVom Grossen zum Kleinen, von der Übersicht ins Detail. Jetzt widmen wir uns dem Tagesgang, das heisst der Verteilung der Besuchenden auf die 24 Tagesstunden je nach Phase.\n\n3a)\n\nBerechnet zuerst den Mittelwert der Totalen Besuchszahlen pro Wochentag pro Stunde pro Phase. (ganz ähnlich wie unter 2a) und speichert das df unter Mean_h.\n\nggplots haben Daten lieber im Format long als wide.\n\n\n3b)\n\nPlottet den Tagesgang, unterteilt nach den 7 Wochentagen nun für unsere 5 Phasen.\n\n\n\n\n\n\nHinweis: - Nutzt zum plotten ggplot() - folgende Codeschnipsel helfen euch:\n\nggplot(Mean_h, aes(x = Stunde, y = Total, colour = Wochentag, linetype = Wochentag))+\n  geom_line(size = 2)+\n  facet_grid(...)\n  ..."
  },
  {
    "objectID": "fallstudie_s/6_Deskr_Analysen_Uebung.html#aufgabe-4-kennzahlen",
    "href": "fallstudie_s/6_Deskr_Analysen_Uebung.html#aufgabe-4-kennzahlen",
    "title": "KW 43+44: Übung Deskriptiv",
    "section": "Aufgabe 4: Kennzahlen",
    "text": "Aufgabe 4: Kennzahlen\nSchliesslich berechnen wir noch einige Kennzahlen (Anzahl Passagen, Richtungsverteilung, …).\n\n4a)\n\nGruppiert nach Phase und Tageszeit und berechnet dieses mal die Summe (nicht den Durchschnitt) Total, IN und OUT (ähnlich wie in 2a und 3a).\nNehmt dafür das vorbereitete df “depo_daytime”\nSpeichert das Ergebnis als .csv\n\n\n\n4b)\nDie Zeitreihen der 5 Phasen unterscheiden sich deutlich voneinander. Totale Summen sind da kaum miteinander vergleichbar, besser eignet sich der Durchschnitt oder der Median.\n\nGruppiert nach Phase und Tageszeit und berechnet den Durchschnitt Total, IN und OUT und speichert das df unter mean_phase_d.\nErgänzt das mit der prozentualen Richtungsverteilung\n\n\nmean_phase_d <- mean_phase_d %>% \n  mutate(Proz_IN = round(100/Total*IN, 1)) %>% # berechnen und auf eine Nachkommastelle runden\n  ...\n\n\n\n4c)\nSchliesslich soll das Ergebnis noch visualisiert werden.\n\nErstellt einen Plot nach untenstehendem Beispiel und speichert ihn:"
  },
  {
    "objectID": "fallstudie_n/1_Vorbemerkung.html",
    "href": "fallstudie_n/1_Vorbemerkung.html",
    "title": "1. Vorbemerkung",
    "section": "",
    "text": "Aktuell dient diese Plattform für die BiEc Fallstudie - Profil N einzig der Bereitstellung von Aufgaben die von euch im Rahmen dieses Fallstudienprojekts erarbeitet werden sollen. Die Aufgaben werden in den meisten Fällen mit Code-Beispielen erläutert oder benötigten Code-snippets resp. Funktionen werden mitgeliefert. Im Laufe des Semesters werden hier ausserdem häppchenweise (mögliche) Lösungen zu den Aufgaben aufgeschaltet. Alles grundlegende Material und alle Unterlagen zu den theoretischen Inputs sind weiterhin und ausschliesslich im Moodlekurs Research Methods - Fallstudie BiEc zu finden. Die für die Aufgaben benötigten Datengrundlagen sind ebenfalls im entsprechenden Abschnitt auf Moodle zu finden. Frohes Schaffen!"
  },
  {
    "objectID": "fallstudie_n/2_Datenverarbeitung_Uebung.html",
    "href": "fallstudie_n/2_Datenverarbeitung_Uebung.html",
    "title": "2. Daten(vor)verarbeitung - Übung",
    "section": "",
    "text": "Vor den eigentlichen Auswertungen müssen einige vorbereitende Arbeiten unternommen werden. Die Zeit, die man hier investiert, wird in der späteren Projektphase um ein vielfaches eingespart. Im Skript soll die Ordnerstruktur des Projekts genannt werden, damit der Arbeitsvorgang auf verschiedenen Rechnern reproduzierbar ist.\nArbeitet mit Projekten, da diese sehr einfach ausgetauscht und somit auch reproduziert werden önnen; es gibt keine absoluten Arbeitspfade sondern nur relative. Der Datenimport (und auch der Export) kann mithilfe dieser relativen Pfaden stark vereinfacht werden. Projekte helfen alles am richtigen Ort zu behalten. (mehr zur Arbeit mit Projekten: Link)"
  },
  {
    "objectID": "fallstudie_n/2_Datenverarbeitung_Uebung.html#aufbau-von-r-skripten",
    "href": "fallstudie_n/2_Datenverarbeitung_Uebung.html#aufbau-von-r-skripten",
    "title": "2. Daten(vor)verarbeitung - Übung",
    "section": "Aufbau von R-Skripten",
    "text": "Aufbau von R-Skripten\nIm Kopf des Skripts zuerst immer den Titel des Projekts sowie die Autor:innen des Skripts nennen. Hier soll auch die Herkunft der Daten ersichtlich sein und falls externe Daten verwendet werden, sollte geklärt werden, wer die Datenherrschaft hat (Rehdaten: Forschungsgruppe WILMA).\n\n#.##################################################################################\n# Daten(vor)verarbeitung Fallstudie WPZ  ####\n# Modul Research Methods, HS22. Autor/in ####\n#.##################################################################################\n\n# Beschreibt zudem folgendes:\n# • Ordnerstruktur; ich verwende hier den Projektordner mit den Unterordnern: \n#   • Skripts\n#   • Data\n#   • Results\n#   • Plots\n# • Verwendete Daten\n\n# Ein Skript soll in R eigentlich immer nach dem selbem Schema aufgebaut sein. \n# Dieses Schema beinhaltet (nach dem bereits erwähnten Kopf des Skripts) 4 Kapitel: \n\n\nDatenimport\nDatenvorverarbeitung\nAnalyse\nVisualisierung\n\nBereitet euer Skript also nach dieser Struktur vor. Nutzt für den Text, welcher nicht Code ist, vor dem Text das Symbol #. Wenn ihr den Text als Titel definieren wollt, der die grobe Struktur des Skripts absteckt, baut in wie in folgendem Beispiel auf:\n\n#.###################################################################################\n# METADATA ####\n#.###################################################################################\n# Datenherkunft ####\n# ...\n\n#.###################################################################################\n# 1. DATENIMPORT ####\n#.###################################################################################"
  },
  {
    "objectID": "fallstudie_n/2_Datenverarbeitung_Uebung.html#libraries-laden",
    "href": "fallstudie_n/2_Datenverarbeitung_Uebung.html#libraries-laden",
    "title": "2. Daten(vor)verarbeitung - Übung",
    "section": "Libraries laden",
    "text": "Libraries laden\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "fallstudie_n/2_Datenverarbeitung_Uebung.html#daten-laden",
    "href": "fallstudie_n/2_Datenverarbeitung_Uebung.html#daten-laden",
    "title": "2. Daten(vor)verarbeitung - Übung",
    "section": "Daten laden",
    "text": "Daten laden\nHerunterladen der Daten der Feldaufnahmen von Moodle (Aufgabe3_Feldaufnahmen_alle_Gruppen.zip), Einlesen, Sichtung der Datensätze und der Datentypen.\n\ndf_team1 <- read_delim(\"fallstudie_n/data/Felderhebungen_Waldstruktur.csv\", delim = \";\")\n\ndf_team2 <- read_delim(\"fallstudie_n/data/Felderhebung_11102022_gr3.csv\", delim = \";\")\n\ndf_team3 <- read_delim(\"fallstudie_n/data/Felderhebung_Waldstruktur_TEAM3_pink_Gruppe 7.csv\",\n    delim = \";\")\n\ndf_team4 <- read_delim(\"fallstudie_n/data/Felderhebungen_Team4_Blau_221011.csv\",\n    delim = \";\")\n\ndf_team5 <- read_delim(\"fallstudie_n/data/Felderhebung TEAM 5.csv\", delim = \";\")\n\ndf_team6 <- read_delim(\"fallstudie_n/data/Team6_Felderhebung.csv\", delim = \";\")\n\n# hier können die Probekreise mit den Angaben zur Anzahl Rehlokalisationen und\n# der LIDAR-basierten Ableitung der Waldstruktur eingelesen werden\n\ndf_reh <- read_delim(\"fallstudie_n/data/Aufgabe3_Reh_Waldstruktur_221013.csv\", delim = \";\")\nstr(df_reh)\n\n# Die eingelesenen Datensätze anschauen und versuchen zu einem Gesamtdatensatz\n# verbinden. Ist der Output zufriedenstellend?\n\ndf_gesamt <- bind_rows(df_team1, df_team2, df_team3, df_team4, df_team5, df_team6)\nstr(df_gesamt)"
  },
  {
    "objectID": "fallstudie_n/2_Datenverarbeitung_Uebung.html#aufgabe-1",
    "href": "fallstudie_n/2_Datenverarbeitung_Uebung.html#aufgabe-1",
    "title": "2. Daten(vor)verarbeitung - Übung",
    "section": "Aufgabe 1",
    "text": "Aufgabe 1\n\n1.1 Einfügen zusätzliche Spalte pro Datensatz mit der Gruppenzugehörigkeit (Team1-6)\n1.2 Spaltenumbenennung damit die Bezeichungen in allen Datensätzen gleich sind und der Gesamtdatensatz zusammengefügt werden kann –> Befehle mutate und rename, mit pipes (alt: %>%, neu: |>) in einem Schritt möglich"
  },
  {
    "objectID": "fallstudie_n/2_Datenverarbeitung_Uebung.html#aufgabe-2",
    "href": "fallstudie_n/2_Datenverarbeitung_Uebung.html#aufgabe-2",
    "title": "2. Daten(vor)verarbeitung - Übung",
    "section": "Aufgabe 2",
    "text": "Aufgabe 2\nZusammenführen der Teildatensätze zu einem Datensatz"
  },
  {
    "objectID": "fallstudie_n/2_Datenverarbeitung_Uebung.html#aufgabe-3",
    "href": "fallstudie_n/2_Datenverarbeitung_Uebung.html#aufgabe-3",
    "title": "2. Daten(vor)verarbeitung - Übung",
    "section": "Aufgabe 3",
    "text": "Aufgabe 3\nVerbinden (join) des Datensatzes der Felderhebungen mit dem Datensatz der Rehe.\nZiel: ein Datensatz mit allen Kreisen der Felderhebung, angereichert mit den Umweltvariablen Understory und Overstory aus den LIDAR-Daten (DG_us, DG_os) aus dem Rehdatensatz. –> Welche Art von join? Welche Spalten zum Verbinden (by = ?) der Datensätze"
  },
  {
    "objectID": "fallstudie_n/2_Datenverarbeitung_Uebung.html#aufgabe-4",
    "href": "fallstudie_n/2_Datenverarbeitung_Uebung.html#aufgabe-4",
    "title": "2. Daten(vor)verarbeitung - Übung",
    "section": "Aufgabe 4",
    "text": "Aufgabe 4\nScatterplot der korrespondondierenden Umweltvariablen aus den Felderhebungen gegen die Umweltvariablen aus den LIDAR-Daten erstellen (zusätzlich Einfärben der Gruppen und Regressionslinie darüberlegen)."
  },
  {
    "objectID": "fallstudie_n/2_Datenverarbeitung_Loesung.html",
    "href": "fallstudie_n/2_Datenverarbeitung_Loesung.html",
    "title": "2. Daten(vor)verarbeitung - Lösung",
    "section": "",
    "text": "Vor den eigentlichen Auswertungen müssen einige vorbereitende Arbeiten unternommen werden. Die Zeit, die man hier investiert, wird in der späteren Projektphase um ein vielfaches eingespart. Im Skript soll die Ordnerstruktur des Projekts genannt werden, damit der Arbeitsvorgang auf verschiedenen Rechnern reproduzierbar ist.\nArbeitet mit Projekten, da diese sehr einfach ausgetauscht und somit auch reproduziert werden önnen; es gibt keine absoluten Arbeitspfade sondern nur relative. Der Datenimport (und auch der Export) kann mithilfe dieser relativen Pfaden stark vereinfacht werden. Projekte helfen alles am richtigen Ort zu behalten. (mehr zur Arbeit mit Projekten: Link)"
  },
  {
    "objectID": "fallstudie_n/2_Datenverarbeitung_Loesung.html#aufbau-von-r-skripten",
    "href": "fallstudie_n/2_Datenverarbeitung_Loesung.html#aufbau-von-r-skripten",
    "title": "2. Daten(vor)verarbeitung - Lösung",
    "section": "Aufbau von R-Skripten",
    "text": "Aufbau von R-Skripten\nIm Kopf des Skripts zuerst immer den Titel des Projekts sowie die Autor:innen des Skripts nennen. Hier soll auch die Herkunft der Daten ersichtlich sein und falls externe Daten verwendet werden, sollte geklärt werden, wer die Datenherrschaft hat (Rehdaten: Forschungsgruppe WILMA).\n\n#.##################################################################################\n# Daten(vor)verarbeitung Fallstudie WPZ  ####\n# Modul Research Methods, HS22. Autor/in ####\n#.##################################################################################\n\n# Beschreibt zudem folgendes:\n# • Ordnerstruktur; ich verwende hier den Projektordner mit den Unterordnern: \n#   • Skripts\n#   • Data\n#   • Results\n#   • Plots\n# • Verwendete Daten\n\n# Ein Skript soll in R eigentlich immer nach dem selbem Schema aufgebaut sein. \n# Dieses Schema beinhaltet (nach dem bereits erwähnten Kopf des Skripts) 4 Kapitel: \n\n\nDatenimport\nDatenvorverarbeitung\nAnalyse\nVisualisierung\n\nBereitet euer Skript also nach dieser Struktur vor. Nutzt für den Text, welcher nicht Code ist, vor dem Text das Symbol #. Wenn ihr den Text als Titel definieren wollt, der die grobe Struktur des Skripts absteckt, baut in wie in folgendem Beispiel auf:\n\n#.###################################################################################\n# METADATA ####\n#.###################################################################################\n# Datenherkunft ####\n# ...\n\n#.###################################################################################\n# 1. DATENIMPORT ####\n#.###################################################################################\n\n\nLibraries laden\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "fallstudie_n/2_Datenverarbeitung_Loesung.html#daten-laden",
    "href": "fallstudie_n/2_Datenverarbeitung_Loesung.html#daten-laden",
    "title": "2. Daten(vor)verarbeitung - Lösung",
    "section": "Daten laden",
    "text": "Daten laden\nHerunterladen der Daten der Feldaufnahme von Moodle, Einlesen, Sichtung der Datensätze und der Datentypen.\n\ndf_team1 <- read_delim(\"fallstudie_n/data/Felderhebungen_Waldstruktur.csv\", delim = \";\")\n\ndf_team2 <- read_delim(\"fallstudie_n/data/Felderhebung_11102022_gr3.csv\", delim = \";\")\n\ndf_team3 <- read_delim(\"fallstudie_n/data/Felderhebung_Waldstruktur_TEAM3_pink_Gruppe 7.csv\", delim = \";\")\n\ndf_team4 <- read_delim(\"fallstudie_n/data/Felderhebungen_Team4_Blau_221011.csv\",delim = \";\")\n\ndf_team5 <- read_delim(\"fallstudie_n/data/Felderhebung TEAM 5.csv\", delim = \";\")\n\ndf_team6 <- read_delim(\"fallstudie_n/data/Team6_Felderhebung.csv\", delim = \";\")\n\n# hier können die Probekreise mit den Angaben zur Anzahl Rehlokalisationen und der \n# LIDAR-basierten Ableitung der Waldstruktur eingelesen werden\n\ndf_reh <- read_delim(\"fallstudie_n/data/Aufgabe3_Reh_Waldstruktur_221013.csv\", delim = \";\")\nstr(df_reh)\n\nspec_tbl_df [305 × 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Anz_reh_lokalisationen: num [1:305] 0 0 0 0 0 0 0 0 0 0 ...\n $ x                     : num [1:305] 684900 684900 684900 684900 684875 ...\n $ y                     : num [1:305] 237100 237125 237150 237175 237075 ...\n $ DG_us                 : num [1:305] 0.0903 0.2717 0.468 0.7407 0.1811 ...\n $ DG_os                 : num [1:305] 0.908 0.959 0.871 0.986 0.86 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   Anz_reh_lokalisationen = col_double(),\n  ..   x = col_double(),\n  ..   y = col_double(),\n  ..   DG_us = col_double(),\n  ..   DG_os = col_double()\n  .. )\n - attr(*, \"problems\")=<externalptr> \n\n# Die eingelesenen Datensätze anschauen und versuchen zu einem Gesamtdatensatz  \n# verbinden. Ist der Output zufriedenstellend?\n\ndf_gesamt <- bind_rows(df_team1, df_team2, df_team3, df_team4, df_team5, df_team6)\nstr(df_gesamt)\n\nspec_tbl_df [150 × 11] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Kreis (r 12.5m)               : num [1:150] 0 1 2 3 4 5 6 7 8 9 ...\n $ X                             : num [1:150] 684900 684875 684875 684875 684850 ...\n $ Y                             : num [1:150] 237175 237125 237175 237250 237225 ...\n $ Deckungsgrad Rubus sp. [%]    : num [1:150] 1 72.5 15 25 15 25 30 60 85 65 ...\n $ DG Strauchschicht [%] (0.5-3m): num [1:150] 50 57.5 65 45 65 70 75 65 65 35 ...\n $ DG Baumschicht [%] (ab 3m)    : num [1:150] 90 55 85 65 70 80 80 50 60 70 ...\n $ Kreis                         : num [1:150] NA NA NA NA NA NA NA NA NA NA ...\n $ x                             : num [1:150] NA NA NA NA NA NA NA NA NA NA ...\n $ y                             : num [1:150] NA NA NA NA NA NA NA NA NA NA ...\n $ Deckungsgrad Rubus sp [%]     : num [1:150] NA NA NA NA NA NA NA NA NA NA ...\n $ DG Rubus sp. [%]              : num [1:150] NA NA NA NA NA NA NA NA NA NA ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   `Kreis (r 12.5m)` = col_double(),\n  ..   X = col_double(),\n  ..   Y = col_double(),\n  ..   `Deckungsgrad Rubus sp. [%]` = col_double(),\n  ..   `DG Strauchschicht [%] (0.5-3m)` = col_double(),\n  ..   `DG Baumschicht [%] (ab 3m)` = col_double()\n  .. )\n - attr(*, \"problems\")=<externalptr>"
  },
  {
    "objectID": "fallstudie_n/2_Datenverarbeitung_Loesung.html#aufgabe-1",
    "href": "fallstudie_n/2_Datenverarbeitung_Loesung.html#aufgabe-1",
    "title": "2. Daten(vor)verarbeitung - Lösung",
    "section": "Aufgabe 1",
    "text": "Aufgabe 1\n1.1 Einügen zusätzliche Spalte pro Datensatz mit der Gruppenzugehörigkeit (Team1-6). 1.2 Spaltenumbenennung damit die Bezeichungen in allen Datensätzen gleich sind und der Gesamtdatensatz zusammengefügt werden kann. –> Befehle mutate und rename, mit pipes (alt: %>%, neu: |>) in einem Schritt möglich\n\n#.#################################################################################\n# 2. DATENVORVERARBEITUNG #####\n#.#################################################################################\n\ndf_team1 <- df_team1 |>\n  mutate(team = \"team1\") |>\n  rename(KreisID = \"Kreis (r 12.5m)\",\n         DG_Rubus = \"Deckungsgrad Rubus sp. [%]\",\n         DG_Strauchschicht = \"DG Strauchschicht [%] (0.5-3m)\",\n         DG_Baumschicht = \"DG Baumschicht [%] (ab 3m)\")\n\ndf_team2 <- df_team2 |>\n  mutate(team = \"team2\") |>\n  rename(KreisID = \"Kreis\",\n         DG_Rubus = \"Deckungsgrad Rubus sp. [%]\",\n         DG_Strauchschicht = \"DG Strauchschicht [%] (0.5-3m)\",\n         DG_Baumschicht = \"DG Baumschicht [%] (ab 3m)\")\n\ndf_team3 <- df_team3 |>\n  mutate(team = \"team3\") |>\n  rename(KreisID = \"Kreis (r 12.5m)\",\n         X = \"x\",\n         Y = \"y\",\n         DG_Rubus = \"Deckungsgrad Rubus sp. [%]\",\n         DG_Strauchschicht = \"DG Strauchschicht [%] (0.5-3m)\",\n         DG_Baumschicht = \"DG Baumschicht [%] (ab 3m)\")\n\ndf_team4 <- df_team4 |>\n  mutate(team = \"team4\") |>\n  rename(KreisID = \"Kreis (r 12.5m)\",\n         DG_Rubus = \"Deckungsgrad Rubus sp. [%]\",\n         DG_Strauchschicht = \"DG Strauchschicht [%] (0.5-3m)\",\n         DG_Baumschicht = \"DG Baumschicht [%] (ab 3m)\")\n\ndf_team5 <- df_team5 |>\n  mutate(team = \"team5\") |>\n  rename(KreisID = \"Kreis\",\n         X = \"x\",\n         Y = \"y\",\n         DG_Rubus = \"Deckungsgrad Rubus sp [%]\",\n         DG_Strauchschicht = \"DG Strauchschicht [%] (0.5-3m)\",\n         DG_Baumschicht = \"DG Baumschicht [%] (ab 3m)\")\n\ndf_team6 <- df_team6 |>\n  mutate(team = \"team6\") |>\n  rename(KreisID = \"Kreis\",\n         X = \"x\",\n         Y = \"y\",\n         DG_Rubus = \"DG Rubus sp. [%]\",\n         DG_Strauchschicht = \"DG Strauchschicht [%] (0.5-3m)\",\n         DG_Baumschicht = \"DG Baumschicht [%] (ab 3m)\")"
  },
  {
    "objectID": "fallstudie_n/2_Datenverarbeitung_Loesung.html#aufgabe-2",
    "href": "fallstudie_n/2_Datenverarbeitung_Loesung.html#aufgabe-2",
    "title": "2. Daten(vor)verarbeitung - Lösung",
    "section": "Aufgabe 2",
    "text": "Aufgabe 2\nZusammenführen der Teildatensätze zu einem Datensatz\n\ndf_gesamt <- bind_rows(df_team1, df_team2, df_team3, df_team4, df_team5, df_team6)"
  },
  {
    "objectID": "fallstudie_n/2_Datenverarbeitung_Loesung.html#aufgabe-3",
    "href": "fallstudie_n/2_Datenverarbeitung_Loesung.html#aufgabe-3",
    "title": "2. Daten(vor)verarbeitung - Lösung",
    "section": "Aufgabe 3",
    "text": "Aufgabe 3\nVerbinden (join) des Datensatzes der Felderhebungen mit dem Datensatz der Rehe.\nZiel: ein Datensatz mit allen Kreisen der Felderhebung, angereichert mit den Umweltvariablen Understory und Overstory aus den LIDAR-Daten (DG_us, DG_os) aus dem Rehdatensatz. –> Welche Art von join? Welche Spalten zum Verbinden (by = ?) der Datensätze\n\ndf_with_LIDAR <- left_join(df_gesamt,df_reh, by = c(\"X\" = \"x\", \"Y\" = \"y\"))\n\n\nAufgabe 4\nScatterplot der korrespondondierenden Umweltvariablen aus den Felderhebungen gegen die Umweltvariablen aus den LIDAR-Daten erstellen (zusätzlich Einfärben der Gruppen und Regressionslinie darüberlegen).\n\n#.#####################################################################################\n# 4. VISUALISERUNG #####\n#.#####################################################################################\n\nggplot(df_with_LIDAR, aes(DG_Strauchschicht, DG_us, color = team)) + geom_point() + \n  stat_smooth(method = \"lm\")\n\n\n\nwrite_delim(df_with_LIDAR, \"data/df_with_lidar.csv\", delim = \";\")\n\nError: Cannot open file for writing:\n* 'C:\\Users\\sigb\\Beni\\WPZ_Fallstudie\\HS22\\data\\df_with_lidar.csv'"
  },
  {
    "objectID": "fallstudie_n/3_Berechnung_Homeranges_Uebung.html",
    "href": "fallstudie_n/3_Berechnung_Homeranges_Uebung.html",
    "title": "3. Aufgabe 3 Berechnung Homeranges - Übung",
    "section": "",
    "text": "ipak <- function(pkg){\nnew.pkg <- pkg[!(pkg %in% installed.packages()[, \"Package\"])]\nif (length(new.pkg))\ninstall.packages(new.pkg, dependencies = TRUE)\nsapply(pkg, require, character.only = TRUE)\n}\npackages <- c(\"sf\", \"raster\", \"tidyverse\", \"adehabitatHR\", \"maptools\", \"sp\", \n              \"ggspatial\", \"rgeos\", \"rgdal\")\nipak(packages)\n\n          sf       raster    tidyverse adehabitatHR     maptools           sp \n        TRUE         TRUE         TRUE         TRUE         TRUE         TRUE \n   ggspatial        rgeos        rgdal \n        TRUE         TRUE         TRUE \n\n\n\n\n\nEinlesen des Gesamtdatensatzes von Moodle, Sichtung des Datensatzes und der Datentypen\n\nRehe <- read_delim(\"fallstudie_n/data/Aufgabe3_Homeranges_Rehe_landforst_20221024.csv\", delim = \";\")\n\nstr(Rehe)\n\nspec_tbl_df [1,452 × 23] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ TierID         : chr [1:1452] \"RE10\" \"RE10\" \"RE10\" \"RE10\" ...\n $ CollarID       : num [1:1452] 13573 13573 13573 13573 13573 ...\n $ UTC_Date       : chr [1:1452] \"31.05.2014\" \"01.06.2014\" \"01.06.2014\" \"01.06.2014\" ...\n $ UTC_Time       : 'hms' num [1:1452] 23:00:44 02:01:02 05:00:44 08:00:43 ...\n  ..- attr(*, \"units\")= chr \"secs\"\n $ LMT_Date       : chr [1:1452] \"01.06.2014\" \"01.06.2014\" \"01.06.2014\" \"01.06.2014\" ...\n $ LMT_Time       : 'hms' num [1:1452] 00:00:44 03:01:02 06:00:44 09:00:43 ...\n  ..- attr(*, \"units\")= chr \"secs\"\n $ Latitude       : num [1:1452] 47.3 47.3 47.3 47.3 47.3 ...\n $ Longitude      : num [1:1452] 8.57 8.57 8.57 8.57 8.57 ...\n $ Height         : num [1:1452] 672 669 689 668 647 ...\n $ DOP            : num [1:1452] 1.8 3.8 3.4 3.2 3.2 1.6 2.4 6.4 2.8 3 ...\n $ X              : num [1:1452] 685838 685842 685877 685827 685763 ...\n $ Y              : num [1:1452] 234281 234287 234300 234281 234264 ...\n $ twilight_start : chr [1:1452] \"31.05.2014 03:55\" \"01.06.2014 03:54\" \"01.06.2014 03:54\" \"01.06.2014 03:54\" ...\n $ sunrise        : chr [1:1452] \"31.05.2014 04:34\" \"01.06.2014 04:33\" \"01.06.2014 04:33\" \"01.06.2014 04:33\" ...\n $ sunset         : chr [1:1452] \"31.05.2014 20:13\" \"01.06.2014 20:14\" \"01.06.2014 20:14\" \"01.06.2014 20:14\" ...\n $ twilight_end   : chr [1:1452] \"31.05.2014 20:51\" \"01.06.2014 20:53\" \"01.06.2014 20:53\" \"01.06.2014 20:53\" ...\n $ moonilumination: num [1:1452] 0.0667 0.1291 0.1291 0.1291 0.1291 ...\n $ daytime        : chr [1:1452] \"N\" \"N\" \"T\" \"T\" ...\n $ ActMean        : num [1:1452] 55.5 42 74 63.7 68 ...\n $ Trajectory     : chr [1:1452] \"RE10\" \"RE10\" \"RE10\" \"RE10\" ...\n $ distance       : num [1:1452] 7.21 37.34 53.49 66.22 92.59 ...\n $ speed          : num [1:1452] 0.000667 0.003463 0.004953 0.006131 0.008574 ...\n $ timelag        : num [1:1452] 10818 10782 10799 10801 10799 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   TierID = col_character(),\n  ..   CollarID = col_double(),\n  ..   UTC_Date = col_character(),\n  ..   UTC_Time = col_time(format = \"\"),\n  ..   LMT_Date = col_character(),\n  ..   LMT_Time = col_time(format = \"\"),\n  ..   Latitude = col_double(),\n  ..   Longitude = col_double(),\n  ..   Height = col_double(),\n  ..   DOP = col_double(),\n  ..   X = col_double(),\n  ..   Y = col_double(),\n  ..   twilight_start = col_character(),\n  ..   sunrise = col_character(),\n  ..   sunset = col_character(),\n  ..   twilight_end = col_character(),\n  ..   moonilumination = col_double(),\n  ..   daytime = col_character(),\n  ..   ActMean = col_double(),\n  ..   Trajectory = col_character(),\n  ..   distance = col_double(),\n  ..   speed = col_double(),\n  ..   timelag = col_double()\n  .. )\n - attr(*, \"problems\")=<externalptr> \n\n\n\n\n\nIm Datensatz Rehe eine neue Spalte mit Datum und Zeit in einer Spalte kreieren. Beim Format hat sich ein Fehler eingeschlichen. Findet ihr ihn?\n\nRehe <- Rehe %>%\n  mutate(UTC_DateTime = as.POSIXct(paste(UTC_Date, UTC_Time), \n                                   format = \"%Y-%m-%d %H:%M:%S\"))\n\n\n\n\nHier einige Zeilen Code, um eine HomeRange zu berechnen.\nHerumschrauben an den Einstellungen von:\n\nhref (in der Funktion kernelUD)\nan der Ausdehung, resp. prozentualer Anteil Punkte in der HR (Funktion getverticeshr)\n\n–> Ziel: eine Karte erstellen mit der Visualiserung mindestens einer HR\n\nx <- Rehe$X[Rehe$TierID== \"RE13\"]    \ny <- Rehe$Y[Rehe$TierID== \"RE13\"]\nxy <- data.frame(cbind (x, y, rep(1, length(x))))       \ncoordinates(xy)<-c(\"x\",\"y\")                             \nproj4string(xy)<-CRS(\"+init=epsg:21781\")  \n\nplot(xy, col = \"blue\", pch = 19, cex = 1.5)\n\n\n\n# Berechnung von href nach: Pebsworth et al. (2012) Evaluating home range techniques: \n# use of Global Positioning System (GPS) collar data from chacma baboons\n\nsigma <- 0.5*(sd(x)+sd(y))                              \nn <- length(x)\nhref <- sigma * n^(-1/6)*0.9  \n\n# scaled reference: href * 0.9\n\nkud <- kernelUD(xy, h=href, grid=25)             \n\n# Berechnung der Home Range (95% Isopleth)\n\nhomerange <- getverticeshr(kud, percent=95)             \n\n# Schreibt HR in den oben beschriebenen Ordner (als Shapefile)\n\nhr <- st_as_sf(homerange)\n\nst_write(hr, dsn= \"Results\", layer=\"HR_RE13\", driver=\"ESRI Shapefile\",  \n         delete_layer = T )\n\nDeleting layer `HR_RE13' using driver `ESRI Shapefile'\nWriting layer `HR_RE13' to data source `Results' using driver `ESRI Shapefile'\nWriting 1 features with 2 fields and geometry type Polygon.\n\n\n\n# mit diesem Befehl kann die HR geplottet werden\n\nggplot(hr, aes(color = \"red\", fill=\"red\")) + \n  geom_sf(size = 1, alpha = 0.3) +\ncoord_sf(datum = sf::st_crs(21781))+\ntheme(\naxis.title = element_blank(),\naxis.text = element_blank(),\naxis.ticks = element_blank(),\nlegend.position=\"none\"\n)\n\n\n\n# und die Punkte der GPS-Lokalisationen darüber gelegt werden \n\nxy_p <- st_as_sf(xy)\n\nggplot(hr, aes(color = \"red\", fill=\"red\")) + \n  geom_sf(size = 1, alpha = 0.3) +\ngeom_sf(data = xy_p, aes(fill = \"red\")) +\ncoord_sf(datum = sf::st_crs(21781))+\ntheme(\naxis.title = element_blank(),\naxis.text = element_blank(),\naxis.ticks = element_blank(),\nlegend.position=\"none\"\n)\n\n\n\n\nCode um die Homerange auf der Landeskarte 1:25000 zu plotten. Transparenz kann mit alpha angepasst werden.\n\npk25_wpz <- brick(\"fallstudie_n/data/pk25_wpz.tif\")\n\nxy_p <- st_as_sf(xy)\n\nggplot(hr, aes(color = \"red\", fill=\"red\")) +\nannotation_spatial(pk25_wpz) +\ngeom_sf(size = 1, alpha = 0.3) +\ngeom_sf(data = xy_p, aes(fill = \"red\")) +\ncoord_sf(datum = sf::st_crs(21781))+\ntheme(\naxis.title = element_blank(),\naxis.text = element_blank(),\naxis.ticks = element_blank(),\nlegend.position=\"none\"\n)\n\n\n\n\nNachbauen des Sampling Grids mit den Kreisen (Wird als Grundlage für Extraktion der Umweltvariablen innerhalb der Homeranges benötigt)\n\nXmin bzw. Ymin des Grids: c(684000, 234000)\ncellsize des Grids: c(25, 25)\nAnzahl Kreise in X und Y Richtung: c(100, 160)\n\n\nx25       <- GridTopology(c(684000, 234000), c(25, 25), c(100, 160)) \ndata25    = data.frame(1:(100*160))           \n# Erstellt aus der GridTopology und den Daten ein SpatialGridDataFrame\ngrid25    <- SpatialGridDataFrame(x25, data25,  proj4string <- CRS(\"+init=epsg:21781\"))\npixel25   <- as(grid25, \"SpatialPixelsDataFrame\")\n\n# zweites Sampling Grid für einen Ausschnitt aufbauen, plotten\n# -> dient nur der Visualisierung des Sampling Grids um einen Eindruck zu erhalten\n\nx       <- GridTopology(c(684200, 236900), c(25, 25), c(35, 35)) \ndata    = data.frame(1:(35*35))           \n# Erstellt aus der GridTopology und den Daten ein SpatialGridDataFrame\ngrid    <- SpatialGridDataFrame(x, data,  proj4string <- CRS(\"+init=epsg:21781\"))\npixel  <- as(grid, \"SpatialPixelsDataFrame\")\n\npoints <- as(pixel, \"SpatialPointsDataFrame\")\n\ngrid_plot <- st_buffer(st_as_sf(points), 12.5)\n\nplot(st_geometry(grid_plot))\n\n\n\nggplot(grid_plot, color = \"black\", fill=NA) + \n  geom_sf() +\ngeom_sf(data = xy_p, color = \"blue\",  ) +\n  geom_sf(data = hr, color = \"red\", fill = NA, size = 2) +\ncoord_sf(datum = sf::st_crs(21781))+\ntheme(\naxis.title = element_blank(),\naxis.text = element_blank(),\naxis.ticks = element_blank(),\nlegend.position=\"none\"\n)\n\n\n\n\n\n\n\nTesten der Variablen der Vegetationsschichten von letzter Woche auf einen linearen Zusammenhang (Korrelation; Funktion cor.test). DG_Baumschicht vs. DG_os / DG_Strauchschicht vs. DG_us aus dem Datensatz df_with_lidar den wir letzte Woche erstellt haben\nDie Theorie zu Korrelation folgt erst ab 31.10.\n\n#| eval: false\n\ndf_with_lidar <- read_delim(\"fallstudie_n/data/df_with_lidar.csv\", delim =\";\")\n\ncor.test(~ DG_Strauchschicht+DG_us, data = df_with_lidar, method=\"pearson\")\n\n\n    Pearson's product-moment correlation\n\ndata:  DG_Strauchschicht and DG_us\nt = 2.6184, df = 148, p-value = 0.009753\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.05190297 0.35858387\nsample estimates:\n      cor \n0.2104143"
  }
]